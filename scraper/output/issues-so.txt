--------------------------------------------------
Heikin Ashi candle code in pine script V5
In pinescript version 4, the Heikin Ashi candle open is calculated as:

```
ha_close = (open + high + low + close)/4
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```
However, it shows compalilation error in pinescript version 5:
```
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```

The compilation error &quot;Undeclared identifier &#39;ha_open&#39;&quot;.

I have no idea what to do to solve this. 
||||||||||||||In pinescript, you must declare variables before using them.  
You should use (for version 5) :

    var float ha_open = na
    ha_close = (open + high + low + close)/4
    ha_open := na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2

`var float ha_open = na` declare the variable as a float and initialize it to `na`

--------------------------------------------------
fatal error: opencv2/opencv_modules.hpp: No such file or directory #include &quot;opencv2/opencv_modules.hpp&quot;
Hello all I am trying to use opencv-c++ API (version 4.4.0) which I have built from source. It is installed in /usr/local/ and I was simply trying to load and display an image using the following code - 
```
#include &lt;iostream&gt;
#include &lt;opencv4/opencv2/opencv.hpp&gt;
#include &lt;opencv4/opencv2/core.hpp&gt;
#include &lt;opencv4/opencv2/imgcodecs.hpp&gt;
#include &lt;opencv4/opencv2/highgui.hpp&gt;
#include &lt;opencv4/opencv2/core/cuda.hpp&gt;

using namespace cv;

int main()
{
    std::string image_path = &quot;13.jpg&quot;;
    cv::Mat img = cv::imreadmulti(image_path, IMREAD_COLOR);
    if(img.empty())
    {
        std::cout&lt;&lt;&quot;COULD NOT READ IMAGE&quot;&lt;&lt;std::endl;
        return 1;
    }
    imshow(&quot;Display Window&quot;, img);
    return 0;
}
```
And when I compile it throws the following error during compilation - 
```
In file included from /CLionProjects/opencvTest/main.cpp:2:
/usr/local/include/opencv4/opencv2/opencv.hpp:48:10: fatal error: opencv2/opencv_modules.hpp: No such file or directory
 #include &quot;opencv2/opencv_modules.hpp&quot;
```
My Cmake is as follows - 

```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
include_directories(&quot;/usr/local/include/opencv4/opencv2/&quot;)
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest PUBLIC &quot;/usr/local/lib/&quot;)
```
I do not know what am I doing wrong here.. This might be a noob question, But I ahev just started using opencv in C++
||||||||||||||The solution is to just include_directories path till `/usr/local/opencv4` and it works perfectly.

However, the best way I believe is to use the `find_package` function. I updated my Cmake to the following and it takes care of linking during build. 
```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
find_package(OpenCV REQUIRED)
include_directories(${OpenCV_INCLUDE_DIRS})
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest ${OpenCV_LIBS})
``` 


--------------------------------------------------
how to get the url parameters from http
I am working in a very rudimentary &quot;routing&quot; system for small CMS in nodejs without express or any framework. My aim is to have very few dependencies. 
For templating I found jrender that works fine in the sample route &quot;hey&quot; below: 

    var http = require(&#39;http&#39;)
    var jsrender = require (&#39;jsrender&#39;);    
    
    var html = jsrender.renderFile(&#39;./templates/hey.html&#39;, {name: &quot;Jim&quot;, age: &quot;22&quot;});
        
    
    http.createServer(function (req, res) {
    	res.writeHead(200, {&#39;Content-Type&#39;: &#39;text/html&#39;}); // http header
    
    	var url = req.url;
    	if(url ===&#39;/about&#39;){
            console.log (req.url)
      		res.write(&quot;hey&quot;); //write a response
      		res.end(); //end the response
            
    	}else if(url ===&#39;/contact&#39;){
      		res.write(&#39;&lt;h1&gt;contact us page&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
            
        }else if(url ===&#39;/hey&#39;){
      		res.write(html); //write a response
      		res.end(); //end the response    
            
    	}else{
      		res.write(&#39;&lt;h1&gt;Hello World!&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
    	}
    
    }).listen(3000, function(){
    	console.log(&quot;Judge Dress live on port 3000&quot;); //the server object listens on port 3000
    }); 


My problem is to get a parameter for a page e.g. /?pages=pagename to have dynamic routes. Is there any way to extact this parameter from req.url ? 

||||||||||||||You can use the node.js built-in 'querystring' module. To get "me" from "http://localhost:3000/about/?pages=me"

    const querystring = require('querystring');     
    console.log(querystring.parse(req.url)["/about/?pages"])

--------------------------------------------------
Regex to match all strings of given format with given exceptions
I&#39;m really struggling with this one. I tried to search from left to right, but still can&#39;t figure this out.

I have a list of strings with random amount of tags, each placed in brackets, randomly positioned within each string. Few examples may look as follows.


```
[tag1][tag4] Desired string - with optional dash [tag10]
[tag1][tag2][tag3] Desired string [tag10]
[tag3][tag1][tag2][tag5] Desired - string (with suffix)
[tag2][tag5][tag4] [Animation] Target string [tag10]
[tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
```

What I&#39;m trying to achieve is to extract from each string the content without tags, which are enclosed in brackets. The only exception is tag **[Animation]** or **[Animations]**. In case, one of these tags appear, I want to extract them as well together with the desired string.

So in case of list above, the desired output would be following. (I don&#39;t care about the whitespace around extracted strings, it will be trimmed afterwards.)

```
Desired string - with optional dash
Desired string
Desired - string (with suffix)
[Animation] Target string
[Animations](prefix)Desired - string (and suffix)
```


Originally, I was using as simple regex as `\[.*?\]`. Which matched all tags in brackets, and I simply replaced everything with empty string.

```python
re_pattern = r&quot;\[.*?\]&quot;
re.sub(re_pattern, &#39;&#39;, dirty_string).strip()
```

However, now I found a need to have an exception for tags **[Animation]** and **[Animations]**, and really can&#39;t figure it out. Your help would be much appreciated.
Thanks.
||||||||||||||You could use the better `regex` module with the following expression:

    \[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*

In `Python`, this could be

    import regex as re
    
    data = """
    [tag1][tag4] Desired string - with optional dash [tag10]
    [tag1][tag2][tag3] Desired string [tag10]
    [tag3][tag1][tag2][tag5] Desired - string (with suffix)
    [tag2][tag5][tag4] [Animation] Target string [tag10]
    [tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
    """
    
    pattern = re.compile(r'\[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*')
    
    print(pattern.sub("", data))

And would yield

    Desired string - with optional dash 
    Desired string 
    Desired - string (with suffix)
    [Animation] Target string 
    [Animations](prefix)Desired - string (and suffix)



--------------------------------------------------
Node.js server that accepts POST requests
I&#39;m trying to allow javascript to communicate with a Node.js server. 

**POST request (web browser)**

    var	http = new XMLHttpRequest();
    var params = &quot;text=stuff&quot;;
    http.open(&quot;POST&quot;, &quot;http://someurl.net:8080&quot;, true);
    
    http.setRequestHeader(&quot;Content-type&quot;, &quot;application/x-www-form-urlencoded&quot;);
    http.setRequestHeader(&quot;Content-length&quot;, params.length);
    http.setRequestHeader(&quot;Connection&quot;, &quot;close&quot;);
    
    alert(http.onreadystatechange);
    http.onreadystatechange = function() {
      if (http.readyState == 4 &amp;&amp; http.status == 200) {
        alert(http.responseText);
      }
    }
    
    http.send(params);

Right now the Node.js server code looks like this. Before it was used for GET requests. I&#39;m not sure how to make it work with POST requests.

**Server (Node.js)**

    var server = http.createServer(function (request, response) {
      var queryData = url.parse(request.url, true).query;
    
      if (queryData.text) {
        convert(&#39;engfemale1&#39;, queryData.text, response);
    	response.writeHead(200, {
    	  &#39;Content-Type&#39;: &#39;audio/mp3&#39;, 
    	  &#39;Content-Disposition&#39;: &#39;attachment; filename=&quot;tts.mp3&quot;&#39;
    	});
      } 
      else {
        response.end(&#39;No text to convert.&#39;);
      }
    }).listen(8080);
||||||||||||||The following code shows how to read values from an HTML form. As @pimvdb said you need to use the request.on('data'...) to capture the contents of the body.
```
const http = require('http')

const server = http.createServer(function(request, response) {
  console.dir(request.param)

  if (request.method == 'POST') {
    console.log('POST')
    var body = ''
    request.on('data', function(data) {
      body += data
      console.log('Partial body: ' + body)
    })
    request.on('end', function() {
      console.log('Body: ' + body)
      response.writeHead(200, {'Content-Type': 'text/html'})
      response.end('post received')
    })
  } else {
    console.log('GET')
    var html = `
			<html>
				<body>
					<form method="post" action="http://localhost:3000">Name: 
						<input type="text" name="name" />
						<input type="submit" value="Submit" />
					</form>
				</body>
			</html>`
    response.writeHead(200, {'Content-Type': 'text/html'})
    response.end(html)
  }
})

const port = 3000
const host = '127.0.0.1'
server.listen(port, host)
console.log(`Listening at http://${host}:${port}`)


```

If you use something like [Express.js][1] and [Bodyparser](https://www.npmjs.com/package/body-parser) then it would look like this since Express will handle the request.body concatenation


```
var express = require('express')
var fs = require('fs')
var app = express()

app.use(express.bodyParser())

app.get('/', function(request, response) {
  console.log('GET /')
  var html = `
    <html>
        <body>
            <form method="post" action="http://localhost:3000">Name: 
                <input type="text" name="name" />
                <input type="submit" value="Submit" />
            </form>
        </body>
    </html>`
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end(html)
})

app.post('/', function(request, response) {
  console.log('POST /')
  console.dir(request.body)
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end('thanks')
})

const port = 3000
app.listen(port)
console.log(`Listening at http://localhost:${port}`)

```

  [1]: http://expressjs.com/


--------------------------------------------------
How can I validate an email address using a regular expression?
Over the years I have slowly developed a [regular expression][1] that validates *most* email addresses correctly, assuming they don&#39;t use an IP address as the server part.

I use it in several PHP programs, and it works most of the time.  However, from time to time I get contacted by someone that is having trouble with a site that uses it, and I end up having to make some adjustment (most recently I realized that I wasn&#39;t allowing four-character [TLDs][2]).

*What is the best regular expression you have or have seen for validating emails?*

I&#39;ve seen several solutions that use functions that use several shorter expressions, but I&#39;d rather have one long complex expression in a simple function instead of several short expression in a more complex function.

  [1]: http://en.wikipedia.org/wiki/Regular_expression
  [2]: https://en.wikipedia.org/wiki/Top-level_domain



||||||||||||||The [fully RFC 822 compliant regex][1] is inefficient and obscure because of its length.  Fortunately, RFC 822 was superseded twice and the current specification for email addresses is [RFC 5322][2].  RFC 5322 leads to a regex that can be understood if studied for a few minutes and is efficient enough for actual use.

One RFC 5322 compliant regex can be found at the top of the page at http://emailregex.com/ but uses the IP address pattern that is floating around the internet with a bug that allows `00` for any of the unsigned byte decimal values in a dot-delimited address, which is illegal.  The rest of it appears to be consistent with the RFC 5322 grammar and passes several tests using `grep -Po`, including cases domain names, IP addresses, bad ones, and account names with and without quotes.

Correcting the `00` bug in the IP pattern, we obtain a working and fairly fast regex.  (Scrape the rendered version, not the markdown, for actual code.)

 > (?:[a-z0-9!#$%&'\*+/=?^\_\`{|}~-]+(?:\\.[a-z0-9!#$%&'\*+/=?^_\`{|}~-]+)\*|"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])\*")@(?:(?:\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?\\.)+\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])

or:

```
(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])
```

Here is [diagram][3] of [finite state machine][4] for above regexp which is more clear than regexp itself
[![enter image description here][5]][5]


The more sophisticated patterns in Perl and PCRE (regex library used e.g. in PHP) can [correctly parse RFC 5322 without a hitch][6]. Python and C# can do that too, but they use a different syntax from those first two. However, if you are forced to use one of the many less powerful pattern-matching languages, then it’s best to use a real parser.

It's also important to understand that validating it per the RFC tells you absolutely nothing about whether that address actually exists at the supplied domain, or whether the person entering the address is its true owner. People sign others up to mailing lists this way all the time. Fixing that requires a fancier kind of validation that involves sending that address a message that includes a confirmation token meant to be entered on the same web page as was the address. 

Confirmation tokens are the only way to know you got the address of the person entering it. This is why most mailing lists now use that mechanism to confirm sign-ups. After all, anybody can put down `president@whitehouse.gov`, and that will even parse as legal, but it isn't likely to be the person at the other end.

For PHP, you should *not* use the pattern given in [Validate an E-Mail Address with PHP, the Right Way][7] from which I quote:

> There is some danger that common usage and widespread sloppy coding will establish a de facto standard for e-mail addresses that is more restrictive than the recorded formal standard.

That is no better than all the other non-RFC patterns. It isn’t even smart enough to handle even [RFC 822][8], let alone RFC 5322. [This one][6], however, is.

If you want to get fancy and pedantic, [implement a complete state engine][9]. A regular expression can only act as a rudimentary filter. The problem with regular expressions is that telling someone that their perfectly valid e-mail address is invalid (a false positive) because your regular expression can't handle it is just rude and impolite from the user's perspective. A state engine for the purpose can both validate and even correct e-mail addresses that would otherwise be considered invalid as it disassembles the e-mail address according to each RFC. This allows for a potentially more pleasing experience, like

>The specified e-mail address 'myemail@address,com' is invalid. Did you mean 'myemail@address.com'?

See also [Validating Email Addresses][10], including the comments. Or [Comparing E-mail Address Validating Regular Expressions][11].

[![Regular expression visualization](https://i.stack.imgur.com/SrUwP.png)](https://i.stack.imgur.com/SrUwP.png)

[Debuggex Demo][12]


  [1]: http://ex-parrot.com/~pdw/Mail-RFC822-Address.html
  [2]: https://datatracker.ietf.org/doc/html/rfc5322
  [3]: https://regexper.com/#(%3F%3A%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B(%3F%3A%5C.%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B)*%7C%22(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21%5Cx23-%5Cx5b%5Cx5d-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)*%22)%40(%3F%3A(%3F%3A%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%5C.)%2B%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%7C%5C%5B(%3F%3A(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D))%5C.)%7B3%7D(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D)%7C%5Ba-z0-9-%5D*%5Ba-z0-9%5D%3A(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21-%5Cx5a%5Cx53-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)%2B)%5C%5D)
  [4]: https://en.wikipedia.org/wiki/Finite-state_machine
  [5]: https://i.stack.imgur.com/YI6KR.png
  [6]: https://stackoverflow.com/questions/201323/what-is-the-best-regular-expression-for-validating-email-addresses/1917982#1917982
  [7]: http://www.linuxjournal.com/article/9585
  [8]: https://datatracker.ietf.org/doc/html/rfc822
  [9]: http://cubicspot.blogspot.com/2012/06/correct-way-to-validate-e-mail-address.html
  [10]: http://worsethanfailure.com/Articles/Validating_Email_Addresses.aspx
  [11]: http://fightingforalostcause.net/misc/2006/compare-email-regex.php
  [12]: https://www.debuggex.com/r/aH_x42NflV8G-GS7

--------------------------------------------------
JPA generating broken SQL when using native query and pageable
Using Spring-Boot 2.7.7, when I attempt to create a native PostgreSQL query that receives a Pageable and outputs a Page, it seems to generate a broken SQL for one of the page attributes

This is the Query I used for the function:

```
    @Query(value =&quot;SELECT * &quot; +
            &quot;FROM propriedade &quot; +
            &quot;INNER JOIN proprietario &quot; +
            &quot;  ON proprietario.id = propriedade.proprietario_id &quot; +
            &quot;WHERE proprietario.nome ILIKE %:proprietario% &quot;,
            nativeQuery = true
    )
    Page&lt;Propriedade&gt; findByProprietarioLikePage(Pageable pageable, @Param(&quot;proprietario&quot;) String proprietario);
```
When I try to call the function, it generates a few SQL commands, but breaks in this one:
```
Hibernate: select count(INNER) FROM propriedade INNER JOIN proprietario   ON proprietario.id = propriedade.proprietario_id WHERE proprietario.nome ILIKE ?   AND condominio_id = ? 
2023-08-07 10:34:57.361  WARN 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42601
2023-08-07 10:34:57.361 ERROR 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: syntax error at or near &quot;)&quot;

```
If I try to run this count on PSQL, I get the same error:
```
ERROR:  syntax error at or near &quot;)&quot;
LINE 1: select count(INNER) FROM propriedade INNER JOIN proprietario...
                          ^
```
The problem seems to be with the generated query to count the entries in the table
||||||||||||||Please, try to add an alias, like "p", after "propriedade". It'll be like this: 

    @Query(value ="SELECT * " +
            "FROM propriedade p " +
            "INNER JOIN proprietario " +
            "  ON proprietario.id = p.proprietario_id " +
            "WHERE proprietario.nome ILIKE %:proprietario% ",
            nativeQuery = true
    )

--------------------------------------------------
Get child node index
In straight up javascript (i.e., no extensions such as jQuery, etc.), is there a way to determine a child node&#39;s index inside of its parent node without iterating over and comparing all children nodes?

E.g.,

    var child = document.getElementById(&#39;my_element&#39;);
    var parent = child.parentNode;
    var childNodes = parent.childNodes;
    var count = childNodes.length;
    var child_index;
    for (var i = 0; i &lt; count; ++i) {
      if (child === childNodes[i]) {
        child_index = i;
        break;
      }
    }

Is there a better way to determine the child&#39;s index?
||||||||||||||you can use the `previousSibling` property to iterate back through the siblings until you get back `null` and count how many siblings you've encountered:

    var i = 0;
    while( (child = child.previousSibling) != null ) 
      i++;
    //at the end i will contain the index.

Please note that in languages like Java, there is a `getPreviousSibling()` function, however in JS this has become a property -- `previousSibling`.

Use [previousElementSibling][2] or [nextElementSibling][1] to ignore text and comment nodes.


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element/nextElementSibling
  [2]: https://developer.mozilla.org/en-US/docs/Web/API/Element/previousElementSibling

--------------------------------------------------
Is there a way to inherit the parent __init__ arguments?
Suppose I have a basic class inheritance:

```
class A:
    def __init__(self, filepath: str, debug=False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, **kwargs):
        super(B, self).__init__(**kwargs)
        self.portnumber = portnumber
```

For typing and completion purposes, I would like to somehow &quot;forward&quot; the list of arguments from `A.__init__()` to `B.__init__()`.


Is there a way to do this? To have a type checker correctly infer the signature for `B.__init__(...)` and have an IDE be able to provide meaningful completions or checks?

---

[edit] after searching a little bit more, here is something that is perhaps closer to what I look:

if I declared `A` and `B` as _dataclasses_ :

```
from dataclasses import dataclass

@dataclass
class A:
    filepath: str
    debug: bool = False

@dataclass
class B(A):
    portnumber: int = 42
```

I can get the following hints in vscode with the standard pylance extension:
[![screen capture of vscode autocompletion][1]][1]

Could there be something similar to target just the `__init__()` method?  
perhaps by explicitly naming the base method that gets &quot;extended&quot; (e.g: a special `@extends(A.__init__)` decorator)?

  [1]: https://i.stack.imgur.com/Xv9L0m.png
||||||||||||||Yes this is possible, but personally I wouldn't recommend it - see below:

```py

from typing import TypedDict, Unpack

class AInterface(TypedDict):
    filepath: str
    debug: bool

class A:
    def __init__(self, **kwargs: Unpack[AInterface]):
        self.filepath = kwargs["filepath"]
        self.debug = kwargs["debug"]

class B(A):
    def __init__(self, portnumber: int, **kwargs: Unpack[AInterface]):
        super().__init__(**kwargs)
        self.portnumber = portnumber
```

By using the `TypedDict` we can structure the kwargs argument giving it a type, and allowing us to pass it through. If you have multiple inheritance you could even combine the interfaces together to produce the current kwargs type. When you use the `__init__` for A and B you still get warned if you miss parts of the `TypedDict`.

I would instead just pass the arguments down to the next layer manually:

```py
class A:
    def __init__(self, filepath: str, debug: bool =False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, filepath: str, debug: bool =False):
        super().__init__(filepath=filepath, debug = debug)
        self.portnumber = portnumber
```


--------------------------------------------------
MySQL / Laravel structure: monolith tables, or thousands of small tables?
**Short Version:**

Our webapp works with sets of scoped `project` data - that is, model relationships that will always relate to models in the same `project`, and strictly never cross over into other `project`s. Security / opacity between projects is a key requirement. The whole scoped relational ecosystem spans ~20 database tables so far.

We are currently managing this ecosystem with ~20 monolith tables, and enforcing opacity / security through code - but we&#39;re losing our grasp on it. We&#39;re considering adopting a structure where each `project` deploys its own clone of these ~20 tables into the same database. Are there any known fundamental drawbacks to having thousands of tables in one database, like increased storage size, slower performance, higher indexing overhead? Our team just doesn&#39;t have the database expertise to speak to flaws that might be introduced by this ourselves. 

If it makes any difference, we&#39;re using Laravel 10 - all models and relationships take advantage of Laravel / Eloquent structures. We&#39;re anticipating at least 200 projects active at a time, with a few being added or nuked each month.

-----

**Long Version:** We have an internal-use project org / management webapp, with some complicated requirements.

In broad strokes, there are `projects`, and each `project` has several related entities - `permissions`, `reports`, `tickets`, `labels`, `ticket_label`, `media`, `notifications`, many more. All related entities are scoped to their project - reports, labels, tickets, etc created inside a project by definition will never be transitioned to another, and no entities are generalized to multiple projects. Our project shareholders are adamant that a project&#39;s information is totally secure and opaque from other projects - no project should know any other project exists, and when a project is deleted there shouldn&#39;t be a trace of it left.

It&#39;s a little messy, because all these entities are binned into single tables with each other, regardless of project. There&#39;s extra work and middleware going into, for example, making sure some bad actor can&#39;t reassign a ticket ID in from an external project to gain access to its data. It&#39;s also complicated deleting a project and ensuring all nested / related data has been wiped - we get pretty far with appropriate foreign_keys and `-&gt;onDelete( &#39;cascade&#39; )`, but as the app gets more developed it&#39;s more and more difficult to **guarantee** that all data has been scrubbed when a project is deleted. There have been a few incidents where orphans containing sensitive information have been discovered. We&#39;ve been improving our tests and code when each is discovered, but it&#39;s becoming clear that we can&#39;t guarantee fallthroughs won&#39;t happen again - shareholders are expressing doubts.

Someone brought up that we can reduce a lot of complexity if we&#39;re able to generate a group of tables each time a new project is created. So, when project `0f9ebA2` is created, it creates tables `0f9ebA2_reports`, `0f9ebA2_tickets`, and so on as well. These tables will be identical structurally, so can all be created from the same migrations. In terms of convenience and cleanliness, the advantages are clear - foreign keys will be pointed to tables with the same prefix, and guarantee IDs outside of the project can&#39;t be assigned. It&#39;s also trivial to ensure all data has been scrubbed - just delete the tables. Many of the cross-pollination protections become obsolete, reduced to just access permissions.

The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach - and it doesn&#39;t seem like a common practice in MySQL, so there aren&#39;t a lot of articles or forums on the subject. We&#39;d like to get another perspective on this, and see if we&#39;re overlooking any fundamental flaws before we pull the trigger - like increased storage size, slower performance, higher indexing overhead. Matters of MySQL architecture and performance, over best practice and opinion.
||||||||||||||A few years ago I managed the databases at a company that had about 10,000 schemas, each schema had the same set of ~120 tables. They did this for similar reasons that you have, to make sure data for different clients is kept separate, for privacy and security reasons.

This was on MySQL 5.1 at the time. We found that after a few tens of thousands of tables on a given server, performance became a problem. It turns out that MySQL has internal data structures corresponding to each table, and they didn't architect this to handle so many tables. So eventually scanning lists of open tables becomes a bottleneck.

We split the schemas over seven servers, so each server didn't have so many tables. About 160,000 tables per server was our maximum.

I gave feedback to the MySQL product manager about this bottleneck, as they were developing a revamped implementation of the data dictionary for MySQL 8.0. He passed this along to the engineers, and they made sure to test scalability up to 1 million tables per server.

So definitely make sure that you use MySQL 8.0 or later to get this improvement.

But even with MySQL 8.0, this doesn't give you unlimited scalability. Eventually if you intend to keep growing, you must develop the capability to store data on more than one database server, and your applications need to have code to switch between database servers.

For example, in our case, one of the db servers had a simple table that stored a list of all the clients and which db server their data was stored on. This list was read at the startup of the app, and held in cache. It was a simple mapping list. Then on any request, the app could quickly tell which of seven db servers it should send the query. All the functions to run queries had an argument which was the db connection to use.

So in a way, "are there any performance limitations" is the wrong question. The assumption should be that there _are_ performance limitations, it's just a matter of how large can you grow until you hit those limits. Assume that you will.

Then the question is how to keep growing beyond those limits, and that means scaling out to multiple servers. Build this into your application design.

---

> The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach

Well, now's your opportunity to exercise your general software engineering skills, and develop some tests. 

You — or _someone_ on your team — hopefully have a degree in Computer Science? Well, approach the problem like a scientist. What type of tests would measure scalability of this kind? Presumably you'd need a lab where you could build a database with lots of tables. You'd need some scripts that can populate those tables, probably with a parameter so you can re-run the test at different scale. You'd need some way to drive query traffic in a repeatable fashion, and measure performance.

Then you need to develop requirements for scalability. What performance is acceptable? What rate of degradation is acceptable? Who gets to decide this?

No one starts with these skills. _They learn as they work._ "I don't have those skills" is not an excuse. They try something, they make mistakes, learn from them, improve their processes. 

You also need to learn that optimization and scalability is not about choosing the right technology. No technology scales if you use it improperly. Scalability comes from architecture, which should be where you have software engineering skills.

--------------------------------------------------
Cannot read properties of undefined (reading [api.reducerPath]) at Object.extractRehydrationInfo after clearing browser data
I have used redux persist with RTK query and redux toolkit. After clearing browser data manually from browser settings,
it could not rehydrate RTK query reducer and showing 

    Uncaught TypeError: Cannot read properties of undefined (reading &#39;notesApi&#39;)
        at Object.extractRehydrationInfo (notesApi.js:18:1)
        at createApi.ts:234:1
        at memoized (defaultMemoize.js:123:1)
        at createApi.ts:260:1
        at memoized (defaultMemoize.js:123:1)
        at createReducer.ts:239:1
        at Array.filter (&lt;anonymous&gt;)
        at reducer (createReducer.ts:236:1)
        at reducer (createSlice.ts:325:1)
        at combination (redux.js:560:1).

Here is the [screenshot of my problem][1].

Official Documentation says 

 - RTK Query supports rehydration via the extractRehydrationInfo option
   on createApi. This function is passed every dispatched action, and
   where it returns a value other than ***undefined***, that value is used to
   rehydrate the API state for fulfilled &amp; errored queries.

But what about ***undefined*** value like in my case?

This is my store




    const reducers = combineReducers({
      userReducer,
      [notesApi.reducerPath]: notesApi.reducer,
    });
    
    const persistConfig = {
      key: &quot;root&quot;,
      storage,
    };
    
    const persistedReducer = persistReducer(
      persistConfig,
      reducers
    );
    
    const store = configureStore({
      reducer: persistedReducer,
      middleware: (getDefaultMiddleware) =&gt;
        getDefaultMiddleware({
          serializableCheck: {
            ignoredActions: [FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER],
          },
        }).concat(notesApi?.middleware),
    });    
    
    export default store;




This is the notesApi



    export const notesApi = createApi({
     reducerPath: &quot;notesApi&quot; ,
      baseQuery: fetchBaseQuery({
        baseUrl: &quot;http://localhost:5000/api/notes/&quot;,
        prepareHeaders: (headers, { getState }) =&gt; {
          const token = getState().userReducer.userInfo.token;
          console.log(token);
          if (token) {
            headers.set(&quot;authorization&quot;, `Bearer ${token}`);
          }
          return headers;
        },
      }),
      extractRehydrationInfo(action, { reducerPath }) {
        if (action.type === REHYDRATE) {
            return action.payload[reducerPath]
        }
      },
      tagTypes: [&quot;notes&quot;],
    
      endpoints: (builder) =&gt; ({
        createNote: builder.mutation({
          query: (data) =&gt; ({
            url: `/create`,
            method: &quot;POST&quot;,
            body: data,
          }),
          invalidatesTags: [&quot;notes&quot;],
        }),
        getSingleNote: builder.query({
          query: (id) =&gt; ({
            url: `/${id}`,
          }),
          providesTags: [&quot;notes&quot;],
        })
    });
    export const {  useGetSingleNoteQuery,
      useCreateNoteMutation,
    } = notesApi;



  [1]: https://i.stack.imgur.com/jqawj.png
||||||||||||||I've run into this issue a few times and it seems to manifest when attempting to rehydrate the store when there isn't anything in localStorage to hydrate from.

The error is saying it can't read `"notesApi"` of undefined when running `extractRehydrationInfo`. `"notesApi"` is the API slice's `reducerPath` value. The action's payload is undefined.

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload[reducerPath]; // <-- action.payload undefined
      }
    },

To resolve this issue I've simply used the Optional Chaining operator on the action payload.

Example:

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload?.[reducerPath];
      }
    },

--------------------------------------------------
Pyspark. spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, java.net.SocketException: Connection reset
I am new to pyspark, and i&#39;m trying to run multiple time series in prophet with pyspark (as distributed computing because i have 100s of times series to predict) but i have error as below. 


```
import time 
start_time = time.time()
sdf = spark.createDataFrame(data)
print(&#39;%0.2f min: Lags&#39; % ((time.time() - start_time) / 60))
sdf.createOrReplaceTempView(&#39;Quantity&#39;)
spark.sql(&quot;select Reseller_City, Business_Unit, count(*) from Quantity group by Reseller_City, Business_Unit order by Reseller_City, Business_Unit&quot;).show()
query = &#39;SELECT Reseller_City, Business_Unit, conditions, black_week, promos, Sales_Date as ds, sum(Rslr_Sales_Quantity) as y FROM Quantity GROUP BY Reseller_City, Business_Unit, conditions, black_week, promos, ds ORDER BY Reseller_City, Business_Unit, ds&#39;
spark.sql(query).show()
sdf.rdd.getNumPartitions()
store_part = (spark.sql(query).repartition(spark.sparkContext.defaultParallelism[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;])).cache()

store_part.explain()

from pyspark.sql.types import *

result_schema =StructType([
  StructField(&#39;ds&#39;,TimestampType()),
  StructField(&#39;Reseller_City&#39;,StringType()),
  StructField(&#39;Business_Unit&#39;,StringType()),
  StructField(&#39;y&#39;,DoubleType()),
  StructField(&#39;yhat&#39;,DoubleType()),
  StructField(&#39;yhat_upper&#39;,DoubleType()),
  StructField(&#39;yhat_lower&#39;,DoubleType())
  ])
from pyspark.sql.functions import pandas_udf, PandasUDFType

@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )
def forecast_sales( store_pd ):
    
    model = Prophet(interval_width=0.95, holidays = lock_down)
    model.add_country_holidays(country_name=&#39;DE&#39;)
    model.add_regressor(&#39;conditions&#39;)
    model.add_regressor(&#39;black_week&#39;)
    model.add_regressor(&#39;promos&#39;)
    
    train = store_pd[store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;]
    future_pd = store_pd[store_pd[&#39;ds&#39;]&gt;=&#39;2021-10-01 00:00:00&#39;]
    model.fit(train[[&#39;ds&#39;, &#39;y&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])


    forecast_pd = model.predict(future_pd[[&#39;ds&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])  

    f_pd = forecast_pd[ [&#39;ds&#39;,&#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ].set_index(&#39;ds&#39;)

    #store_pd = store_pd.filter(store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;)

    st_pd = future_pd[[&#39;ds&#39;,&#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;]].set_index(&#39;ds&#39;)

    results_pd = f_pd.join( st_pd, how=&#39;left&#39; )
    results_pd.reset_index(level=0, inplace=True)

    results_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]] = future_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]].iloc[0]

    return results_pd[ [&#39;ds&#39;, &#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;, &#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ]
results = (store_part.groupBy([&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]).apply(forecast_sales).withColumn(&#39;training date&#39;, current_date() ))
results.cache()
results.show()
``` 

All the lines are executed perfectly but error the come from **results.show()**  line  I dont understand where i have done wrong, Much appreciated if someone helps me 

```
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-46-8c647e8bf4d9&gt; in &lt;module&gt;
----&gt; 1 results.show()

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\dataframe.py in show(self, n, truncate, vertical)
    438         &quot;&quot;&quot;
    439         if isinstance(truncate, bool) and truncate:
--&gt; 440             print(self._jdf.showString(n, 20, vertical))
    441         else:
    442             print(self._jdf.showString(n, int(truncate), vertical))

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\utils.py in deco(*a, **kw)
    126     def deco(*a, **kw):
    127         try:
--&gt; 128             return f(*a, **kw)
    129         except py4j.protocol.Py4JJavaError as e:
    130             converted = convert_exception(e.java_exception)

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
--&gt; 328                     format(target_id, &quot;.&quot;, name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o128.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 1243, Grogu.profiflitzer.local, executor driver): java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	... 1 more

```  
||||||||||||||You can also set the os env variables by following the below steps,
run this before SparkSession/SparkContext

    import os
    import sys
    
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

It worked for me

--------------------------------------------------
Calculate difference between 2 date / times in Oracle SQL
I have a table as follows:

    Filename - varchar
    Creation Date - Date format dd/mm/yyyy hh24:mi:ss
    Oldest cdr date - Date format dd/mm/yyyy hh24:mi:ss

How can I calcuate the difference in hours minutes and seconds (and possibly days) between the two dates in Oracle SQL?

Thanks


||||||||||||||You can substract dates in Oracle. This will give you the difference in days. Multiply by 24 to get hours, and so on.

    SQL> select oldest - creation from my_table;


If your date is stored as character data, you have to convert it to a date type first.


    SQL> select 24 * (to_date('2009-07-07 22:00', 'YYYY-MM-DD hh24:mi') 
                 - to_date('2009-07-07 19:30', 'YYYY-MM-DD hh24:mi')) diff_hours 
           from dual;
    
    DIFF_HOURS
    ----------
           2.5

---
*Note*:

This answer applies to dates represented by the Oracle data type `DATE`.
Oracle also has a data type `TIMESTAMP`, which can also represent a date (with time). If you subtract `TIMESTAMP` values, you get an `INTERVAL`; to extract numeric values, use the `EXTRACT` function.

--------------------------------------------------
Excel VBA Macro Returning &quot;Subscript Out of Range&quot; for Incremental Function Converting Hyperlinks to Raw URLs
I am writing an Excel VBA macro that is working fine except for one specific function, and I cannot figure out why it is failing; I&#39;m not a programmer, although I do have some understanding of the basic logic, just little/no experience at writing it, so apologies in advance if there is any confusion imparted by my attempted explanations. The error returned when attempting to execute the code in question is &quot;Run-time error code &#39;9&#39;: subscript out of range&quot;, and I&#39;ve copied the relevant code snippets below:

    &#39; Define variable for worksheet in question
    Dim wsSales As Worksheet
    Set wsSales = ThisWorkbook.Sheets(&quot;Sales&quot;)

    &#39; Find last row with data in it
    Dim lastRowSales As Long
    lastRowSales = wsSales.Cells(Rows.Count, &quot;J&quot;).End(xlUp).Row

    &#39; Loop through column J and convert hyperlinks to raw URLs
    For i = 2 To lastRowSales
        If wsSales.Cells(i, &quot;J&quot;).Hyperlinks.Count &gt; 0 Then
            wsSales.Cells(i, &quot;J&quot;).Value = wsSales.Hyperlinks(i).Address
        End If
    Next i`

- For extra info/context, column J of the Sales sheet referenced contains hyperlinked text (e.g., &quot;Object Name&quot; that points to a URL in a sales-related webpage), and I&#39;m trying to get the actual URL for each row in the range so I can output it elsewhere. Row 1 is a header row, so I&#39;m starting with &#39;i = 2&#39; to ignore it accordingly.
- What the above code ends up doing is partially successful, but specifically fails on the last row for some reason. So if I have, for example, 100 rows in column J of the Sales sheet (99 rows with data and 1 header row), it will successfully convert any hyperlinked values to a URL for the first 99 rows, but row 100 does not convert and Excel spits out the &#39;subscript out of range&#39; error. When looking at the highlighted code that failed after clicking &#39;Debug&#39; on the error pop-up in the VBA Editor, it is specifically the &#39;wsSales.Hyperlinks(i).Address&#39; part that returns a value of &#39;&lt;subscript out of range&gt;&#39;.
- Additionally, it does not actually convert things quite properly; for example, say that row 50 has a hyperlinked text string in it. Rather than converting cell J50 to show the URL that was in J50, it actually shows the URL for J51, and it does this for the entire range (where it&#39;s showing the URL of the cell below it, not the cell itself).
- If I start with &#39;i = 1&#39; instead to include checking the header row (which will never have a hyperlink, but I figured was worth testing), the function works identically - same behavior, same error, no difference at all relative to starting with &#39;i = 2&#39;. That seems to imply to me the error is somewhere either in the logic before the function actually executes or my references in the function itself.
- I have also tested the above code with &quot;wsSales.Hyperlinks(1).address&quot; (1 instead of i) and it ends up completing successfully but using the same URL for the entire column J, so there seems to be a flaw with that logic as an alternative (presumably the static reference for the Hyperlinks object).  The same is true if I use &#39;2&#39; instead of &#39;1&#39;, so I suspect that using any digit will give me the same core problem.
- I feel like there must be something wrong with either my function or some variable I&#39;ve defined that is causing this, but after looking extensively through my code and attempting to &#39;rubber ducky&#39; troubleshoot it, I&#39;m still coming up blank.


I&#39;ve used essentially the exact same logic for multiple other formulas that compose the rest of the larger macro and they all work properly, but this function specifically fails to work as expected; every other &#39;for i = # To [value]&#39; iterates successfully and commenting out the above code snippet from the larger macro enables the full macro to work exactly as expected, just not this function. Does anyone have any thoughts or suggestions for why this may be failing to function as expected? Any ideas for what logic I should check, what may be failing, or a better way to do this? Any advice would be greatly appreciated, thanks!
||||||||||||||Refer to the hyperlink in *each specific cell*, not the [`Worksheet.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.worksheet.hyperlinks) collection:
```
For i = 2 To lastRowSales
    If wsSales.Cells(i, "J").Hyperlinks.Count > 0 Then
        wsSales.Cells(i, "J").Value = wsSales.Cells(i, "J").Hyperlinks(1).Address
    End If
Next i`
```
In other words, you want to use the [`Range.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.range.hyperlinks) property.

If you did want to use the `Worksheet.Hyperlinks` approach:

```
Dim h As Hyperlink
For Each h In wsSales.Hyperlinks
    h.Range.Value = h.Address
Next
```

--------------------------------------------------
How to send email attachments?
I am having problems understanding how to email an attachment using Python. I have successfully emailed simple messages with the `smtplib`. Could someone please explain how to send an attachment in an email. I know there are other posts online but as a Python beginner I find them hard to understand. 

    
||||||||||||||Here's another:

    import smtplib
    from os.path import basename
    from email.mime.application import MIMEApplication
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    from email.utils import COMMASPACE, formatdate
    
    
    def send_mail(send_from, send_to, subject, text, files=None,
                  server="127.0.0.1"):
        assert isinstance(send_to, list)
    
        msg = MIMEMultipart()
        msg['From'] = send_from
        msg['To'] = COMMASPACE.join(send_to)
        msg['Date'] = formatdate(localtime=True)
        msg['Subject'] = subject
 
        msg.attach(MIMEText(text))

        for f in files or []:
            with open(f, "rb") as fil:
                part = MIMEApplication(
                    fil.read(),
                    Name=basename(f)
                )
            # After the file is closed
            part['Content-Disposition'] = 'attachment; filename="%s"' % basename(f)
            msg.attach(part)

    
        smtp = smtplib.SMTP(server)
        smtp.sendmail(send_from, send_to, msg.as_string())
        smtp.close()


It's much the same as the first example... But it should be easier to drop in.

  [1]: http://snippets.dzone.com/posts/show/2038

--------------------------------------------------
Get handle to desktop / shell window
In one of my programs, I need to test if the user is currently focusing the desktop/shell window. Currently, I&#39;m using `GetShellWindow()` from *user32.dll* and compare the result to `GetForegroundWindow()`.

This approach is working until someone changes the desktop wallpaper, but as soon as the wallpaper is changed the handle from `GetShellWindow()` doesn&#39;t match the one from `GetForegroundWindow()` anymore and I don&#39;t quite get why that is. (**OS:** Windows 7 32bit)

Is there a better approach to check if the desktop is focused? Preferably one that won&#39;t be broken if the user changes the wallpaper?

**EDIT:** I designed a workaround: I&#39;m testing the handle to have a child of class `SHELLDLL_DefView`. If it has, the desktop is on focus. Whilst, it&#39;s working at my PC that doesn&#39;t mean it will work all the time.
||||||||||||||The thing changed a little bit since there are slideshows as wallpaper available in Windows 7.
You are right with WorkerW, but this works only with wallpaper is set to slideshow effect. 

When there is set the wallpaper mode to slideshow, you have to search for a window of class `WorkerW` and check the children, whether there is a `SHELLDLL_DefView`.
If there is no slideshow, you can use the good old `GetShellWindow()`.

I had the same problem some months ago and I wrote a function for getting the right window. Unfortunately I can't find it. But the following should work. Only the Win32 Imports are missing:

    public enum DesktopWindow
    {
        ProgMan,
        SHELLDLL_DefViewParent,
        SHELLDLL_DefView,
        SysListView32
    }
    
    public static IntPtr GetDesktopWindow(DesktopWindow desktopWindow)
    {
        IntPtr _ProgMan = GetShellWindow();
        IntPtr _SHELLDLL_DefViewParent = _ProgMan;
        IntPtr _SHELLDLL_DefView = FindWindowEx(_ProgMan, IntPtr.Zero, "SHELLDLL_DefView", null);
        IntPtr _SysListView32 = FindWindowEx(_SHELLDLL_DefView, IntPtr.Zero, "SysListView32", "FolderView");
    
        if (_SHELLDLL_DefView == IntPtr.Zero)
        {
            EnumWindows((hwnd, lParam) =>
            {
                if (GetClassName(hwnd) == "WorkerW")
                {
                    IntPtr child = FindWindowEx(hwnd, IntPtr.Zero, "SHELLDLL_DefView", null);
                    if (child != IntPtr.Zero)
                    {
                        _SHELLDLL_DefViewParent = hwnd;
                        _SHELLDLL_DefView = child;
                        _SysListView32 = FindWindowEx(child, IntPtr.Zero, "SysListView32", "FolderView"); ;
                        return false;
                    }
                }
                return true;
            }, IntPtr.Zero);
        }
    
        switch (desktopWindow)
        {
            case DesktopWindow.ProgMan:
                return _ProgMan;
            case DesktopWindow.SHELLDLL_DefViewParent:
                return _SHELLDLL_DefViewParent;
            case DesktopWindow.SHELLDLL_DefView:
                return _SHELLDLL_DefView;
            case DesktopWindow.SysListView32:
                return _SysListView32;
            default:
                return IntPtr.Zero;
        }
    }

In your case you would call `GetDesktopWindow(DesktopWindow.SHELLDLL_DefViewParent);` to get the top-level window for checking whether it is the foreground window.

--------------------------------------------------
Keil compiler v5 to v.6
I&#39;m forced to switch from ARMCC v5 to CLANG(v.6). Here is the problem. 
I have some struct that includes a pointer to the function which gets as a parameter pointer to the same structure. 
So I do 

```
struct _some_struct_s;
typedef void (*callback_f)(struct _some_struct *p);
 
typedef struct {
  callback_f fn;
  int        x; 
} some_type_s;

// init function
void init_some_struct (some_struct *p, callback_f f) {
  p-&gt;fn = f;
  p-&gt;x = 0;
}
```
In another file I&#39;m writing the callback() and calling init_some_struct()
```
some_type_s  my_struc;
void callback (some_type_s *p) {
  p-&gt;x++;
}
init_some_struct (&amp;my_struc, callback);
```
I had no issues with compiler 5 but a warning with version 6.
***
```
warning: incompatible function pointer types passing &#39;void (some_struct_s *)&#39; to parameter of type &#39;callback_f&#39; (aka &#39;void (*)(struct _some_struct_s *)&#39;) [-Wincompatible-function-pointer-types]
```
What can I do to avoid having this warning?


What can I do to avoid having this warning?

||||||||||||||1. `typedef (*callback_f)(struct _some_struct *p);` - you alias the type as function of pointer which returns `int`.

It should be `typedef void (*callback_f)(struct some_struct *p);`

2.     typedef struct {
            callback_f *fn;
   `fn` is a pointer to pointer to function. It should be `callback_f fn;`

```
typedef struct some_struct _some_struct_s;
typedef void callback_f(struct _some_struct *p);
 
typedef struct some_struct{
  callback_f *fn;
  int        x; 
} some_struct_s;

// init function
void init_some_struct (some_struct *p, callback_f *f) {
  p->fn = f;
  p->x = 0;
}
```



--------------------------------------------------
How to create mutually exclusive fields in Pydantic
I am using Pydantic to model an object. How can I make two fields mutually exclusive?

For instance, if I have the following model:

    class MyModel(pydantic.BaseModel):
        a: typing.Optional[str]
        b: typing.Optional[str]

I want field `a` and field `b` to be mutually exclusive. I want only one of them to be set. Is there a way to achieve that?
||||||||||||||You can use pydantic.validator decorator to add custom validations.

```lang-python
from typing import Optional
from pydantic import BaseModel, validator

class MyModel(BaseModel):
    a: Optional[str]
    b: Optional[str]

    @validator("b", always=True)
    def mutually_exclusive(cls, v, values):
        if values["a"] is not None and v:
            raise ValueError("'a' and 'b' are mutually exclusive.")

        return v
```

--------------------------------------------------
Web automation with Selenium + python and google chrome 115.x &gt;
How to use selenium and chrome CFT for web automation from chrome version 115.x using python?

I have an automation script that worked fine until chrome version 114.x. From version 115.x it stopped working due to the version update, but also due to the new method with chrome cft.
||||||||||||||After upgrading to Chrome version 115.x, my automation stopped working, and chrome driver versions were no longer released, because from chrome version 115.x, automations are performed by CFT (chrome for test ), which as I understand this browser remains static until user action, preventing automations from stopping due to automatic chrome updates and need for crhome driver replacement.
The problem was solved with the solution below:


```
# using selenium 4.8 and python 3.9

from selenium import webdriver
from selenium.webdriver.chrome.options import Options


options = Options()
options.binary_location = 'path to chrome.exe'
## this is the chromium for testing which can be downloaded from the link given below

driver = webdriver.Chrome(chrome_options = options, executable_path = 'path to chromedriver.exe')
## must be the same as the downloaded version of chrome cft.
```
As of today, the files can be downloaded from: https://googlechromelabs.github.io/chrome-for-testing/

Prefer the stable version and download the compatible browser and chromedriver.

The rest of the code continues to work.

source: https://stackoverflow.com/questions/45500606/set-chrome-browser-binary-through-chromedriver-in-python

--------------------------------------------------
How can I list the taints on Kubernetes nodes?
The [docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint) are great about explaining how to set a taint on a node, or remove one. And I can use `kubectl describe node` to get a verbose description of one node, including its taints. But what if I&#39;ve forgotten the name of the taint I created, or which nodes I set it on? Can I list all of my nodes, with any taints that exist on them?
||||||||||||||<!-- language-all: lang-bash -->

    kubectl get nodes -o json | jq '.items[].spec'

which will give the complete spec with node name, or:

    kubectl get nodes -o json | jq '.items[].spec.taints'

will produce the list of the taints per each node

--------------------------------------------------
How to write unitTest for methods using a stream as a parameter
I have class `ImportProvider` , and I want write unit test for Import method.

But this should be unit test, so I don&#39;t want to read from file to stream.
Any idea?

  

    public class ImportProvider : IImportProvider
    { 
         public bool Import(Stream stream)
         {
             //Do import
        
             return isImported;
         }
    }
        
    public interface IImportProvider
    {
          bool Import(Stream input);
    }

This is unit test:

    [TestMethod]
    public void ImportProvider_Test()
    {
        // Arrange           
        var importRepository = new Mock&lt;IImportRepository&gt;(); 
        var imp = new ImportProvider(importRepository.Object);
        //Do setup...

        // Act
        var test_Stream = ?????????????
        // This working but not option:
        //test_Stream = File.Open(&quot;C:/ExcelFile.xls&quot;, FileMode.Open, FileAccess.Read);
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }
||||||||||||||Use a MemoryStream. Not sure what your function expects, but to stuff a UTF-8 string into it for example:

    //Act
    using (var test_Stream = new MemoryStream(Encoding.UTF8.GetBytes("whatever")))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }

EDIT: If you need an Excel file, and you are unable to read files from disk, could you add an Excel file as an embedded resource in your test project? See [How to embed and access resources by using Visual C#][1]

You can then read as a stream like this:

    //Act
    using (var test_Stream = this.GetType().Assembly.GetManifestResourceStream("excelFileResource"))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }


  [1]: https://support.microsoft.com/en-us/kb/319292

--------------------------------------------------
Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
What is causing this build error:

```

- Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
- Plugin Repositories (could not resolve plugin artifact &#39;com.android.application:com.android.application.gradle.plugin:7.0.3&#39;)
  Searched in the following repositories:
    Gradle Central Plugin Repository
    Google
```

in `build.gradle` file

Expecting a successful android build
||||||||||||||In my case `settings.gradle` file was missing. You can create a file and place into project root folder.

**settings.gradle**:

    pluginManagement {
        repositories {
            gradlePluginPortal()
            google()
            mavenCentral()
        }
    }
    dependencyResolutionManagement {
        repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)
        repositories {
            google()
            mavenCentral()
        }
    }
    rootProject.name = "android-geocode"
    include ':app'

--------------------------------------------------
How do I use an API key/secret on Binance&#39;s TestNet?
Following the instructions here, https://docs.binance.org/smart-chain/wallet/arkane.html, I created a Binance SmartChain account with its &quot;0x&quot; prefixed wallet address.  I then added funds.  What I can&#39;t figure out is how I get a TestNet API key and secret so that I can test my Python API calls.  I create the client like so

	from binance.client import Client
	...
	auth_client = Client(key, b64secret)
     if account.testing:
     	auth_client.API_URL = &#39;https://testnet.binance.vision/api&#39;
 
How do I get an API key tied to my Binance SmartChain address?
||||||||||||||You have to create your API credentials from [here][1] and pass the testnet variable into the Client constructor. See the
[documentation][2].

```python
auth_client = Client(key, b64secret, testnet=True)
```

does the job.


  [1]: https://testnet.binance.vision/
  [2]: https://python-binance.readthedocs.io/en/latest/binance.html?highlight=client#binance.client.Client.__init__

--------------------------------------------------
org.gradle.kotlin.kotlin-dsl was not found
I am getting the following error while running the build

    FAILURE: Build failed with an exception.
    
    * Where:
    Build file &#39;/home/charming/mainframer/bigovlog_android/buildSrc/build.gradle.kts&#39; line: 4
    
    * What went wrong:
    Plugin [id: &#39;org.gradle.kotlin.kotlin-dsl&#39;, version: &#39;1.2.6&#39;] was not found in any of the following sources:
    
    - Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
    - Plugin Repositories (could not resolve plugin artifact &#39;org.gradle.kotlin.kotlin-dsl:org.gradle.kotlin.kotlin-dsl.gradle.plugin:1.2.6&#39;)
      Searched in the following repositories:
        Gradle Central Plugin Repository

my buildSrc/build.gradle.kts

    repositories {
        jcenter()
    }
    plugins {
        `kotlin-dsl`
        id(&quot;groovy&quot;)
    }
    dependencies{
        gradleApi()
        localGroovy()
    }

I tried everything but still not working
||||||||||||||Did you check that Android Studio wasn't running in Offline Mode? Take a look at `Preferences/Build, Execution, Deployment/Gradle/Global Gradle settings` and see if Offline Work is checked.

--------------------------------------------------
Typesetting New Functions in LaTeX
So, I just have a little question:

What is the &quot;best way&quot; to typeset new functions in LaTeX which aren&#39;t already included in the various packages?  Right now I&#39;m just using `\mbox` as my go-to method,  but I just was wondering if there was a more &quot;acceptable way of doing it (as with mbox, I have to make sure to include spaces around the text of the functions in order for it to not look too strange)

Here is an example:

    $y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$

which comes out looking like:

![$y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$][1]

Don&#39;t get me wrong... I think it looks fine, but I was just looking for some opinions (as far as best practices go).

  [1]: http://adamnbowen.com/images/error_function.jpg
||||||||||||||Use `\DeclareMathOperator` from package `amsmath`. For example,

```tex
\documentclass{article}
\usepackage{amsmath}
\DeclareMathOperator\erfi{Erfi}

\begin{document}
Consider $x + y + \erfi(t) = z$ for example.
\end{document}
```

produces

[![result][1]][1]

If you only need it once, you can also use `\operatorname`: you get the same output as above with

```tex
\documentclass{article}
\usepackage{amsmath}
\begin{document}
Consider $x + y + \operatorname{Erfi}(t) = z$ for example.
\end{document}
```

If you cannot use the `amsmath` package for some reason, you can manually do `\mathop{\mathrm{Erfi}}` like:

```
\documentclass{article}
\begin{document}
Consider $x + y + \mathop{\mathrm{Erfi}}(t) = z$ for example.
\end{document}
```

See the always-useful TeX FAQ, specifically [Defining a new log-like function in LaTeX](https://texfaq.org/FAQ-newfunction).

  [1]: https://i.stack.imgur.com/9GCor.png

--------------------------------------------------
C# change a string variable with List or Array
I have some static strings 

    static string   Robocopy_Mirror = &quot;[Robocopy_Mirror]&quot;; 
    static string   Robocopy_Copy = &quot;[Robocopy_Copy]&quot;;
    static string   Network_Path_1 = &quot;[Network_Path_1]&quot;;    // \\NAS\Sync\
    static string   Lokal_Path_1 = &quot;[Lokal_Path_1]&quot;;      // X:\Sync\

And I thought I could save some lines of code if I put them in a List and change the values in the List with a loop.

    List&lt;string&gt; variableListe = new List&lt;string&gt;()  
    {   
        Robocopy_Mirror , Robocopy_Copy , Network_Path_1, Lokal_Path_1, 
        File_Network_Sync_1, File_Lokal_Sync_1, File_Network_Sync_2, File_Lokal_Sync_2
    };


But I can&#39;t change the static variables. I guess the List object does not change the static variable? Is there a quick way to change it? 

    for (int i = 0; i &lt; variableListe.Count-1; i++)
    {
        variableListe[i] = AppConfig.ElementAt(configPathPosition);
    }
    Console.WriteLine(Robocopy_Mirror); 

    // prints  &quot;[Robocopy_Mirror]&quot; instead of like C:\robocopy


||||||||||||||The static variables store references to string objects in memory. The elements in the list also store references to the same string objects in memory, but _each element is it's own variable_ and those elements _do not store references to the static variables;_ they refer to the string objects directly. 

When you change an element in the list, you're changing the variable in the list to point to a new object in a new memory location. The static variables do not change and continue to refer to the same unchanged strings as they did before.


--------------------------------------------------
How do I run curl command from within a Kubernetes pod
I have the following questions:

1. I am logged into a Kubernetes pod using the following command:

        ./cluster/kubectl.sh exec my-nginx-0onux -c my-nginx -it bash

    The &#39;ip addr show&#39; command shows its assigned the ip of the pod. Since pod is a logical concept, I am assuming I am logged into a docker container and not a pod, In which case, the pod IP is same as docker container IP. Is that understanding correct?

2. from a Kubernetes node, I do `sudo docker ps` and then do the following:-

        sudo docker exec  71721cb14283 -it &#39;/bin/bash&#39;

    This doesn&#39;t work. Does someone know what I am doing wrong?

3. I want to access the nginx service I created, from within the pod using curl. How can I install curl within this pod or container to access the service from inside. I want to do this to understand the network connectivity.
||||||||||||||Here is how you get a curl command line within a kubernetes network to test and explore your internal REST endpoints.

To get a prompt of a busybox running inside the network, execute the following command. (A tip is to use one unique container per developer.)

```sh
kubectl run curl-<YOUR NAME> --image=radial/busyboxplus:curl -i --tty --rm
```

You may omit the --rm and keep the instance running for later re-usage. To reuse it later, type:

```sh
kubectl attach <POD ID> -c curl-<YOUR NAME> -i -t
```

Using the command `kubectl get pods` you can see all running POD's. The `<POD ID>` is something similar to `curl-yourname-944940652-fvj28`.

**EDIT:** Note that you need to login to google cloud from your terminal (once) before you can do this! Here is an example, make sure to put in your zone, cluster and project: 
```sh
gcloud container clusters get-credentials example-cluster --zone europe-west1-c --project example-148812
```

--------------------------------------------------
Execute CURL with kubectl
I am trying to execute `curl` command with `kubectl` like 

    kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;

Gives belob error

	OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused &quot;exec: \&quot;kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39;\&quot;: 
	stat kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;: no such file or directory&quot; 
    :unknown command terminated with exit code 126
I have tried to escape the quotes but no luck. Then I tried simple curl 

    kubectl exec -it POD_NAME curl http://localhost:8080/xyz

This gives proper output as excepted. Any help with this 

Update: 

But when I run interactive (`kubectl exec -it POD_NAME /bin/bash`) mode of container and then run the curl inside the container works like champ
||||||||||||||i think you need to do something like this:

```
kubectl exec POD_NAME curl "-X PUT http://localhost:8080/abc -H \"Content-Type: application/json\" -d '{\"name\":\"aaa\",\"no\":\"10\"}' "
```

what the error suggests is that its trying to interpret everything inside `""` as a single command, not as a command with parameters. so its essentially looking for an executable called that

--------------------------------------------------
Open in Safari with UIActivityViewController?
I&#39;m sharing a URL via UIActivityViewController. I&#39;d like to see &quot;Open in Safari&quot; or &quot;Open in browser&quot; appear on the share sheet, but it doesn&#39;t. Is there a way to make this happen?

Note: I am not interested in solutions that involve adding somebody else&#39;s library to my app. I want to understand how to do this, not just get it to happen.

||||||||||||||Yes, you could add your custom action to Share sheet in iOS


You would have to copy this class.

    class MyActivity: UIActivity {
        var _activityTitle: String
        var _activityImage: UIImage?
        var activityItems = [Any]()
        var action: ([Any]) -> Void
        
        init(title: String, image: UIImage?, performAction: @escaping ([Any]) -> Void) {
            _activityTitle = title
            _activityImage = image
            action = performAction
            super.init()
        }
        override var activityTitle: String? {
            return _activityTitle
        }
    
        override var activityImage: UIImage? {
            return _activityImage
        }
        override var activityType: UIActivity.ActivityType {
            return UIActivity.ActivityType(rawValue: "com.someUnique.identifier")
        }
    
        override class var activityCategory: UIActivity.Category {
            return .action
        }
        override func canPerform(withActivityItems activityItems: [Any]) -> Bool {
            return true
        }
        override func prepare(withActivityItems activityItems: [Any]) {
            self.activityItems = activityItems
        }
        override func perform() {
            action(activityItems)
            activityDidFinish(true)
        }
    }

Please go through the class you might need to change a few things.

This is how you use it.

        let customItem = MyActivity(title: "Open in Safari", image: UIImage(systemName: "safari")  ) { sharedItems in
            guard let url = sharedItems[0] as? URL else { return }
            UIApplication.shared.open(url)
        }

        let items = [URL(string: "https://www.apple.com")!]
        let ac = UIActivityViewController(activityItems: items, applicationActivities: [customItem])
        ac.excludedActivityTypes = [.postToFacebook]
        present(ac, animated: true)

I have done this for one action, and tested it, it works.

Similarly you could do it for other custom actions.

For more on it refer this link.
[Link To Detailed Post][1]


  [1]: https://www.hackingwithswift.com/articles/118/uiactivityviewcontroller-by-example


--------------------------------------------------
How to cache playwright-python contexts for testing?
I am doing some web scraping using [`playwright-python&gt;=1.41`][1], and have to launch the browser in a headed mode (e.g. `launch(headless=False)`.

For CI testing, I would like to somehow cache the headed interactions with Chromium, to enable offline testing:

- First invocation: uses Chromium to make real-world HTTP transactions
- Later invocations: uses Chromium, but all HTTP transactions read from a cache

How can this be done? I can&#39;t find any clear answers on how to do this.

  [1]: https://github.com/microsoft/playwright-python
||||||||||||||It might solve your problem using HAR-file recording:
1. Run the first test while [recording a HAR-file][1]
2. Storing the HAR-file as an artifact, in your repo or similar in your CI environment
3. Running test again [with recorded HAR-file][2]

Here is how to do that with `playwright==1.41.1` and `pytest-playwright==0.3.3`:

```python
import pathlib

import pytest
from playwright.sync_api import Browser, Playwright

CACHE_DIR = pathlib.Path(__file__).parent / "cache"


@pytest.fixture(name="example_har", scope="session")
def fixture_example_har(playwright: Playwright) -> pathlib.Path:
    har_file = CACHE_DIR / "example.har"
    with (
        playwright.chromium.launch(headless=False) as browser,
        browser.new_page() as page,
    ):
        page.route_from_har(har_file, url="*/**", update=True)
        page.goto("https://example.com/")
    return har_file


def test_caching(browser: Browser, example_har: pathlib.Path) -> None:
    with browser.new_context(offline=True) as context:
        page = context.new_page()
        page.route_from_har(example_har, url="*/**")
        page.goto("https://example.com/")
```

  [1]: https://playwright.dev/python/docs/mock#recording-a-har-file
  [2]: https://playwright.dev/python/docs/mock#replaying-from-har

--------------------------------------------------
Python: Using .format() on a Unicode-escaped string
I am using Python 2.6.5. My code requires the use of the &quot;more than or equal to&quot; sign. Here it goes:  

    &gt;&gt;&gt; s = u&#39;\u2265&#39;
    &gt;&gt;&gt; print s
    &gt;&gt;&gt; ≥
    &gt;&gt;&gt; print &quot;{0}&quot;.format(s)
    Traceback (most recent call last):
         File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; 
    UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\u2265&#39;
      in position 0: ordinal not in range(128)`  

Why do I get this error? Is there a right way to do this? I need to use the `.format()` function.

||||||||||||||Just make the second string also a unicode string

    >>> s = u'\u2265'
    >>> print s
    ≥
    >>> print "{0}".format(s)
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    UnicodeEncodeError: 'ascii' codec can't encode character u'\u2265' in position 0: ordinal not in range(128)
    >>> print u"{0}".format(s)
    ≥
    >>> 



--------------------------------------------------
Only Content controls are allowed directly in a content page that contains Content controls in ASP.NET
I have an application which has a master page and child pages. My application is working fine on local host (on my intranet). But as soon as I put it on a server that is on the internet, I get the error shown below after clicking on any menus.

&gt; Only Content controls are allowed directly in a content page that contains Content controls.

![screenshot][1]




  [1]: http://i.stack.imgur.com/b21sZ.png
||||||||||||||
Double and triple check your opening and closing Content tags throughout your child pages.

**Confirm that they** 

 - are in existence
 - are spelled correctly
 - have an ID
 - have runat="server"
 - have the correct ContentPlaceHolderID

--------------------------------------------------
Apollo Client is not reading variables passed in using useQuery hook
Having a weird issue passing variables into the useQuery hook.

The query:
```
const GET_USER_BY_ID= gql`
  query($id: ID!) {
    getUser(id: $id) {
      id
      fullName
      role
    }
  }
`;
```
Calling the query:
```
const DisplayUser: React.FC&lt;{ id: string }&gt; = ({ id }) =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID, {
    variables: { id },
  });

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Rendering the component:
```
&lt;DisplayUser id=&quot;5e404fa72b819d1410a3164c&quot; /&gt;
```

This yields the error: 
```
&quot;Argument \&quot;id\&quot; of required type \&quot;ID!\&quot; was provided the variable \&quot;$id\&quot; which was not provided a runtime value.&quot;
```

Calling the query from GraphQL Playground returns the expected result:
```
{
  &quot;data&quot;: {
    &quot;getUser&quot;: {
      &quot;id&quot;: &quot;5e404fa72b819d1410a3164c&quot;,
      &quot;fullName&quot;: &quot;Test 1&quot;,
      &quot;role&quot;: &quot;USER&quot;
    }
  }
}
```
And calling the query without a variable but instead hard-coding the id:
```
const GET_USER_BY_ID = gql`
  query {
    getUser(id: &quot;5e404fa72b819d1410a3164c&quot;) {
      id
      fullName
      role
    }
  }
`;

const DisplayUser: React.FC = () =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID);

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Also returns the expected result.

I have also attempted to test a similar query that takes `firstName: String!` as a parameter which also yields an error saying that the variable was not provided a runtime value. This query also works as expected when hard-coding a value in the query string.

This project was started today and uses `&quot;apollo-boost&quot;: &quot;^0.4.7&quot;`, `&quot;graphql&quot;: &quot;^14.6.0&quot;`, and `&quot;react-apollo&quot;: &quot;^3.1.3&quot;`.
||||||||||||||[Solved]

In reading through the stack trace I noticed the issue was referencing `graphql-query-complexity` which I was using for validationRules. I removed the validation rules and now everything works! Granted I don't have validation at the moment but at least I can work from here. Thanks to everyone who took the time to respond!

--------------------------------------------------
Why it is a StackOverFlow Exception?
Why following code throws `StackoverflowException`? 

    class Foo
    {
        Foo foo = new Foo();
    }
    class Program
    {
        static void Main(string[] args)
        {
            new Foo();
        }
    }
||||||||||||||In `Main` you create a new `Foo` object, invoking its constructor.
Inside the `Foo` constructor, you create a different `Foo` instance, again invoking the `Foo` constructor.

This leads to infinite recursion and a `StackOverflowException` being thrown.

--------------------------------------------------
Function to aggregate json
Assume I have a gcs bucket with json files with the following structure:

```
[
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeid&quot;: &quot;Y1&quot;,
    &quot;storeName&quot;: &quot;alibaba1&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.8/3.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y2&quot;,
     &quot;storeName&quot;: &quot;alibaba2&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.7/2.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y3&quot;,
     &quot;storeName&quot;: &quot;alibaba3&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;2.7/4.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y4&quot;,
     &quot;storeName&quot;: &quot;alibaba4&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;3.7/5.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  }
]
```

What I want to do is to aggregate the different values by summing ```a, b,c, d, f,g``` and taking the average of ```e``` to return one single ```json``` like

```
[
{
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;a&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;b&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;c&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;d&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;e&quot;: &quot;average over all first instance/average over all second instance&quot;,
    &quot;f&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;g&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
  }
]
``` 

Not that any of the values in ```*/*/*``` could be NaN and that the data in ```e``` could be a string ```data unvavailable```.

In have created this function 

```
def format_large_numbers_optimized(value):
    abs_values = np.abs(value)
    mask = abs_values &gt;= 1e6
    formatted_values = np.where(mask, 
                                np.char.add(np.round(value / 1e6, 2).astype(str), &quot;M&quot;), 
                                np.round(value, 2).astype(str))
    return formatted_values

def process_json_data_optimized(json_list):
    result = {}
    keys = set(json_list[0].keys()) - {&#39;Id&#39;, &#39;Name&#39;, &#39;storeid&#39;, &#39;storeName&#39;}
    for key in keys:
        result[key] = {&#39;values&#39;: []}
    for json_data in json_list:
        for key in keys:
            value = json_data.get(key, &#39;0&#39;)  
            result[key][&#39;values&#39;].append(value)
    for key in keys:
        all_values_processed = []
        for value in result[key][&#39;values&#39;]:
            if isinstance(value, str) and &#39;/&#39; in value:
                processed_values = [float(v) if v != &#39;data unavailable&#39; else 0 for v in value.split(&#39;/&#39;)]
            elif isinstance(value, float) or isinstance(value, int):
                processed_values = [value]
            else:
                processed_values = [0.0]  
            all_values_processed.append(processed_values)
        numeric_values = np.array(all_values_processed)
        if numeric_values.ndim == 1:
            numeric_values = numeric_values[:, np.newaxis]
        summed_values = np.sum(numeric_values, axis=0)
        formatted_summed_values = &#39;/&#39;.join(format_large_numbers_optimized(summed_values))
        result[key][&#39;summed&#39;] = formatted_summed_values
    processed_result = {key: data[&#39;summed&#39;] for key, data in result.items()}
    processed_result[&#39;Id&#39;] = json_list[0][&#39;Id&#39;]
    processed_result[&#39;Name&#39;] = json_list[0][&#39;Name&#39;]
    return processed_result
```

But it does not create what I expect. I am a at a total loss. Would really appreciate any help.
||||||||||||||Note that you are placing the values as lists `all_values_processed`.
Assuming that the `/` character is just a separator, and that what you want by replacing `all_values_processed.append(processed_values)` by `all_values_processed += processed_values`. Or even better you could just aggregate the values.

For instance you could have a function to aggregate like this

```lang-py
import math
def agg_func(value, initial):
  v_count, v_sum = initial
  if isinstance(value, str) and '/' in value:
    for v in value.split('/'):
      if v != 'data unavailable' :
         v = float(v)
         if not math.isnan(v):
           v_sum += v
           v_count += 1
  elif isinstance(value, float) or isinstance(value, int):
    if not math.isnan(value):
      v_sum += value
      v_count += 1
  return v_count, v_sum
```

A function that aggregate the given keys in the json

```lang-py
def agg_json(v_list, fields):
  state = {k: (0, 0) for k in fields}
  for item in v_list:
    for k in fields:
      if k in item:
        state[k] = agg(item[k], state[k])
  return state
```


Now
```lang-py
state = agg_json(json_list, ['a', 'b', 'c', 'd', 'e', 'f', 'g'])
```

will give you a dictionary with tuples containing the count and the sum for each field. To get your final answer you could do

```lang-py
result = {k: v[1] / v[0] if k == 'e' else v[1] for k, v in state.items()}
```

--------------------------------------------------
color text in divs with two colors using css only - tricky
OK, let me rewrite my question in another words so it looks clear and interesting: [jsFiddle][1]


  [1]: http://jsfiddle.net/xY6T3/1/

I need a pure css solution that colorizes the lines of text in the color depending whether the line is odd or even.

The example of code could be :

    &lt;div class=&quot;main&quot;&gt;
        &lt;div class=&quot;zipcode12345&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode23456&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode90033&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode11321&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;

Is it possible to make it with css? As you see [@ jsFiddle][1], it is not colorized as expected.

So, the main div is &quot;main&quot;.
The inner `div`s always have class names in format &quot;zipcodeXXXXX&quot;, as you see.
The number of zipcodeXXXXX is variable, the number of `myclass` is variable.
However, the odd lines should be always red and the even lines should be always blue.
Does pure css solution exist?

That would be kind of 

    .myclass:nth-child(2n+1){
     color:red;
    }
    .myclass:nth-child(2n){
     color:blue;
    }

if we could igonre `&quot;zipcodeXXXXX&quot;` divs, right?

Thank you.
||||||||||||||Simply apply different odd/even rules to the parent elements as well as the child elements:

<!-- language: lang-css -->

    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(odd),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(even) {
        color: red;
    }
    
    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(even),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(odd) {
        color: blue;
    }

[**JSFiddle demo**][1].


  [1]: http://jsfiddle.net/xY6T3/9/

--------------------------------------------------
How to populate columns in a table using JavaScript
I need to create a simple table using JavaScript based on an array with nested objects, which should have only two columns. In the first cell of the first column of the table, the Processor header is specified, after which the corresponding processor models are written to the lower cells. In the first cell of the second column, the Processor frequency header is indicated, after which the frequencies are written to the lower cells. I was able to generate code for this task, but it doesn&#39;t work correctly. Instead of writing keys to column cells after the first iteration, for some reason, it takes into account unnecessary objects. That&#39;s why you get an undefined value in the table headers and a re-duplication. Please tell me how to solve this problem. 

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    let processorFrequency = [
        {
            titleOne : &#39;Processor&#39;, values : [
                {name : &#39;80386LC (1988г.)&#39;},
                {name : &#39;80486DX4 (1994г.)&#39;},
                {name : &#39;Pentium MMX (1997г.)&#39;},
                {name : &#39;Pentium II (1998г.)&#39;},
                {name : &#39;Pentium III (1999г.)&#39;},
                {name : &#39;Pentium IV&#39;},
                {name : &#39;Athlon-Athlon XP&#39;},
                {name : &#39;Athlon 64&#39;},

            ]
        },
        {
            titleTwo : &#39;Processor frequency&#39;, values : [
                {name : &#39;33-60&#39;},
                {name : &#39;80-133&#39;},
                {name : &#39;160-233&#39;},
                {name : &#39;260-550&#39;},
                {name : &#39;300-1400&#39;},
                {name : &#39;1600-3800&#39;},
                {name : &#39;1400-3200&#39;},
                {name : &#39;2600-3800&#39;},
            ]
    },
    ]



    function Test(){
        let table = document.getElementsByTagName(&#39;table&#39;)[0];
        for (let i = 0; i &lt; processorFrequency.length; i++) {
            var pf = processorFrequency[i];
            let tableRow = document.createElement(&#39;tr&#39;);
            let tdOne = document.createElement(&#39;td&#39;);
            let tdTwo = document.createElement(&#39;td&#39;);
            let txtOne = document.createTextNode(pf.titleOne);
            let txtTwo = document.createTextNode(pf.titleTwo);
            
            tdOne.className = &#39;head&#39;;
            tdTwo.className = &#39;head&#39;;

            tdOne.appendChild(txtOne);
            tdTwo.appendChild(txtTwo);

            tableRow.appendChild(tdOne);
            tableRow.appendChild(tdTwo);
            table.appendChild(tableRow);

            var values = pf.values;
            for (let j = 0; j &lt; values.length; j++) {
                let value = values[j];
                let tableRow = document.createElement(&#39;tr&#39;);
                let td = document.createElement(&#39;td&#39;);
                let txt = document.createTextNode(value.name);
                td.appendChild(txt);
                tableRow.appendChild(td);
                table.appendChild(tableRow);
            }
        }
    } 

    Test();

&lt;!-- language: lang-css --&gt;

    table td, table th {
      border: 1px solid black;
      padding: 5px;
    }

&lt;!-- language: lang-html --&gt;

    &lt;table&gt;&lt;!-- Contents will be created via JavaScript --&gt;
    &lt;/table&gt;

&lt;!-- end snippet --&gt;


||||||||||||||Seems like the issue is that you are creating a new row for each processor AND frequency value, which results in extra rows being added to the table. The correct way should be create a single row for each processor, with two cells (one for the processor model and one for the processor frequency):

    let processorFrequency = [
        {
            titleOne: 'Processor', values: [
                { name: '80386LC (1988г.)' },
                { name: '80486DX4 (1994г.)' },
                { name: 'Pentium MMX (1997г.)' },
                { name: 'Pentium II (1998г.)' },
                { name: 'Pentium III (1999г.)' },
                { name: 'Pentium IV' },
                { name: 'Athlon-Athlon XP' },
                { name: 'Athlon 64' },
            ]
        },
        {
            titleTwo: 'Processor frequency', values: [
                { name: '33-60' },
                { name: '80-133' },
                { name: '160-233' },
                { name: '260-550' },
                { name: '300-1400' },
                { name: '1600-3800' },
                { name: '1400-3200' },
                { name: '2600-3800' },
            ]
        },
    ];
    
    function Test() {
        let table = document.getElementsByTagName('table')[0];
    
        for (let i = 0; i < processorFrequency[0].values.length; i++) {
            let tableRow = document.createElement('tr');
            
            // Processor Name Cell
            let tdOne = document.createElement('td');
            let txtOne = document.createTextNode(processorFrequency[0].values[i].name);
            tdOne.appendChild(txtOne);
            tableRow.appendChild(tdOne);
    
            // Processor Frequency Cell
            let tdTwo = document.createElement('td');
            let txtTwo = document.createTextNode(processorFrequency[1].values[i].name);
            tdTwo.appendChild(txtTwo);
            tableRow.appendChild(tdTwo);
    
            table.appendChild(tableRow);
        }
    }
    
    Test();

--------------------------------------------------
System.UnauthorizedAccessException: Access to the path &quot;...&quot; is denied
  I have C# wpf installation done with .net using click once installation. All works fine. Then I have the following code which is part of the installed program:

    String destinationPath = System.Windows.Forms.Application.StartupPath + &quot;\\&quot; + fileName;
    File.Copy(path, destinationPath, true);
    this.DialogResult = true;
    this.Close();

But I get this error:

&gt;System.UnauthorizedAccessException: Access to the path C:\user\pc\appdata\local\apps\2.0.......  is denied.
&gt;
&gt;at System.IO.File.InternalCopy(String sourceFileName, String destFileName, Boolean overwrite, Boolean checkHost)
&gt;       at System.IO.File.Copy(String sourceFileName, String destFileName, Boolean overwrite)

Is it a permission error or do I need to tweak something in my code?

What puzzles me is why the user is able to install the program using click once into that directory without any issues, but uploading a file to it doesn&#39;t work?
||||||||||||||When installing an application the installer usually asks for administrative privileges. If the user chooses "Yes" the program will run and have read and write access to a larger variety of paths than what a normal user has. If the case is such that the installer did not ask for administrative privileges, it might just be that ClickOnce automatically runs under some sort of elevated privileges.

I'd suggest you write to the local appdata folder instead, but if you feel you really want to write to the very same directory as your application you must first run your app with administrator privileges.

To make your application always ask for administrator privileges you can modify your app's manifest file and set the `requestedExecutionLevel` tag's `level` attribute to `requireAdministrator`:

    <requestedExecutionLevel level="requireAdministrator" uiAccess="false" />

You can read a bit more in [**How do I force my .NET application to run as administrator?**](https://stackoverflow.com/questions/2818179/how-do-i-force-my-net-application-to-run-as-administrator)

--------------------------------------------------
Create a NuGet package for .NET8 MAUI with Azure DevOps
I have created a `.NET8 MAUI Class Library` to use in MAUI projects. The repo is in `Azure DevOps` and I was trying to build and publish the package via NuGet.

For that, I wrote a YAML file

    trigger:
    - main
    
    pool:
      vmImage: ubuntu-latest
    
    steps:
    - task: UseDotNet@2
      displayName: &#39;Use dotnet 8&#39;
      inputs:
        version: &#39;8.0.x&#39;
    - task: CmdLine@2
      inputs:
        script: &#39;dotnet workload install maui&#39;
    - task: DotNetCoreCLI@2
      displayName: Restore packages
      inputs:
        command: &#39;restore&#39;
        feedsToUse: &#39;select&#39;
        vstsFeed: &#39;c800d0d7-e2af-4567-997f-de7cf7888e6c&#39;
    - task: DotNetCoreCLI@2
      displayName: Build project
      inputs:
        command: &#39;build&#39;
        projects: &#39;**/PSC.Maui.Components.BottomSheet.csproj&#39;
        arguments: &#39;--configuration $(buildConfiguration)&#39;

When the pipeline runs, I get this error

    Generating script.
    Script contents:
    dotnet workload install maui
    ========================== Starting Command Output ===========================
    /usr/bin/bash --noprofile --norc /home/vsts/work/_temp/42901c0d-f407-4f75-912b-f93132efa865.sh
    Workload ID maui isn&#39;t supported on this platform.
    
    ##[error]Bash exited with code &#39;1&#39;.
    Finishing: CmdLine


[![enter image description here][1]][1]

Then, I tried to create the NuGet package locally, but it was not recognized by the NuGet website when I uploaded it.

How can I change the pipeline?

  [1]: https://i.stack.imgur.com/loBv9.png
||||||||||||||.Net MAUI does not support Linux, therefore you can neither build to it or from it.  

See [here](https://learn.microsoft.com/en-us/dotnet/maui/supported-platforms?view=net-maui-8.0)  

--------------------------------------------------
How to specify multiple locators for Selenium web element using the FindBy and PageFactory mechanisms
I like to use `PageFactory` with `@FindBy` annotations in my automation framework to auto-locate elements in my page object classes. 

I have one WebElement for which I need to be able to specify a couple of different locators. I thought FindBys was my solution, but apparently, that is not how it works. It&#39;s the equivalent of `driver.findElement(option1).findelement.(option2)`. That&#39;s not what I need. I need something that will find an element by one or the other locators. If one doesn&#39;t work, then use the other locator. Is there a way to do this in Selenium using FindBy annotations?
||||||||||||||There is apparently a new feature in Selenium as of May this year -- the @FindAll annotation that does exactly what I need;

http://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/support/FindAll.html
http://selenium.10932.n7.nabble.com/Pull-Request-62-Add-a-FindAll-annotation-to-the-Java-Page-Factory-td24814.html

--------------------------------------------------
Store cout from function as string
I have a function that takes in a vector of integers and outputs them via `std::cout`. 

    #include &lt;iostream&gt;
    #include &lt;vector&gt;
    
    void final_sol(std::vector&lt;int&gt; list){
        for (int i ; i &lt; list.size() ; i++){
            std::cout &lt;&lt; list[i] &lt;&lt; &quot; &quot;;
        }
    }
    
    int main(){
        std::vector&lt;int&gt; list = {1, 2, 3, 4, 5};
        final_sol(list);
        return 0;
    }
However, from this point I would like to have a way to quickly obtain the outputs of `final_sol(vector)` as a string. One way to do this would be to modify the original function to also create the string. However, I am not interested in modifying `final_sol(vector)`. Is there another way I could store the outputs as a string?
||||||||||||||Provide overload:
```
void final_sol(std::ostream& out, const std::vector<int>& list){
    for (int i = 0; i < list.size() ; i++){
        out << list[i] << " ";
    }
}

void final_sol(const std::vector<int>& list){
    final_sol(std::cout, list);
}
```
This way you existing calling code will not be impacted - most probably this is what you want: not modifying function signature. Not what you described: not do not modifying implementation of final_sol.

Then you can do:
```cpp
std::ostringstream str;
final_sol(str, list);
auto s = str.str()
```


--------------------------------------------------
Heikin Ashi candle code in pine script V5
In pinescript version 4, the Heikin Ashi candle open is calculated as:

```
ha_close = (open + high + low + close)/4
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```
However, it shows compalilation error in pinescript version 5:
```
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```

The compilation error &quot;Undeclared identifier &#39;ha_open&#39;&quot;.

I have no idea what to do to solve this. 
||||||||||||||In pinescript, you must declare variables before using them.  
You should use (for version 5) :

    var float ha_open = na
    ha_close = (open + high + low + close)/4
    ha_open := na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2

`var float ha_open = na` declare the variable as a float and initialize it to `na`

--------------------------------------------------
fatal error: opencv2/opencv_modules.hpp: No such file or directory #include &quot;opencv2/opencv_modules.hpp&quot;
Hello all I am trying to use opencv-c++ API (version 4.4.0) which I have built from source. It is installed in /usr/local/ and I was simply trying to load and display an image using the following code - 
```
#include &lt;iostream&gt;
#include &lt;opencv4/opencv2/opencv.hpp&gt;
#include &lt;opencv4/opencv2/core.hpp&gt;
#include &lt;opencv4/opencv2/imgcodecs.hpp&gt;
#include &lt;opencv4/opencv2/highgui.hpp&gt;
#include &lt;opencv4/opencv2/core/cuda.hpp&gt;

using namespace cv;

int main()
{
    std::string image_path = &quot;13.jpg&quot;;
    cv::Mat img = cv::imreadmulti(image_path, IMREAD_COLOR);
    if(img.empty())
    {
        std::cout&lt;&lt;&quot;COULD NOT READ IMAGE&quot;&lt;&lt;std::endl;
        return 1;
    }
    imshow(&quot;Display Window&quot;, img);
    return 0;
}
```
And when I compile it throws the following error during compilation - 
```
In file included from /CLionProjects/opencvTest/main.cpp:2:
/usr/local/include/opencv4/opencv2/opencv.hpp:48:10: fatal error: opencv2/opencv_modules.hpp: No such file or directory
 #include &quot;opencv2/opencv_modules.hpp&quot;
```
My Cmake is as follows - 

```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
include_directories(&quot;/usr/local/include/opencv4/opencv2/&quot;)
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest PUBLIC &quot;/usr/local/lib/&quot;)
```
I do not know what am I doing wrong here.. This might be a noob question, But I ahev just started using opencv in C++
||||||||||||||The solution is to just include_directories path till `/usr/local/opencv4` and it works perfectly.

However, the best way I believe is to use the `find_package` function. I updated my Cmake to the following and it takes care of linking during build. 
```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
find_package(OpenCV REQUIRED)
include_directories(${OpenCV_INCLUDE_DIRS})
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest ${OpenCV_LIBS})
``` 


--------------------------------------------------
how to get the url parameters from http
I am working in a very rudimentary &quot;routing&quot; system for small CMS in nodejs without express or any framework. My aim is to have very few dependencies. 
For templating I found jrender that works fine in the sample route &quot;hey&quot; below: 

    var http = require(&#39;http&#39;)
    var jsrender = require (&#39;jsrender&#39;);    
    
    var html = jsrender.renderFile(&#39;./templates/hey.html&#39;, {name: &quot;Jim&quot;, age: &quot;22&quot;});
        
    
    http.createServer(function (req, res) {
    	res.writeHead(200, {&#39;Content-Type&#39;: &#39;text/html&#39;}); // http header
    
    	var url = req.url;
    	if(url ===&#39;/about&#39;){
            console.log (req.url)
      		res.write(&quot;hey&quot;); //write a response
      		res.end(); //end the response
            
    	}else if(url ===&#39;/contact&#39;){
      		res.write(&#39;&lt;h1&gt;contact us page&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
            
        }else if(url ===&#39;/hey&#39;){
      		res.write(html); //write a response
      		res.end(); //end the response    
            
    	}else{
      		res.write(&#39;&lt;h1&gt;Hello World!&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
    	}
    
    }).listen(3000, function(){
    	console.log(&quot;Judge Dress live on port 3000&quot;); //the server object listens on port 3000
    }); 


My problem is to get a parameter for a page e.g. /?pages=pagename to have dynamic routes. Is there any way to extact this parameter from req.url ? 

||||||||||||||You can use the node.js built-in 'querystring' module. To get "me" from "http://localhost:3000/about/?pages=me"

    const querystring = require('querystring');     
    console.log(querystring.parse(req.url)["/about/?pages"])

--------------------------------------------------
Regex to match all strings of given format with given exceptions
I&#39;m really struggling with this one. I tried to search from left to right, but still can&#39;t figure this out.

I have a list of strings with random amount of tags, each placed in brackets, randomly positioned within each string. Few examples may look as follows.


```
[tag1][tag4] Desired string - with optional dash [tag10]
[tag1][tag2][tag3] Desired string [tag10]
[tag3][tag1][tag2][tag5] Desired - string (with suffix)
[tag2][tag5][tag4] [Animation] Target string [tag10]
[tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
```

What I&#39;m trying to achieve is to extract from each string the content without tags, which are enclosed in brackets. The only exception is tag **[Animation]** or **[Animations]**. In case, one of these tags appear, I want to extract them as well together with the desired string.

So in case of list above, the desired output would be following. (I don&#39;t care about the whitespace around extracted strings, it will be trimmed afterwards.)

```
Desired string - with optional dash
Desired string
Desired - string (with suffix)
[Animation] Target string
[Animations](prefix)Desired - string (and suffix)
```


Originally, I was using as simple regex as `\[.*?\]`. Which matched all tags in brackets, and I simply replaced everything with empty string.

```python
re_pattern = r&quot;\[.*?\]&quot;
re.sub(re_pattern, &#39;&#39;, dirty_string).strip()
```

However, now I found a need to have an exception for tags **[Animation]** and **[Animations]**, and really can&#39;t figure it out. Your help would be much appreciated.
Thanks.
||||||||||||||You could use the better `regex` module with the following expression:

    \[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*

In `Python`, this could be

    import regex as re
    
    data = """
    [tag1][tag4] Desired string - with optional dash [tag10]
    [tag1][tag2][tag3] Desired string [tag10]
    [tag3][tag1][tag2][tag5] Desired - string (with suffix)
    [tag2][tag5][tag4] [Animation] Target string [tag10]
    [tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
    """
    
    pattern = re.compile(r'\[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*')
    
    print(pattern.sub("", data))

And would yield

    Desired string - with optional dash 
    Desired string 
    Desired - string (with suffix)
    [Animation] Target string 
    [Animations](prefix)Desired - string (and suffix)



--------------------------------------------------
Node.js server that accepts POST requests
I&#39;m trying to allow javascript to communicate with a Node.js server. 

**POST request (web browser)**

    var	http = new XMLHttpRequest();
    var params = &quot;text=stuff&quot;;
    http.open(&quot;POST&quot;, &quot;http://someurl.net:8080&quot;, true);
    
    http.setRequestHeader(&quot;Content-type&quot;, &quot;application/x-www-form-urlencoded&quot;);
    http.setRequestHeader(&quot;Content-length&quot;, params.length);
    http.setRequestHeader(&quot;Connection&quot;, &quot;close&quot;);
    
    alert(http.onreadystatechange);
    http.onreadystatechange = function() {
      if (http.readyState == 4 &amp;&amp; http.status == 200) {
        alert(http.responseText);
      }
    }
    
    http.send(params);

Right now the Node.js server code looks like this. Before it was used for GET requests. I&#39;m not sure how to make it work with POST requests.

**Server (Node.js)**

    var server = http.createServer(function (request, response) {
      var queryData = url.parse(request.url, true).query;
    
      if (queryData.text) {
        convert(&#39;engfemale1&#39;, queryData.text, response);
    	response.writeHead(200, {
    	  &#39;Content-Type&#39;: &#39;audio/mp3&#39;, 
    	  &#39;Content-Disposition&#39;: &#39;attachment; filename=&quot;tts.mp3&quot;&#39;
    	});
      } 
      else {
        response.end(&#39;No text to convert.&#39;);
      }
    }).listen(8080);
||||||||||||||The following code shows how to read values from an HTML form. As @pimvdb said you need to use the request.on('data'...) to capture the contents of the body.
```
const http = require('http')

const server = http.createServer(function(request, response) {
  console.dir(request.param)

  if (request.method == 'POST') {
    console.log('POST')
    var body = ''
    request.on('data', function(data) {
      body += data
      console.log('Partial body: ' + body)
    })
    request.on('end', function() {
      console.log('Body: ' + body)
      response.writeHead(200, {'Content-Type': 'text/html'})
      response.end('post received')
    })
  } else {
    console.log('GET')
    var html = `
			<html>
				<body>
					<form method="post" action="http://localhost:3000">Name: 
						<input type="text" name="name" />
						<input type="submit" value="Submit" />
					</form>
				</body>
			</html>`
    response.writeHead(200, {'Content-Type': 'text/html'})
    response.end(html)
  }
})

const port = 3000
const host = '127.0.0.1'
server.listen(port, host)
console.log(`Listening at http://${host}:${port}`)


```

If you use something like [Express.js][1] and [Bodyparser](https://www.npmjs.com/package/body-parser) then it would look like this since Express will handle the request.body concatenation


```
var express = require('express')
var fs = require('fs')
var app = express()

app.use(express.bodyParser())

app.get('/', function(request, response) {
  console.log('GET /')
  var html = `
    <html>
        <body>
            <form method="post" action="http://localhost:3000">Name: 
                <input type="text" name="name" />
                <input type="submit" value="Submit" />
            </form>
        </body>
    </html>`
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end(html)
})

app.post('/', function(request, response) {
  console.log('POST /')
  console.dir(request.body)
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end('thanks')
})

const port = 3000
app.listen(port)
console.log(`Listening at http://localhost:${port}`)

```

  [1]: http://expressjs.com/


--------------------------------------------------
How can I validate an email address using a regular expression?
Over the years I have slowly developed a [regular expression][1] that validates *most* email addresses correctly, assuming they don&#39;t use an IP address as the server part.

I use it in several PHP programs, and it works most of the time.  However, from time to time I get contacted by someone that is having trouble with a site that uses it, and I end up having to make some adjustment (most recently I realized that I wasn&#39;t allowing four-character [TLDs][2]).

*What is the best regular expression you have or have seen for validating emails?*

I&#39;ve seen several solutions that use functions that use several shorter expressions, but I&#39;d rather have one long complex expression in a simple function instead of several short expression in a more complex function.

  [1]: http://en.wikipedia.org/wiki/Regular_expression
  [2]: https://en.wikipedia.org/wiki/Top-level_domain



||||||||||||||The [fully RFC 822 compliant regex][1] is inefficient and obscure because of its length.  Fortunately, RFC 822 was superseded twice and the current specification for email addresses is [RFC 5322][2].  RFC 5322 leads to a regex that can be understood if studied for a few minutes and is efficient enough for actual use.

One RFC 5322 compliant regex can be found at the top of the page at http://emailregex.com/ but uses the IP address pattern that is floating around the internet with a bug that allows `00` for any of the unsigned byte decimal values in a dot-delimited address, which is illegal.  The rest of it appears to be consistent with the RFC 5322 grammar and passes several tests using `grep -Po`, including cases domain names, IP addresses, bad ones, and account names with and without quotes.

Correcting the `00` bug in the IP pattern, we obtain a working and fairly fast regex.  (Scrape the rendered version, not the markdown, for actual code.)

 > (?:[a-z0-9!#$%&'\*+/=?^\_\`{|}~-]+(?:\\.[a-z0-9!#$%&'\*+/=?^_\`{|}~-]+)\*|"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])\*")@(?:(?:\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?\\.)+\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])

or:

```
(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])
```

Here is [diagram][3] of [finite state machine][4] for above regexp which is more clear than regexp itself
[![enter image description here][5]][5]


The more sophisticated patterns in Perl and PCRE (regex library used e.g. in PHP) can [correctly parse RFC 5322 without a hitch][6]. Python and C# can do that too, but they use a different syntax from those first two. However, if you are forced to use one of the many less powerful pattern-matching languages, then it’s best to use a real parser.

It's also important to understand that validating it per the RFC tells you absolutely nothing about whether that address actually exists at the supplied domain, or whether the person entering the address is its true owner. People sign others up to mailing lists this way all the time. Fixing that requires a fancier kind of validation that involves sending that address a message that includes a confirmation token meant to be entered on the same web page as was the address. 

Confirmation tokens are the only way to know you got the address of the person entering it. This is why most mailing lists now use that mechanism to confirm sign-ups. After all, anybody can put down `president@whitehouse.gov`, and that will even parse as legal, but it isn't likely to be the person at the other end.

For PHP, you should *not* use the pattern given in [Validate an E-Mail Address with PHP, the Right Way][7] from which I quote:

> There is some danger that common usage and widespread sloppy coding will establish a de facto standard for e-mail addresses that is more restrictive than the recorded formal standard.

That is no better than all the other non-RFC patterns. It isn’t even smart enough to handle even [RFC 822][8], let alone RFC 5322. [This one][6], however, is.

If you want to get fancy and pedantic, [implement a complete state engine][9]. A regular expression can only act as a rudimentary filter. The problem with regular expressions is that telling someone that their perfectly valid e-mail address is invalid (a false positive) because your regular expression can't handle it is just rude and impolite from the user's perspective. A state engine for the purpose can both validate and even correct e-mail addresses that would otherwise be considered invalid as it disassembles the e-mail address according to each RFC. This allows for a potentially more pleasing experience, like

>The specified e-mail address 'myemail@address,com' is invalid. Did you mean 'myemail@address.com'?

See also [Validating Email Addresses][10], including the comments. Or [Comparing E-mail Address Validating Regular Expressions][11].

[![Regular expression visualization](https://i.stack.imgur.com/SrUwP.png)](https://i.stack.imgur.com/SrUwP.png)

[Debuggex Demo][12]


  [1]: http://ex-parrot.com/~pdw/Mail-RFC822-Address.html
  [2]: https://datatracker.ietf.org/doc/html/rfc5322
  [3]: https://regexper.com/#(%3F%3A%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B(%3F%3A%5C.%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B)*%7C%22(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21%5Cx23-%5Cx5b%5Cx5d-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)*%22)%40(%3F%3A(%3F%3A%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%5C.)%2B%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%7C%5C%5B(%3F%3A(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D))%5C.)%7B3%7D(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D)%7C%5Ba-z0-9-%5D*%5Ba-z0-9%5D%3A(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21-%5Cx5a%5Cx53-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)%2B)%5C%5D)
  [4]: https://en.wikipedia.org/wiki/Finite-state_machine
  [5]: https://i.stack.imgur.com/YI6KR.png
  [6]: https://stackoverflow.com/questions/201323/what-is-the-best-regular-expression-for-validating-email-addresses/1917982#1917982
  [7]: http://www.linuxjournal.com/article/9585
  [8]: https://datatracker.ietf.org/doc/html/rfc822
  [9]: http://cubicspot.blogspot.com/2012/06/correct-way-to-validate-e-mail-address.html
  [10]: http://worsethanfailure.com/Articles/Validating_Email_Addresses.aspx
  [11]: http://fightingforalostcause.net/misc/2006/compare-email-regex.php
  [12]: https://www.debuggex.com/r/aH_x42NflV8G-GS7

--------------------------------------------------
JPA generating broken SQL when using native query and pageable
Using Spring-Boot 2.7.7, when I attempt to create a native PostgreSQL query that receives a Pageable and outputs a Page, it seems to generate a broken SQL for one of the page attributes

This is the Query I used for the function:

```
    @Query(value =&quot;SELECT * &quot; +
            &quot;FROM propriedade &quot; +
            &quot;INNER JOIN proprietario &quot; +
            &quot;  ON proprietario.id = propriedade.proprietario_id &quot; +
            &quot;WHERE proprietario.nome ILIKE %:proprietario% &quot;,
            nativeQuery = true
    )
    Page&lt;Propriedade&gt; findByProprietarioLikePage(Pageable pageable, @Param(&quot;proprietario&quot;) String proprietario);
```
When I try to call the function, it generates a few SQL commands, but breaks in this one:
```
Hibernate: select count(INNER) FROM propriedade INNER JOIN proprietario   ON proprietario.id = propriedade.proprietario_id WHERE proprietario.nome ILIKE ?   AND condominio_id = ? 
2023-08-07 10:34:57.361  WARN 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42601
2023-08-07 10:34:57.361 ERROR 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: syntax error at or near &quot;)&quot;

```
If I try to run this count on PSQL, I get the same error:
```
ERROR:  syntax error at or near &quot;)&quot;
LINE 1: select count(INNER) FROM propriedade INNER JOIN proprietario...
                          ^
```
The problem seems to be with the generated query to count the entries in the table
||||||||||||||Please, try to add an alias, like "p", after "propriedade". It'll be like this: 

    @Query(value ="SELECT * " +
            "FROM propriedade p " +
            "INNER JOIN proprietario " +
            "  ON proprietario.id = p.proprietario_id " +
            "WHERE proprietario.nome ILIKE %:proprietario% ",
            nativeQuery = true
    )

--------------------------------------------------
Get child node index
In straight up javascript (i.e., no extensions such as jQuery, etc.), is there a way to determine a child node&#39;s index inside of its parent node without iterating over and comparing all children nodes?

E.g.,

    var child = document.getElementById(&#39;my_element&#39;);
    var parent = child.parentNode;
    var childNodes = parent.childNodes;
    var count = childNodes.length;
    var child_index;
    for (var i = 0; i &lt; count; ++i) {
      if (child === childNodes[i]) {
        child_index = i;
        break;
      }
    }

Is there a better way to determine the child&#39;s index?
||||||||||||||you can use the `previousSibling` property to iterate back through the siblings until you get back `null` and count how many siblings you've encountered:

    var i = 0;
    while( (child = child.previousSibling) != null ) 
      i++;
    //at the end i will contain the index.

Please note that in languages like Java, there is a `getPreviousSibling()` function, however in JS this has become a property -- `previousSibling`.

Use [previousElementSibling][2] or [nextElementSibling][1] to ignore text and comment nodes.


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element/nextElementSibling
  [2]: https://developer.mozilla.org/en-US/docs/Web/API/Element/previousElementSibling

--------------------------------------------------
Is there a way to inherit the parent __init__ arguments?
Suppose I have a basic class inheritance:

```
class A:
    def __init__(self, filepath: str, debug=False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, **kwargs):
        super(B, self).__init__(**kwargs)
        self.portnumber = portnumber
```

For typing and completion purposes, I would like to somehow &quot;forward&quot; the list of arguments from `A.__init__()` to `B.__init__()`.


Is there a way to do this? To have a type checker correctly infer the signature for `B.__init__(...)` and have an IDE be able to provide meaningful completions or checks?

---

[edit] after searching a little bit more, here is something that is perhaps closer to what I look:

if I declared `A` and `B` as _dataclasses_ :

```
from dataclasses import dataclass

@dataclass
class A:
    filepath: str
    debug: bool = False

@dataclass
class B(A):
    portnumber: int = 42
```

I can get the following hints in vscode with the standard pylance extension:
[![screen capture of vscode autocompletion][1]][1]

Could there be something similar to target just the `__init__()` method?  
perhaps by explicitly naming the base method that gets &quot;extended&quot; (e.g: a special `@extends(A.__init__)` decorator)?

  [1]: https://i.stack.imgur.com/Xv9L0m.png
||||||||||||||Yes this is possible, but personally I wouldn't recommend it - see below:

```py

from typing import TypedDict, Unpack

class AInterface(TypedDict):
    filepath: str
    debug: bool

class A:
    def __init__(self, **kwargs: Unpack[AInterface]):
        self.filepath = kwargs["filepath"]
        self.debug = kwargs["debug"]

class B(A):
    def __init__(self, portnumber: int, **kwargs: Unpack[AInterface]):
        super().__init__(**kwargs)
        self.portnumber = portnumber
```

By using the `TypedDict` we can structure the kwargs argument giving it a type, and allowing us to pass it through. If you have multiple inheritance you could even combine the interfaces together to produce the current kwargs type. When you use the `__init__` for A and B you still get warned if you miss parts of the `TypedDict`.

I would instead just pass the arguments down to the next layer manually:

```py
class A:
    def __init__(self, filepath: str, debug: bool =False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, filepath: str, debug: bool =False):
        super().__init__(filepath=filepath, debug = debug)
        self.portnumber = portnumber
```


--------------------------------------------------
MySQL / Laravel structure: monolith tables, or thousands of small tables?
**Short Version:**

Our webapp works with sets of scoped `project` data - that is, model relationships that will always relate to models in the same `project`, and strictly never cross over into other `project`s. Security / opacity between projects is a key requirement. The whole scoped relational ecosystem spans ~20 database tables so far.

We are currently managing this ecosystem with ~20 monolith tables, and enforcing opacity / security through code - but we&#39;re losing our grasp on it. We&#39;re considering adopting a structure where each `project` deploys its own clone of these ~20 tables into the same database. Are there any known fundamental drawbacks to having thousands of tables in one database, like increased storage size, slower performance, higher indexing overhead? Our team just doesn&#39;t have the database expertise to speak to flaws that might be introduced by this ourselves. 

If it makes any difference, we&#39;re using Laravel 10 - all models and relationships take advantage of Laravel / Eloquent structures. We&#39;re anticipating at least 200 projects active at a time, with a few being added or nuked each month.

-----

**Long Version:** We have an internal-use project org / management webapp, with some complicated requirements.

In broad strokes, there are `projects`, and each `project` has several related entities - `permissions`, `reports`, `tickets`, `labels`, `ticket_label`, `media`, `notifications`, many more. All related entities are scoped to their project - reports, labels, tickets, etc created inside a project by definition will never be transitioned to another, and no entities are generalized to multiple projects. Our project shareholders are adamant that a project&#39;s information is totally secure and opaque from other projects - no project should know any other project exists, and when a project is deleted there shouldn&#39;t be a trace of it left.

It&#39;s a little messy, because all these entities are binned into single tables with each other, regardless of project. There&#39;s extra work and middleware going into, for example, making sure some bad actor can&#39;t reassign a ticket ID in from an external project to gain access to its data. It&#39;s also complicated deleting a project and ensuring all nested / related data has been wiped - we get pretty far with appropriate foreign_keys and `-&gt;onDelete( &#39;cascade&#39; )`, but as the app gets more developed it&#39;s more and more difficult to **guarantee** that all data has been scrubbed when a project is deleted. There have been a few incidents where orphans containing sensitive information have been discovered. We&#39;ve been improving our tests and code when each is discovered, but it&#39;s becoming clear that we can&#39;t guarantee fallthroughs won&#39;t happen again - shareholders are expressing doubts.

Someone brought up that we can reduce a lot of complexity if we&#39;re able to generate a group of tables each time a new project is created. So, when project `0f9ebA2` is created, it creates tables `0f9ebA2_reports`, `0f9ebA2_tickets`, and so on as well. These tables will be identical structurally, so can all be created from the same migrations. In terms of convenience and cleanliness, the advantages are clear - foreign keys will be pointed to tables with the same prefix, and guarantee IDs outside of the project can&#39;t be assigned. It&#39;s also trivial to ensure all data has been scrubbed - just delete the tables. Many of the cross-pollination protections become obsolete, reduced to just access permissions.

The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach - and it doesn&#39;t seem like a common practice in MySQL, so there aren&#39;t a lot of articles or forums on the subject. We&#39;d like to get another perspective on this, and see if we&#39;re overlooking any fundamental flaws before we pull the trigger - like increased storage size, slower performance, higher indexing overhead. Matters of MySQL architecture and performance, over best practice and opinion.
||||||||||||||A few years ago I managed the databases at a company that had about 10,000 schemas, each schema had the same set of ~120 tables. They did this for similar reasons that you have, to make sure data for different clients is kept separate, for privacy and security reasons.

This was on MySQL 5.1 at the time. We found that after a few tens of thousands of tables on a given server, performance became a problem. It turns out that MySQL has internal data structures corresponding to each table, and they didn't architect this to handle so many tables. So eventually scanning lists of open tables becomes a bottleneck.

We split the schemas over seven servers, so each server didn't have so many tables. About 160,000 tables per server was our maximum.

I gave feedback to the MySQL product manager about this bottleneck, as they were developing a revamped implementation of the data dictionary for MySQL 8.0. He passed this along to the engineers, and they made sure to test scalability up to 1 million tables per server.

So definitely make sure that you use MySQL 8.0 or later to get this improvement.

But even with MySQL 8.0, this doesn't give you unlimited scalability. Eventually if you intend to keep growing, you must develop the capability to store data on more than one database server, and your applications need to have code to switch between database servers.

For example, in our case, one of the db servers had a simple table that stored a list of all the clients and which db server their data was stored on. This list was read at the startup of the app, and held in cache. It was a simple mapping list. Then on any request, the app could quickly tell which of seven db servers it should send the query. All the functions to run queries had an argument which was the db connection to use.

So in a way, "are there any performance limitations" is the wrong question. The assumption should be that there _are_ performance limitations, it's just a matter of how large can you grow until you hit those limits. Assume that you will.

Then the question is how to keep growing beyond those limits, and that means scaling out to multiple servers. Build this into your application design.

---

> The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach

Well, now's your opportunity to exercise your general software engineering skills, and develop some tests. 

You — or _someone_ on your team — hopefully have a degree in Computer Science? Well, approach the problem like a scientist. What type of tests would measure scalability of this kind? Presumably you'd need a lab where you could build a database with lots of tables. You'd need some scripts that can populate those tables, probably with a parameter so you can re-run the test at different scale. You'd need some way to drive query traffic in a repeatable fashion, and measure performance.

Then you need to develop requirements for scalability. What performance is acceptable? What rate of degradation is acceptable? Who gets to decide this?

No one starts with these skills. _They learn as they work._ "I don't have those skills" is not an excuse. They try something, they make mistakes, learn from them, improve their processes. 

You also need to learn that optimization and scalability is not about choosing the right technology. No technology scales if you use it improperly. Scalability comes from architecture, which should be where you have software engineering skills.

--------------------------------------------------
Cannot read properties of undefined (reading [api.reducerPath]) at Object.extractRehydrationInfo after clearing browser data
I have used redux persist with RTK query and redux toolkit. After clearing browser data manually from browser settings,
it could not rehydrate RTK query reducer and showing 

    Uncaught TypeError: Cannot read properties of undefined (reading &#39;notesApi&#39;)
        at Object.extractRehydrationInfo (notesApi.js:18:1)
        at createApi.ts:234:1
        at memoized (defaultMemoize.js:123:1)
        at createApi.ts:260:1
        at memoized (defaultMemoize.js:123:1)
        at createReducer.ts:239:1
        at Array.filter (&lt;anonymous&gt;)
        at reducer (createReducer.ts:236:1)
        at reducer (createSlice.ts:325:1)
        at combination (redux.js:560:1).

Here is the [screenshot of my problem][1].

Official Documentation says 

 - RTK Query supports rehydration via the extractRehydrationInfo option
   on createApi. This function is passed every dispatched action, and
   where it returns a value other than ***undefined***, that value is used to
   rehydrate the API state for fulfilled &amp; errored queries.

But what about ***undefined*** value like in my case?

This is my store




    const reducers = combineReducers({
      userReducer,
      [notesApi.reducerPath]: notesApi.reducer,
    });
    
    const persistConfig = {
      key: &quot;root&quot;,
      storage,
    };
    
    const persistedReducer = persistReducer(
      persistConfig,
      reducers
    );
    
    const store = configureStore({
      reducer: persistedReducer,
      middleware: (getDefaultMiddleware) =&gt;
        getDefaultMiddleware({
          serializableCheck: {
            ignoredActions: [FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER],
          },
        }).concat(notesApi?.middleware),
    });    
    
    export default store;




This is the notesApi



    export const notesApi = createApi({
     reducerPath: &quot;notesApi&quot; ,
      baseQuery: fetchBaseQuery({
        baseUrl: &quot;http://localhost:5000/api/notes/&quot;,
        prepareHeaders: (headers, { getState }) =&gt; {
          const token = getState().userReducer.userInfo.token;
          console.log(token);
          if (token) {
            headers.set(&quot;authorization&quot;, `Bearer ${token}`);
          }
          return headers;
        },
      }),
      extractRehydrationInfo(action, { reducerPath }) {
        if (action.type === REHYDRATE) {
            return action.payload[reducerPath]
        }
      },
      tagTypes: [&quot;notes&quot;],
    
      endpoints: (builder) =&gt; ({
        createNote: builder.mutation({
          query: (data) =&gt; ({
            url: `/create`,
            method: &quot;POST&quot;,
            body: data,
          }),
          invalidatesTags: [&quot;notes&quot;],
        }),
        getSingleNote: builder.query({
          query: (id) =&gt; ({
            url: `/${id}`,
          }),
          providesTags: [&quot;notes&quot;],
        })
    });
    export const {  useGetSingleNoteQuery,
      useCreateNoteMutation,
    } = notesApi;



  [1]: https://i.stack.imgur.com/jqawj.png
||||||||||||||I've run into this issue a few times and it seems to manifest when attempting to rehydrate the store when there isn't anything in localStorage to hydrate from.

The error is saying it can't read `"notesApi"` of undefined when running `extractRehydrationInfo`. `"notesApi"` is the API slice's `reducerPath` value. The action's payload is undefined.

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload[reducerPath]; // <-- action.payload undefined
      }
    },

To resolve this issue I've simply used the Optional Chaining operator on the action payload.

Example:

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload?.[reducerPath];
      }
    },

--------------------------------------------------
Pyspark. spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, java.net.SocketException: Connection reset
I am new to pyspark, and i&#39;m trying to run multiple time series in prophet with pyspark (as distributed computing because i have 100s of times series to predict) but i have error as below. 


```
import time 
start_time = time.time()
sdf = spark.createDataFrame(data)
print(&#39;%0.2f min: Lags&#39; % ((time.time() - start_time) / 60))
sdf.createOrReplaceTempView(&#39;Quantity&#39;)
spark.sql(&quot;select Reseller_City, Business_Unit, count(*) from Quantity group by Reseller_City, Business_Unit order by Reseller_City, Business_Unit&quot;).show()
query = &#39;SELECT Reseller_City, Business_Unit, conditions, black_week, promos, Sales_Date as ds, sum(Rslr_Sales_Quantity) as y FROM Quantity GROUP BY Reseller_City, Business_Unit, conditions, black_week, promos, ds ORDER BY Reseller_City, Business_Unit, ds&#39;
spark.sql(query).show()
sdf.rdd.getNumPartitions()
store_part = (spark.sql(query).repartition(spark.sparkContext.defaultParallelism[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;])).cache()

store_part.explain()

from pyspark.sql.types import *

result_schema =StructType([
  StructField(&#39;ds&#39;,TimestampType()),
  StructField(&#39;Reseller_City&#39;,StringType()),
  StructField(&#39;Business_Unit&#39;,StringType()),
  StructField(&#39;y&#39;,DoubleType()),
  StructField(&#39;yhat&#39;,DoubleType()),
  StructField(&#39;yhat_upper&#39;,DoubleType()),
  StructField(&#39;yhat_lower&#39;,DoubleType())
  ])
from pyspark.sql.functions import pandas_udf, PandasUDFType

@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )
def forecast_sales( store_pd ):
    
    model = Prophet(interval_width=0.95, holidays = lock_down)
    model.add_country_holidays(country_name=&#39;DE&#39;)
    model.add_regressor(&#39;conditions&#39;)
    model.add_regressor(&#39;black_week&#39;)
    model.add_regressor(&#39;promos&#39;)
    
    train = store_pd[store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;]
    future_pd = store_pd[store_pd[&#39;ds&#39;]&gt;=&#39;2021-10-01 00:00:00&#39;]
    model.fit(train[[&#39;ds&#39;, &#39;y&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])


    forecast_pd = model.predict(future_pd[[&#39;ds&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])  

    f_pd = forecast_pd[ [&#39;ds&#39;,&#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ].set_index(&#39;ds&#39;)

    #store_pd = store_pd.filter(store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;)

    st_pd = future_pd[[&#39;ds&#39;,&#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;]].set_index(&#39;ds&#39;)

    results_pd = f_pd.join( st_pd, how=&#39;left&#39; )
    results_pd.reset_index(level=0, inplace=True)

    results_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]] = future_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]].iloc[0]

    return results_pd[ [&#39;ds&#39;, &#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;, &#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ]
results = (store_part.groupBy([&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]).apply(forecast_sales).withColumn(&#39;training date&#39;, current_date() ))
results.cache()
results.show()
``` 

All the lines are executed perfectly but error the come from **results.show()**  line  I dont understand where i have done wrong, Much appreciated if someone helps me 

```
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-46-8c647e8bf4d9&gt; in &lt;module&gt;
----&gt; 1 results.show()

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\dataframe.py in show(self, n, truncate, vertical)
    438         &quot;&quot;&quot;
    439         if isinstance(truncate, bool) and truncate:
--&gt; 440             print(self._jdf.showString(n, 20, vertical))
    441         else:
    442             print(self._jdf.showString(n, int(truncate), vertical))

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\utils.py in deco(*a, **kw)
    126     def deco(*a, **kw):
    127         try:
--&gt; 128             return f(*a, **kw)
    129         except py4j.protocol.Py4JJavaError as e:
    130             converted = convert_exception(e.java_exception)

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
--&gt; 328                     format(target_id, &quot;.&quot;, name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o128.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 1243, Grogu.profiflitzer.local, executor driver): java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	... 1 more

```  
||||||||||||||You can also set the os env variables by following the below steps,
run this before SparkSession/SparkContext

    import os
    import sys
    
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

It worked for me

--------------------------------------------------
Calculate difference between 2 date / times in Oracle SQL
I have a table as follows:

    Filename - varchar
    Creation Date - Date format dd/mm/yyyy hh24:mi:ss
    Oldest cdr date - Date format dd/mm/yyyy hh24:mi:ss

How can I calcuate the difference in hours minutes and seconds (and possibly days) between the two dates in Oracle SQL?

Thanks


||||||||||||||You can substract dates in Oracle. This will give you the difference in days. Multiply by 24 to get hours, and so on.

    SQL> select oldest - creation from my_table;


If your date is stored as character data, you have to convert it to a date type first.


    SQL> select 24 * (to_date('2009-07-07 22:00', 'YYYY-MM-DD hh24:mi') 
                 - to_date('2009-07-07 19:30', 'YYYY-MM-DD hh24:mi')) diff_hours 
           from dual;
    
    DIFF_HOURS
    ----------
           2.5

---
*Note*:

This answer applies to dates represented by the Oracle data type `DATE`.
Oracle also has a data type `TIMESTAMP`, which can also represent a date (with time). If you subtract `TIMESTAMP` values, you get an `INTERVAL`; to extract numeric values, use the `EXTRACT` function.

--------------------------------------------------
Excel VBA Macro Returning &quot;Subscript Out of Range&quot; for Incremental Function Converting Hyperlinks to Raw URLs
I am writing an Excel VBA macro that is working fine except for one specific function, and I cannot figure out why it is failing; I&#39;m not a programmer, although I do have some understanding of the basic logic, just little/no experience at writing it, so apologies in advance if there is any confusion imparted by my attempted explanations. The error returned when attempting to execute the code in question is &quot;Run-time error code &#39;9&#39;: subscript out of range&quot;, and I&#39;ve copied the relevant code snippets below:

    &#39; Define variable for worksheet in question
    Dim wsSales As Worksheet
    Set wsSales = ThisWorkbook.Sheets(&quot;Sales&quot;)

    &#39; Find last row with data in it
    Dim lastRowSales As Long
    lastRowSales = wsSales.Cells(Rows.Count, &quot;J&quot;).End(xlUp).Row

    &#39; Loop through column J and convert hyperlinks to raw URLs
    For i = 2 To lastRowSales
        If wsSales.Cells(i, &quot;J&quot;).Hyperlinks.Count &gt; 0 Then
            wsSales.Cells(i, &quot;J&quot;).Value = wsSales.Hyperlinks(i).Address
        End If
    Next i`

- For extra info/context, column J of the Sales sheet referenced contains hyperlinked text (e.g., &quot;Object Name&quot; that points to a URL in a sales-related webpage), and I&#39;m trying to get the actual URL for each row in the range so I can output it elsewhere. Row 1 is a header row, so I&#39;m starting with &#39;i = 2&#39; to ignore it accordingly.
- What the above code ends up doing is partially successful, but specifically fails on the last row for some reason. So if I have, for example, 100 rows in column J of the Sales sheet (99 rows with data and 1 header row), it will successfully convert any hyperlinked values to a URL for the first 99 rows, but row 100 does not convert and Excel spits out the &#39;subscript out of range&#39; error. When looking at the highlighted code that failed after clicking &#39;Debug&#39; on the error pop-up in the VBA Editor, it is specifically the &#39;wsSales.Hyperlinks(i).Address&#39; part that returns a value of &#39;&lt;subscript out of range&gt;&#39;.
- Additionally, it does not actually convert things quite properly; for example, say that row 50 has a hyperlinked text string in it. Rather than converting cell J50 to show the URL that was in J50, it actually shows the URL for J51, and it does this for the entire range (where it&#39;s showing the URL of the cell below it, not the cell itself).
- If I start with &#39;i = 1&#39; instead to include checking the header row (which will never have a hyperlink, but I figured was worth testing), the function works identically - same behavior, same error, no difference at all relative to starting with &#39;i = 2&#39;. That seems to imply to me the error is somewhere either in the logic before the function actually executes or my references in the function itself.
- I have also tested the above code with &quot;wsSales.Hyperlinks(1).address&quot; (1 instead of i) and it ends up completing successfully but using the same URL for the entire column J, so there seems to be a flaw with that logic as an alternative (presumably the static reference for the Hyperlinks object).  The same is true if I use &#39;2&#39; instead of &#39;1&#39;, so I suspect that using any digit will give me the same core problem.
- I feel like there must be something wrong with either my function or some variable I&#39;ve defined that is causing this, but after looking extensively through my code and attempting to &#39;rubber ducky&#39; troubleshoot it, I&#39;m still coming up blank.


I&#39;ve used essentially the exact same logic for multiple other formulas that compose the rest of the larger macro and they all work properly, but this function specifically fails to work as expected; every other &#39;for i = # To [value]&#39; iterates successfully and commenting out the above code snippet from the larger macro enables the full macro to work exactly as expected, just not this function. Does anyone have any thoughts or suggestions for why this may be failing to function as expected? Any ideas for what logic I should check, what may be failing, or a better way to do this? Any advice would be greatly appreciated, thanks!
||||||||||||||Refer to the hyperlink in *each specific cell*, not the [`Worksheet.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.worksheet.hyperlinks) collection:
```
For i = 2 To lastRowSales
    If wsSales.Cells(i, "J").Hyperlinks.Count > 0 Then
        wsSales.Cells(i, "J").Value = wsSales.Cells(i, "J").Hyperlinks(1).Address
    End If
Next i`
```
In other words, you want to use the [`Range.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.range.hyperlinks) property.

If you did want to use the `Worksheet.Hyperlinks` approach:

```
Dim h As Hyperlink
For Each h In wsSales.Hyperlinks
    h.Range.Value = h.Address
Next
```

--------------------------------------------------
How to send email attachments?
I am having problems understanding how to email an attachment using Python. I have successfully emailed simple messages with the `smtplib`. Could someone please explain how to send an attachment in an email. I know there are other posts online but as a Python beginner I find them hard to understand. 

    
||||||||||||||Here's another:

    import smtplib
    from os.path import basename
    from email.mime.application import MIMEApplication
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    from email.utils import COMMASPACE, formatdate
    
    
    def send_mail(send_from, send_to, subject, text, files=None,
                  server="127.0.0.1"):
        assert isinstance(send_to, list)
    
        msg = MIMEMultipart()
        msg['From'] = send_from
        msg['To'] = COMMASPACE.join(send_to)
        msg['Date'] = formatdate(localtime=True)
        msg['Subject'] = subject
 
        msg.attach(MIMEText(text))

        for f in files or []:
            with open(f, "rb") as fil:
                part = MIMEApplication(
                    fil.read(),
                    Name=basename(f)
                )
            # After the file is closed
            part['Content-Disposition'] = 'attachment; filename="%s"' % basename(f)
            msg.attach(part)

    
        smtp = smtplib.SMTP(server)
        smtp.sendmail(send_from, send_to, msg.as_string())
        smtp.close()


It's much the same as the first example... But it should be easier to drop in.

  [1]: http://snippets.dzone.com/posts/show/2038

--------------------------------------------------
Get handle to desktop / shell window
In one of my programs, I need to test if the user is currently focusing the desktop/shell window. Currently, I&#39;m using `GetShellWindow()` from *user32.dll* and compare the result to `GetForegroundWindow()`.

This approach is working until someone changes the desktop wallpaper, but as soon as the wallpaper is changed the handle from `GetShellWindow()` doesn&#39;t match the one from `GetForegroundWindow()` anymore and I don&#39;t quite get why that is. (**OS:** Windows 7 32bit)

Is there a better approach to check if the desktop is focused? Preferably one that won&#39;t be broken if the user changes the wallpaper?

**EDIT:** I designed a workaround: I&#39;m testing the handle to have a child of class `SHELLDLL_DefView`. If it has, the desktop is on focus. Whilst, it&#39;s working at my PC that doesn&#39;t mean it will work all the time.
||||||||||||||The thing changed a little bit since there are slideshows as wallpaper available in Windows 7.
You are right with WorkerW, but this works only with wallpaper is set to slideshow effect. 

When there is set the wallpaper mode to slideshow, you have to search for a window of class `WorkerW` and check the children, whether there is a `SHELLDLL_DefView`.
If there is no slideshow, you can use the good old `GetShellWindow()`.

I had the same problem some months ago and I wrote a function for getting the right window. Unfortunately I can't find it. But the following should work. Only the Win32 Imports are missing:

    public enum DesktopWindow
    {
        ProgMan,
        SHELLDLL_DefViewParent,
        SHELLDLL_DefView,
        SysListView32
    }
    
    public static IntPtr GetDesktopWindow(DesktopWindow desktopWindow)
    {
        IntPtr _ProgMan = GetShellWindow();
        IntPtr _SHELLDLL_DefViewParent = _ProgMan;
        IntPtr _SHELLDLL_DefView = FindWindowEx(_ProgMan, IntPtr.Zero, "SHELLDLL_DefView", null);
        IntPtr _SysListView32 = FindWindowEx(_SHELLDLL_DefView, IntPtr.Zero, "SysListView32", "FolderView");
    
        if (_SHELLDLL_DefView == IntPtr.Zero)
        {
            EnumWindows((hwnd, lParam) =>
            {
                if (GetClassName(hwnd) == "WorkerW")
                {
                    IntPtr child = FindWindowEx(hwnd, IntPtr.Zero, "SHELLDLL_DefView", null);
                    if (child != IntPtr.Zero)
                    {
                        _SHELLDLL_DefViewParent = hwnd;
                        _SHELLDLL_DefView = child;
                        _SysListView32 = FindWindowEx(child, IntPtr.Zero, "SysListView32", "FolderView"); ;
                        return false;
                    }
                }
                return true;
            }, IntPtr.Zero);
        }
    
        switch (desktopWindow)
        {
            case DesktopWindow.ProgMan:
                return _ProgMan;
            case DesktopWindow.SHELLDLL_DefViewParent:
                return _SHELLDLL_DefViewParent;
            case DesktopWindow.SHELLDLL_DefView:
                return _SHELLDLL_DefView;
            case DesktopWindow.SysListView32:
                return _SysListView32;
            default:
                return IntPtr.Zero;
        }
    }

In your case you would call `GetDesktopWindow(DesktopWindow.SHELLDLL_DefViewParent);` to get the top-level window for checking whether it is the foreground window.

--------------------------------------------------
Keil compiler v5 to v.6
I&#39;m forced to switch from ARMCC v5 to CLANG(v.6). Here is the problem. 
I have some struct that includes a pointer to the function which gets as a parameter pointer to the same structure. 
So I do 

```
struct _some_struct_s;
typedef void (*callback_f)(struct _some_struct *p);
 
typedef struct {
  callback_f fn;
  int        x; 
} some_type_s;

// init function
void init_some_struct (some_struct *p, callback_f f) {
  p-&gt;fn = f;
  p-&gt;x = 0;
}
```
In another file I&#39;m writing the callback() and calling init_some_struct()
```
some_type_s  my_struc;
void callback (some_type_s *p) {
  p-&gt;x++;
}
init_some_struct (&amp;my_struc, callback);
```
I had no issues with compiler 5 but a warning with version 6.
***
```
warning: incompatible function pointer types passing &#39;void (some_struct_s *)&#39; to parameter of type &#39;callback_f&#39; (aka &#39;void (*)(struct _some_struct_s *)&#39;) [-Wincompatible-function-pointer-types]
```
What can I do to avoid having this warning?


What can I do to avoid having this warning?

||||||||||||||1. `typedef (*callback_f)(struct _some_struct *p);` - you alias the type as function of pointer which returns `int`.

It should be `typedef void (*callback_f)(struct some_struct *p);`

2.     typedef struct {
            callback_f *fn;
   `fn` is a pointer to pointer to function. It should be `callback_f fn;`

```
typedef struct some_struct _some_struct_s;
typedef void callback_f(struct _some_struct *p);
 
typedef struct some_struct{
  callback_f *fn;
  int        x; 
} some_struct_s;

// init function
void init_some_struct (some_struct *p, callback_f *f) {
  p->fn = f;
  p->x = 0;
}
```



--------------------------------------------------
How to create mutually exclusive fields in Pydantic
I am using Pydantic to model an object. How can I make two fields mutually exclusive?

For instance, if I have the following model:

    class MyModel(pydantic.BaseModel):
        a: typing.Optional[str]
        b: typing.Optional[str]

I want field `a` and field `b` to be mutually exclusive. I want only one of them to be set. Is there a way to achieve that?
||||||||||||||You can use pydantic.validator decorator to add custom validations.

```lang-python
from typing import Optional
from pydantic import BaseModel, validator

class MyModel(BaseModel):
    a: Optional[str]
    b: Optional[str]

    @validator("b", always=True)
    def mutually_exclusive(cls, v, values):
        if values["a"] is not None and v:
            raise ValueError("'a' and 'b' are mutually exclusive.")

        return v
```

--------------------------------------------------
Web automation with Selenium + python and google chrome 115.x &gt;
How to use selenium and chrome CFT for web automation from chrome version 115.x using python?

I have an automation script that worked fine until chrome version 114.x. From version 115.x it stopped working due to the version update, but also due to the new method with chrome cft.
||||||||||||||After upgrading to Chrome version 115.x, my automation stopped working, and chrome driver versions were no longer released, because from chrome version 115.x, automations are performed by CFT (chrome for test ), which as I understand this browser remains static until user action, preventing automations from stopping due to automatic chrome updates and need for crhome driver replacement.
The problem was solved with the solution below:


```
# using selenium 4.8 and python 3.9

from selenium import webdriver
from selenium.webdriver.chrome.options import Options


options = Options()
options.binary_location = 'path to chrome.exe'
## this is the chromium for testing which can be downloaded from the link given below

driver = webdriver.Chrome(chrome_options = options, executable_path = 'path to chromedriver.exe')
## must be the same as the downloaded version of chrome cft.
```
As of today, the files can be downloaded from: https://googlechromelabs.github.io/chrome-for-testing/

Prefer the stable version and download the compatible browser and chromedriver.

The rest of the code continues to work.

source: https://stackoverflow.com/questions/45500606/set-chrome-browser-binary-through-chromedriver-in-python

--------------------------------------------------
How can I list the taints on Kubernetes nodes?
The [docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint) are great about explaining how to set a taint on a node, or remove one. And I can use `kubectl describe node` to get a verbose description of one node, including its taints. But what if I&#39;ve forgotten the name of the taint I created, or which nodes I set it on? Can I list all of my nodes, with any taints that exist on them?
||||||||||||||<!-- language-all: lang-bash -->

    kubectl get nodes -o json | jq '.items[].spec'

which will give the complete spec with node name, or:

    kubectl get nodes -o json | jq '.items[].spec.taints'

will produce the list of the taints per each node

--------------------------------------------------
How to write unitTest for methods using a stream as a parameter
I have class `ImportProvider` , and I want write unit test for Import method.

But this should be unit test, so I don&#39;t want to read from file to stream.
Any idea?

  

    public class ImportProvider : IImportProvider
    { 
         public bool Import(Stream stream)
         {
             //Do import
        
             return isImported;
         }
    }
        
    public interface IImportProvider
    {
          bool Import(Stream input);
    }

This is unit test:

    [TestMethod]
    public void ImportProvider_Test()
    {
        // Arrange           
        var importRepository = new Mock&lt;IImportRepository&gt;(); 
        var imp = new ImportProvider(importRepository.Object);
        //Do setup...

        // Act
        var test_Stream = ?????????????
        // This working but not option:
        //test_Stream = File.Open(&quot;C:/ExcelFile.xls&quot;, FileMode.Open, FileAccess.Read);
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }
||||||||||||||Use a MemoryStream. Not sure what your function expects, but to stuff a UTF-8 string into it for example:

    //Act
    using (var test_Stream = new MemoryStream(Encoding.UTF8.GetBytes("whatever")))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }

EDIT: If you need an Excel file, and you are unable to read files from disk, could you add an Excel file as an embedded resource in your test project? See [How to embed and access resources by using Visual C#][1]

You can then read as a stream like this:

    //Act
    using (var test_Stream = this.GetType().Assembly.GetManifestResourceStream("excelFileResource"))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }


  [1]: https://support.microsoft.com/en-us/kb/319292

--------------------------------------------------
Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
What is causing this build error:

```

- Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
- Plugin Repositories (could not resolve plugin artifact &#39;com.android.application:com.android.application.gradle.plugin:7.0.3&#39;)
  Searched in the following repositories:
    Gradle Central Plugin Repository
    Google
```

in `build.gradle` file

Expecting a successful android build
||||||||||||||In my case `settings.gradle` file was missing. You can create a file and place into project root folder.

**settings.gradle**:

    pluginManagement {
        repositories {
            gradlePluginPortal()
            google()
            mavenCentral()
        }
    }
    dependencyResolutionManagement {
        repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)
        repositories {
            google()
            mavenCentral()
        }
    }
    rootProject.name = "android-geocode"
    include ':app'

--------------------------------------------------
How do I use an API key/secret on Binance&#39;s TestNet?
Following the instructions here, https://docs.binance.org/smart-chain/wallet/arkane.html, I created a Binance SmartChain account with its &quot;0x&quot; prefixed wallet address.  I then added funds.  What I can&#39;t figure out is how I get a TestNet API key and secret so that I can test my Python API calls.  I create the client like so

	from binance.client import Client
	...
	auth_client = Client(key, b64secret)
     if account.testing:
     	auth_client.API_URL = &#39;https://testnet.binance.vision/api&#39;
 
How do I get an API key tied to my Binance SmartChain address?
||||||||||||||You have to create your API credentials from [here][1] and pass the testnet variable into the Client constructor. See the
[documentation][2].

```python
auth_client = Client(key, b64secret, testnet=True)
```

does the job.


  [1]: https://testnet.binance.vision/
  [2]: https://python-binance.readthedocs.io/en/latest/binance.html?highlight=client#binance.client.Client.__init__

--------------------------------------------------
org.gradle.kotlin.kotlin-dsl was not found
I am getting the following error while running the build

    FAILURE: Build failed with an exception.
    
    * Where:
    Build file &#39;/home/charming/mainframer/bigovlog_android/buildSrc/build.gradle.kts&#39; line: 4
    
    * What went wrong:
    Plugin [id: &#39;org.gradle.kotlin.kotlin-dsl&#39;, version: &#39;1.2.6&#39;] was not found in any of the following sources:
    
    - Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
    - Plugin Repositories (could not resolve plugin artifact &#39;org.gradle.kotlin.kotlin-dsl:org.gradle.kotlin.kotlin-dsl.gradle.plugin:1.2.6&#39;)
      Searched in the following repositories:
        Gradle Central Plugin Repository

my buildSrc/build.gradle.kts

    repositories {
        jcenter()
    }
    plugins {
        `kotlin-dsl`
        id(&quot;groovy&quot;)
    }
    dependencies{
        gradleApi()
        localGroovy()
    }

I tried everything but still not working
||||||||||||||Did you check that Android Studio wasn't running in Offline Mode? Take a look at `Preferences/Build, Execution, Deployment/Gradle/Global Gradle settings` and see if Offline Work is checked.

--------------------------------------------------
Typesetting New Functions in LaTeX
So, I just have a little question:

What is the &quot;best way&quot; to typeset new functions in LaTeX which aren&#39;t already included in the various packages?  Right now I&#39;m just using `\mbox` as my go-to method,  but I just was wondering if there was a more &quot;acceptable way of doing it (as with mbox, I have to make sure to include spaces around the text of the functions in order for it to not look too strange)

Here is an example:

    $y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$

which comes out looking like:

![$y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$][1]

Don&#39;t get me wrong... I think it looks fine, but I was just looking for some opinions (as far as best practices go).

  [1]: http://adamnbowen.com/images/error_function.jpg
||||||||||||||Use `\DeclareMathOperator` from package `amsmath`. For example,

```tex
\documentclass{article}
\usepackage{amsmath}
\DeclareMathOperator\erfi{Erfi}

\begin{document}
Consider $x + y + \erfi(t) = z$ for example.
\end{document}
```

produces

[![result][1]][1]

If you only need it once, you can also use `\operatorname`: you get the same output as above with

```tex
\documentclass{article}
\usepackage{amsmath}
\begin{document}
Consider $x + y + \operatorname{Erfi}(t) = z$ for example.
\end{document}
```

If you cannot use the `amsmath` package for some reason, you can manually do `\mathop{\mathrm{Erfi}}` like:

```
\documentclass{article}
\begin{document}
Consider $x + y + \mathop{\mathrm{Erfi}}(t) = z$ for example.
\end{document}
```

See the always-useful TeX FAQ, specifically [Defining a new log-like function in LaTeX](https://texfaq.org/FAQ-newfunction).

  [1]: https://i.stack.imgur.com/9GCor.png

--------------------------------------------------
C# change a string variable with List or Array
I have some static strings 

    static string   Robocopy_Mirror = &quot;[Robocopy_Mirror]&quot;; 
    static string   Robocopy_Copy = &quot;[Robocopy_Copy]&quot;;
    static string   Network_Path_1 = &quot;[Network_Path_1]&quot;;    // \\NAS\Sync\
    static string   Lokal_Path_1 = &quot;[Lokal_Path_1]&quot;;      // X:\Sync\

And I thought I could save some lines of code if I put them in a List and change the values in the List with a loop.

    List&lt;string&gt; variableListe = new List&lt;string&gt;()  
    {   
        Robocopy_Mirror , Robocopy_Copy , Network_Path_1, Lokal_Path_1, 
        File_Network_Sync_1, File_Lokal_Sync_1, File_Network_Sync_2, File_Lokal_Sync_2
    };


But I can&#39;t change the static variables. I guess the List object does not change the static variable? Is there a quick way to change it? 

    for (int i = 0; i &lt; variableListe.Count-1; i++)
    {
        variableListe[i] = AppConfig.ElementAt(configPathPosition);
    }
    Console.WriteLine(Robocopy_Mirror); 

    // prints  &quot;[Robocopy_Mirror]&quot; instead of like C:\robocopy


||||||||||||||The static variables store references to string objects in memory. The elements in the list also store references to the same string objects in memory, but _each element is it's own variable_ and those elements _do not store references to the static variables;_ they refer to the string objects directly. 

When you change an element in the list, you're changing the variable in the list to point to a new object in a new memory location. The static variables do not change and continue to refer to the same unchanged strings as they did before.


--------------------------------------------------
How do I run curl command from within a Kubernetes pod
I have the following questions:

1. I am logged into a Kubernetes pod using the following command:

        ./cluster/kubectl.sh exec my-nginx-0onux -c my-nginx -it bash

    The &#39;ip addr show&#39; command shows its assigned the ip of the pod. Since pod is a logical concept, I am assuming I am logged into a docker container and not a pod, In which case, the pod IP is same as docker container IP. Is that understanding correct?

2. from a Kubernetes node, I do `sudo docker ps` and then do the following:-

        sudo docker exec  71721cb14283 -it &#39;/bin/bash&#39;

    This doesn&#39;t work. Does someone know what I am doing wrong?

3. I want to access the nginx service I created, from within the pod using curl. How can I install curl within this pod or container to access the service from inside. I want to do this to understand the network connectivity.
||||||||||||||Here is how you get a curl command line within a kubernetes network to test and explore your internal REST endpoints.

To get a prompt of a busybox running inside the network, execute the following command. (A tip is to use one unique container per developer.)

```sh
kubectl run curl-<YOUR NAME> --image=radial/busyboxplus:curl -i --tty --rm
```

You may omit the --rm and keep the instance running for later re-usage. To reuse it later, type:

```sh
kubectl attach <POD ID> -c curl-<YOUR NAME> -i -t
```

Using the command `kubectl get pods` you can see all running POD's. The `<POD ID>` is something similar to `curl-yourname-944940652-fvj28`.

**EDIT:** Note that you need to login to google cloud from your terminal (once) before you can do this! Here is an example, make sure to put in your zone, cluster and project: 
```sh
gcloud container clusters get-credentials example-cluster --zone europe-west1-c --project example-148812
```

--------------------------------------------------
Execute CURL with kubectl
I am trying to execute `curl` command with `kubectl` like 

    kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;

Gives belob error

	OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused &quot;exec: \&quot;kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39;\&quot;: 
	stat kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;: no such file or directory&quot; 
    :unknown command terminated with exit code 126
I have tried to escape the quotes but no luck. Then I tried simple curl 

    kubectl exec -it POD_NAME curl http://localhost:8080/xyz

This gives proper output as excepted. Any help with this 

Update: 

But when I run interactive (`kubectl exec -it POD_NAME /bin/bash`) mode of container and then run the curl inside the container works like champ
||||||||||||||i think you need to do something like this:

```
kubectl exec POD_NAME curl "-X PUT http://localhost:8080/abc -H \"Content-Type: application/json\" -d '{\"name\":\"aaa\",\"no\":\"10\"}' "
```

what the error suggests is that its trying to interpret everything inside `""` as a single command, not as a command with parameters. so its essentially looking for an executable called that

--------------------------------------------------
Open in Safari with UIActivityViewController?
I&#39;m sharing a URL via UIActivityViewController. I&#39;d like to see &quot;Open in Safari&quot; or &quot;Open in browser&quot; appear on the share sheet, but it doesn&#39;t. Is there a way to make this happen?

Note: I am not interested in solutions that involve adding somebody else&#39;s library to my app. I want to understand how to do this, not just get it to happen.

||||||||||||||Yes, you could add your custom action to Share sheet in iOS


You would have to copy this class.

    class MyActivity: UIActivity {
        var _activityTitle: String
        var _activityImage: UIImage?
        var activityItems = [Any]()
        var action: ([Any]) -> Void
        
        init(title: String, image: UIImage?, performAction: @escaping ([Any]) -> Void) {
            _activityTitle = title
            _activityImage = image
            action = performAction
            super.init()
        }
        override var activityTitle: String? {
            return _activityTitle
        }
    
        override var activityImage: UIImage? {
            return _activityImage
        }
        override var activityType: UIActivity.ActivityType {
            return UIActivity.ActivityType(rawValue: "com.someUnique.identifier")
        }
    
        override class var activityCategory: UIActivity.Category {
            return .action
        }
        override func canPerform(withActivityItems activityItems: [Any]) -> Bool {
            return true
        }
        override func prepare(withActivityItems activityItems: [Any]) {
            self.activityItems = activityItems
        }
        override func perform() {
            action(activityItems)
            activityDidFinish(true)
        }
    }

Please go through the class you might need to change a few things.

This is how you use it.

        let customItem = MyActivity(title: "Open in Safari", image: UIImage(systemName: "safari")  ) { sharedItems in
            guard let url = sharedItems[0] as? URL else { return }
            UIApplication.shared.open(url)
        }

        let items = [URL(string: "https://www.apple.com")!]
        let ac = UIActivityViewController(activityItems: items, applicationActivities: [customItem])
        ac.excludedActivityTypes = [.postToFacebook]
        present(ac, animated: true)

I have done this for one action, and tested it, it works.

Similarly you could do it for other custom actions.

For more on it refer this link.
[Link To Detailed Post][1]


  [1]: https://www.hackingwithswift.com/articles/118/uiactivityviewcontroller-by-example


--------------------------------------------------
How to cache playwright-python contexts for testing?
I am doing some web scraping using [`playwright-python&gt;=1.41`][1], and have to launch the browser in a headed mode (e.g. `launch(headless=False)`.

For CI testing, I would like to somehow cache the headed interactions with Chromium, to enable offline testing:

- First invocation: uses Chromium to make real-world HTTP transactions
- Later invocations: uses Chromium, but all HTTP transactions read from a cache

How can this be done? I can&#39;t find any clear answers on how to do this.

  [1]: https://github.com/microsoft/playwright-python
||||||||||||||It might solve your problem using HAR-file recording:
1. Run the first test while [recording a HAR-file][1]
2. Storing the HAR-file as an artifact, in your repo or similar in your CI environment
3. Running test again [with recorded HAR-file][2]

Here is how to do that with `playwright==1.41.1` and `pytest-playwright==0.3.3`:

```python
import pathlib

import pytest
from playwright.sync_api import Browser, Playwright

CACHE_DIR = pathlib.Path(__file__).parent / "cache"


@pytest.fixture(name="example_har", scope="session")
def fixture_example_har(playwright: Playwright) -> pathlib.Path:
    har_file = CACHE_DIR / "example.har"
    with (
        playwright.chromium.launch(headless=False) as browser,
        browser.new_page() as page,
    ):
        page.route_from_har(har_file, url="*/**", update=True)
        page.goto("https://example.com/")
    return har_file


def test_caching(browser: Browser, example_har: pathlib.Path) -> None:
    with browser.new_context(offline=True) as context:
        page = context.new_page()
        page.route_from_har(example_har, url="*/**")
        page.goto("https://example.com/")
```

  [1]: https://playwright.dev/python/docs/mock#recording-a-har-file
  [2]: https://playwright.dev/python/docs/mock#replaying-from-har

--------------------------------------------------
Python: Using .format() on a Unicode-escaped string
I am using Python 2.6.5. My code requires the use of the &quot;more than or equal to&quot; sign. Here it goes:  

    &gt;&gt;&gt; s = u&#39;\u2265&#39;
    &gt;&gt;&gt; print s
    &gt;&gt;&gt; ≥
    &gt;&gt;&gt; print &quot;{0}&quot;.format(s)
    Traceback (most recent call last):
         File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; 
    UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\u2265&#39;
      in position 0: ordinal not in range(128)`  

Why do I get this error? Is there a right way to do this? I need to use the `.format()` function.

||||||||||||||Just make the second string also a unicode string

    >>> s = u'\u2265'
    >>> print s
    ≥
    >>> print "{0}".format(s)
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    UnicodeEncodeError: 'ascii' codec can't encode character u'\u2265' in position 0: ordinal not in range(128)
    >>> print u"{0}".format(s)
    ≥
    >>> 



--------------------------------------------------
Only Content controls are allowed directly in a content page that contains Content controls in ASP.NET
I have an application which has a master page and child pages. My application is working fine on local host (on my intranet). But as soon as I put it on a server that is on the internet, I get the error shown below after clicking on any menus.

&gt; Only Content controls are allowed directly in a content page that contains Content controls.

![screenshot][1]




  [1]: http://i.stack.imgur.com/b21sZ.png
||||||||||||||
Double and triple check your opening and closing Content tags throughout your child pages.

**Confirm that they** 

 - are in existence
 - are spelled correctly
 - have an ID
 - have runat="server"
 - have the correct ContentPlaceHolderID

--------------------------------------------------
Apollo Client is not reading variables passed in using useQuery hook
Having a weird issue passing variables into the useQuery hook.

The query:
```
const GET_USER_BY_ID= gql`
  query($id: ID!) {
    getUser(id: $id) {
      id
      fullName
      role
    }
  }
`;
```
Calling the query:
```
const DisplayUser: React.FC&lt;{ id: string }&gt; = ({ id }) =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID, {
    variables: { id },
  });

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Rendering the component:
```
&lt;DisplayUser id=&quot;5e404fa72b819d1410a3164c&quot; /&gt;
```

This yields the error: 
```
&quot;Argument \&quot;id\&quot; of required type \&quot;ID!\&quot; was provided the variable \&quot;$id\&quot; which was not provided a runtime value.&quot;
```

Calling the query from GraphQL Playground returns the expected result:
```
{
  &quot;data&quot;: {
    &quot;getUser&quot;: {
      &quot;id&quot;: &quot;5e404fa72b819d1410a3164c&quot;,
      &quot;fullName&quot;: &quot;Test 1&quot;,
      &quot;role&quot;: &quot;USER&quot;
    }
  }
}
```
And calling the query without a variable but instead hard-coding the id:
```
const GET_USER_BY_ID = gql`
  query {
    getUser(id: &quot;5e404fa72b819d1410a3164c&quot;) {
      id
      fullName
      role
    }
  }
`;

const DisplayUser: React.FC = () =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID);

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Also returns the expected result.

I have also attempted to test a similar query that takes `firstName: String!` as a parameter which also yields an error saying that the variable was not provided a runtime value. This query also works as expected when hard-coding a value in the query string.

This project was started today and uses `&quot;apollo-boost&quot;: &quot;^0.4.7&quot;`, `&quot;graphql&quot;: &quot;^14.6.0&quot;`, and `&quot;react-apollo&quot;: &quot;^3.1.3&quot;`.
||||||||||||||[Solved]

In reading through the stack trace I noticed the issue was referencing `graphql-query-complexity` which I was using for validationRules. I removed the validation rules and now everything works! Granted I don't have validation at the moment but at least I can work from here. Thanks to everyone who took the time to respond!

--------------------------------------------------
Why it is a StackOverFlow Exception?
Why following code throws `StackoverflowException`? 

    class Foo
    {
        Foo foo = new Foo();
    }
    class Program
    {
        static void Main(string[] args)
        {
            new Foo();
        }
    }
||||||||||||||In `Main` you create a new `Foo` object, invoking its constructor.
Inside the `Foo` constructor, you create a different `Foo` instance, again invoking the `Foo` constructor.

This leads to infinite recursion and a `StackOverflowException` being thrown.

--------------------------------------------------
Function to aggregate json
Assume I have a gcs bucket with json files with the following structure:

```
[
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeid&quot;: &quot;Y1&quot;,
    &quot;storeName&quot;: &quot;alibaba1&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.8/3.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y2&quot;,
     &quot;storeName&quot;: &quot;alibaba2&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.7/2.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y3&quot;,
     &quot;storeName&quot;: &quot;alibaba3&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;2.7/4.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y4&quot;,
     &quot;storeName&quot;: &quot;alibaba4&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;3.7/5.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  }
]
```

What I want to do is to aggregate the different values by summing ```a, b,c, d, f,g``` and taking the average of ```e``` to return one single ```json``` like

```
[
{
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;a&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;b&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;c&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;d&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;e&quot;: &quot;average over all first instance/average over all second instance&quot;,
    &quot;f&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;g&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
  }
]
``` 

Not that any of the values in ```*/*/*``` could be NaN and that the data in ```e``` could be a string ```data unvavailable```.

In have created this function 

```
def format_large_numbers_optimized(value):
    abs_values = np.abs(value)
    mask = abs_values &gt;= 1e6
    formatted_values = np.where(mask, 
                                np.char.add(np.round(value / 1e6, 2).astype(str), &quot;M&quot;), 
                                np.round(value, 2).astype(str))
    return formatted_values

def process_json_data_optimized(json_list):
    result = {}
    keys = set(json_list[0].keys()) - {&#39;Id&#39;, &#39;Name&#39;, &#39;storeid&#39;, &#39;storeName&#39;}
    for key in keys:
        result[key] = {&#39;values&#39;: []}
    for json_data in json_list:
        for key in keys:
            value = json_data.get(key, &#39;0&#39;)  
            result[key][&#39;values&#39;].append(value)
    for key in keys:
        all_values_processed = []
        for value in result[key][&#39;values&#39;]:
            if isinstance(value, str) and &#39;/&#39; in value:
                processed_values = [float(v) if v != &#39;data unavailable&#39; else 0 for v in value.split(&#39;/&#39;)]
            elif isinstance(value, float) or isinstance(value, int):
                processed_values = [value]
            else:
                processed_values = [0.0]  
            all_values_processed.append(processed_values)
        numeric_values = np.array(all_values_processed)
        if numeric_values.ndim == 1:
            numeric_values = numeric_values[:, np.newaxis]
        summed_values = np.sum(numeric_values, axis=0)
        formatted_summed_values = &#39;/&#39;.join(format_large_numbers_optimized(summed_values))
        result[key][&#39;summed&#39;] = formatted_summed_values
    processed_result = {key: data[&#39;summed&#39;] for key, data in result.items()}
    processed_result[&#39;Id&#39;] = json_list[0][&#39;Id&#39;]
    processed_result[&#39;Name&#39;] = json_list[0][&#39;Name&#39;]
    return processed_result
```

But it does not create what I expect. I am a at a total loss. Would really appreciate any help.
||||||||||||||Note that you are placing the values as lists `all_values_processed`.
Assuming that the `/` character is just a separator, and that what you want by replacing `all_values_processed.append(processed_values)` by `all_values_processed += processed_values`. Or even better you could just aggregate the values.

For instance you could have a function to aggregate like this

```lang-py
import math
def agg_func(value, initial):
  v_count, v_sum = initial
  if isinstance(value, str) and '/' in value:
    for v in value.split('/'):
      if v != 'data unavailable' :
         v = float(v)
         if not math.isnan(v):
           v_sum += v
           v_count += 1
  elif isinstance(value, float) or isinstance(value, int):
    if not math.isnan(value):
      v_sum += value
      v_count += 1
  return v_count, v_sum
```

A function that aggregate the given keys in the json

```lang-py
def agg_json(v_list, fields):
  state = {k: (0, 0) for k in fields}
  for item in v_list:
    for k in fields:
      if k in item:
        state[k] = agg(item[k], state[k])
  return state
```


Now
```lang-py
state = agg_json(json_list, ['a', 'b', 'c', 'd', 'e', 'f', 'g'])
```

will give you a dictionary with tuples containing the count and the sum for each field. To get your final answer you could do

```lang-py
result = {k: v[1] / v[0] if k == 'e' else v[1] for k, v in state.items()}
```

--------------------------------------------------
color text in divs with two colors using css only - tricky
OK, let me rewrite my question in another words so it looks clear and interesting: [jsFiddle][1]


  [1]: http://jsfiddle.net/xY6T3/1/

I need a pure css solution that colorizes the lines of text in the color depending whether the line is odd or even.

The example of code could be :

    &lt;div class=&quot;main&quot;&gt;
        &lt;div class=&quot;zipcode12345&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode23456&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode90033&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode11321&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;

Is it possible to make it with css? As you see [@ jsFiddle][1], it is not colorized as expected.

So, the main div is &quot;main&quot;.
The inner `div`s always have class names in format &quot;zipcodeXXXXX&quot;, as you see.
The number of zipcodeXXXXX is variable, the number of `myclass` is variable.
However, the odd lines should be always red and the even lines should be always blue.
Does pure css solution exist?

That would be kind of 

    .myclass:nth-child(2n+1){
     color:red;
    }
    .myclass:nth-child(2n){
     color:blue;
    }

if we could igonre `&quot;zipcodeXXXXX&quot;` divs, right?

Thank you.
||||||||||||||Simply apply different odd/even rules to the parent elements as well as the child elements:

<!-- language: lang-css -->

    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(odd),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(even) {
        color: red;
    }
    
    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(even),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(odd) {
        color: blue;
    }

[**JSFiddle demo**][1].


  [1]: http://jsfiddle.net/xY6T3/9/

--------------------------------------------------
How to populate columns in a table using JavaScript
I need to create a simple table using JavaScript based on an array with nested objects, which should have only two columns. In the first cell of the first column of the table, the Processor header is specified, after which the corresponding processor models are written to the lower cells. In the first cell of the second column, the Processor frequency header is indicated, after which the frequencies are written to the lower cells. I was able to generate code for this task, but it doesn&#39;t work correctly. Instead of writing keys to column cells after the first iteration, for some reason, it takes into account unnecessary objects. That&#39;s why you get an undefined value in the table headers and a re-duplication. Please tell me how to solve this problem. 

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    let processorFrequency = [
        {
            titleOne : &#39;Processor&#39;, values : [
                {name : &#39;80386LC (1988г.)&#39;},
                {name : &#39;80486DX4 (1994г.)&#39;},
                {name : &#39;Pentium MMX (1997г.)&#39;},
                {name : &#39;Pentium II (1998г.)&#39;},
                {name : &#39;Pentium III (1999г.)&#39;},
                {name : &#39;Pentium IV&#39;},
                {name : &#39;Athlon-Athlon XP&#39;},
                {name : &#39;Athlon 64&#39;},

            ]
        },
        {
            titleTwo : &#39;Processor frequency&#39;, values : [
                {name : &#39;33-60&#39;},
                {name : &#39;80-133&#39;},
                {name : &#39;160-233&#39;},
                {name : &#39;260-550&#39;},
                {name : &#39;300-1400&#39;},
                {name : &#39;1600-3800&#39;},
                {name : &#39;1400-3200&#39;},
                {name : &#39;2600-3800&#39;},
            ]
    },
    ]



    function Test(){
        let table = document.getElementsByTagName(&#39;table&#39;)[0];
        for (let i = 0; i &lt; processorFrequency.length; i++) {
            var pf = processorFrequency[i];
            let tableRow = document.createElement(&#39;tr&#39;);
            let tdOne = document.createElement(&#39;td&#39;);
            let tdTwo = document.createElement(&#39;td&#39;);
            let txtOne = document.createTextNode(pf.titleOne);
            let txtTwo = document.createTextNode(pf.titleTwo);
            
            tdOne.className = &#39;head&#39;;
            tdTwo.className = &#39;head&#39;;

            tdOne.appendChild(txtOne);
            tdTwo.appendChild(txtTwo);

            tableRow.appendChild(tdOne);
            tableRow.appendChild(tdTwo);
            table.appendChild(tableRow);

            var values = pf.values;
            for (let j = 0; j &lt; values.length; j++) {
                let value = values[j];
                let tableRow = document.createElement(&#39;tr&#39;);
                let td = document.createElement(&#39;td&#39;);
                let txt = document.createTextNode(value.name);
                td.appendChild(txt);
                tableRow.appendChild(td);
                table.appendChild(tableRow);
            }
        }
    } 

    Test();

&lt;!-- language: lang-css --&gt;

    table td, table th {
      border: 1px solid black;
      padding: 5px;
    }

&lt;!-- language: lang-html --&gt;

    &lt;table&gt;&lt;!-- Contents will be created via JavaScript --&gt;
    &lt;/table&gt;

&lt;!-- end snippet --&gt;


||||||||||||||Seems like the issue is that you are creating a new row for each processor AND frequency value, which results in extra rows being added to the table. The correct way should be create a single row for each processor, with two cells (one for the processor model and one for the processor frequency):

    let processorFrequency = [
        {
            titleOne: 'Processor', values: [
                { name: '80386LC (1988г.)' },
                { name: '80486DX4 (1994г.)' },
                { name: 'Pentium MMX (1997г.)' },
                { name: 'Pentium II (1998г.)' },
                { name: 'Pentium III (1999г.)' },
                { name: 'Pentium IV' },
                { name: 'Athlon-Athlon XP' },
                { name: 'Athlon 64' },
            ]
        },
        {
            titleTwo: 'Processor frequency', values: [
                { name: '33-60' },
                { name: '80-133' },
                { name: '160-233' },
                { name: '260-550' },
                { name: '300-1400' },
                { name: '1600-3800' },
                { name: '1400-3200' },
                { name: '2600-3800' },
            ]
        },
    ];
    
    function Test() {
        let table = document.getElementsByTagName('table')[0];
    
        for (let i = 0; i < processorFrequency[0].values.length; i++) {
            let tableRow = document.createElement('tr');
            
            // Processor Name Cell
            let tdOne = document.createElement('td');
            let txtOne = document.createTextNode(processorFrequency[0].values[i].name);
            tdOne.appendChild(txtOne);
            tableRow.appendChild(tdOne);
    
            // Processor Frequency Cell
            let tdTwo = document.createElement('td');
            let txtTwo = document.createTextNode(processorFrequency[1].values[i].name);
            tdTwo.appendChild(txtTwo);
            tableRow.appendChild(tdTwo);
    
            table.appendChild(tableRow);
        }
    }
    
    Test();

--------------------------------------------------
System.UnauthorizedAccessException: Access to the path &quot;...&quot; is denied
  I have C# wpf installation done with .net using click once installation. All works fine. Then I have the following code which is part of the installed program:

    String destinationPath = System.Windows.Forms.Application.StartupPath + &quot;\\&quot; + fileName;
    File.Copy(path, destinationPath, true);
    this.DialogResult = true;
    this.Close();

But I get this error:

&gt;System.UnauthorizedAccessException: Access to the path C:\user\pc\appdata\local\apps\2.0.......  is denied.
&gt;
&gt;at System.IO.File.InternalCopy(String sourceFileName, String destFileName, Boolean overwrite, Boolean checkHost)
&gt;       at System.IO.File.Copy(String sourceFileName, String destFileName, Boolean overwrite)

Is it a permission error or do I need to tweak something in my code?

What puzzles me is why the user is able to install the program using click once into that directory without any issues, but uploading a file to it doesn&#39;t work?
||||||||||||||When installing an application the installer usually asks for administrative privileges. If the user chooses "Yes" the program will run and have read and write access to a larger variety of paths than what a normal user has. If the case is such that the installer did not ask for administrative privileges, it might just be that ClickOnce automatically runs under some sort of elevated privileges.

I'd suggest you write to the local appdata folder instead, but if you feel you really want to write to the very same directory as your application you must first run your app with administrator privileges.

To make your application always ask for administrator privileges you can modify your app's manifest file and set the `requestedExecutionLevel` tag's `level` attribute to `requireAdministrator`:

    <requestedExecutionLevel level="requireAdministrator" uiAccess="false" />

You can read a bit more in [**How do I force my .NET application to run as administrator?**](https://stackoverflow.com/questions/2818179/how-do-i-force-my-net-application-to-run-as-administrator)

--------------------------------------------------
Create a NuGet package for .NET8 MAUI with Azure DevOps
I have created a `.NET8 MAUI Class Library` to use in MAUI projects. The repo is in `Azure DevOps` and I was trying to build and publish the package via NuGet.

For that, I wrote a YAML file

    trigger:
    - main
    
    pool:
      vmImage: ubuntu-latest
    
    steps:
    - task: UseDotNet@2
      displayName: &#39;Use dotnet 8&#39;
      inputs:
        version: &#39;8.0.x&#39;
    - task: CmdLine@2
      inputs:
        script: &#39;dotnet workload install maui&#39;
    - task: DotNetCoreCLI@2
      displayName: Restore packages
      inputs:
        command: &#39;restore&#39;
        feedsToUse: &#39;select&#39;
        vstsFeed: &#39;c800d0d7-e2af-4567-997f-de7cf7888e6c&#39;
    - task: DotNetCoreCLI@2
      displayName: Build project
      inputs:
        command: &#39;build&#39;
        projects: &#39;**/PSC.Maui.Components.BottomSheet.csproj&#39;
        arguments: &#39;--configuration $(buildConfiguration)&#39;

When the pipeline runs, I get this error

    Generating script.
    Script contents:
    dotnet workload install maui
    ========================== Starting Command Output ===========================
    /usr/bin/bash --noprofile --norc /home/vsts/work/_temp/42901c0d-f407-4f75-912b-f93132efa865.sh
    Workload ID maui isn&#39;t supported on this platform.
    
    ##[error]Bash exited with code &#39;1&#39;.
    Finishing: CmdLine


[![enter image description here][1]][1]

Then, I tried to create the NuGet package locally, but it was not recognized by the NuGet website when I uploaded it.

How can I change the pipeline?

  [1]: https://i.stack.imgur.com/loBv9.png
||||||||||||||.Net MAUI does not support Linux, therefore you can neither build to it or from it.  

See [here](https://learn.microsoft.com/en-us/dotnet/maui/supported-platforms?view=net-maui-8.0)  

--------------------------------------------------
How to specify multiple locators for Selenium web element using the FindBy and PageFactory mechanisms
I like to use `PageFactory` with `@FindBy` annotations in my automation framework to auto-locate elements in my page object classes. 

I have one WebElement for which I need to be able to specify a couple of different locators. I thought FindBys was my solution, but apparently, that is not how it works. It&#39;s the equivalent of `driver.findElement(option1).findelement.(option2)`. That&#39;s not what I need. I need something that will find an element by one or the other locators. If one doesn&#39;t work, then use the other locator. Is there a way to do this in Selenium using FindBy annotations?
||||||||||||||There is apparently a new feature in Selenium as of May this year -- the @FindAll annotation that does exactly what I need;

http://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/support/FindAll.html
http://selenium.10932.n7.nabble.com/Pull-Request-62-Add-a-FindAll-annotation-to-the-Java-Page-Factory-td24814.html

--------------------------------------------------
Store cout from function as string
I have a function that takes in a vector of integers and outputs them via `std::cout`. 

    #include &lt;iostream&gt;
    #include &lt;vector&gt;
    
    void final_sol(std::vector&lt;int&gt; list){
        for (int i ; i &lt; list.size() ; i++){
            std::cout &lt;&lt; list[i] &lt;&lt; &quot; &quot;;
        }
    }
    
    int main(){
        std::vector&lt;int&gt; list = {1, 2, 3, 4, 5};
        final_sol(list);
        return 0;
    }
However, from this point I would like to have a way to quickly obtain the outputs of `final_sol(vector)` as a string. One way to do this would be to modify the original function to also create the string. However, I am not interested in modifying `final_sol(vector)`. Is there another way I could store the outputs as a string?
||||||||||||||Provide overload:
```
void final_sol(std::ostream& out, const std::vector<int>& list){
    for (int i = 0; i < list.size() ; i++){
        out << list[i] << " ";
    }
}

void final_sol(const std::vector<int>& list){
    final_sol(std::cout, list);
}
```
This way you existing calling code will not be impacted - most probably this is what you want: not modifying function signature. Not what you described: not do not modifying implementation of final_sol.

Then you can do:
```cpp
std::ostringstream str;
final_sol(str, list);
auto s = str.str()
```


--------------------------------------------------
FastAPI runs api-calls in serial instead of parallel fashion
I have the following code:

```python
import time
from fastapi import FastAPI, Request
    
app = FastAPI()
    
@app.get(&quot;/ping&quot;)
async def ping(request: Request):
        print(&quot;Hello&quot;)
        time.sleep(5)
        print(&quot;bye&quot;)
        return {&quot;ping&quot;: &quot;pong!&quot;}
```
If I run my code on localhost - e.g., `http://localhost:8501/ping` - in different tabs of the same browser window, I get:
```
Hello
bye
Hello
bye
```
instead of:
```
Hello
Hello
bye
bye
```
I have read about using `httpx`, but still, I cannot have a true parallelization. What&#39;s the problem?
||||||||||||||As per [FastAPI's documentation][1]:

> When you declare a path operation function with normal `def` instead
> of `async def`, it is run in an external threadpool **that is then
> `await`ed**, instead of being called directly (as it would block the
> server).

also, as described [here][2]:

> If you are using a third party library that communicates with
> something (a database, an API, the file system, etc.) and doesn't have
> support for using `await`, (this is currently the case for most
> database libraries), then declare your path operation functions as
> normally, with just `def`.
> 
> If your application (somehow) doesn't have to communicate with
> anything else and wait for it to respond, use `async def`.
> 
> If you just don't know, use normal `def`.
> 
> **Note**: You can mix `def` and `async def` in your path operation functions as much as you need and define each one using the best
> option for you. FastAPI will do the right thing with them.
> 
> Anyway, in any of the cases above, FastAPI **will still work
> asynchronously** and be extremely fast.
> 
> But by following the steps above, it will be able to do some
> performance optimizations.



Thus, `def` endpoints (in the context of asynchronous programming, a function defined with just `def` is called *synchronous* function), in FastAPI, run in a separate thread from an external threadpool that is then `await`ed, and hence, FastAPI will still work *asynchronously*. In other words, the server will process requests to such endpoints *concurrently*. Whereas, `async def` endpoints run in the [`event loop`][3]&mdash;on the main (single) thread&mdash;that is, the server will also process requests to such endpoints *concurrently*/*asynchronously*, **as long as there is** an [`await`][4] call to non-blocking I/O-bound operations inside such `async def` endpoints/routes, such as *waiting* for (1) data from the client to be sent through the network, (2) contents of a file in the disk to be read, (3) a database operation to finish, etc., (have a look [here][5]). If, however, an endpoint defined with `async def` does not `await` for something inside, in order to give up time for other tasks in the `event loop` to run (e.g., requests to the same or other endpoints, background tasks, etc.), each request to such an endpoint will have to be completely finished (i.e., exit the endpoint), before returning control back to the `event loop` and allow other tasks to run. In other words, in such cases, the server will process requests *sequentially*. **Note** that the same concept not only applies to FastAPI endpoints, but also to [`StreamingResponse`'s generator function][6] (see [`StreamingResponse`][7] class implementation), as well as [`Background Tasks`][8] (see [`BackgroundTask`][9] class implementation); hence, after reading this answer to the end, you should be able to decide whether you should define a FastAPI endpoint, `StreamingResponse`'s generator, or background task function with `def` or `async def`. 

The keyword `await` (which works only within an `async def` function) passes function control back to the `event loop`. In other words, it suspends the execution of the surrounding [coroutine][10] (i.e., a coroutine object is the result of calling an `async def` function), and tells the `event loop` to let some other task run, until that `await`ed task is completed. **Note** that just because you may define a custom function with `async def` and then `await` it inside your `async def` endpoint, it doesn't mean that your code will work asynchronously, if that custom function contains, for example, calls to `time.sleep()`, CPU-bound tasks, non-async I/O libraries, or any other blocking call that is incompatible with asynchronous Python code. In FastAPI, for example, when using the `async` methods of [`UploadFile`][11], such as `await file.read()` and `await file.write()`, FastAPI/Starlette, behind the scenes, actually runs such *synchronous* [File objects' methods][12] in a separate thread from the external threadpool (using the `async` [`run_in_threadpool()`][13] function) and `await`s it; otherwise, such methods/operations would block the `event loop`&mdash;you can find out more by looking at the [implementation of the `UploadFile` class][14]. The number of worker threads of that external threadpool can be adjusted as required&mdash;please have a look at [this answer][15] for more details.

**Note**  that `async` does not mean *parallel*, but *concurrently*. Asynchronous code with [`async` and `await` is many times summarised as using coroutines][16]. **Coroutines** are collaborative (or [cooperatively multitasked][17]), meaning that "at any given time, a program with coroutines is running **only** one of its coroutines, and this running coroutine suspends its execution only when it explicitly requests to be suspended" (see [here][18] and [here][19] for more info on coroutines). As described in [this article][20]:

> Specifically, whenever execution of a currently-running coroutine
> reaches an `await` expression, the coroutine may be suspended, and
> another previously-suspended coroutine may resume execution if what it
> was suspended on has since returned a value. Suspension can also
> happen when an `async for` block requests the next value from an
> asynchronous iterator or when an `async with` block is entered or
> exited, as these operations use `await` under the hood.

If, however, a blocking I/O-bound or CPU-bound operation was directly executed/called inside an `async def` function/endpoint, it would **block the main thread**, and hence, the `event loop` (as the `event loop` runs in the main thread). Hence, a blocking operation such as `time.sleep()` in an `async def` endpoint would block the entire server (as in the code example provided in your question). Thus, if your endpoint is not going to make any `async` calls, you could declare it with normal `def` instead, in which case, FastAPI would run it in a separate thread from the external threadpool and `await` it, as explained earlier (more solutions are given in the following sections). Example:
```python
@app.get("/ping")
def ping(request: Request):
	#print(request.client)
	print("Hello")
	time.sleep(5)
	print("bye")
	return "pong"
```

Otherwise, if the functions that you had to execute inside the endpoint are `async` functions that you had to `await`, you should define your endpoint with `async def`. To demonstrate this, the example below uses the [`asyncio.sleep()`][21] function (from the [`asyncio`][22] library), which provides a non-blocking sleep operation. The `await asyncio.sleep()` method will suspend the execution of the surrounding coroutine (until the sleep operation is completed), thus allowing other tasks in the `event loop` to run. Similar examples are given [here][23] and [here][24] as well.
```python
import asyncio
 
@app.get("/ping")
async def ping(request: Request):
	#print(request.client)
	print("Hello")
	await asyncio.sleep(5)
	print("bye")
	return "pong"
```

**Both** the endpoints above will print out the specified messages to the screen in the same order as mentioned in your question&mdash;if two requests arrived at around the same time&mdash;that is:
```text
Hello
Hello
bye
bye
```

### Important Note
When you call your endpoint for the second (third, and so on) time, please remember to do that from **a tab that is isolated from the browser's main session**; otherwise, succeeding requests (i.e., coming after the first one) will be blocked by the browser (on **client side**), as the browser will be waiting for response from the server for the previous request before sending the next one. You can confirm that by using `print(request.client)` inside the endpoint, where you would see the `hostname` and `port` number being the same for all incoming requests&mdash;if requests were initiated from tabs opened in the same browser window/session)&mdash;and hence, those requests would be processed sequentially, because of the browser sending them sequentially in the first place. To **solve** this, you could either:
1. Reload the same tab (as is running), or
2. Open a new tab in an Incognito Window, or
3. Use a different browser/client to send the request, or
4. Use the `httpx` library to [make asynchronous HTTP requests][25], along with the [*awaitable*][26] [`asyncio.gather()`][27], which allows executing multiple asynchronous operations concurrently and then returns a list of results in the **same** order the awaitables (tasks) were passed to that function (have a look at [this answer][28] for more details).

   **Example**:
   ```python
   import httpx
   import asyncio

   URLS = ['http://127.0.0.1:8000/ping'] * 2

   async def send(url, client):
       return await client.get(url, timeout=10)

   async def main():
       async with httpx.AsyncClient() as client:
           tasks = [send(url, client) for url in URLS]
           responses = await asyncio.gather(*tasks)
           print(*[r.json() for r in responses], sep='\n')

   asyncio.run(main())
   ```
   In case you had to call different endpoints that may take different time to process a request, and you would like to print the response out on client side as soon as it is returned from the server&mdash;instead of waiting for `asyncio.gather()` to gather the results of all tasks and print them out in the same order the tasks were passed to the `send()` function&mdash;you could replace the `send()` function of the example above with the one shown below:
   ```
   async def send(url, client):
       res = await client.get(url, timeout=10)
       print(res.json())
       return res
   ```

`Async`/`await` and Blocking I/O-bound or CPU-bound Operations
--------------------------------------

If you are required to use `async def` (as you might need to `await` for coroutines inside your endpoint), but also have some _synchronous_ blocking I/O-bound or CPU-bound operation (long-running computation task) that will block the `event loop` (essentially, the entire server) and won't let other requests to go through, for example:
```python 
@app.post("/ping")
async def ping(file: UploadFile = File(...)):
    print("Hello")
	try:
		contents = await file.read()
		res = cpu_bound_task(contents)  # this will block the event loop
	finally:
		await file.close()
	print("bye")
    return "pong"
```

then:

1. You should check whether you could change your endpoint's definition to normal `def` instead of `async def`. For example, if the only method in your endpoint that has to be awaited is the one reading the file contents (as you mentioned in the comments section below), you could instead declare the type of the endpoint's parameter as `bytes` (i.e., `file: bytes = File()`) and thus, FastAPI would read the file for you and you would receive the contents as `bytes`. Hence, there would be no need to use `await file.read()`. Please note that the above approach should work for small files, as the enitre file contents would be stored into memory (see the [documentation on `File` Parameters][29]); and hence, if your system does not have enough RAM available to accommodate the accumulated data (if, for example, you have 8GB of RAM, you can’t load a 50GB file), your application may end up crashing. Alternatively, you could call the `.read()` method of the [`SpooledTemporaryFile`][30] directly (which can be accessed through the `.file` attribute of the `UploadFile` object), so that again you don't have to `await` the `.read()` method&mdash;and as you can now declare your endpoint with normal `def`, each request will run in a **separate thread** (example is given below). For more details on how to upload a `File`, as well how Starlette/FastAPI uses `SpooledTemporaryFile` behind the scenes, please have a look at [this answer][31] and [this answer][32].

   ```python 
   @app.post("/ping")
   def ping(file: UploadFile = File(...)):
       print("Hello")
	   try:
		   contents = file.file.read()
		   res = cpu_bound_task(contents)
	   finally:
		   file.file.close()
       print("bye")
       return "pong"
   ```

2. Use FastAPI's (Starlette's) [`run_in_threadpool()`][13] function from the `concurrency` module&mdash;as @tiangolo suggested [here][33]&mdash;which "will run the function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked" (see [here][34]). As described by @tiangolo [here][35], "`run_in_threadpool` is an `await`able function; the first parameter is a normal function, the following parameters are passed to that function directly. It supports both *sequence* arguments and *keyword* arguments".

   ```python
   from fastapi.concurrency import run_in_threadpool

   res = await run_in_threadpool(cpu_bound_task, contents)
   ```

3. Alternatively, use `asyncio`'s [`loop.run_in_executor()`][36]&mdash;after obtaining the running `event loop` using [`asyncio.get_running_loop()`][37]&mdash;to run the task, which, in this case, you can `await` for it to complete and return the result(s), before moving on to the next line of code. Passing `None` to the *executor* argument, the *default* executor will be used; which is a [`ThreadPoolExecutor`][38]:

   ```python
   import asyncio

   loop = asyncio.get_running_loop()
   res = await loop.run_in_executor(None, cpu_bound_task, contents)
   ```
   or, if you would like to [pass keyword arguments][39] instead, you could use a `lambda` expression (e.g., `lambda: cpu_bound_task(some_arg=contents)`), or, preferably, [`functools.partial()`][40], which is specifically recommended in the documentation for [`loop.run_in_executor()`][36]:
   ```python
   import asyncio
   from functools import partial

   loop = asyncio.get_running_loop()
   res = await loop.run_in_executor(None, partial(cpu_bound_task, some_arg=contents))
   ```

   In Python 3.9+, you could also use [`asyncio.to_thread()`][41] to asynchronously run a synchronous function in a separate thread&mdash;which, essentially, uses `await loop.run_in_executor(None, func_call)` under the hood, as can been seen in the [implementation of `asyncio.to_thread()`][42]. The `to_thread()` function takes the name of a blocking function to execute, as well as any arguments (`*args` and/or `**kwargs`) to the function, and then returns a coroutine that can be `await`ed. Example:
   ```
   import asyncio

   res = await asyncio.to_thread(cpu_bound_task, contents)
   ```
    
   **Note** that as explained in [**this answer**][15], passing `None` to the `executor` argument **does not** create a new `ThreadPoolExecutor` every time you call `await loop.run_in_executor(None, ...)`, but instead re-uses the *default* executor with the *default* number of worker threads (i.e., `min(32, os.cpu_count() + 4)`). Thus, depending on the requirements of your application, that number might be quite low. In that case, you should rather use a custom [`ThreadPoolExecutor`][38]. For instance:
   ```python
   import asyncio
   import concurrent.futures

   loop = asyncio.get_running_loop()
   with concurrent.futures.ThreadPoolExecutor() as pool:
	   res = await loop.run_in_executor(pool, cpu_bound_task, contents)
   ```
   I would strongly recommend having a look at the linked answer above to learn about the difference between using `run_in_threadpool()` and `run_in_executor()`, as well as how to create a re-usable custom `ThreadPoolExecutor` at the application startup, and adjust the number of maximum worker threads as needed.

4. `ThreadPoolExecutor` will successfully prevent the `event loop` from being blocked, but won't give you the **performance improvement** you would expect from running **code in parallel**; especially, when one needs to perform `CPU-bound` tasks, such as the ones described [here][43] (e.g., audio or image processing, machine learning, and so on). It is thus preferable to **run CPU-bound tasks in a separate process**&mdash;using [`ProcessPoolExecutor`][44], as shown below&mdash;which, again, you can integrate with `asyncio`, in order to `await` it to finish its work and return the result(s). As described [here][45], it is important to protect the entry point of the program to avoid recursive spawning of subprocesses, etc. Basically, your code must be under [`if __name__ == '__main__'`][46]. 

   ```python
   import concurrent.futures
   
   loop = asyncio.get_running_loop()
   with concurrent.futures.ProcessPoolExecutor() as pool:
       res = await loop.run_in_executor(pool, cpu_bound_task, contents) 
   ```
   Again, I'd suggest having a look at the linked answer earlier on how to create a re-usable `ProcessPoolExecutor` at the application startup. You might find [this answer][47] helpful as well.

5. Use **more [workers][48]** to take advantage of multi-core CPUs, in order to run multiple processes in parallel and be able to serve more requests. For example, `uvicorn main:app --workers 4` (if you are using [Gunicorn as a process manager with Uvicorn workers][49], please have a look at [**this answer**][50]). When using 1 worker, only one process is run. When using multiple workers, this will spawn multiple processes (all single threaded). Each process has a separate Global Interpreter Lock (GIL), as well as its own `event loop`, which runs in the main thread of each process and executes all tasks in its thread. That means, there is only one thread that can take a lock on the interpreter of each process; unless, of course, you employ additional threads, either outside or inside the `event loop`, e.g., when using a `ThreadPoolExecutor` with `loop.run_in_executor`, or defining endpoints/background tasks/`StreamingResponse`'s generator with normal `def` instead of `async def`, as well as when calling `UploadFile`'s methods (see the first two paragraphs of this answer for more details).

   **Note:** Each worker ["has its own things, variables and memory"][51]. This means that `global` variables/objects, etc., won't be shared across the processes/workers. In this case, you should consider using a database storage, or  Key-Value stores (Caches), as described [here][52] and [here][53]. Additionally, note that "if you are consuming a large amount of memory in your code, **each process** will consume an equivalent amount of memory".


6. If you need to perform **heavy background computation** and you don't necessarily need it to be run by the same process (for example, you don't need to share memory, variables, etc), you might benefit from using other bigger tools like [Celery][54], as described in [FastAPI's documentation][55].


  [1]: https://fastapi.tiangolo.com/async/#path-operation-functions
  [2]: https://fastapi.tiangolo.com/async/#concurrency-and-async-await
  [3]: https://docs.python.org/3/library/asyncio-eventloop.html
  [4]: https://stackoverflow.com/questions/38865050/is-await-in-python3-cooperative-multitasking
  [5]: https://fastapi.tiangolo.com/async/#asynchronous-code
  [6]: https://stackoverflow.com/a/75760884/17865804
  [7]: https://github.com/encode/starlette/blob/31164e346b9bd1ce17d968e1301c3bb2c23bb418/starlette/responses.py#L235
  [8]: https://fastapi.tiangolo.com/tutorial/background-tasks/
  [9]: https://github.com/encode/starlette/blob/33f46a13625bcca4b7520e33be299a23b2e2b26c/starlette/background.py#L15
  [10]: https://docs.python.org/3/library/asyncio-task.html#coroutines
  [11]: https://fastapi.tiangolo.com/tutorial/request-files/#uploadfile
  [12]: https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects
  [13]: https://github.com/encode/starlette/blob/b8ea367b4304a98653ec8ce9c794ad0ba6dcaf4b/starlette/concurrency.py#L35
  [14]: https://github.com/encode/starlette/blob/048643adc21e75b668567fc6bcdd3650b89044ea/starlette/datastructures.py#L426
  [15]: https://stackoverflow.com/a/77941425/17865804
  [16]: https://fastapi.tiangolo.com/async/#coroutines
  [17]: https://en.wikipedia.org/wiki/Cooperative_multitasking
  [18]: https://stackoverflow.com/questions/553704/what-is-a-coroutine
  [19]: https://stackoverflow.com/questions/1934715/difference-between-a-coroutine-and-a-thread
  [20]: https://jwodder.github.io/kbits/posts/pyasync-fundam/
  [21]: https://docs.python.org/3/library/asyncio-task.html#asyncio.sleep
  [22]: https://docs.python.org/3/library/asyncio.html
  [23]: https://docs.python.org/3/library/asyncio-task.html#coroutine
  [24]: https://stackoverflow.com/a/56730924
  [25]: https://www.python-httpx.org/async/#making-async-requests
  [26]: https://docs.python.org/3/library/asyncio-task.html#awaitables
  [27]: https://docs.python.org/3/library/asyncio-task.html#asyncio.gather
  [28]: https://stackoverflow.com/a/74239367/17865804
  [29]: https://fastapi.tiangolo.com/tutorial/request-files/#define-file-parameters
  [30]: https://docs.python.org/3/library/tempfile.html#tempfile.SpooledTemporaryFile
  [31]: https://stackoverflow.com/a/70657621/17865804
  [32]: https://stackoverflow.com/a/70667530/17865804
  [33]: https://github.com/tiangolo/fastapi/issues/1066#issuecomment-612940187
  [34]: https://bocadilloproject.github.io/guide/async.html#common-patterns
  [35]: https://gitter.im/tiangolo/fastapi?at=5ce550f675d9a575a625feb7
  [36]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor
  [37]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop
  [38]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
  [39]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio-pass-keywords
  [40]: https://docs.python.org/3/library/functools.html#functools.partial
  [41]: https://docs.python.org/3/library/asyncio-task.html#asyncio.to_thread
  [42]: https://github.com/python/cpython/blob/c5660ae96f2ab5732c68c301ce9a63009f432d93/Lib/asyncio/threads.py#L12
  [43]: https://fastapi.tiangolo.com/async/#is-concurrency-better-than-parallelism
  [44]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
  [45]: https://stackoverflow.com/q/15900366
  [46]: https://stackoverflow.com/questions/419163/what-does-if-name-main-do
  [47]: https://stackoverflow.com/a/77862153/17865804
  [48]: https://fastapi.tiangolo.com/deployment/server-workers/
  [49]: https://fastapi.tiangolo.com/deployment/server-workers/#gunicorn-with-uvicorn-workers
  [50]: https://stackoverflow.com/a/71613757/17865804
  [51]: https://fastapi.tiangolo.com/deployment/concepts/#memory-per-process
  [52]: https://stackoverflow.com/a/71537393/17865804
  [53]: https://stackoverflow.com/a/65699375/17865804
  [54]: https://docs.celeryq.dev/
  [55]: https://fastapi.tiangolo.com/tutorial/background-tasks/#caveat

--------------------------------------------------
Is it possible to access Svelte store from external js files?
I am wondering if i would be able to access my *Svelte* store values from a plain .js file.

I am trying to write functions returning a dynamic value based on a store value, to import them in any component.
But in a plain .js file I can&#39;t just access the store value with the $ sign..

Quick exemple of a basic function that uses a store value and could be used on multiple components: 

```js
//in .svelte

function add() {
    $counter = $counter + 1;
}
```

*EDIT: rephrasing a bit*

*EDIT:*
Found a solution but i don&#39;t really know if it&#39;s really optimized..

```js
//in .js file

import { get } from &quot;svelte/store&quot;;
import { counter } from &quot;./stores&quot;;

export function add() {
    var counterRef = get(counter);
    counter.set(counterRef + 1);
}
```
||||||||||||||In addition to rixo's answer, a better way to implement `add` is to use the store's `update` method:

```js
import { counter } from "./stores";

export function add() {
    counter.update(n => n + 1);
}
```

You could also create a [custom store](https://svelte.dev/tutorial/custom-stores) that implemented that logic.

--------------------------------------------------
Is CP437 decoding broken for control characters?
According to the [Wikipedia page for Code Page 437](https://en.wikipedia.org/wiki/Code_page_437) the byte values `\x01` through `\x1f` should decode to graphic characters, e.g. `b&#39;\x01&#39;` equates to ☺ `&#39;\u263A&#39;`. But that&#39;s not what `decode` produces:

    &gt;&gt;&gt; b&#39;\x01&#39;.decode(&#39;cp437&#39;)
    &#39;\x01&#39;

That was Python 3.6 but 2.7 does the same, for all 31 byte values.
||||||||||||||While there were graphics associated with the byte range `\x01` through `\x1f`, those graphics were only used in some contexts. In other contexts, those code points would be interpreted as control characters, as in ASCII. Quoting an [IBM page on CP437][1]:

> Code points X'01' through X'1F' and X'7F' may be controls or graphics depending on context. For displays the hexadecimal code in a memory-mapped 
video display buffer is a graphic. For printers the graphics context is established by a preceding control sequence in the data stream. There are two 
such control sequences: ESC X'5C' and ESC X'5E' named Print All Characters and Print Single Character respectively. In other situations the code 
points in question are used as controls.



Python's CP437 decoding is based on the [Unicode mappings on Unicode.org][2], which use the control character interpretation.

The [Unicode FAQ implies][3] that "The correct Unicode mappings for the special graphic characters (01-1F, 7F) of CP437 and other DOS-type code pages" should be available at https://www.unicode.org/Public/MAPPINGS, but digging down there only turns up the mappings with the control characters, and a [page][4] linking to several IBM websites. Digging through IBM's sites turns up ftp://ftp.software.ibm.com/software/globalization/gcoc/attachments/CP00437.txt, which gives graphical mappings for `\x01`-`\x1f` in terms of IBM's [GCGID system][5], but not in terms of Unicode.

I don't know if there actually *is* an official mapping, from either IBM or Unicode, that gives canonical Unicode mappings for `\x01`-`\x1f` in terms of the graphical interpretation of CP437.


  [1]: http://www-01.ibm.com/software/globalization/cp/cp00437.html
  [2]: ftp://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/PC/CP437.TXT
  [3]: http://unicode.org/faq/char_combmark.html#5
  [4]: http://www.unicode.org/Public/MAPPINGS/VENDORS/IBM/IBM_conversions.html
  [5]: https://www-01.ibm.com/software/globalization/gcgid/gcgid.html

--------------------------------------------------
Converting from Python-Polars to Rust-Polars
I have the following working Python polars code. I am learning Rust and am interested in converting Python to Rust.

```
df = df.with_columns(pl.concat([pl.col(base).slice(0, period).rolling_mean(period), pl.col(base).slice(period,None)]).alias(&#39;con&#39;))
```
How to convert the same in Rust? It might be very trivial, still not sure where I am going wrong.

```
let rolling_options = RollingOptions {
        window_size : Duration::parse(duration_str.as_str()),
        ..Default::default()
    };
let a = col(base).slice(0,period).rolling_mean(rolling_options);
let b = col(base).slice(period,lit(Null {}));

let temp_df = df.with_column(concat([a, b], UnionArgs::default()));
```
I keep getting the following error

&gt;mismatched types
expected enum `Expr`
   found enum `Result&lt;LazyFrame, PolarsError&gt;`

When i checked the data types of **a** and **b**, to my surprise they are **LazyFrame** and not **Expr**

[rolling_mean][1] as per the document returns **Expr** and so does [slice][2]. Not sure what I am missing.


  [1]: https://docs.rs/polars/latest/polars/prelude/enum.Expr.html#method.rolling_mean
  [2]: https://docs.rs/polars/latest/polars/prelude/enum.Expr.html#method.slice
||||||||||||||The result datatype you are getting is an enum meant to represent whether the operation was successful (returns a lazyFrame) or it failed (returns polarserror).

you should be able to 'uwnrap' the result

This link should cover the different ways to handle errors/results in rust:
https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html

--------------------------------------------------
Is Element.tagName always uppercase?
Reading at [MDN about Element.tagName][1] it states:

&gt;On HTML elements in DOM trees flagged as HTML documents, tagName returns the element name in the uppercase form.

My question is: is this trustable? Does IE (old and modern) behave as expected? Is this likely to change? or is it better to always work with `el.tagName.toLowerCase()`?


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element.tagName
||||||||||||||You don't have to `toLowerCase` or whatever, browsers do behave the same on this point (surprisingly huh?).

About the rationale, once I had discussion with a colleague who's very professional on W3C standards. One of his opinions is that using uppercase TAGNAME would be much easier to recognize them out of user content. That's quite persuasive for me.

-------------
**Edit:** As @adjenks says, XHTML doctype returns mixed-case tagName *if the document is served as `Content-Type: application/xhtml+xml`*. Test page: http://programming.enthuses.me/tag-node-case.php?doc=x

Technically, please read this spec for more info: http://www.w3.org/TR/REC-DOM-Level-1/level-one-core.html#ID-745549614

> Note that this (tagName) is **case-preserving in XML**, as are all of the operations of the DOM. The HTML DOM returns the tagName of an HTML element in the canonical uppercase form, regardless of the case in the source HTML document.

As of asker's question: this is trustable. Breaking change is not likely to happen in HTML spec.

--------------------------------------------------
How to validate more than one field of a Pydantic model?
I want to validate three model Fields of a Pydantic model. To do this, I am importing [`root_validator`][1] from pydantic, however I am getting the error below:
```py3
from pydantic import BaseModel, ValidationError, root_validator
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ImportError: cannot import name &#39;root_validator&#39; from &#39;pydantic&#39; (C:\Users\Lenovo\AppData\Local\Programs\Python\Python38-32\lib\site-packages\pydantic\__init__.py)
```

I tried this:
```python
@validator
def validate_all(cls, v, values, **kwargs):
    ...
```

I am inheriting my pydantic model from some common fields parent model. Values showing only parent class fields, but not my child class fields. For example:

```py3
class Parent(BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str
    
    @validator
    def validate_all(cls, v, values, **kwargs):
        #here values showing only (name and comment) but not address and phone.
        ...
```


  [1]: https://pydantic-docs.helpmanual.io/usage/validators/#root-validators
||||||||||||||To extend on the answer of `Rahul R`, this example shows in more detail how to use the `pydantic` validators.

This example contains all the necessary information to answer your question.

Note, that there is also the option to use a `@root_validator`, as mentioned by `Kentgrav`, see the example at the bottom of the post for more details.

```python
import pydantic

class Parent(pydantic.BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str

    # If you want to apply the Validator to the fields "name", "comments", "address", "phone"
    @pydantic.validator("name", "comments", "address", "phone")
    @classmethod
    def validate_all_fields_one_by_one(cls, field_value):
        # Do the validation instead of printing
        print(f"{cls}: Field value {field_value}")

        return field_value  # this is the value written to the class field

    # if you want to validate to content of "phone" using the other fields of the Parent and Child class
    @pydantic.validator("phone")
    @classmethod
    def validate_one_field_using_the_others(cls, field_value, values, field, config):
        parent_class_name = values["name"]
        parent_class_address = values["address"] # works because "address" is already validated once we validate "phone"
        # Do the validation instead of printing
        print(f"{field_value} is the {field.name} of {parent_class_name}")

        return field_value 

Customer(name="Peter", comments="Pydantic User", address="Home", phone="117")
```
**Output**
```cmd
<class '__main__.Customer'>: Field value Peter
<class '__main__.Customer'>: Field value Pydantic User
<class '__main__.Customer'>: Field value Home
<class '__main__.Customer'>: Field value 117
117 is the phone number of Peter
Customer(name='Peter', comments='Pydantic User', address='Home', phone='117')
```

To answer your question in more detail:

Add the fields to validate to the `@validator` decorator directly above the validation function.
- `@validator("name")` uses the field value of `"name"` (e.g. `"Peter"`) as input to the validation function. All fields of the class and its parent classes can be added to the `@validator` decorator.
- the validation function (`validate_all_fields_one_by_one`) then uses the field value as the second argument (`field_value`) for which to validate the input. The return value of the validation function is written to the class field. The signature of the validation function is `def validate_something(cls, field_value)` where the function and variable names can be chosen arbitrarily (but the first argument should be `cls`). According to Arjan (https://youtu.be/Vj-iU-8_xLs?t=329), also the `@classmethod` decorator should be added.


If the goal is to validate one field by using other (already validated) fields of the parent and child class, the full signature of the validation function is `def validate_something(cls, field_value, values, field, config)` (the argument names `values`,`field` and `config` **must** match) where the value of the fields can be accessed with the field name as key (e.g. `values["comments"]`).

**Edit1**: If you want to check only input values of a certain type, you could use the following structure:
```python
@validator("*") # validates all fields
def validate_if_float(cls, value):
    if isinstance(value, float):
        # do validation here
    return value
```

**Edit2**: Easier way to validate all fields together using `@root_validator`:
```python
import pydantic

class Parent(pydantic.BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str

    @pydantic.root_validator()
    @classmethod
    def validate_all_fields_at_the_same_time(cls, field_values):
        # Do the validation instead of printing
        print(f"{cls}: Field values are: {field_values}")
        assert field_values["name"] != "invalid_name", f"Name `{field_values['name']}` not allowed."
        return field_values
```

**Output**:

```python
Customer(name="valid_name", comments="", address="Street 7", phone="079")
<class '__main__.Customer'>: Field values are: {'name': 'valid_name', 'comments': '', 'address': 'Street 7', 'phone': '079'}
Customer(name='valid_name', comments='', address='Street 7', phone='079')
```

```python
Customer(name="invalid_name", comments="", address="Street 7", phone="079")
ValidationError: 1 validation error for Customer
__root__
  Name `invalid_name` not allowed. (type=assertion_error)
```

--------------------------------------------------
What is Python&#39;s bytes type actually used for?
Could somebody explain the general purpose of [the bytes type in Python 3](https://docs.python.org/3/library/stdtypes.html#bytes-objects), or give some examples where it is preferred over other data types? 

I see that the advantage of [bytearrays](https://docs.python.org/3/library/stdtypes.html#bytearray-objects) over strings is their mutability, but what about bytes? So far, the only situation where I actually needed it was sending and receiving data through sockets; is there something else? 
||||||||||||||Possible duplicate of [what is the difference between a string and a byte string][1]

In short, the bytes type is a sequence of bytes that have been encoded and are ready to be stored in memory/disk. There are many types of encodings (utf-8, utf-16, windows-1255), which all handle the bytes differently. The bytes object can be decoded into a str type.

The str type is a sequence of unicode characters. The str needs to be encoded to be stored, but is mutable and an abstraction of the bytes logic. 

There is a strong relationship between `str` and `bytes`. `bytes` can be decoded into a `str`, and `str`s can be encoded into bytes. 

You typically only have to use `bytes` when you encounter a string in the wild with a unique encoding, or when a library requires it. `str` , especially in python3, will handle the rest. 

More reading [here][2] and [here][3]



  [1]: https://stackoverflow.com/questions/6224052/what-is-the-difference-between-a-string-and-a-byte-string
  [2]: https://eli.thegreenplace.net/2012/01/30/the-bytesstr-dichotomy-in-python-3
  [3]: https://betterprogramming.pub/strings-unicode-and-bytes-in-python-3-everything-you-always-wanted-to-know-27dc02ff2686

--------------------------------------------------
Process terminated. Couldn&#39;t find a valid ICU package installed on the system in Asp.Net Core 3 - ubuntu
I am trying to run a Asp.Net Core 3 application in Ubuntu 19.10 thru terminal using `dotnet run` command but it does not seem to work. I get this error.

&gt; ```none
&gt; Process terminated. Couldn&#39;t find a valid ICU package installed on the system.
&gt; Set the configuration flag System.Globalization.Invariant to true if you want
&gt; to run with no globalization support.   
&gt;  at System.Environment.FailFast(System.String)   
&gt;  at System.Globalization.GlobalizationMode.GetGlobalizationInvariantMode()
&gt;  at System.Globalization.GlobalizationMode..cctor()   
&gt;  at System.Globalization.CultureData.CreateCultureWithInvariantData()   
&gt;  at System.Globalization.CultureData.get_Invariant()   
&gt;  at System.Globalization.CultureInfo..cctor()   
&gt;  at System.StringComparer..cctor()   
&gt;  at System.StringComparer.get_OrdinalIgnoreCase()   
&gt;  at Microsoft.Extensions.Configuration.ConfigurationProvider..ctor()   
&gt;  at Microsoft.Extensions.Configuration.EnvironmentVariables.EnvironmentVariablesConfigurationSource.Build(Microsoft.Extensions.Configuration.IConfigurationBuilder)
&gt;  at Microsoft.Extensions.Configuration.ConfigurationBuilder.Build()   
&gt;  at Microsoft.AspNetCore.Hosting.GenericWebHostBuilder..ctor(Microsoft.Extensions.Hosting.IHostBuilder)
&gt;  at Microsoft.Extensions.Hosting.GenericHostWebHostBuilderExtensions.ConfigureWebHost(Microsoft.Extensions.Hosting.IHostBuilder, System.Action&#39;1&lt;Microsoft.AspNetCore.Hosting.IWebHostBuilder&gt;)   
&gt;  at Microsoft.Extensions.Hosting.GenericHostBuilderExtensions.ConfigureWebHostDefaults(Microsoft.Extensions.Hosting.IHostBuilder, System.Action&#39;1&lt;Microsoft.AspNetCore.Hosting.IWebHostBuilder&gt;)   
&gt;  at WebApplication.Program.CreateHostBuilder(System.String[])   
&gt;  at WebApplication.Program.Main(System.String[])
&gt; ```

I installed the dotnet core sdk using the ubuntu store and after that I also installed Rider IDE.

The weird thing here is that when I run the app using Rider it runs fine, the only issue is using terminal dotnet core commands.

Does anybody know what might be the issue ?

The application is created using Rider. I don&#39;t think that this plays a role but just as a side fact.

I know there are also other ways to install dotnet core in ubuntu but since the sdk is available in the ubuntu story I thought it should work out of the box and of course its an easier choice.

Also tried this [one](https://stackoverflow.com/questions/58132275/ci-cannot-build-net-project-fails-with-couldnt-find-a-valid-icu-package-ins) but does not seem to work for me. Still the same issue happens after running the commands.

||||||||||||||The alternative solution as described in [Microsoft documentation][1] is to set environment variable before running your app 

    export DOTNET_SYSTEM_GLOBALIZATION_INVARIANT=1


  [1]: https://learn.microsoft.com/en-us/dotnet/core/run-time-config/globalization

--------------------------------------------------
Python: Should I save PyPi packages offline as a backup?
**My Python projects heavily depends on PyPi packages**.&lt;br&gt;
I want to make sure that: in any time in the future: the packages required by my apps will always be available online on PyPi.&lt;br&gt;
For example:-&lt;br&gt;
I found a project on Github that requires PyQt4.&lt;br&gt;
when I tried to run it on my Linux machine,&lt;br&gt;
it crashed on startup because it can&#39;t find PyQt4 package on PyPi.&lt;br&gt;
&gt; NB: I know that PyQt4 is deprecated

I searched a lot to find an archive for PyPi that still holds PyQt4 package, but I couldn&#39;t find them anywhere.&lt;br&gt;

so I had to rewrite that app to make it work on PyQt5.&lt;br&gt;
I only changed the code related to the UI (ie: PyQt4).&lt;br&gt;
other functions were still working.&lt;br&gt;

so the only problem with that app was that PyQt4 package was removed from PyPi.&lt;br&gt;
&lt;hr&gt;&lt;br&gt;
so, my question is: should I save a backup of the PyPi packages I use ?
||||||||||||||**TL;DR**

YES if you want availability... The next big question is **how** best to keep a backup version of the dependencies? There are some suggestions at the end of the answer.

**Long Verion:**

Your questions touches on the concept of "Availability" which one of the three pillars of Information Assurance (or Information Security). The other two pillars are Confidentiality and Integrity... The CIA triad.

PyPi packages are maintained by the owners of those packages, a project that depends on a package and list it as a dependency must take into account the possibility that the owner of the package will pull the package or a version of the package out of PyPi at any moment.

Important python packages with many dependencies usually are maintained by foundations or organisations that are more responsible with dealing with downstream dependants packages and projects. However keeping support for old packages is very costly and requires extra effort and usually maintainers sets a date for end of support, or publish a the package lifecycle where they state when a specific version will be removed from the public PyPi server.

Once that happens, the dependant have to update the code (as you did), or provide the original dependency via alternative means.

This topic is very important for procurement in Libraries, Universities, Labs, Companies, and Government Agencies where a software tool might have dependencies on other software packages (or ecosystem), and where "availability" should be addressed adequately. Addressing might mean ensuring high availability, but it could also mean living with the risk of losing availability of the packages... The choices you make for "security" of your project should be informed by a risk analysis.

Now to make sure that dependencies are always available... I quickly compiled the following list. Note that each option has pros and cons. You should evaluate these and other options based on your needs:

 1. Store the virtual environment along with the code. Once you create a virtual environment and install the packages you require for the project in that virtual environment, you can keep the virtual environment for posterity.
 2. Host your own PyPi instance (or mirror) and keep a copy of packages you depend upon hosted on it https://packaging.python.org/en/latest/guides/hosting-your-own-index/ 
 3. Use an "artifact management tool" such as Artifactory from https://jfrog.com/artifact-management/, where you can not only host python packages but also docker images, nmap packages, and other kinds of artifacts.
 4. Get the source code of all dependencies, and always build from source.

Let me know if you think of other ideas so that I add them to the list...

--------------------------------------------------
How to create a windowless application in C#?
I am new to C# and want to make a program that runs without a console/GUI, but can&#39;t figure out how. Is that even possible?

The only options I found were minimizing the window or hiding it AFTER start, but I want it to start without a window/visible in the task bar. I am aware of the fact that I (of course) won&#39;t be able to write to the console...
||||||||||||||What you're looking for is a windows service. You can create one, or at least I see the option to (in VS2022), when creating a new project.

Just create new project and search `Windows Service` and check the one that says `C#`. Pay attention though, one says VB.

Creating a window form and then hiding it.. unless you actually want to bring it up at some point, would be something I would advise against.

    using System;
    using System.Collections.Generic;
    using System.Linq;
    using System.ServiceProcess;
    using System.Text;
    using System.Threading.Tasks;
    
    namespace TestService
    {
        internal static class Program
        {
            /// <summary>
            /// The main entry point for the application.
            /// </summary>
            static void Main()
            {
                ServiceBase[] ServicesToRun;
                ServicesToRun = new ServiceBase[]
                {
                    new Service1()
                };
                ServiceBase.Run(ServicesToRun);
            }
        }
    }



--------------------------------------------------
How to draw tiled image with QT
I&#39;m writing interface with C++/Qt in QtCreator&#39;s designer. What element to chose to make as a rect with some background image?

And the second question: how to draw tiled image? I have and image with size (1&#215;50) and I want to render it for the parent width. Any ideas?

-----------------

    mTopMenuBg = QPixmap(&quot;images/top_menu_bg.png&quot;);
    mTopMenuBrush = QBrush(mTopMenuBg);
    mTopMenuBrush.setStyle(Qt::TexturePattern);
    mTopMenuBrush.setTexture(mTopMenuBg);
    
    ui-&gt;graphicsView-&gt;setBackgroundBrush(mTopMenuBrush);

&gt; QBrush: Incorrect use of
&gt; TexturePattern
||||||||||||||If you just want to show an image you can use [QImage][1].  To make a background with the image tiled construct a [QBrush][2] with the QImage.  Then, if you were using [QGraphicsScene][3] for example, you could set the bursh as the background brush.

Here is an example which fills the entire main window with the tiled image "document.png":

    int main(int argc, char *argv[]) {
    	QApplication app(argc, argv);
    	QMainWindow *mainWindow = new QMainWindow();
    
    	QGraphicsScene *scene = new QGraphicsScene(100, 100, 100, 100);
    	QGraphicsView *view = new QGraphicsView(scene);
    	mainWindow->setCentralWidget(view);
    
    	QImage *image = new QImage("document.png");
    	if(image->isNull()) {
    		std::cout << "Failed to load the image." <<std::endl;
    	} else {
    		QBrush *brush = new QBrush(*image);
    		view->setBackgroundBrush(*brush);
    	}
    
    	mainWindow->show();
    	return app.exec();
    }

The resulting app:  
![screen shot][4]


Alternatively, it seems that you could use [style sheets][5] with any widget and change the [background-image][6] property on the widget.  This has more integration with QtDesigner as you can set the style sheet and image in [QtDesigner][7].


  [1]: https://doc.qt.io/qt-6/qimage.html
  [2]: https://doc.qt.io/qt-6/qbrush.html
  [3]: https://doc.qt.io/qt-6/qgraphicsscene.html
  [4]: http://i.stack.imgur.com/1LK7W.jpg
  [5]: https://doc.qt.io/qt-6/stylesheet-reference.html
  [6]: https://doc.qt.io/qt-6/stylesheet-reference.html#background-image-prop
  [7]: https://doc.qt.io/qt-6/designer-stylesheet.html

--------------------------------------------------
Autofilter and set field using variable
I&#39;m using a serch criteria to get the number/index location of a column to use in the field section of the autofilter. Getting an error &quot;runtime err 1004: Autofilter method of range class failed&quot; not sure if it&#39;s possible. I can see in the degug the variable is holding the correct number

```

Private Sub cmdExtract1_Click()
Dim ws As Worksheet
    Dim lngLastRow As Long
    Dim rngData As Range
    Dim iColNumber As Integer
    
    
     Dim strSearch As String
    Dim aCell As Range
  
    Set ws = Worksheets(&quot;Detail Excel&quot;)
    ws.Activate


    &#39;Identify the last row and use that info to set up the Range
    With ws
    ws.Range(&quot;1:1&quot;).Select
        lngLastRow = ActiveSheet.Cells.Find(&quot;*&quot;, SearchOrder:=xlByRows, SearchDirection:=xlPrevious).Row

strSearch = &quot;Deleted App&quot;

    Set aCell = Sheet1.Rows(1).Find(What:=strSearch, LookIn:=xlValues, _
    LookAt:=xlWhole, SearchOrder:=xlByRows, SearchDirection:=xlNext, _
    MatchCase:=False, SearchFormat:=False)
    
     iColNumber = aCell.Column
         
    End With
    

&#39;Offer Date: include dates, remove blanks
Application.DisplayAlerts = False &#39;switching off the alert button
ws.Range(&quot;A1&quot; &amp; &quot;:y&quot; &amp; lngLastRow).AutoFilter Field:=iColNumber, Criteria1:=&quot;&quot;
ws.Range(&quot;A2&quot; &amp; &quot;:y&quot; &amp; lngLastRow).SpecialCells(xlCellTypeVisible).Delete
Application.DisplayAlerts = True &#39;switching on the alert button

On Error Resume Next
ws.ShowAllData
```




search variable then set the cell column number to a variable
||||||||||||||If you only want to delete rows with blanks you could do that without AutoFilter:

```
Private Sub cmdExtract1_Click()
    Dim ws As Worksheet, strSearch As String, m As Variant
  
    Set ws = Worksheets("Detail Excel")
    strSearch = "Deleted App"
    
    m = Application.match(strSearch, ws.rows(1), 0)
    
    If IsError(m) Then 'no header match?
        MsgBox "Header '" & strSearch & "' not found!", vbExclamation
        Exit Sub
    Else
        With ws.Range(ws.Cells(2, m), ws.Cells(rows.count, m).End(xlUp))
            On Error Resume Next 'ignore error in case no blanks
            .SpecialCells(xlCellTypeBlanks).EntireRow.Delete
            On Error GoTo 0      'stop ignoring errors
        End With
    End If

End Sub
```

EDIT: using autofilter

```
Private Sub cmdExtract1_Click()
    Dim ws As Worksheet, strSearch As String, m As Variant
    Dim lr As Long, lc As Long
  
    Set ws = Worksheets("Detail Excel")
    strSearch = "Deleted App"
    
    m = Application.match(strSearch, ws.rows(1), 0)
    
    If IsError(m) Then 'no header match?
        MsgBox "Header '" & strSearch & "' not found!", vbExclamation
        Exit Sub
    Else
        lr = LastOccupiedRow(ws)
        lc = ws.Cells(1, ws.Columns.count).End(xlToLeft).Column 'last header
        With ws.Range("A1", ws.Cells(lr, lc))
            .AutoFilter Field:=m, Criteria1:=""
            .Offset(1).SpecialCells(xlCellTypeVisible).EntireRow.Delete
        End With
        ws.ShowAllData
    End If
End Sub

Function LastOccupiedRow(ws As Worksheet) As Long
    Dim f As Range
    Set f = ws.Cells.Find("*", SearchOrder:=xlByRows, SearchDirection:=xlPrevious)
    If Not f Is Nothing Then LastOccupiedRow = f.row
End Function
```

--------------------------------------------------
How do I decrypt email from the href value
I am trying to decrypt email from the href value.
I encountered this problem while doing a web scraping task using python.

```html
&lt;a href=&quot;javascript:linkTo_UnCryptMailto(%27ocknvq%2CkphqBngjocpp0ej%27);&quot;&gt;
```

https://i.stack.imgur.com/yB8vo.png

 Whenever I click on *Email*, it directs me to Outlook mail application.From there I can get the email.However, it want the email without actually going to the mail application and in the console.

I have tried various decryption methods discussed on this website but it didn&#39;t work.Can someone give me a hint about what type of method for encryption is used?

||||||||||||||If you are looking to reverse engineer the encryption, you will need to go through the site's source and look for the JS function `linkTo_UnCryptMailto`.

More simply though, if you open dev tools (Ctrl + Shift + I) on chrome and click on the console, you should just be able to type `linkTo_UnCryptMailto(%27ocknvq%2CkphqBngjocpp0ej%27)` and view the result since the function appears to be global.

--------------------------------------------------
Show Bootstrap modal using (#myModal).modal(&#39;show&#39;) from Angular component
I have an Angular 8 project that is using bootstrap. I am trying to show the modal dialog inside my component using the `$(&#39;myModal&#39;).modal(&#39;show&#39;)` inside my component&#39;s *Typescript* file.

Here&#39;s my component&#39;s file:

    import {Component, OnInit} from &#39;@angular/core&#39;;
    import {Router} from &#39;@angular/router&#39;;
    // import * as $ from &#39;jquery&#39;;
    import * as bootstrap from &#39;bootstrap&#39;;
    
    @Component({
      selector: &#39;app-xyz&#39;,
      templateUrl: &#39;./xyz.component.html&#39;,
      styleUrls: [&#39;./xyz.css&#39;]
    })
    export class XyzComponent implements OnInit {
    
      constructor(private router: Router) {
      }
    
      ngOnInit() {
      }
    
      submit() {
        $(&#39;#confirm&#39;).modal(&#39;show&#39;);
      }
    
    }

Upon invoking the submit() funciton on click I get the following error: `ERROR TypeError: $(...).modal is not a function`

I installed bootstrap and jquery using `npm install bootstrap` --save and `npm install jquery --save`.

I even installed *ngx-bootstrap*.

However, when I uncomment the line importing *jQuery* I get a different error: `ERROR TypeError: jquery__WEBPACK_IMPORTED_MODULE_3__(...).modal is not a function`
||||||||||||||Check your angular.json file to make sure that the Jquery js file is included in your scripts array.

`"scripts": [
    "node_modules/jquery/dist/jquery.min.js",
]`

Then in your component TS file declare var $ instead of trying to import it:

`declare var $: any;`

That should allow you to trigger the modal via

`$('#confirm').modal();`

Also make sure that the HTML is correct. Example Modal:


    <div class="modal fade" id="confirm" tabindex="-1" role="dialog" data-backdrop="static" aria-labelledby="noDataModalCenterTitle" aria-hidden="true">
        <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
                <div class="modal-header" style="background-color:lightgrey">
                    <h5 class="modal-title" id="noDataModalLongTitle">No Data</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                    </button>
                </div>
                <div class="modal-body">
                    <i class="fas fa-check fa-4x mb-3 animated rotateIn"></i>
                    No Data Found. Please expand your search criteria and try again.
                </div>
                <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Ok</button>
                </div>
            </div>
        </div>
    </div>

Hope this helps!

--------------------------------------------------
cypher: Count distinct paths between two nodes disregarding link type
I&#39;m use cypher to count the number of paths between two nodes.  I have this query:

    MATCH p=(a)-[*1..2]-(b)
    where ID(a) = &quot;node1&quot;
    and ID(b) = &quot;node2&quot;
    return COUNT(p)

However, many of my nodes have multiple links to the same node with different relationship types.  I&#39;d like to ONLY count the distinct paths regardless of the relationship type.  

For example, the paths returned may be as follows:

    (node1)-[rel_type_a]-(node3)-[rel_type_b]-(node2) 
    (node1)-[rel_type_c]-(node3)-[rel_type_d]-(node2) 
    (node1)-[rel_type_e]-(node3)-[rel_type_f]-(node2) 
The query above counts this as 3 paths, but I only want to count this as a single path since all of the nodes are are the same, I&#39;m not interested in the relationship types.

Thanks in advance!

||||||||||||||I have found the following query that works:

    MATCH p=(a)-[*1..2]-(b)
    where ID(a) = "node1"
    and ID(b) = "node2"
    return count(distinct(nodes(p)))

If there is a better way to do this I'm all ears!

--------------------------------------------------
OffsetDateTime date object not getting stored in db the way date is set in object
I have a model which has startDateTime field which is storing DateTime in OffsetDateTime format.

```
startDateTime: 2023-07-25T04:40:46.143-08:00
```

However when we store the above object in our PostgreSQL db on GCP, it is getting stored in below format:  
`2023-07-25 12:40:46.143+00`

It looks like it is adjusting the above object to UTC time.  
But my requirement is, it sohuld store in the same format which is there in object and **should not adjust** to UTC time.

I explored the methods given [here](https://docs.oracle.com/javase/8/docs/api/java/time/OffsetDateTime.html), but none of the methods is satisfying my requirement.

Can someone please suggest if there is a way to acheive this. Any help would be appreciated.

Code I used and input is ```startDateTime: 2023-07-25T12:40:46.143Z```
and ``` &quot;timeZoneOffset&quot;: &quot;UTC-08:00&quot; ```. I am converting input to expected OffsetDateTime based on timezoneOffset value.
```
 val actualDateTime: OffsetDateTime = OffsetDateTime.parse(startDateTime)
            val zoneOffset: ZoneOffset = ZoneOffset.of(timeZoneOffset.replace(&quot;UTC&quot;, &quot;&quot;))
            val expectedDateTime: OffsetDateTime = actualDateTime.withOffsetSameInstant(zoneOffset)
            return expectedDateTime.toString()
```

DDL:
```
CREATE TABLE IF NOT EXISTS TRANSACTION
(
    id                uuid                     DEFAULT,
    start_date_time   TIMESTAMP WITH TIME ZONE NOT NULL,
    end_date_time     TIMESTAMP WITH TIME ZONE NOT NULL,
    currency          VARCHAR(5)               NOT NULL,
    country           VARCHAR(50)              NOT NULL,
    created_at        TIMESTAMP WITH TIME ZONE NOT NULL,
    created_by        VARCHAR (255)            NOT NULL,
    startDateTime     TIMESTAMP WITH TIME ZONE NOT NULL
)
```
||||||||||||||Your best practice is to think of time zone as a presentation attribute and the timestamp itself as an instant in time.  If your business need requires that you preserve the timezone of the input field, then you must store that in another field outside of the timestamp.

The type "timestamp with timezone" is used by postgresql only to interpret the timezone offset from an input string.  That timezone data is not preserved by the database.  The timestamp is always stored in UTC, and the time zone is immediately lost.

--------------------------------------------------
Chrome Devtools formatter for javascript proxy
I&#39;ve recently started using proxies in one of my projects.  The one downside of this has been that when inspecting the object in a debugger, it&#39;s now wrapped by the proxy [javascript proxy](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy).

[![enter image description here][1]][1]

Intead of seeing `[[Handler]],[[Target]],[[isRevoked]]` I would prefer to just see the object referenced by `[[Target]]`.

It&#39;s a minor inconvenience but I think that it could be solved with a [Chrome Devtools custom formatter](https://www.mattzeunert.com/2016/02/19/custom-chrome-devtools-object-formatters.html).

Seems like this would be fairly common, but I can&#39;t find any existing formatters.  Just wanted to double check that there wasn&#39;t already one out there before I go down the road of writing my own.


  [1]: https://i.stack.imgur.com/alW5J.png
||||||||||||||So it turns out this is quite difficult to achieve.  The first problem is that it's [impossible to identify a Proxy][1] without:

[A:][2] Adding a custom symbol to your proxy implementation (if you control the Proxy init code)

[B:][3] Overriding the `window.Proxy` prototype and using a Weakset to basically track every proxy init 

On top of that, there is no way to access to original `[[Target]]` object.  However, running `JSON.parse(JSON.stringify(obj))` does seems to work well for just `console.log` purposes.

Assuming you don't have control to modify the Proxy handler, this is what your solution would look like:

```
// track all proxies in weakset (allows GC)
const proxy_set = new WeakSet();
window.Proxy = new Proxy(Proxy, {
      construct(target, args) {
        const proxy = new target(args[0], args[1]);
        proxy_set.add(proxy);
        return proxy;
      },
});

window.devtoolsFormatters = [{
  header(obj: any) {
    try {
      if (!proxy_set.has(obj)) {
        return null;
      }
      return ['object', {object: JSON.parse(JSON.stringify(obj))}]; //hack... but seems to work
    } catch (e) {
      return null;
    }
},
  hasBody() {
      return false;
  },
}];
```

  [1]: https://stackoverflow.com/questions/36372611/how-to-test-if-an-object-is-a-proxy
  [2]: https://stackoverflow.com/a/37198132/800619
  [3]: https://stackoverflow.com/a/53463589/800619

--------------------------------------------------
How to select n columns from a matrix minimizing a given function
I must buy **one of each product**, but I can visit **no more then n shops**. Which n shops should I choose to spend the least amount of money? Products are not divisible, every shop have full inventory.

|           | Shop A | Shop B | Shop C |
| --------- | ------ | ------ | ------ |
| Product 1 | $10.00 | $12.00 | $9.99  |
| Product 2 | $8.50  | $9.99  | $7.99  |
| Product 3 | $15.00 | $14.50 | $16.99 |


So I need to  minimize
``df.min(axis=1).sum() ``, where df represents any combination of n columns.

Can I do better than check all the combinations by brute force? Greedy approach, or dynamic programming don&#39;t work here. Sorting columns also doesn&#39;t help, because a shop with half of its products prohibitively expensive and the other half almost free, can have a biggest total sum of its products, but still be the best candidate.
||||||||||||||This code solves the problem. Sadly I am not interested in the output itself, but in complexity. How many steps should I do when trying to simulate underlying algorithm on paper?


    import pulp
    
    # You could formulate this as in integer linear program with binary variables:
    problem = pulp.LpProblem("Product Purchase Problem", pulp.LpMinimize)
    
    # Define the decision variables
    stores = ["Store 1", "Store 2", "Store 3"]
    products = ["Product 1", "Product 2", "Product 3"]
    
    # Xij = 1 if product j is purchased at store i, else = 0;
    X = pulp.LpVariable.dicts("X", [(i, j) for i in stores for j in products], cat="Binary")
    # Si = 1 if store i is available, else = 0.
    S = pulp.LpVariable.dicts("S", stores, cat="Binary")
    
    # Define the objective function
    C = {
        ("Store 1", "Product 1"): 1,
        ("Store 1", "Product 2"): 3,
        ("Store 1", "Product 3"): 4,
        ("Store 2", "Product 1"): 3,
        ("Store 2", "Product 2"): 1,
        ("Store 2", "Product 3"): 3,
        ("Store 3", "Product 1"): 2,
        ("Store 3", "Product 2"): 3,
        ("Store 3", "Product 3"): 1,
    }
    
    # Minimize Σij CijXij, where Cij is the cost of purchasing product j from store i
    problem += pulp.lpSum([C[(i, j)] * X[(i, j)] for i in stores for j in products])
    
    # Subject to constraints
    for j in products:
    # Σi Xij = 1 for all products j (each product is purchased at some store)
        problem += pulp.lpSum([X[(i, j)] for i in stores]) == 1
        for i in stores:
    # Xij <= Si for all i,j (can only purchase at store i if store i is available)
            problem += pulp.lpSum(X[(i, j)]) <= S[i]
    
    # Subject to constraint Σi Si <= n (at most n stores available)
    problem += pulp.lpSum([S[i] for i in stores]) <= 2
    
    # Solve the problem
    problem.solve()
    
    # Print the results
    for v in problem.variables():
        print(v.name, "=", v.varValue)
    print("Total Cost =", pulp.value(problem.objective))

--------------------------------------------------
Django - Search Query Results Loading Incorrectly
On my search results page for some reason when a logged in user makes a search query in my gaming application all the results display instead of the query itself.

What needs to get fixed with what I currently have?

I’m building a gaming application where logged in users have access to play different games based on their rank in our application… so when a user makes a search for lets say the letter ``d`` all the unlocked games and locked games that have the letter ``d`` should display. I&#39;m currently getting that but the search query shows all results that don&#39;t inlcude letter ``d``. 


This is only happening when a user is logged in. When a user is logged out the search works correctly, a search query for ``d`` will show results of every game with the character ``d`` in it. 

Any help is gladly appreciated!

Thanks!

Below is my code. 

**models.py**

    class Game_Info(models.Model):
        id = models.IntegerField(primary_key=True, unique=True, blank=True, editable=False)
        game_title = models.CharField(max_length=100, null=True)
        game_rank = models.IntegerField(default=1)
        game_image = models.ImageField(default=&#39;default.png&#39;, upload_to=&#39;game_covers&#39;, null=True, blank=True)
    
    class User_Info(models.Model):
        id = models.IntegerField(primary_key=True, blank=True)
        image = models.ImageField(default=&#39;/profile_pics/default.png&#39;, upload_to=&#39;profile_pics&#39;, null=True, blank=True)
        user = models.OneToOneField(settings.AUTH_USER_MODEL,blank=True, null=True, on_delete=models.CASCADE)
        rank = models.IntegerField(default=1)    


**views.py**

    def is_valid_queryparam(param):
        return param != &#39;&#39; and param is not None
    
    def search_filter_view(request):
        user_profile_games_filter = Game_Info.objects.all()
        user_profile = User_Info.objects.all()
        title_query = request.POST.get(&#39;q&#39;)  
    
        if is_valid_queryparam(title_query):
            user_profile_games_filter = user_profile_games_filter.filter(game_title__icontains=title_query)
    
        if request.user.is_authenticated:
            user_profile = User_Info.objects.filter(user=request.user)
            user_profile_game_obj = User_Info.objects.get(user=request.user)
            user_profile_rank = int(user_profile_game_obj.rank)
    
    
            user_profile_games_filter = Game_Info.objects.annotate(
                user_unlocked_game=Case(
                    When(game_rank__lte=user_profile_rank, then=Value(True)),
                    default=Value(False),
                    output_field=BooleanField()
                )
            )
    
            context = {
                &#39;user_profile&#39;: user_profile,  
                &#39;user_profile_games_filter&#39;: user_profile_games_filter,
                &#39;title_query&#39; : title_query
            }
    
        else:
            context = {
                &#39;user_profile&#39;: user_profile,
                &#39;user_profile_games_filter&#39;: user_profile_games_filter,
                &#39;title_query&#39; : title_query
           }
    
        return render(request, &quot;search_results.html&quot;, context)


**search.html**

    &lt;h1&gt;Results for &amp;#34;{{ title_query }}&amp;#34;&lt;/h1&gt;
    
    
                {% for content in user_profile_games_filter %}
                    {% if content.user_unlocked_game %}
                            &lt;!-- unlocked games logic --&gt;
                            &lt;a class=&quot;game-tile-container&quot; href=&quot;{% url &#39;detail&#39; content.pk %}&quot;&gt;
                                &lt;li class=&quot;results_info&quot;&gt;
                                    &lt;img src= &quot;{{ content.game_image.url }}&quot;&gt; 
                                    &lt;span class=&quot;results_title&quot;&gt;{{ content.game_title }}&lt;/span&gt;
                                &lt;/li&gt;
                            &lt;/a&gt;
            
                            {% else %}
                            &lt;!-- locked games logic --&gt;
                            &lt;a class=&quot;game-tile-container&quot; href=&quot;{% url &#39;detail&#39; content.pk %}&quot;&gt;
                                &lt;li class=&quot;results_info&quot;&gt;
                                    &lt;div class=&quot;locked_game&quot;&gt;
                                        &lt;img class=&quot;lock-img&quot; src={% static &#39;images/treasure-chest-closed-alt.png&#39; %} /&gt;
                                        &lt;img src= &quot;{{ content.game_image.url }}&quot;&gt; 
                                        &lt;button class=&quot;level-up&quot;&gt;Reach level {{ content.game_rank }} to unlock&lt;/button&gt;
                                    &lt;/div&gt;
                                    &lt;span class=&quot;results_title&quot;&gt;{{ content.game_title }}&lt;/span&gt;
                
                                &lt;/li&gt;
                            &lt;/a&gt;
                    {% endif %}
                {% endfor %}


  

||||||||||||||Issue:

    user_profile_games_filter = Game_Info.objects.annotate(
                user_unlocked_game=Case(
                    When(game_rank__lte=user_profile_rank, then=Value(True)),
                    default=Value(False),
                    output_field=BooleanField()
                )
            )

**SOLUTION** for anyone new; found after a long search and talk in the comments!:

Create a new variable where you store a list of the ID's of the filtered objects, in this case is the variable `user_profile_games_filter`:

    NEW_VARIABLE = user_profile_games_filter.values_list('id', flat=True)

Next, with the filtered objects, assign it to the desired object, making sure that you filter the id using `id__in` and it should be equal to the list of IDs. No loop needed!

    user_profile_games_filter = Game_Info.objects.filter(id__in=NEW_VARIABLE).annotate(...)

--------------------------------------------------
How to extract the Vector from an OpenAI Embeddings Call?
I use nearly the same code as here in this Git Repo to get Embeddings from OpenAI:
https://gist.github.com/limcheekin/997de2ae0757cd46db796f162c3dd58c

    oai = OpenAI(
    # This is the default and can be omitted
    api_key=&quot;sk-.....&quot;,
    )

    def get_embedding(text_to_embed, openai):
       
        response = openai.embeddings.create(
            model= &quot;text-embedding-ada-002&quot;,
            input=[text_to_embed]
        )
        
        return response
    
    embedding_raw = get_embedding(text,oai)

According to the Git Repo the Vector should be in `response[&#39;data&#39;][0][&#39;embedding&#39;]`. But it isn&#39;t in my case.

When I print the response Variable, I got this:

    print(embedding_raw)

Output: 

    CreateEmbeddingResponse(data=[Embedding(embedding=[0.009792150929570198, -0.01779201813042164, 0.011846082285046577, -0.0036859565880149603, -0.0013213189085945487, 0.00037509595858864486,..... -0.0121011883020401, -0.015751168131828308], index=0, object=&#39;embedding&#39;)], model=&#39;text-embedding-ada-002&#39;, object=&#39;list&#39;, usage=Usage(prompt_tokens=360, total_tokens=360))


Sorry, I&#39;m new to python, but how can I access the vector data?
||||||||||||||Simply return just the embedding vector as follows:

    def get_embedding(text_to_embed, openai):
       
        response = openai.embeddings.create(
            model= "text-embedding-ada-002",
            input=[text_to_embed]
        )
        
        return response.data[0].embedding # Change this

    embedding_raw = get_embedding(text,oai)

--------------------------------------------------
React Redux Reducer: &#39;this.props.tasks.map is not a function&#39; error
I am making a React Redux example; however, I ran into an issue and get the error below:

&gt; TypeError: this.props.tasks.map is not a function
[Learn More]

I have tried many things and I cannot seem to understand why this is not working. I believe it is when the allReducers maps the tasks from the Tasks function. I have fixed this error back and forth but then it would complain it was undefined. I would fix that and loop back to this issue. Any help would be appreciated. Im sure I am making a simple mistake. Below are my following files

**App.js**
&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    import React from &#39;react&#39;;
    import TaskBoard from &quot;../containers/task-board&quot;;
    require(&#39;../../scss/style.scss&#39;);

    const App = () =&gt; (
        &lt;div&gt;
            &lt;h2&gt;Task List&lt;/h2&gt;
            &lt;hr /&gt;
            &lt;TaskBoard/&gt;
        &lt;/div&gt;
    );

    export default App;


&lt;!-- end snippet --&gt;

**index.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

        import {combineReducers} from &#39;redux&#39;;
        import {Tasks} from &#39;./reducer-tasks&#39;;
        const allReducers = combineReducers({
            tasks: Tasks
        });

        export default allReducers

&lt;!-- end snippet --&gt;

**task-board.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

        import React, {Component} from &#39;react&#39;;
        import {bindActionCreators} from &#39;redux&#39;;
        import {connect} from &#39;react-redux&#39;;
        import {deleteTaskAction} from &#39;../actions/ActionIndex&#39;;
        import {editTaskAction} from &#39;../actions/ActionIndex&#39;;
        class TaskBoard extends Component {
            renderList() {
                return this.props.tasks.map((task) =&gt; {
                    if(task.status == &quot;pending&quot;){
                        return (&lt;li key={task.id}&gt;
                            {task.id} {task.description}
                            &lt;button type=&quot;button&quot;&gt;Finish&lt;/button&gt;
                            &lt;button type=&quot;button&quot;&gt;Edit&lt;/button&gt;
                            &lt;button onClick={() =&gt; this.props.deleteTask(task)} type=&quot;button&quot;&gt;Delete&lt;/button&gt;
                        &lt;/li&gt;
                    );
                }
            });
        }
        render() {
            if (!this.props.tasks) {
                console.log(this.props.tasks);
                return (&lt;div&gt;You currently have no tasks, please first create one...&lt;/div&gt;);
            }
            return (
                &lt;div&gt;
                    {this.renderList()}
                &lt;/div&gt;
            );
        }
    }
        function mapStateToProps(state) {
            return {
                tasks: state.tasks
            };
        }
        function matchDispatchToProps(dispatch){
            return bindActionCreators(
            {
                deleteTask: deleteTaskAction,
                editTask: editTaskAction
            }, dispatch)
        }
        export default connect(mapStateToProps,matchDispatchToProps)(TaskBoard);

&lt;!-- end snippet --&gt;

**reducer-tasks.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    const initialState = {
    	tasks: [
            {
                id: 1,
                description: &quot;This is a task&quot;,
                status: &quot;pending&quot;
            },
            {
                id: 2,
                description: &quot;This is another task&quot;,
                status: &quot;pending&quot;
            },
            {
                id: 3,
                description: &quot;This is an easy task&quot;,
                status: &quot;pending&quot; 

            }
    	]
    }

    export function Tasks (state = initialState, action) {
        switch (action.type) {
            case &#39;ADD_TASK&#39;:
                return Object.assign({}, state, {
                	tasks: [
                		...state.tasks,
                		{
                			description: action.text,
                			status: action.status
                		}
                	]
                })
                break;

            case &#39;EDIT_TASK&#39;:
                return action.payload;
                break;

            case &#39;DELETE_TASK&#39;:
                return Object.assign({}, state, {
                	status: action.status
                })
                break;
        }

        return state;
    }

&lt;!-- end snippet --&gt;

**actionindex.js**
&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;


        export const addTaskAction = (task) =&gt; {
            return {
                type: &#39;ADD_TASK&#39;,
                text: &quot;Here is a sample description&quot;,
                status: &quot;pending&quot;
            }
        };
        export const deleteTaskAction = (task) =&gt; {
            return {
                type: &#39;DELETE_TASK&#39;,
                status: &quot;deleted&quot;
            }
        };
        export const editTaskAction = (task) =&gt; {
            return {
                type: &#39;EDIT_TASK&#39;,
                payload: task
            }
        };

&lt;!-- end snippet --&gt;


||||||||||||||It's because the function 'map' can only be used for arrays, not for objects.

If you print out this.props.tasks in the render function of task-board.js you'll see that it's an OBJECT which contains the tasks array, not the actual tasks array itself.

So to fix this it's quite easy, instead of:

        return this.props.tasks.map((task) => {

it's 

        return this.props.tasks.tasks.map((task) => {

Then it works

--------------------------------------------------
Google Chrome Extension - waiting until page loads
In my Google Chrome Extension, I have a [Content Script][1] (content.js) and a [Background Page][2] (background.html). I have context.js checking for a keyword that appears on the page. However, I want to wait until the page is fully loaded until I search the page, because the keyword may occur at the bottom of the page. 

See [Page action by content][3] sandwich example ([files][4]), this is basically what I am doing. If you load the extension you&#39;ll see the extension only works when the word &quot;sandwich&quot; appears at the top of the page.


  [1]: http://code.google.com/chrome/extensions/content_scripts.html
  [2]: http://code.google.com/chrome/extensions/background_pages.html
  [3]: http://code.google.com/chrome/extensions/samples.html
  [4]: http://src.chromium.org/viewvc/chrome/trunk/src/chrome/common/extensions/docs/examples/api/pageAction/pageaction_by_content/
||||||||||||||Try to add this to the "content_scripts" part of your manifest.json file.
```json
"run_at": "document_end"
```
https://developer.chrome.com/docs/extensions/mv3/content_scripts/

--------------------------------------------------
No matching constructor for initialization of &#39;v8::ScriptOrigin&#39; || candidate constructor (the implicit move constructor) not viable
I am using invoking a  class inside my project src c++ file as
```
ScriptOrigin script_origin(
        isolate_,
        script_name,
        Integer::New(isolate_, 0),                            // line offset
        Integer::New(isolate_, 0),                            // column offset
        False(isolate_),                                      // isCrossOrigin
        Local&lt;Integer&gt;(),                                     // scriptId
        Local&lt;Value&gt;(),                                       // sourceMapURL
        False(isolate_),                                      // isOpaque
        False(isolate_),                                      // isWASM
        su.IsNoModule() ? False(isolate_) : True(isolate_));
  ```
Tried hardcoding the Local&lt;Integer&gt;() as a random integer value also . Still getting the below error

    error: no matching constructor for initialization of &#39;v8::ScriptOrigin&#39;
          ScriptOrigin script_origin(
    note: candidate constructor (the implicit copy constructor) not viable: requires 1 argument, but 10 were provided 
    note: candidate constructor (the implicit move constructor) not viable: requires 1 argument, but 10 were provided


Am i invoking the class wrongly? i do not understand the mismatch arguments note


This is the source code of the class inside a v8 include file

```
 class V8_EXPORT ScriptOrigin {
 public:
  V8_INLINE ScriptOrigin(Isolate* isolate, Local&lt;Value&gt; resource_name,
                         int resource_line_offset = 0,
                         int resource_column_offset = 0,
                         bool resource_is_shared_cross_origin = false,
                         int script_id = -1,
                         Local&lt;Value&gt; source_map_url = Local&lt;Value&gt;(),
                         bool resource_is_opaque = false, bool is_wasm = false,
                         bool is_module = false,
                         Local&lt;Data&gt; host_defined_options = Local&lt;Data&gt;())
      : v8_isolate_(isolate),
        resource_name_(resource_name),
        resource_line_offset_(resource_line_offset),
        resource_column_offset_(resource_column_offset),
        options_(resource_is_shared_cross_origin, resource_is_opaque, is_wasm,
                 is_module),
        script_id_(script_id),
        source_map_url_(source_map_url),
        host_defined_options_(host_defined_options) {
    VerifyHostDefinedOptions();
  }
};
```


||||||||||||||Many of the parameters you're passing have the wrong type. For example, `Integer::New(isolate_, 0)` produces a `v8::Local<v8::Integer>`, not a C++ `int`. Similarly, `False(isolate_)` produces a `v8::Local<v8::Boolean>`, not a C++ `bool`.

This isn't really related to V8: whenever working with C++, you need to care about the types of your values.

The part where the compiler says ...
```none
note: candidate constructor (the implicit copy constructor) not 
viable: requires 1 argument, but 10 were provided 

note: candidate constructor (the implicit move constructor) not 
viable: requires 1 argument, but 10 were provided
```
... is just about that it _tried_ to match the 10 arguments you supplied with the copy constructor and move constructor (both of which expect 1 argument only), but that failed.

--------------------------------------------------
SSIS Excel Destination Editor closes unexpectedly
I&#39;m fairly new to SSIS, but what I&#39;m trying to do should be simple:

I have a Data Flow task that has an OLE DB Source feeding into an Excel Destination. The issue though, is I can&#39;t configure the Excel Destination correctly. I&#39;m able to connect my Excel connection manager, but when I hit the &quot;New...&quot; button next to the &quot;Name of the Excel sheet&quot; dropdown, the Excel Destination Editor window just closes instead of opening a different dialog. In the image below, I highlighted the button that&#39;s closing the window.

![SSIS Button](https://i.stack.imgur.com/BR2TV.png)

In general, I&#39;m following this guide [How to use SSIS to Export to Excel][1] (current step is just above the second to last image in the article).


  [1]: http://knowlton-group.com/using-ssis-to-export-data-to-excel/
||||||||||||||I think the issue lies with the Excel connection instead. Once I changed the output file to .xls instead of .xlsx and changed the connection to Excel 97-2003, I was able to create a new Excel Sheet for the file.

--------------------------------------------------
Is it possible to display toolbar options below textarea in Quilljs editor?
How to display toolbar below `textarea`.

My code: 
    

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    var quill = new Quill(&#39;#txtMessage&#39;, {
      theme: &#39;snow&#39;,
      modules: {
        toolbar: {
          container: [
            [&#39;bold&#39;, &#39;italic&#39;, &#39;underline&#39;],
            [{
              &#39;list&#39;: &#39;ordered&#39;
            }, {
              &#39;list&#39;: &#39;bullet&#39;
            }],
            [&#39;clean&#39;],
            [&#39;code-block&#39;],
            [{
              &#39;variables&#39;: [&#39;{Name}&#39;, &#39;{Email}&#39;]
            }],
          ],
          handlers: {
            &quot;variables&quot;: function(value) {
              if (value) {
                const cursorPosition = this.quill.getSelection().index;
                this.quill.insertText(cursorPosition, value);
                this.quill.setSelection(cursorPosition + value.length);
              }
            }
          }
        }
      }
    });

    // Variables
    const placeholderPickerItems = Array.prototype.slice.call(document.querySelectorAll(&#39;.ql-variables .ql-picker-item&#39;));
    placeholderPickerItems.forEach(item =&gt; item.textContent = item.dataset.value);
    document.querySelector(&#39;.ql-variables .ql-picker-label&#39;).innerHTML = &#39;Variables&#39; + document.querySelector(&#39;.ql-variables .ql-picker-label&#39;).innerHTML;

&lt;!-- language: lang-html --&gt;

    &lt;script src=&quot;//cdn.quilljs.com/1.3.6/quill.js&quot;&gt;&lt;/script&gt;
    &lt;link href=&quot;//cdn.quilljs.com/1.3.6/quill.snow.css&quot; rel=&quot;stylesheet&quot;/&gt;
    &lt;link href=&quot;//cdn.quilljs.com/1.3.6/quill.bubble.css&quot; rel=&quot;stylesheet&quot;/&gt;

    &lt;div id=&quot;txtMessage&quot;&gt;&lt;/div&gt;

&lt;!-- end snippet --&gt;

Output for the above code:
[![enter image description here][1]][1]

I want output as follows:
[![enter image description here][2]][2]
  [1]: https://i.stack.imgur.com/PQtJv.png
  [2]: https://i.stack.imgur.com/bQlmC.png

How to accomplish above result.
||||||||||||||I can't see why not use only css.

Something like this:

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    var quill = new Quill('#editor-container', {
      modules: {
        toolbar: [
          [{
            header: [1, 2, false]
          }],
          ['bold', 'italic', 'underline'],
          ['image', 'code-block']
        ]
      },
      placeholder: 'Compose an epic...',
      theme: 'snow' // or 'bubble'
    });

<!-- language: lang-css -->

    #editor-container {
      height: 375px;
    }

    .editor-wrapper {
      position: relative;
    }

    .ql-toolbar {
      position: absolute;
      bottom: 0;
      width: 100%;
      transform: translateY(100%);
    }

<!-- language: lang-html -->

    <script src="//cdn.quilljs.com/1.3.6/quill.js"></script>
    <link href="//cdn.quilljs.com/1.3.6/quill.snow.css" rel="stylesheet"/>
    <link href="//cdn.quilljs.com/1.3.6/quill.bubble.css" rel="stylesheet"/>
    <div class="editor-wrapper">
      <div id="editor-container">
      </div>
    </div>

<!-- end snippet -->

https://codepen.io/moshfeu/pen/wXwqmg

--------------------------------------------------
Can I install Visual Studio without Admin rights?
I use a machine where I don&#39;t have administrator rights. I&#39;ve been able to run programs without admin rights by extracting the program&#39;s .zip file to a directory I have created on my desktop. However, I can&#39;t find such a .zip file for Visual Studio.

Is there a way to install Visual Studio Community Edition without administrator rights?


||||||||||||||Practically no. Visual Studio (Express and above, excluding VS Code) consists of multiple components that must be installed as admin, and will be required for the app you're debugging to be available as system-wide component. It *might* be possible to use [ThinApp](https://www.vmware.com/products/thinapp.html) or its equivalent, but ThinApp can't even work with [VS 2010](https://www.vmware.com/support/thinapp4/doc/releasenotes_thinapp52.html) and it was by far the best of its class.

A (resource intensive) alternative to get VS on any PC will be packaging a VM with VS installed, either creating one yourself or get a [ready-made](https://developer.microsoft.com/en-us/windows/downloads/virtual-machines) ones. VirtuaBox is available as [portable fork](http://www.vbox.me/) if you can't even get Hyper-V tools installed. But this still require kernel drivers installation, which means at least one-time admin access. Depending on your internet connection & budget, it might be more practical to setup a VPS with VS installed, then remote there.

--------------------------------------------------
ggplot2: how to produce smaller points
I have a large dataset that I am plotting using a scatter plot. These points have a unique combination of x,y and therefore they don&#39;t overlap, but some of them are very close to each other therefore I&#39;m plotitng them with small size.  

1- How to produce smaller point symbols (smaller `size`) so that the areas are proportional. In this example, the last point does not have an area proportional to the `size`. I was expecting it 10 smaller than the middle one e.g.:

    df &lt;- data.frame(c1 = 1:3, c2 = c(1,1,1))
    ggplot(df) + geom_point(aes(x= c1, y = c2), size = c(1, 0.1, 0.01)) 

2- How does the `size` in ggplot2 matches the R graphics `cex` argument e.g.:  `plot(df$c2 ~ df$c1, cex = c(1, 0.1, 0.01))`. 
Thanks
||||||||||||||There is a `size = ` argument to `geom_point`, but you either specify a size for all points:

    + geom_point(size = 0.5)

Or you map the size to one of the columns in your data using `aes`:

    + geom_point(aes(size = c2))

In the latter case, you can control the range of sizes using `scale_size_continuous`. The default is min = 1, max = 6. To get _e.g._ min = 2, max = 8:

    + geom_point(aes(size = c2)) + scale_size_continuous(range = c(2, 8))

- Note that the "ggplot2 way" is to map data to geoms, not to assign values to each observation
- and no, size here is different to `cex`

--------------------------------------------------
Can&#39;t close Excel completely using win32com on Python
This is my code, and I found many answers for [VBA][1], .NET framework and is pretty strange. When I execute this, Excel closes.

    from win32com.client import DispatchEx
    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wbs.Close()
    excel.Quit()
    wbs = None
    excel = None # &lt;-- Excel Closes here

But when I do the following, it does not close.

    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wb = wbs.Open(&#39;D:\\Xaguar\\A1.xlsm&#39;)
    wb.Close(False)
    wbs.Close()
    excel.Quit()
    wb = None
    wbs = None
    excel = None  # &lt;-- NOT Closing !!!

I found some possible answer in Stack Overflow question *[Excel process remains open after interop; traditional method not working][2]*. The problem is that is not Python, and I don&#39;t find `Marshal.ReleaseComObject` and `GC`. I looked over all the demos on `...site-packages/win32com` and others.

Even it does not bother me if I can just get the PID and kill it.

I found a workaround in *[Kill process based on window name (win32)][3]*.

May be not the proper way, but a workround is:

    def close_excel_by_force(excel):
        import win32process
        import win32gui
        import win32api
        import win32con

        # Get the window&#39;s process id&#39;s
        hwnd = excel.Hwnd
        t, p = win32process.GetWindowThreadProcessId(hwnd)
        # Ask window nicely to close
        win32gui.PostMessage(hwnd, win32con.WM_CLOSE, 0, 0)
        # Allow some time for app to close
        time.sleep(10)
        # If the application didn&#39;t close, force close
        try:
            handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, p)
            if handle:
                win32api.TerminateProcess(handle, 0)
                win32api.CloseHandle(handle)
        except:
            pass

    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wb = wbs.Open(&#39;D:\\Xaguar\\A1.xlsm&#39;)
    wb.Close(False)
    wbs.Close()
    excel.Quit()
    wb = None
    wbs = None
    close_excel_by_force(excel) # &lt;--- YOU #@#$# DIEEEEE!! DIEEEE!!!

  [1]: http://en.wikipedia.org/wiki/Visual_Basic_for_Applications
  [2]: https://stackoverflow.com/questions/8977571/excel-process-remains-open-after-interop-traditional-method-not-working
  [3]: http://python.6.n6.nabble.com/Kill-process-based-on-window-name-win32-td1042063.html

||||||||||||||Try this:

    wbs.Close()
    excel.Quit()
    del excel # this line removed it from task manager in my case


--------------------------------------------------
Log4j2 createOnDemand=&quot;true&quot; does not allow creation of new file on a daily basis
`Log4j2 createOnDemand=&quot;true&quot;` does not allow creation of new file on a daily basis in-spite of using `RollingFile Appenders` with `TimeBasedTriggeringPolicy`.

Below is my `log4j2.xml` file.

I have two `appenders`, one is for all logs, another is for a custom purpose, which needs to be generated only on demand, but the `createOnDemand` is overriding the Rolling nature of the log and it is not allowing to create new log file for the custom log.

    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;Configuration status=&quot;WARN&quot;&gt;
    	&lt;Appenders&gt;
    		&lt;RollingFile name=&quot;App&quot; 
    				fileName=&quot;app.log&quot; 
    				filePattern=&quot;app.%d{yyyy-MM-dd}.log&quot;&gt;
    			&lt;PatternLayout pattern=&quot;[%t] %d{yyyy-MM-dd HH:mm:ss,SSS zzz} %-5p %l - %m%n&quot; /&gt;
    			&lt;Policies&gt;
    				&lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;
    			&lt;/Policies&gt;
    		&lt;/RollingFile&gt;
    		&lt;RollingFile name=&quot;custom&quot;
    				 fileName=&quot;appCustom.log&quot;
    				 filePattern=&quot;appCustom.%d{yyyy-MM-dd-HH-mm}.log&quot;
    				 createOnDemand=&quot;true&quot;&gt;
    		&lt;PatternLayout pattern=&quot;[%t] %d{yyyy-MM-dd HH:mm:ss,SSS zzz} %-5p %l - %m%n&quot; /&gt;
    		&lt;Policies&gt;
    			&lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;
    		&lt;/Policies&gt;
    		&lt;/RollingFile&gt;
    	&lt;/Appenders&gt;
    	&lt;Loggers&gt;
    		&lt;Logger name=&quot;AppLogger&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
    				&lt;AppenderRef ref=&quot;App&quot;/&gt;
    			&lt;/Logger&gt;
    		&lt;Logger name=&quot;customLogger&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
    			&lt;AppenderRef ref=&quot;custom&quot;/&gt;
    		&lt;/Logger&gt;
    		&lt;Root level=&quot;info&quot;&gt;
    				&lt;AppenderRef ref=&quot;file&quot; /&gt;
    		&lt;/Root&gt;
    	&lt;/Loggers&gt;
    &lt;/Configuration&gt;
||||||||||||||I have found the fix for the above issue.
This was an existing bug in lo4j2 which is fixed in the version - [2.13.1][1]

Below are the links :

https://issues.apache.org/jira/browse/LOG4J2-2759

https://logging.apache.org/log4j/2.x/changes-report.html#a2.13.3

I was using 2.11.0

Upgrading resolved my issue.


  [1]: https://blogs.apache.org/logging/entry/log4j-2-13-1-released

--------------------------------------------------
How to get the Slack DM channel ID of the Slack App
I have created a simple Slack App app where the only purpose is to send a message to a channel. I understand that there is the `conversations.list` API to list all public channels to get the correct ID. 

However, as a first step, I just want to send the message to the app channel itself. If I use the D... ID it works as expected. No invite by the channel is needed. But how do I get this ID? `conversations.list` only returns publich channels, but no the app channel itself.
||||||||||||||In Slack, there is no such thing as an app's channel. There is a DM channel between every user and your app/bot. In these terms, to send a DM message from your app/bot to the user, you need to know `ID` of this user and specify it as a `channel` argument of the `postMessage` API request.

--------------------------------------------------
How do I make a custom class that&#39;s serializable with dataclasses.asdict()?
I&#39;m trying to use a dataclass as a (more strongly typed) dictionary in my application, and found this strange behavior when using a custom type subclassing `list` within the dataclass. I&#39;m using Python 3.11.3 on Windows.

```python
from dataclasses import dataclass, asdict

class CustomFloatList(list):
    def __init__(self, args):
        for i, arg in enumerate(args):
            assert isinstance(arg, float), f&quot;Expected index {i} to be a float, but it&#39;s a {type(arg).__name__}&quot;

        super().__init__(args)

    @classmethod
    def from_list(cls, l: list[float]):
        return cls(l)

@dataclass
class Poc:
    x: CustomFloatList

p = Poc(x=CustomFloatList.from_list([3.0]))
print(p)  # Prints Poc(x=[3.0])
print(p.x)  # Prints [3.0]
print(asdict(p))  # Prints {&#39;x&#39;: []}
```

This does not occur if I use a regular list[float], but I&#39;m using a custom class here to enforce some runtime constraints.

How do I do this correctly?

I&#39;m open to just using `.__dict__` directly, but I thought `asdict()` was the more &quot;official&quot; way to handle this

A simple modification makes the code behave as expected, but is slightly less efficient:

```python
from dataclasses import dataclass, asdict

class CustomFloatList(list):
    def __init__(self, args):
        dup_args = list(args)
        for i, arg in enumerate(dup_args):
            assert isinstance(arg, float), f&quot;Expected index {i} to be a float, but it&#39;s a {type(arg).__name__}&quot;

        super().__init__(dup_args)

    @classmethod
    def from_list(cls, l: list[float]):
        return cls(l)

@dataclass
class Poc:
    x: CustomFloatList

p = Poc(x=CustomFloatList.from_list([3.0]))
print(p)
print(p.x)
print(asdict(p))
```
||||||||||||||If you look at the [source code of `asdict`](https://github.com/python/cpython/blob/d334122d2295a4863384676a3ce313a831b12335/Lib/dataclasses.py#L1364), you'll see that passes a generator expression that recursively calls itself on the elements of a list when it encounters a list:
```
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)
```



 But *your implementation depletes any iterator it gets in `__init__` before the `super` call*. 

Don't do that. You'll have to "cache" the values if you want to use the superclass constructor. Something like:

```
class CustomFloatList(list):
    def __init__(self, args):
        args = list(args)
        for i, arg in enumerate(args):
            assert isinstance(arg, float), f"Expected index {i} to be a float, but it's a {type(arg).__name__}"

        super().__init__(args)
```

Or perhaps:

```
class CustomFloatList(list):
    def __init__(self, args):
        super().__init__(args)
        for i, arg in enumerate(self):
            if not isinstance(arg, float):
                raise TypeError(f"Expected index {i} to be a float, but it's a {type(arg).__name__}")
```

--------------------------------------------------
How would I run PHP code when input box changes?
I&#39;m trying to do something with PHP when a text box changes. I can do this with JavaScript with onchange but that doesn&#39;t work with PHP.


I already have a PHP function in my already existing PHP code. When a text box value changes, I want it to run the function.

||||||||||||||The reason you can do it in JavaScript is because you're in the same scope...the client side. PHP is a server-side language, so it has no notion of what is happening on the client-side, unless you explicitly tell it.

To tell PHP to evaluate, and possible return the response of a function call, you have to pass it the value of the input using a network call, such as a fetch or ajax request.

Your question shows that you really don't understand PHP, and you need to learn the fundamentals of it. PHP does not run client-side, and JavaScript does not run server-side (again, unless you're using Node, in which case you wouldn't be using PHP).

--------------------------------------------------
Listener method using Spring and ActiveMQ throws &quot;Property name cannot be null&quot; warnings repeatedly
I&#39;ve attempted to implement ActiveMQ using Spring in two places. Both implementations have had this issue. Sending either an HTTP Request using postman or directly entering a message in the ActiveMQ console causes the following Error to be repeated infinitely:

```
2024-02-02T17:08:56.317-06:00 ERROR 2264 --- [ntContainer#0-1] c.j.a.config.JmsConfig$JMSErrorHandler   : Error in listener
```

```
org.springframework.jms.listener.adapter.ListenerExecutionFailedException: Listener method &#39;public void com.jackhodge.activemqlearning.consumer.component.MessageConsumer.messageListener(com.jackhodge.activemqlearning.model.SystemMessage)&#39; threw exception
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:118) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.onMessage(MessagingMessageListenerAdapter.java:84) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:783) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:741) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:719) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:333) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:270) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1258) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1248) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:1141) ~[spring-jms-6.1.3.jar:6.1.3]
  at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]
Caused by: java.lang.NullPointerException: Property name cannot be null
  at org.apache.activemq.command.ActiveMQMessage.getObjectProperty(ActiveMQMessage.java:575) ~[activemq-client-jakarta-5.18.3.jar:5.18.3]
  at org.apache.activemq.command.ActiveMQMessage.getStringProperty(ActiveMQMessage.java:683) ~[activemq-client-jakarta-5.18.3.jar:5.18.3]
  at org.springframework.jms.support.converter.MappingJackson2MessageConverter.getJavaTypeForMessage(MappingJackson2MessageConverter.java:456) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.support.converter.MappingJackson2MessageConverter.fromMessage(MappingJackson2MessageConverter.java:241) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener.extractMessage(AbstractAdaptableMessageListener.java:250) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter.extractPayload(AbstractAdaptableMessageListener.java:472) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter$LazyResolutionMessage.unwrapPayload(AbstractAdaptableMessageListener.java:539) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter$LazyResolutionMessage.getPayload(AbstractAdaptableMessageListener.java:521) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.annotation.support.PayloadMethodArgumentResolver.resolveArgument(PayloadMethodArgumentResolver.java:122) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:118) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:147) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:115) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:110) ~[spring-jms-6.1.3.jar:6.1.3]
  ... 10 common frames omitted
```

What stands out is `Error in listener [...] Property name cannot be null`.

The error still occurs when I don&#39;t do anything in my listener, and Logs/Breakpoints in the `messageListener` aren&#39;t sent nor activated.

Here&#39;s the simple app in which the error is occurring:

**JmsConfig**

```java
@Configuration
@EnableJms
public class JmsConfig {

    Logger logger = LoggerFactory.getLogger(JMSErrorHandler.class);
    
    @Bean
    public DefaultJmsListenerContainerFactory jmsListenerContainerFactory(
            ConnectionFactory connectionFactory,
            DefaultJmsListenerContainerFactoryConfigurer configurer,
            JMSErrorHandler defaultErrorHandler){
        DefaultJmsListenerContainerFactory jmsListenerContainerFactory = new DefaultJmsListenerContainerFactory();
    
        jmsListenerContainerFactory.setConnectionFactory(connectionFactory);
        jmsListenerContainerFactory.setConcurrency(&quot;1&quot;); // start w/ 5 consumers; auto-scale to 10 consumers as necessary
    
        jmsListenerContainerFactory.setErrorHandler(defaultErrorHandler);
        jmsListenerContainerFactory.setMessageConverter(this.jacksonJmsMessageConverter());
    
        configurer.configure(jmsListenerContainerFactory, connectionFactory);
        return jmsListenerContainerFactory;
    
    }
    
    
    @Service
    public class JMSErrorHandler implements ErrorHandler {
        @Override
        public void handleError(Throwable t) {
            logger.error(&quot;Error in listener &quot;, t);
        }
    }
    
    @Bean
    public MessageConverter jacksonJmsMessageConverter() {
        MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();
        converter.setTargetType(MessageType.TEXT);
        converter.setObjectMapper(new ObjectMapper());
        return converter;
    }

**PublishController**

```java
package com.jackhodge.activemqlearning.controller;

import com.jackhodge.activemqlearning.model.SystemMessage;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.jms.core.JmsTemplate;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RestController;

@RestControllerpublic class PublishController {// helperclass for sending/receiving messages
    // Spring JMS abstraction API: Distills and simplifies process; abstracts away boilerplate code
    private JmsTemplate jmsTemplate;

    @Autowired
    public PublishController(JmsTemplate jmsTemplate) {
        this.jmsTemplate = jmsTemplate;
    }

    // post method to trigger publishing of messages
    // Requests to here at sent to the Messaging Broker
    @PostMapping(&quot;/publishMessage&quot;)
    public ResponseEntity&lt;String&gt; publishMessage(@RequestBody SystemMessage systemMessage){
        try{
            jmsTemplate.convertAndSend(&quot;jackhodge-queue&quot;, systemMessage.toString());
            return new ResponseEntity&lt;&gt;(&quot;I, Jack Hodge, sent your message.&quot;, HttpStatus.OK);
        } catch (Exception e){
            return new ResponseEntity&lt;&gt;(e.getMessage(), HttpStatus.INTERNAL_SERVER_ERROR);
        }

    }
}
```

**SystemMessage**

```java
package com.jackhodge.activemqlearning.model;

import lombok.Setter;
import org.springframework.beans.factory.annotation.Autowired;
//import java.io.Serializable;

@Setter
public class SystemMessage {
    private String source;
    private String message;
    
    public SystemMessage(String source, String message) {
    
        this.source = source;
        this.message = message;
    }
    
    
    @Override
    public String toString() {
        return &quot;SystemMessage{&quot; +
                &quot;source=&#39;&quot; + source + &#39;\&#39;&#39; +
                &quot;, message=&#39;&quot; + message + &#39;\&#39;&#39; +
                &#39;}&#39;;
    }

}
```

**MessageConsumer**

```java
package com.jackhodge.activemqlearning.consumer.component;

import com.jackhodge.activemqlearning.model.SystemMessage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.jms.annotation.JmsListener;
import org.springframework.stereotype.Component;

@Componentpublic class MessageConsumer {

    public static final Logger LOGGER = LoggerFactory.getLogger(MessageConsumer.class);
    
    // Consumes from the Messaging broker
    @JmsListener(destination = &quot;jackhodge-queue&quot;)
    public void messageListener(SystemMessage systemMessage){
        LOGGER.info(&quot;Message Received {}&quot;, systemMessage);
    }

}
```

I&#39;m somewhat new to Swing and ActiveMQ and this problem has stumped me -- every avenue of breakpoints/logging/message sources as far as I&#39;m capable of has been tried. Thank you!

Sending this request via both Postman and the ActiveMQ console both resulted in the same Error regardless:

```
{     
    &quot;source&quot;:&quot;jeff bezo&quot;,     
    &quot;message&quot;:&quot;hello&quot; 
}
```
||||||||||||||I fixed this by setting setTypeIdPropertyName in my MessageConverter Bean:


    @Bean
    public MessageConverter jacksonJmsMessageConverter(){
            MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();
            converter.setTargetType(MessageType.TEXT);
            converter.setTypeIdPropertyName("_type");
            converter.setObjectMapper(new ObjectMapper());
            return converter;
        }




--------------------------------------------------
Start and kill background process within one Makefile recipe
Within one `make` recipe, I am trying to:
1. Run a server process in the background
2. Run a command that uses the server, in the foreground
3. Kill the background server process after foreground task completes

The below is where I am at (inspired by https://stackoverflow.com/a/30171236), but it doesn&#39;t quite work because `SERVER_PID` is currently empty (possibly related to this empty PID: https://stackoverflow.com/q/5768034).

```make
run-server:	## Run the server in the foreground.
	run server

run-kill-server:	## Run the server while using it.
	@$(MAKE) run-server&amp;
	export SERVER_PID=$! &amp;&amp; cmd-that-uses-server &amp;&amp; kill $(SERVER_PID)
```

https://stackoverflow.com/questions/7668311/makefile-run-processes-in-background is also related, except I believe it &quot;leaks&quot; the background processes, the recipes there don&#39;t try to `kill` spawned background processes
||||||||||||||> Within one make recipe, I am trying to:
> 
>  1.  Run a server process in the background
>  2.  Run a command that uses the server, in the foreground
>  3.  Kill the background server process after foreground task completes

But you're *not* doing all that in one recipe.  You're using two.

Bringing it all into one recipe would be a step in the right direction, though It will not in itself provide a complete solution.

> `SERVER_PID` is currently empty

Yes, because your recipe is setting *shell* variable `SERVER_PID`, but trying to reference a *`make`* variable of that name.  And also trying to reference a `make` variable named `!`, where you appear to want the shell variable of that name.  You need to escape your `$` by doubling it when you want to pass it through to the shell.

Additionally, each logical line of your recipe runs in a separate shell.  In the one where you define and later user `SERVER_PID`, no background process is ever run.

You probably want something more like this:
```
run-job:
        run server & export SERVER_PID=$$!; cmd-that-uses-server; kill $${SERVER_PID}
```

--------------------------------------------------
python while loop if all conditions are equal then do another random choice from list
This is my python code:
```python
import secrets
from time import sleep

ids = [{&#39;id&#39;: number} for number in range(1, 5+1)]

rand1 = secrets.choice(ids)
rand2 = secrets.choice(ids)
rand3 = secrets.choice(ids)

n = 0
while rand1[&#39;id&#39;] == rand2[&#39;id&#39;] == rand3[&#39;id&#39;]:
        n += 1
        print(&#39;Before&#39;)
        print(rand1[&#39;id&#39;], rand2[&#39;id&#39;], rand3[&#39;id&#39;])
        sleep(1)
        rand1 = secrets.choice(ids)
        rand2 = secrets.choice(ids)
        rand3 = secrets.choice(ids)
        print(&#39;After&#39;)
        print(rand1[&#39;id&#39;], rand2[&#39;id&#39;], rand3[&#39;id&#39;])
```
I&#39;m going to reach this:

&gt; do the while loop and choose a random id until none of the
&gt; rand1[&#39;id&#39;], rand2[&#39;id&#39;] and rand3[&#39;id&#39;] are equal.
&gt; 
&gt; Even two of them are equal, then do another for loop.
||||||||||||||Looping is not the right way to do this.  Just shuffle and deal:
```
import random

nums = list(range(1,5+1))
random.shuffle(nums)
ids = [{'id': n} for n in nums[:3]]
```

--------------------------------------------------
Avoiding duplicate tasks in celery broker
I want to create the following flow using celery configuration\api: 

 - Send TaskA(argB) Only if celery queue has no TaskA(argB) already pending

Is it possible? how?
||||||||||||||I cannot think of a way but to 

 1. Retrieve all executing and scheduled tasks via [`celery inspect`](http://docs.celeryproject.org/en/latest/userguide/workers.html#inspecting-workers)
    
 2. Iterate through them to see if your task is there.

check [this](https://stackoverflow.com/questions/5544629/retrieve-list-of-tasks-in-a-queue-in-celery) SO question to see how the first point is done.

good luck

--------------------------------------------------
How do I late-resolve * in .csproj files?
I have a `.csproj` file that looks like this:

````
&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;net8.0&lt;/TargetFramework&gt;
    &lt;LangVersion&gt;12.0&lt;/LangVersion&gt;
    &lt;DebugType&gt;portable&lt;/DebugType&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
	&lt;None Remove=&quot;*.dat&quot; /&gt;
    &lt;None Include=&quot;*.dat&quot; CopyToOutputDirectory=&quot;Always&quot; /&gt;
  &lt;/ItemGroup&gt;

  &lt;Target Name=&quot;PrecompileScript&quot; BeforeTargets=&quot;BeforeCompile&quot;&gt;
    &lt;Exec Command=&quot;dotnet run -c $(Configuration) --no-build --project ../DatGenerator/DatGenerator.csproj -directory $(ProjectDir)&quot; /&gt;
  &lt;/Target&gt;
&lt;/Project&gt;
````

The problem is `*.dat` is expanded too soon and doesn&#39;t actually pick up any files. How do I expand `*.dat` after the `&lt;Exec` directive that emits the `*.dat` runs? No, `-directory $(ProjectDir)/bin/$(Configuration)/$(TargetFramework)` isn&#39;t right. That doesn&#39;t work at all; see how there isn&#39;t a `&lt;OutputType&gt;exe&lt;/OutputType&gt;`. The copy build outputs built-in functionality needs to work.

Listing each `.dat` file manually is pretty bad. I need to pick up changes automatically here.
||||||||||||||To late-resolve a wildcard, move the item `Include` inside a target.

e.g. Change your code to

```C#
<Project Sdk="Microsoft.NET.Sdk">
  <PropertyGroup>
    <TargetFramework>net8.0</TargetFramework>
    <LangVersion>12.0</LangVersion>
    <DebugType>portable</DebugType>
  </PropertyGroup>

  <Target Name="PrecompileScript" BeforeTargets="BeforeBuild">
    <Exec Command="dotnet run -c $(Configuration) --no-build --project ../DatGenerator/DatGenerator.csproj -directory $(ProjectDir)" />
    <ItemGroup>
      <None Remove="*.dat" />
      <None Include="*.dat" CopyToOutputDirectory="PreserveNewest" />
    </ItemGroup>
  </Target>
</Project>
```

"[How MSBuild builds projects][1]" explains the evaluation and execution phases but, briefly:

 - When building a project, MSBuild has an evaluation phase followed by an executuion phase.
 - 'Top level' `PropertyGroup` and `ItemGroup` elements are evaluated in the evalution phase.
 - Target order is determined in the evaluation phase.
 - Targets are run in the execution phase.
 - `PropertyGroup` and `ItemGroup` elements within a target are evaluated when the target is run.

**Note** I updated the answer to change from using `BeforeCompile` to using `BeforeBuild`.

  [1]: https://learn.microsoft.com/en-us/visualstudio/msbuild/build-process-overview?view=vs-2022

--------------------------------------------------
react-google-maps/api DirectionsService keeps rerendering itself
I have written this code in react JS to using &quot;react-google-maps/api&quot; to calculate route between two points. Now my google map keeps rerendering itself until it gives &quot;DIRECTIONS_ROUTE: OVER_QUERY_LIMIT&quot; error. I don&#39;t know what&#39;s the issue. Help would be appreciated because I am a beginner in react and google-API and also I haven&#39;t found a lot of guides of google API in react.

Here is my code:

    import React from &quot;react&quot;;
    import {
      GoogleMap,
      useLoadScript,
      DirectionsService,
      DirectionsRenderer,
    } from &quot;@react-google-maps/api&quot;;
    
    const libraries = [&quot;places&quot;, &quot;directions&quot;];
    const mapContainerStyle = {
      width: &quot;100%&quot;,
      height: &quot;50vh&quot;,
    };
    const center = {
      lat: 31.582045,
      lng: 74.329376,
    };
    const options = {};
    
    const MainMaps = () =&gt; {
      const { isLoaded, loadError } = useLoadScript({
        googleMapsApiKey: &quot;********&quot;,
        libraries,
      });
    
      const [origin2, setOrigin2] = React.useState(&quot;lahore&quot;);
      const [destination2, setDestination2] = React.useState(&quot;gujranwala&quot;);
      const [response, setResponse] = React.useState(null);
    
      const directionsCallback = (response) =&gt; {
        console.log(response);
    
        if (response !== null) {
          if (response.status === &quot;OK&quot;) {
            setResponse(response);
          } else {
            console.log(&quot;response: &quot;, response);
          }
        }
      };
    
      const mapRef = React.useRef();
      const onMapLoad = React.useCallback((map) =&gt; {
        mapRef.current = map;
      }, []);
      if (loadError) return &quot;Error loading maps&quot;;
      if (!isLoaded) return &quot;loading maps&quot;;
    
      const DirectionsServiceOption = {
        destination: destination2,
        origin: origin2,
        travelMode: &quot;DRIVING&quot;,
      };
    
      return (
        &lt;div&gt;
          &lt;GoogleMap
            mapContainerStyle={mapContainerStyle}
            zoom={8}
            center={center}
            onLoad={onMapLoad}
          &gt;
            {response !== null &amp;&amp; (
              &lt;DirectionsRenderer
                options={{
                  directions: response,
                }}
              /&gt;
            )}
    
            &lt;DirectionsService
              options={DirectionsServiceOption}
              callback={directionsCallback}
            /&gt;
          &lt;/GoogleMap&gt;
        &lt;/div&gt;
      );
    };
    
    export default MainMaps;




||||||||||||||The rendering issue appears to be with the library itself. One alternative I can suggest is to instead use/load Google Maps API script instead of relying on 3rd party libraries. This way, you can just follow the [official documentation](https://developers.google.com/maps/documentation) provided by Google.

By loading the script, we can now follow their [Directions API documentation](https://developers.google.com/maps/documentation/javascript/directions):

Here is a sample app for your reference: https://stackblitz.com/edit/react-directions-64165413

`App.js`
```

    import React, { Component } from 'react';
    import { render } from 'react-dom';
    import Map from './components/map';
    import "./style.css";
    
    class App extends Component {
     
      render() {
        return (
           <Map 
            id="myMap"
            options={{
              center: { lat: 31.582045, lng: 74.329376 },
              zoom: 8
            }}
          />
        );
      }
    }
    
    export default App;

```

`map.js`
```

    import React, { Component } from "react";
    import { render } from "react-dom";
    
    class Map extends Component {
      constructor(props) {
        super(props);
        this.state = {
          map: "",
          origin: "",
          destination: ""
        };
        this.handleInputChange = this.handleInputChange.bind(this); 
        this.onSubmit = this.onSubmit.bind(this);
      }
    
      onScriptLoad() {
        this.state.map = new window.google.maps.Map(
          document.getElementById(this.props.id),
          this.props.options
        );
      }
    
      componentDidMount() {
        if (!window.google) {
          var s = document.createElement("script");
          s.type = "text/javascript";
          s.src = `https://maps.google.com/maps/api/js?key=YOUR_API_KEY`;
          var x = document.getElementsByTagName("script")[0];
          x.parentNode.insertBefore(s, x);
          // Below is important.
          //We cannot access google.maps until it's finished loading
          s.addEventListener("load", e => {
            this.onScriptLoad();
          });
        } else {
          this.onScriptLoad();
        }
      }
    
      onSubmit(event) {    
        this.calculateAndDisplayRoute();
        event.preventDefault();
      }
    
      calculateAndDisplayRoute() {
        var directionsService = new google.maps.DirectionsService();
        var directionsRenderer = new google.maps.DirectionsRenderer();
        directionsRenderer.setMap(this.state.map);
        directionsService.route(
          {
            origin: { query: this.state.origin },
            destination: { query: this.state.destination },
            travelMode: "DRIVING"
          },
          function(response, status) {
            if (status === "OK") {
              directionsRenderer.setDirections(response);
            } else {
              window.alert("Directions request failed due to " + status);
            }
          }
        );
        
      }
    
      handleInputChange(event) {
        const target = event.target;
        const value = target.type === "checkbox" ? target.checked : target.value;
        const name = target.name;
    
        this.setState({
          [name]: value
        });
      }
      addMarker(latLng) {
        var marker = new window.google.maps.Marker({
          position: { lat: -33.8569, lng: 151.2152 },
          map: this.state.map,
          title: "Hello Sydney!"
        });
        var marker = new google.maps.Marker({
          position: latLng,
          map: this.state.map
        });
      }
    
      render() {
        return (
          <div>
            <input
              id="origin"
              name="origin"
              value={this.state.origin}
              placeholder="Origin"
              onChange={this.handleInputChange}
            />
            <input
              id="destination"
              name="destination"
              value={this.state.destination}
              placeholder="Destination"
              onChange={this.handleInputChange}
            />
            <button id="submit" onClick={this.onSubmit}>
              Search
            </button>
            <div className="map" id={this.props.id} />
          </div>
        );
      }
    }
    
    export default Map;

```

--------------------------------------------------
At what point does binary search become more efficient than sequential search?
I&#39;ve been learning a lot about algorithms lately, and the binary searched is hailed for its efficiency in finding an item in large amounts of **sorted** data. But what if the data is not sorted to begin with? at what points does a binary search provide an efficiency boost against sequential search, with binary search having to sort the given array first off THEN search. I&#39;m interested in seeing at what points binary search passes over sequential search, if anyone has tested this before i would love to see some results.

Given an array foo[BUFF] with 14 elements

    1 3 6 3 1 87 56 -2 4 61 4 9 81 7

I would assume a sequential sort would be more efficient to find a given number, let&#39;s say... 3, because binary search would need to first sort the array **THEN** search for the number 3. BUT:

Given an array bar[BUFF] with one thousand elements held

    1 2 4 9 -2 3 8 9 4 12 4 56 //continued

A call to sort then binary search should in theory be more efficiently if i am not mistaken.


||||||||||||||In an unsorted array where no information is known, you are going to have to do linear time search.

Linear time search checks each element once, so it's complexity is `O(n)`. Comparing that to sorting. Sorting algorithms which must check each element more than once and have a complexity of `O(n * log n)`. So to even get it sorted is slower than a sequential search. Even though binary search is `O(log n)` it's pretty useless when you just have arbitrarily ordered data.

If your going to search for stuff multiple times though, consider sorting first as it'll increase your efficiency in the long run.

--------------------------------------------------
GPG error: http://archive.debian.org lenny/updates Release: The following signatures were invalid: KEYEXPIRED 1356982504
I am getting following error while update my source lists

    $ sudo apt-get update
    
    Reading package lists... Done
    
    W: GPG error: http://archive.debian.org lenny/updates Release: The following signatures were invalid: KEYEXPIRED 1356982504
    
    W: You may want to run apt-get update to correct these problems



**How to resolve this issue?**

||||||||||||||To find any expired repository keys and their IDs, use apt-key as follows:

      apt-key list | grep expired

You will get a result similar to the following:

      pub   4096R/BE1DB1F1 2011-03-29 [expired: 2014-03-28]

The key ID is the bit after the / i.e. BE1DB1F1 in this case.

To update the key, run

      sudo apt-key adv --recv-keys --keyserver keys.gnupg.net BE1DB1F1



--------------------------------------------------
Select a Dictionary&lt;T1, T2&gt; with LINQ
I have used the &quot;select&quot; keyword and extension method to return an `IEnumerable&lt;T&gt;` with LINQ, but I have a need to return a generic `Dictionary&lt;T1, T2&gt;` and can&#39;t figure it out.  The example I learned this from used something in a form similar to the following: 

    IEnumerable&lt;T&gt; coll = from x in y 
        select new SomeClass{ prop1 = value1, prop2 = value2 };

I&#39;ve also done the same thing with extension methods.  I assumed that since the items in a  `Dictionary&lt;T1, T2&gt;` can be iterated as `KeyValuePair&lt;T1, T2&gt;` that I could just replace &quot;SomeClass&quot; in the above example with &quot;`new KeyValuePair&lt;T1, T2&gt; { ...`&quot;, but that didn&#39;t work (Key and Value were marked as readonly, so I could not compile this code).

Is this possible, or do I need to do this in multiple steps?

Thanks.


||||||||||||||The extensions methods also provide a [ToDictionary][1] extension.  It is fairly simple to use, the general usage is passing a lambda selector for the key and getting the object as the value, but you can pass a lambda selector for both key and value.

    class SomeObject
    {
        public int ID { get; set; }
        public string Name { get; set; }
    }

    SomeObject[] objects = new SomeObject[]
    {
        new SomeObject { ID = 1, Name = "Hello" },
        new SomeObject { ID = 2, Name = "World" }
    };

    Dictionary<int, string> objectDictionary = 
        objects.ToDictionary(
            o => o.ID, 
            o => o.Name);

Then `objectDictionary[1]` Would contain the value "Hello"

  [1]: http://msdn.microsoft.com/en-us/library/system.linq.enumerable.todictionary.aspx

--------------------------------------------------
Heikin Ashi candle code in pine script V5
In pinescript version 4, the Heikin Ashi candle open is calculated as:

```
ha_close = (open + high + low + close)/4
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```
However, it shows compalilation error in pinescript version 5:
```
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```

The compilation error &quot;Undeclared identifier &#39;ha_open&#39;&quot;.

I have no idea what to do to solve this. 
||||||||||||||In pinescript, you must declare variables before using them.  
You should use (for version 5) :

    var float ha_open = na
    ha_close = (open + high + low + close)/4
    ha_open := na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2

`var float ha_open = na` declare the variable as a float and initialize it to `na`

--------------------------------------------------
fatal error: opencv2/opencv_modules.hpp: No such file or directory #include &quot;opencv2/opencv_modules.hpp&quot;
Hello all I am trying to use opencv-c++ API (version 4.4.0) which I have built from source. It is installed in /usr/local/ and I was simply trying to load and display an image using the following code - 
```
#include &lt;iostream&gt;
#include &lt;opencv4/opencv2/opencv.hpp&gt;
#include &lt;opencv4/opencv2/core.hpp&gt;
#include &lt;opencv4/opencv2/imgcodecs.hpp&gt;
#include &lt;opencv4/opencv2/highgui.hpp&gt;
#include &lt;opencv4/opencv2/core/cuda.hpp&gt;

using namespace cv;

int main()
{
    std::string image_path = &quot;13.jpg&quot;;
    cv::Mat img = cv::imreadmulti(image_path, IMREAD_COLOR);
    if(img.empty())
    {
        std::cout&lt;&lt;&quot;COULD NOT READ IMAGE&quot;&lt;&lt;std::endl;
        return 1;
    }
    imshow(&quot;Display Window&quot;, img);
    return 0;
}
```
And when I compile it throws the following error during compilation - 
```
In file included from /CLionProjects/opencvTest/main.cpp:2:
/usr/local/include/opencv4/opencv2/opencv.hpp:48:10: fatal error: opencv2/opencv_modules.hpp: No such file or directory
 #include &quot;opencv2/opencv_modules.hpp&quot;
```
My Cmake is as follows - 

```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
include_directories(&quot;/usr/local/include/opencv4/opencv2/&quot;)
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest PUBLIC &quot;/usr/local/lib/&quot;)
```
I do not know what am I doing wrong here.. This might be a noob question, But I ahev just started using opencv in C++
||||||||||||||The solution is to just include_directories path till `/usr/local/opencv4` and it works perfectly.

However, the best way I believe is to use the `find_package` function. I updated my Cmake to the following and it takes care of linking during build. 
```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
find_package(OpenCV REQUIRED)
include_directories(${OpenCV_INCLUDE_DIRS})
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest ${OpenCV_LIBS})
``` 


--------------------------------------------------
how to get the url parameters from http
I am working in a very rudimentary &quot;routing&quot; system for small CMS in nodejs without express or any framework. My aim is to have very few dependencies. 
For templating I found jrender that works fine in the sample route &quot;hey&quot; below: 

    var http = require(&#39;http&#39;)
    var jsrender = require (&#39;jsrender&#39;);    
    
    var html = jsrender.renderFile(&#39;./templates/hey.html&#39;, {name: &quot;Jim&quot;, age: &quot;22&quot;});
        
    
    http.createServer(function (req, res) {
    	res.writeHead(200, {&#39;Content-Type&#39;: &#39;text/html&#39;}); // http header
    
    	var url = req.url;
    	if(url ===&#39;/about&#39;){
            console.log (req.url)
      		res.write(&quot;hey&quot;); //write a response
      		res.end(); //end the response
            
    	}else if(url ===&#39;/contact&#39;){
      		res.write(&#39;&lt;h1&gt;contact us page&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
            
        }else if(url ===&#39;/hey&#39;){
      		res.write(html); //write a response
      		res.end(); //end the response    
            
    	}else{
      		res.write(&#39;&lt;h1&gt;Hello World!&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
    	}
    
    }).listen(3000, function(){
    	console.log(&quot;Judge Dress live on port 3000&quot;); //the server object listens on port 3000
    }); 


My problem is to get a parameter for a page e.g. /?pages=pagename to have dynamic routes. Is there any way to extact this parameter from req.url ? 

||||||||||||||You can use the node.js built-in 'querystring' module. To get "me" from "http://localhost:3000/about/?pages=me"

    const querystring = require('querystring');     
    console.log(querystring.parse(req.url)["/about/?pages"])

--------------------------------------------------
Regex to match all strings of given format with given exceptions
I&#39;m really struggling with this one. I tried to search from left to right, but still can&#39;t figure this out.

I have a list of strings with random amount of tags, each placed in brackets, randomly positioned within each string. Few examples may look as follows.


```
[tag1][tag4] Desired string - with optional dash [tag10]
[tag1][tag2][tag3] Desired string [tag10]
[tag3][tag1][tag2][tag5] Desired - string (with suffix)
[tag2][tag5][tag4] [Animation] Target string [tag10]
[tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
```

What I&#39;m trying to achieve is to extract from each string the content without tags, which are enclosed in brackets. The only exception is tag **[Animation]** or **[Animations]**. In case, one of these tags appear, I want to extract them as well together with the desired string.

So in case of list above, the desired output would be following. (I don&#39;t care about the whitespace around extracted strings, it will be trimmed afterwards.)

```
Desired string - with optional dash
Desired string
Desired - string (with suffix)
[Animation] Target string
[Animations](prefix)Desired - string (and suffix)
```


Originally, I was using as simple regex as `\[.*?\]`. Which matched all tags in brackets, and I simply replaced everything with empty string.

```python
re_pattern = r&quot;\[.*?\]&quot;
re.sub(re_pattern, &#39;&#39;, dirty_string).strip()
```

However, now I found a need to have an exception for tags **[Animation]** and **[Animations]**, and really can&#39;t figure it out. Your help would be much appreciated.
Thanks.
||||||||||||||You could use the better `regex` module with the following expression:

    \[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*

In `Python`, this could be

    import regex as re
    
    data = """
    [tag1][tag4] Desired string - with optional dash [tag10]
    [tag1][tag2][tag3] Desired string [tag10]
    [tag3][tag1][tag2][tag5] Desired - string (with suffix)
    [tag2][tag5][tag4] [Animation] Target string [tag10]
    [tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
    """
    
    pattern = re.compile(r'\[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*')
    
    print(pattern.sub("", data))

And would yield

    Desired string - with optional dash 
    Desired string 
    Desired - string (with suffix)
    [Animation] Target string 
    [Animations](prefix)Desired - string (and suffix)



--------------------------------------------------
Node.js server that accepts POST requests
I&#39;m trying to allow javascript to communicate with a Node.js server. 

**POST request (web browser)**

    var	http = new XMLHttpRequest();
    var params = &quot;text=stuff&quot;;
    http.open(&quot;POST&quot;, &quot;http://someurl.net:8080&quot;, true);
    
    http.setRequestHeader(&quot;Content-type&quot;, &quot;application/x-www-form-urlencoded&quot;);
    http.setRequestHeader(&quot;Content-length&quot;, params.length);
    http.setRequestHeader(&quot;Connection&quot;, &quot;close&quot;);
    
    alert(http.onreadystatechange);
    http.onreadystatechange = function() {
      if (http.readyState == 4 &amp;&amp; http.status == 200) {
        alert(http.responseText);
      }
    }
    
    http.send(params);

Right now the Node.js server code looks like this. Before it was used for GET requests. I&#39;m not sure how to make it work with POST requests.

**Server (Node.js)**

    var server = http.createServer(function (request, response) {
      var queryData = url.parse(request.url, true).query;
    
      if (queryData.text) {
        convert(&#39;engfemale1&#39;, queryData.text, response);
    	response.writeHead(200, {
    	  &#39;Content-Type&#39;: &#39;audio/mp3&#39;, 
    	  &#39;Content-Disposition&#39;: &#39;attachment; filename=&quot;tts.mp3&quot;&#39;
    	});
      } 
      else {
        response.end(&#39;No text to convert.&#39;);
      }
    }).listen(8080);
||||||||||||||The following code shows how to read values from an HTML form. As @pimvdb said you need to use the request.on('data'...) to capture the contents of the body.
```
const http = require('http')

const server = http.createServer(function(request, response) {
  console.dir(request.param)

  if (request.method == 'POST') {
    console.log('POST')
    var body = ''
    request.on('data', function(data) {
      body += data
      console.log('Partial body: ' + body)
    })
    request.on('end', function() {
      console.log('Body: ' + body)
      response.writeHead(200, {'Content-Type': 'text/html'})
      response.end('post received')
    })
  } else {
    console.log('GET')
    var html = `
			<html>
				<body>
					<form method="post" action="http://localhost:3000">Name: 
						<input type="text" name="name" />
						<input type="submit" value="Submit" />
					</form>
				</body>
			</html>`
    response.writeHead(200, {'Content-Type': 'text/html'})
    response.end(html)
  }
})

const port = 3000
const host = '127.0.0.1'
server.listen(port, host)
console.log(`Listening at http://${host}:${port}`)


```

If you use something like [Express.js][1] and [Bodyparser](https://www.npmjs.com/package/body-parser) then it would look like this since Express will handle the request.body concatenation


```
var express = require('express')
var fs = require('fs')
var app = express()

app.use(express.bodyParser())

app.get('/', function(request, response) {
  console.log('GET /')
  var html = `
    <html>
        <body>
            <form method="post" action="http://localhost:3000">Name: 
                <input type="text" name="name" />
                <input type="submit" value="Submit" />
            </form>
        </body>
    </html>`
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end(html)
})

app.post('/', function(request, response) {
  console.log('POST /')
  console.dir(request.body)
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end('thanks')
})

const port = 3000
app.listen(port)
console.log(`Listening at http://localhost:${port}`)

```

  [1]: http://expressjs.com/


--------------------------------------------------
How can I validate an email address using a regular expression?
Over the years I have slowly developed a [regular expression][1] that validates *most* email addresses correctly, assuming they don&#39;t use an IP address as the server part.

I use it in several PHP programs, and it works most of the time.  However, from time to time I get contacted by someone that is having trouble with a site that uses it, and I end up having to make some adjustment (most recently I realized that I wasn&#39;t allowing four-character [TLDs][2]).

*What is the best regular expression you have or have seen for validating emails?*

I&#39;ve seen several solutions that use functions that use several shorter expressions, but I&#39;d rather have one long complex expression in a simple function instead of several short expression in a more complex function.

  [1]: http://en.wikipedia.org/wiki/Regular_expression
  [2]: https://en.wikipedia.org/wiki/Top-level_domain



||||||||||||||The [fully RFC 822 compliant regex][1] is inefficient and obscure because of its length.  Fortunately, RFC 822 was superseded twice and the current specification for email addresses is [RFC 5322][2].  RFC 5322 leads to a regex that can be understood if studied for a few minutes and is efficient enough for actual use.

One RFC 5322 compliant regex can be found at the top of the page at http://emailregex.com/ but uses the IP address pattern that is floating around the internet with a bug that allows `00` for any of the unsigned byte decimal values in a dot-delimited address, which is illegal.  The rest of it appears to be consistent with the RFC 5322 grammar and passes several tests using `grep -Po`, including cases domain names, IP addresses, bad ones, and account names with and without quotes.

Correcting the `00` bug in the IP pattern, we obtain a working and fairly fast regex.  (Scrape the rendered version, not the markdown, for actual code.)

 > (?:[a-z0-9!#$%&'\*+/=?^\_\`{|}~-]+(?:\\.[a-z0-9!#$%&'\*+/=?^_\`{|}~-]+)\*|"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])\*")@(?:(?:\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?\\.)+\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])

or:

```
(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])
```

Here is [diagram][3] of [finite state machine][4] for above regexp which is more clear than regexp itself
[![enter image description here][5]][5]


The more sophisticated patterns in Perl and PCRE (regex library used e.g. in PHP) can [correctly parse RFC 5322 without a hitch][6]. Python and C# can do that too, but they use a different syntax from those first two. However, if you are forced to use one of the many less powerful pattern-matching languages, then it’s best to use a real parser.

It's also important to understand that validating it per the RFC tells you absolutely nothing about whether that address actually exists at the supplied domain, or whether the person entering the address is its true owner. People sign others up to mailing lists this way all the time. Fixing that requires a fancier kind of validation that involves sending that address a message that includes a confirmation token meant to be entered on the same web page as was the address. 

Confirmation tokens are the only way to know you got the address of the person entering it. This is why most mailing lists now use that mechanism to confirm sign-ups. After all, anybody can put down `president@whitehouse.gov`, and that will even parse as legal, but it isn't likely to be the person at the other end.

For PHP, you should *not* use the pattern given in [Validate an E-Mail Address with PHP, the Right Way][7] from which I quote:

> There is some danger that common usage and widespread sloppy coding will establish a de facto standard for e-mail addresses that is more restrictive than the recorded formal standard.

That is no better than all the other non-RFC patterns. It isn’t even smart enough to handle even [RFC 822][8], let alone RFC 5322. [This one][6], however, is.

If you want to get fancy and pedantic, [implement a complete state engine][9]. A regular expression can only act as a rudimentary filter. The problem with regular expressions is that telling someone that their perfectly valid e-mail address is invalid (a false positive) because your regular expression can't handle it is just rude and impolite from the user's perspective. A state engine for the purpose can both validate and even correct e-mail addresses that would otherwise be considered invalid as it disassembles the e-mail address according to each RFC. This allows for a potentially more pleasing experience, like

>The specified e-mail address 'myemail@address,com' is invalid. Did you mean 'myemail@address.com'?

See also [Validating Email Addresses][10], including the comments. Or [Comparing E-mail Address Validating Regular Expressions][11].

[![Regular expression visualization](https://i.stack.imgur.com/SrUwP.png)](https://i.stack.imgur.com/SrUwP.png)

[Debuggex Demo][12]


  [1]: http://ex-parrot.com/~pdw/Mail-RFC822-Address.html
  [2]: https://datatracker.ietf.org/doc/html/rfc5322
  [3]: https://regexper.com/#(%3F%3A%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B(%3F%3A%5C.%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B)*%7C%22(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21%5Cx23-%5Cx5b%5Cx5d-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)*%22)%40(%3F%3A(%3F%3A%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%5C.)%2B%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%7C%5C%5B(%3F%3A(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D))%5C.)%7B3%7D(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D)%7C%5Ba-z0-9-%5D*%5Ba-z0-9%5D%3A(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21-%5Cx5a%5Cx53-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)%2B)%5C%5D)
  [4]: https://en.wikipedia.org/wiki/Finite-state_machine
  [5]: https://i.stack.imgur.com/YI6KR.png
  [6]: https://stackoverflow.com/questions/201323/what-is-the-best-regular-expression-for-validating-email-addresses/1917982#1917982
  [7]: http://www.linuxjournal.com/article/9585
  [8]: https://datatracker.ietf.org/doc/html/rfc822
  [9]: http://cubicspot.blogspot.com/2012/06/correct-way-to-validate-e-mail-address.html
  [10]: http://worsethanfailure.com/Articles/Validating_Email_Addresses.aspx
  [11]: http://fightingforalostcause.net/misc/2006/compare-email-regex.php
  [12]: https://www.debuggex.com/r/aH_x42NflV8G-GS7

--------------------------------------------------
JPA generating broken SQL when using native query and pageable
Using Spring-Boot 2.7.7, when I attempt to create a native PostgreSQL query that receives a Pageable and outputs a Page, it seems to generate a broken SQL for one of the page attributes

This is the Query I used for the function:

```
    @Query(value =&quot;SELECT * &quot; +
            &quot;FROM propriedade &quot; +
            &quot;INNER JOIN proprietario &quot; +
            &quot;  ON proprietario.id = propriedade.proprietario_id &quot; +
            &quot;WHERE proprietario.nome ILIKE %:proprietario% &quot;,
            nativeQuery = true
    )
    Page&lt;Propriedade&gt; findByProprietarioLikePage(Pageable pageable, @Param(&quot;proprietario&quot;) String proprietario);
```
When I try to call the function, it generates a few SQL commands, but breaks in this one:
```
Hibernate: select count(INNER) FROM propriedade INNER JOIN proprietario   ON proprietario.id = propriedade.proprietario_id WHERE proprietario.nome ILIKE ?   AND condominio_id = ? 
2023-08-07 10:34:57.361  WARN 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42601
2023-08-07 10:34:57.361 ERROR 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: syntax error at or near &quot;)&quot;

```
If I try to run this count on PSQL, I get the same error:
```
ERROR:  syntax error at or near &quot;)&quot;
LINE 1: select count(INNER) FROM propriedade INNER JOIN proprietario...
                          ^
```
The problem seems to be with the generated query to count the entries in the table
||||||||||||||Please, try to add an alias, like "p", after "propriedade". It'll be like this: 

    @Query(value ="SELECT * " +
            "FROM propriedade p " +
            "INNER JOIN proprietario " +
            "  ON proprietario.id = p.proprietario_id " +
            "WHERE proprietario.nome ILIKE %:proprietario% ",
            nativeQuery = true
    )

--------------------------------------------------
Get child node index
In straight up javascript (i.e., no extensions such as jQuery, etc.), is there a way to determine a child node&#39;s index inside of its parent node without iterating over and comparing all children nodes?

E.g.,

    var child = document.getElementById(&#39;my_element&#39;);
    var parent = child.parentNode;
    var childNodes = parent.childNodes;
    var count = childNodes.length;
    var child_index;
    for (var i = 0; i &lt; count; ++i) {
      if (child === childNodes[i]) {
        child_index = i;
        break;
      }
    }

Is there a better way to determine the child&#39;s index?
||||||||||||||you can use the `previousSibling` property to iterate back through the siblings until you get back `null` and count how many siblings you've encountered:

    var i = 0;
    while( (child = child.previousSibling) != null ) 
      i++;
    //at the end i will contain the index.

Please note that in languages like Java, there is a `getPreviousSibling()` function, however in JS this has become a property -- `previousSibling`.

Use [previousElementSibling][2] or [nextElementSibling][1] to ignore text and comment nodes.


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element/nextElementSibling
  [2]: https://developer.mozilla.org/en-US/docs/Web/API/Element/previousElementSibling

--------------------------------------------------
Is there a way to inherit the parent __init__ arguments?
Suppose I have a basic class inheritance:

```
class A:
    def __init__(self, filepath: str, debug=False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, **kwargs):
        super(B, self).__init__(**kwargs)
        self.portnumber = portnumber
```

For typing and completion purposes, I would like to somehow &quot;forward&quot; the list of arguments from `A.__init__()` to `B.__init__()`.


Is there a way to do this? To have a type checker correctly infer the signature for `B.__init__(...)` and have an IDE be able to provide meaningful completions or checks?

---

[edit] after searching a little bit more, here is something that is perhaps closer to what I look:

if I declared `A` and `B` as _dataclasses_ :

```
from dataclasses import dataclass

@dataclass
class A:
    filepath: str
    debug: bool = False

@dataclass
class B(A):
    portnumber: int = 42
```

I can get the following hints in vscode with the standard pylance extension:
[![screen capture of vscode autocompletion][1]][1]

Could there be something similar to target just the `__init__()` method?  
perhaps by explicitly naming the base method that gets &quot;extended&quot; (e.g: a special `@extends(A.__init__)` decorator)?

  [1]: https://i.stack.imgur.com/Xv9L0m.png
||||||||||||||Yes this is possible, but personally I wouldn't recommend it - see below:

```py

from typing import TypedDict, Unpack

class AInterface(TypedDict):
    filepath: str
    debug: bool

class A:
    def __init__(self, **kwargs: Unpack[AInterface]):
        self.filepath = kwargs["filepath"]
        self.debug = kwargs["debug"]

class B(A):
    def __init__(self, portnumber: int, **kwargs: Unpack[AInterface]):
        super().__init__(**kwargs)
        self.portnumber = portnumber
```

By using the `TypedDict` we can structure the kwargs argument giving it a type, and allowing us to pass it through. If you have multiple inheritance you could even combine the interfaces together to produce the current kwargs type. When you use the `__init__` for A and B you still get warned if you miss parts of the `TypedDict`.

I would instead just pass the arguments down to the next layer manually:

```py
class A:
    def __init__(self, filepath: str, debug: bool =False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, filepath: str, debug: bool =False):
        super().__init__(filepath=filepath, debug = debug)
        self.portnumber = portnumber
```


--------------------------------------------------
MySQL / Laravel structure: monolith tables, or thousands of small tables?
**Short Version:**

Our webapp works with sets of scoped `project` data - that is, model relationships that will always relate to models in the same `project`, and strictly never cross over into other `project`s. Security / opacity between projects is a key requirement. The whole scoped relational ecosystem spans ~20 database tables so far.

We are currently managing this ecosystem with ~20 monolith tables, and enforcing opacity / security through code - but we&#39;re losing our grasp on it. We&#39;re considering adopting a structure where each `project` deploys its own clone of these ~20 tables into the same database. Are there any known fundamental drawbacks to having thousands of tables in one database, like increased storage size, slower performance, higher indexing overhead? Our team just doesn&#39;t have the database expertise to speak to flaws that might be introduced by this ourselves. 

If it makes any difference, we&#39;re using Laravel 10 - all models and relationships take advantage of Laravel / Eloquent structures. We&#39;re anticipating at least 200 projects active at a time, with a few being added or nuked each month.

-----

**Long Version:** We have an internal-use project org / management webapp, with some complicated requirements.

In broad strokes, there are `projects`, and each `project` has several related entities - `permissions`, `reports`, `tickets`, `labels`, `ticket_label`, `media`, `notifications`, many more. All related entities are scoped to their project - reports, labels, tickets, etc created inside a project by definition will never be transitioned to another, and no entities are generalized to multiple projects. Our project shareholders are adamant that a project&#39;s information is totally secure and opaque from other projects - no project should know any other project exists, and when a project is deleted there shouldn&#39;t be a trace of it left.

It&#39;s a little messy, because all these entities are binned into single tables with each other, regardless of project. There&#39;s extra work and middleware going into, for example, making sure some bad actor can&#39;t reassign a ticket ID in from an external project to gain access to its data. It&#39;s also complicated deleting a project and ensuring all nested / related data has been wiped - we get pretty far with appropriate foreign_keys and `-&gt;onDelete( &#39;cascade&#39; )`, but as the app gets more developed it&#39;s more and more difficult to **guarantee** that all data has been scrubbed when a project is deleted. There have been a few incidents where orphans containing sensitive information have been discovered. We&#39;ve been improving our tests and code when each is discovered, but it&#39;s becoming clear that we can&#39;t guarantee fallthroughs won&#39;t happen again - shareholders are expressing doubts.

Someone brought up that we can reduce a lot of complexity if we&#39;re able to generate a group of tables each time a new project is created. So, when project `0f9ebA2` is created, it creates tables `0f9ebA2_reports`, `0f9ebA2_tickets`, and so on as well. These tables will be identical structurally, so can all be created from the same migrations. In terms of convenience and cleanliness, the advantages are clear - foreign keys will be pointed to tables with the same prefix, and guarantee IDs outside of the project can&#39;t be assigned. It&#39;s also trivial to ensure all data has been scrubbed - just delete the tables. Many of the cross-pollination protections become obsolete, reduced to just access permissions.

The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach - and it doesn&#39;t seem like a common practice in MySQL, so there aren&#39;t a lot of articles or forums on the subject. We&#39;d like to get another perspective on this, and see if we&#39;re overlooking any fundamental flaws before we pull the trigger - like increased storage size, slower performance, higher indexing overhead. Matters of MySQL architecture and performance, over best practice and opinion.
||||||||||||||A few years ago I managed the databases at a company that had about 10,000 schemas, each schema had the same set of ~120 tables. They did this for similar reasons that you have, to make sure data for different clients is kept separate, for privacy and security reasons.

This was on MySQL 5.1 at the time. We found that after a few tens of thousands of tables on a given server, performance became a problem. It turns out that MySQL has internal data structures corresponding to each table, and they didn't architect this to handle so many tables. So eventually scanning lists of open tables becomes a bottleneck.

We split the schemas over seven servers, so each server didn't have so many tables. About 160,000 tables per server was our maximum.

I gave feedback to the MySQL product manager about this bottleneck, as they were developing a revamped implementation of the data dictionary for MySQL 8.0. He passed this along to the engineers, and they made sure to test scalability up to 1 million tables per server.

So definitely make sure that you use MySQL 8.0 or later to get this improvement.

But even with MySQL 8.0, this doesn't give you unlimited scalability. Eventually if you intend to keep growing, you must develop the capability to store data on more than one database server, and your applications need to have code to switch between database servers.

For example, in our case, one of the db servers had a simple table that stored a list of all the clients and which db server their data was stored on. This list was read at the startup of the app, and held in cache. It was a simple mapping list. Then on any request, the app could quickly tell which of seven db servers it should send the query. All the functions to run queries had an argument which was the db connection to use.

So in a way, "are there any performance limitations" is the wrong question. The assumption should be that there _are_ performance limitations, it's just a matter of how large can you grow until you hit those limits. Assume that you will.

Then the question is how to keep growing beyond those limits, and that means scaling out to multiple servers. Build this into your application design.

---

> The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach

Well, now's your opportunity to exercise your general software engineering skills, and develop some tests. 

You — or _someone_ on your team — hopefully have a degree in Computer Science? Well, approach the problem like a scientist. What type of tests would measure scalability of this kind? Presumably you'd need a lab where you could build a database with lots of tables. You'd need some scripts that can populate those tables, probably with a parameter so you can re-run the test at different scale. You'd need some way to drive query traffic in a repeatable fashion, and measure performance.

Then you need to develop requirements for scalability. What performance is acceptable? What rate of degradation is acceptable? Who gets to decide this?

No one starts with these skills. _They learn as they work._ "I don't have those skills" is not an excuse. They try something, they make mistakes, learn from them, improve their processes. 

You also need to learn that optimization and scalability is not about choosing the right technology. No technology scales if you use it improperly. Scalability comes from architecture, which should be where you have software engineering skills.

--------------------------------------------------
Cannot read properties of undefined (reading [api.reducerPath]) at Object.extractRehydrationInfo after clearing browser data
I have used redux persist with RTK query and redux toolkit. After clearing browser data manually from browser settings,
it could not rehydrate RTK query reducer and showing 

    Uncaught TypeError: Cannot read properties of undefined (reading &#39;notesApi&#39;)
        at Object.extractRehydrationInfo (notesApi.js:18:1)
        at createApi.ts:234:1
        at memoized (defaultMemoize.js:123:1)
        at createApi.ts:260:1
        at memoized (defaultMemoize.js:123:1)
        at createReducer.ts:239:1
        at Array.filter (&lt;anonymous&gt;)
        at reducer (createReducer.ts:236:1)
        at reducer (createSlice.ts:325:1)
        at combination (redux.js:560:1).

Here is the [screenshot of my problem][1].

Official Documentation says 

 - RTK Query supports rehydration via the extractRehydrationInfo option
   on createApi. This function is passed every dispatched action, and
   where it returns a value other than ***undefined***, that value is used to
   rehydrate the API state for fulfilled &amp; errored queries.

But what about ***undefined*** value like in my case?

This is my store




    const reducers = combineReducers({
      userReducer,
      [notesApi.reducerPath]: notesApi.reducer,
    });
    
    const persistConfig = {
      key: &quot;root&quot;,
      storage,
    };
    
    const persistedReducer = persistReducer(
      persistConfig,
      reducers
    );
    
    const store = configureStore({
      reducer: persistedReducer,
      middleware: (getDefaultMiddleware) =&gt;
        getDefaultMiddleware({
          serializableCheck: {
            ignoredActions: [FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER],
          },
        }).concat(notesApi?.middleware),
    });    
    
    export default store;




This is the notesApi



    export const notesApi = createApi({
     reducerPath: &quot;notesApi&quot; ,
      baseQuery: fetchBaseQuery({
        baseUrl: &quot;http://localhost:5000/api/notes/&quot;,
        prepareHeaders: (headers, { getState }) =&gt; {
          const token = getState().userReducer.userInfo.token;
          console.log(token);
          if (token) {
            headers.set(&quot;authorization&quot;, `Bearer ${token}`);
          }
          return headers;
        },
      }),
      extractRehydrationInfo(action, { reducerPath }) {
        if (action.type === REHYDRATE) {
            return action.payload[reducerPath]
        }
      },
      tagTypes: [&quot;notes&quot;],
    
      endpoints: (builder) =&gt; ({
        createNote: builder.mutation({
          query: (data) =&gt; ({
            url: `/create`,
            method: &quot;POST&quot;,
            body: data,
          }),
          invalidatesTags: [&quot;notes&quot;],
        }),
        getSingleNote: builder.query({
          query: (id) =&gt; ({
            url: `/${id}`,
          }),
          providesTags: [&quot;notes&quot;],
        })
    });
    export const {  useGetSingleNoteQuery,
      useCreateNoteMutation,
    } = notesApi;



  [1]: https://i.stack.imgur.com/jqawj.png
||||||||||||||I've run into this issue a few times and it seems to manifest when attempting to rehydrate the store when there isn't anything in localStorage to hydrate from.

The error is saying it can't read `"notesApi"` of undefined when running `extractRehydrationInfo`. `"notesApi"` is the API slice's `reducerPath` value. The action's payload is undefined.

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload[reducerPath]; // <-- action.payload undefined
      }
    },

To resolve this issue I've simply used the Optional Chaining operator on the action payload.

Example:

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload?.[reducerPath];
      }
    },

--------------------------------------------------
Pyspark. spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, java.net.SocketException: Connection reset
I am new to pyspark, and i&#39;m trying to run multiple time series in prophet with pyspark (as distributed computing because i have 100s of times series to predict) but i have error as below. 


```
import time 
start_time = time.time()
sdf = spark.createDataFrame(data)
print(&#39;%0.2f min: Lags&#39; % ((time.time() - start_time) / 60))
sdf.createOrReplaceTempView(&#39;Quantity&#39;)
spark.sql(&quot;select Reseller_City, Business_Unit, count(*) from Quantity group by Reseller_City, Business_Unit order by Reseller_City, Business_Unit&quot;).show()
query = &#39;SELECT Reseller_City, Business_Unit, conditions, black_week, promos, Sales_Date as ds, sum(Rslr_Sales_Quantity) as y FROM Quantity GROUP BY Reseller_City, Business_Unit, conditions, black_week, promos, ds ORDER BY Reseller_City, Business_Unit, ds&#39;
spark.sql(query).show()
sdf.rdd.getNumPartitions()
store_part = (spark.sql(query).repartition(spark.sparkContext.defaultParallelism[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;])).cache()

store_part.explain()

from pyspark.sql.types import *

result_schema =StructType([
  StructField(&#39;ds&#39;,TimestampType()),
  StructField(&#39;Reseller_City&#39;,StringType()),
  StructField(&#39;Business_Unit&#39;,StringType()),
  StructField(&#39;y&#39;,DoubleType()),
  StructField(&#39;yhat&#39;,DoubleType()),
  StructField(&#39;yhat_upper&#39;,DoubleType()),
  StructField(&#39;yhat_lower&#39;,DoubleType())
  ])
from pyspark.sql.functions import pandas_udf, PandasUDFType

@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )
def forecast_sales( store_pd ):
    
    model = Prophet(interval_width=0.95, holidays = lock_down)
    model.add_country_holidays(country_name=&#39;DE&#39;)
    model.add_regressor(&#39;conditions&#39;)
    model.add_regressor(&#39;black_week&#39;)
    model.add_regressor(&#39;promos&#39;)
    
    train = store_pd[store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;]
    future_pd = store_pd[store_pd[&#39;ds&#39;]&gt;=&#39;2021-10-01 00:00:00&#39;]
    model.fit(train[[&#39;ds&#39;, &#39;y&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])


    forecast_pd = model.predict(future_pd[[&#39;ds&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])  

    f_pd = forecast_pd[ [&#39;ds&#39;,&#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ].set_index(&#39;ds&#39;)

    #store_pd = store_pd.filter(store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;)

    st_pd = future_pd[[&#39;ds&#39;,&#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;]].set_index(&#39;ds&#39;)

    results_pd = f_pd.join( st_pd, how=&#39;left&#39; )
    results_pd.reset_index(level=0, inplace=True)

    results_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]] = future_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]].iloc[0]

    return results_pd[ [&#39;ds&#39;, &#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;, &#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ]
results = (store_part.groupBy([&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]).apply(forecast_sales).withColumn(&#39;training date&#39;, current_date() ))
results.cache()
results.show()
``` 

All the lines are executed perfectly but error the come from **results.show()**  line  I dont understand where i have done wrong, Much appreciated if someone helps me 

```
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-46-8c647e8bf4d9&gt; in &lt;module&gt;
----&gt; 1 results.show()

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\dataframe.py in show(self, n, truncate, vertical)
    438         &quot;&quot;&quot;
    439         if isinstance(truncate, bool) and truncate:
--&gt; 440             print(self._jdf.showString(n, 20, vertical))
    441         else:
    442             print(self._jdf.showString(n, int(truncate), vertical))

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\utils.py in deco(*a, **kw)
    126     def deco(*a, **kw):
    127         try:
--&gt; 128             return f(*a, **kw)
    129         except py4j.protocol.Py4JJavaError as e:
    130             converted = convert_exception(e.java_exception)

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
--&gt; 328                     format(target_id, &quot;.&quot;, name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o128.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 1243, Grogu.profiflitzer.local, executor driver): java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	... 1 more

```  
||||||||||||||You can also set the os env variables by following the below steps,
run this before SparkSession/SparkContext

    import os
    import sys
    
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

It worked for me

--------------------------------------------------
Calculate difference between 2 date / times in Oracle SQL
I have a table as follows:

    Filename - varchar
    Creation Date - Date format dd/mm/yyyy hh24:mi:ss
    Oldest cdr date - Date format dd/mm/yyyy hh24:mi:ss

How can I calcuate the difference in hours minutes and seconds (and possibly days) between the two dates in Oracle SQL?

Thanks


||||||||||||||You can substract dates in Oracle. This will give you the difference in days. Multiply by 24 to get hours, and so on.

    SQL> select oldest - creation from my_table;


If your date is stored as character data, you have to convert it to a date type first.


    SQL> select 24 * (to_date('2009-07-07 22:00', 'YYYY-MM-DD hh24:mi') 
                 - to_date('2009-07-07 19:30', 'YYYY-MM-DD hh24:mi')) diff_hours 
           from dual;
    
    DIFF_HOURS
    ----------
           2.5

---
*Note*:

This answer applies to dates represented by the Oracle data type `DATE`.
Oracle also has a data type `TIMESTAMP`, which can also represent a date (with time). If you subtract `TIMESTAMP` values, you get an `INTERVAL`; to extract numeric values, use the `EXTRACT` function.

--------------------------------------------------
Excel VBA Macro Returning &quot;Subscript Out of Range&quot; for Incremental Function Converting Hyperlinks to Raw URLs
I am writing an Excel VBA macro that is working fine except for one specific function, and I cannot figure out why it is failing; I&#39;m not a programmer, although I do have some understanding of the basic logic, just little/no experience at writing it, so apologies in advance if there is any confusion imparted by my attempted explanations. The error returned when attempting to execute the code in question is &quot;Run-time error code &#39;9&#39;: subscript out of range&quot;, and I&#39;ve copied the relevant code snippets below:

    &#39; Define variable for worksheet in question
    Dim wsSales As Worksheet
    Set wsSales = ThisWorkbook.Sheets(&quot;Sales&quot;)

    &#39; Find last row with data in it
    Dim lastRowSales As Long
    lastRowSales = wsSales.Cells(Rows.Count, &quot;J&quot;).End(xlUp).Row

    &#39; Loop through column J and convert hyperlinks to raw URLs
    For i = 2 To lastRowSales
        If wsSales.Cells(i, &quot;J&quot;).Hyperlinks.Count &gt; 0 Then
            wsSales.Cells(i, &quot;J&quot;).Value = wsSales.Hyperlinks(i).Address
        End If
    Next i`

- For extra info/context, column J of the Sales sheet referenced contains hyperlinked text (e.g., &quot;Object Name&quot; that points to a URL in a sales-related webpage), and I&#39;m trying to get the actual URL for each row in the range so I can output it elsewhere. Row 1 is a header row, so I&#39;m starting with &#39;i = 2&#39; to ignore it accordingly.
- What the above code ends up doing is partially successful, but specifically fails on the last row for some reason. So if I have, for example, 100 rows in column J of the Sales sheet (99 rows with data and 1 header row), it will successfully convert any hyperlinked values to a URL for the first 99 rows, but row 100 does not convert and Excel spits out the &#39;subscript out of range&#39; error. When looking at the highlighted code that failed after clicking &#39;Debug&#39; on the error pop-up in the VBA Editor, it is specifically the &#39;wsSales.Hyperlinks(i).Address&#39; part that returns a value of &#39;&lt;subscript out of range&gt;&#39;.
- Additionally, it does not actually convert things quite properly; for example, say that row 50 has a hyperlinked text string in it. Rather than converting cell J50 to show the URL that was in J50, it actually shows the URL for J51, and it does this for the entire range (where it&#39;s showing the URL of the cell below it, not the cell itself).
- If I start with &#39;i = 1&#39; instead to include checking the header row (which will never have a hyperlink, but I figured was worth testing), the function works identically - same behavior, same error, no difference at all relative to starting with &#39;i = 2&#39;. That seems to imply to me the error is somewhere either in the logic before the function actually executes or my references in the function itself.
- I have also tested the above code with &quot;wsSales.Hyperlinks(1).address&quot; (1 instead of i) and it ends up completing successfully but using the same URL for the entire column J, so there seems to be a flaw with that logic as an alternative (presumably the static reference for the Hyperlinks object).  The same is true if I use &#39;2&#39; instead of &#39;1&#39;, so I suspect that using any digit will give me the same core problem.
- I feel like there must be something wrong with either my function or some variable I&#39;ve defined that is causing this, but after looking extensively through my code and attempting to &#39;rubber ducky&#39; troubleshoot it, I&#39;m still coming up blank.


I&#39;ve used essentially the exact same logic for multiple other formulas that compose the rest of the larger macro and they all work properly, but this function specifically fails to work as expected; every other &#39;for i = # To [value]&#39; iterates successfully and commenting out the above code snippet from the larger macro enables the full macro to work exactly as expected, just not this function. Does anyone have any thoughts or suggestions for why this may be failing to function as expected? Any ideas for what logic I should check, what may be failing, or a better way to do this? Any advice would be greatly appreciated, thanks!
||||||||||||||Refer to the hyperlink in *each specific cell*, not the [`Worksheet.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.worksheet.hyperlinks) collection:
```
For i = 2 To lastRowSales
    If wsSales.Cells(i, "J").Hyperlinks.Count > 0 Then
        wsSales.Cells(i, "J").Value = wsSales.Cells(i, "J").Hyperlinks(1).Address
    End If
Next i`
```
In other words, you want to use the [`Range.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.range.hyperlinks) property.

If you did want to use the `Worksheet.Hyperlinks` approach:

```
Dim h As Hyperlink
For Each h In wsSales.Hyperlinks
    h.Range.Value = h.Address
Next
```

--------------------------------------------------
How to send email attachments?
I am having problems understanding how to email an attachment using Python. I have successfully emailed simple messages with the `smtplib`. Could someone please explain how to send an attachment in an email. I know there are other posts online but as a Python beginner I find them hard to understand. 

    
||||||||||||||Here's another:

    import smtplib
    from os.path import basename
    from email.mime.application import MIMEApplication
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    from email.utils import COMMASPACE, formatdate
    
    
    def send_mail(send_from, send_to, subject, text, files=None,
                  server="127.0.0.1"):
        assert isinstance(send_to, list)
    
        msg = MIMEMultipart()
        msg['From'] = send_from
        msg['To'] = COMMASPACE.join(send_to)
        msg['Date'] = formatdate(localtime=True)
        msg['Subject'] = subject
 
        msg.attach(MIMEText(text))

        for f in files or []:
            with open(f, "rb") as fil:
                part = MIMEApplication(
                    fil.read(),
                    Name=basename(f)
                )
            # After the file is closed
            part['Content-Disposition'] = 'attachment; filename="%s"' % basename(f)
            msg.attach(part)

    
        smtp = smtplib.SMTP(server)
        smtp.sendmail(send_from, send_to, msg.as_string())
        smtp.close()


It's much the same as the first example... But it should be easier to drop in.

  [1]: http://snippets.dzone.com/posts/show/2038

--------------------------------------------------
Get handle to desktop / shell window
In one of my programs, I need to test if the user is currently focusing the desktop/shell window. Currently, I&#39;m using `GetShellWindow()` from *user32.dll* and compare the result to `GetForegroundWindow()`.

This approach is working until someone changes the desktop wallpaper, but as soon as the wallpaper is changed the handle from `GetShellWindow()` doesn&#39;t match the one from `GetForegroundWindow()` anymore and I don&#39;t quite get why that is. (**OS:** Windows 7 32bit)

Is there a better approach to check if the desktop is focused? Preferably one that won&#39;t be broken if the user changes the wallpaper?

**EDIT:** I designed a workaround: I&#39;m testing the handle to have a child of class `SHELLDLL_DefView`. If it has, the desktop is on focus. Whilst, it&#39;s working at my PC that doesn&#39;t mean it will work all the time.
||||||||||||||The thing changed a little bit since there are slideshows as wallpaper available in Windows 7.
You are right with WorkerW, but this works only with wallpaper is set to slideshow effect. 

When there is set the wallpaper mode to slideshow, you have to search for a window of class `WorkerW` and check the children, whether there is a `SHELLDLL_DefView`.
If there is no slideshow, you can use the good old `GetShellWindow()`.

I had the same problem some months ago and I wrote a function for getting the right window. Unfortunately I can't find it. But the following should work. Only the Win32 Imports are missing:

    public enum DesktopWindow
    {
        ProgMan,
        SHELLDLL_DefViewParent,
        SHELLDLL_DefView,
        SysListView32
    }
    
    public static IntPtr GetDesktopWindow(DesktopWindow desktopWindow)
    {
        IntPtr _ProgMan = GetShellWindow();
        IntPtr _SHELLDLL_DefViewParent = _ProgMan;
        IntPtr _SHELLDLL_DefView = FindWindowEx(_ProgMan, IntPtr.Zero, "SHELLDLL_DefView", null);
        IntPtr _SysListView32 = FindWindowEx(_SHELLDLL_DefView, IntPtr.Zero, "SysListView32", "FolderView");
    
        if (_SHELLDLL_DefView == IntPtr.Zero)
        {
            EnumWindows((hwnd, lParam) =>
            {
                if (GetClassName(hwnd) == "WorkerW")
                {
                    IntPtr child = FindWindowEx(hwnd, IntPtr.Zero, "SHELLDLL_DefView", null);
                    if (child != IntPtr.Zero)
                    {
                        _SHELLDLL_DefViewParent = hwnd;
                        _SHELLDLL_DefView = child;
                        _SysListView32 = FindWindowEx(child, IntPtr.Zero, "SysListView32", "FolderView"); ;
                        return false;
                    }
                }
                return true;
            }, IntPtr.Zero);
        }
    
        switch (desktopWindow)
        {
            case DesktopWindow.ProgMan:
                return _ProgMan;
            case DesktopWindow.SHELLDLL_DefViewParent:
                return _SHELLDLL_DefViewParent;
            case DesktopWindow.SHELLDLL_DefView:
                return _SHELLDLL_DefView;
            case DesktopWindow.SysListView32:
                return _SysListView32;
            default:
                return IntPtr.Zero;
        }
    }

In your case you would call `GetDesktopWindow(DesktopWindow.SHELLDLL_DefViewParent);` to get the top-level window for checking whether it is the foreground window.

--------------------------------------------------
Keil compiler v5 to v.6
I&#39;m forced to switch from ARMCC v5 to CLANG(v.6). Here is the problem. 
I have some struct that includes a pointer to the function which gets as a parameter pointer to the same structure. 
So I do 

```
struct _some_struct_s;
typedef void (*callback_f)(struct _some_struct *p);
 
typedef struct {
  callback_f fn;
  int        x; 
} some_type_s;

// init function
void init_some_struct (some_struct *p, callback_f f) {
  p-&gt;fn = f;
  p-&gt;x = 0;
}
```
In another file I&#39;m writing the callback() and calling init_some_struct()
```
some_type_s  my_struc;
void callback (some_type_s *p) {
  p-&gt;x++;
}
init_some_struct (&amp;my_struc, callback);
```
I had no issues with compiler 5 but a warning with version 6.
***
```
warning: incompatible function pointer types passing &#39;void (some_struct_s *)&#39; to parameter of type &#39;callback_f&#39; (aka &#39;void (*)(struct _some_struct_s *)&#39;) [-Wincompatible-function-pointer-types]
```
What can I do to avoid having this warning?


What can I do to avoid having this warning?

||||||||||||||1. `typedef (*callback_f)(struct _some_struct *p);` - you alias the type as function of pointer which returns `int`.

It should be `typedef void (*callback_f)(struct some_struct *p);`

2.     typedef struct {
            callback_f *fn;
   `fn` is a pointer to pointer to function. It should be `callback_f fn;`

```
typedef struct some_struct _some_struct_s;
typedef void callback_f(struct _some_struct *p);
 
typedef struct some_struct{
  callback_f *fn;
  int        x; 
} some_struct_s;

// init function
void init_some_struct (some_struct *p, callback_f *f) {
  p->fn = f;
  p->x = 0;
}
```



--------------------------------------------------
How to create mutually exclusive fields in Pydantic
I am using Pydantic to model an object. How can I make two fields mutually exclusive?

For instance, if I have the following model:

    class MyModel(pydantic.BaseModel):
        a: typing.Optional[str]
        b: typing.Optional[str]

I want field `a` and field `b` to be mutually exclusive. I want only one of them to be set. Is there a way to achieve that?
||||||||||||||You can use pydantic.validator decorator to add custom validations.

```lang-python
from typing import Optional
from pydantic import BaseModel, validator

class MyModel(BaseModel):
    a: Optional[str]
    b: Optional[str]

    @validator("b", always=True)
    def mutually_exclusive(cls, v, values):
        if values["a"] is not None and v:
            raise ValueError("'a' and 'b' are mutually exclusive.")

        return v
```

--------------------------------------------------
Web automation with Selenium + python and google chrome 115.x &gt;
How to use selenium and chrome CFT for web automation from chrome version 115.x using python?

I have an automation script that worked fine until chrome version 114.x. From version 115.x it stopped working due to the version update, but also due to the new method with chrome cft.
||||||||||||||After upgrading to Chrome version 115.x, my automation stopped working, and chrome driver versions were no longer released, because from chrome version 115.x, automations are performed by CFT (chrome for test ), which as I understand this browser remains static until user action, preventing automations from stopping due to automatic chrome updates and need for crhome driver replacement.
The problem was solved with the solution below:


```
# using selenium 4.8 and python 3.9

from selenium import webdriver
from selenium.webdriver.chrome.options import Options


options = Options()
options.binary_location = 'path to chrome.exe'
## this is the chromium for testing which can be downloaded from the link given below

driver = webdriver.Chrome(chrome_options = options, executable_path = 'path to chromedriver.exe')
## must be the same as the downloaded version of chrome cft.
```
As of today, the files can be downloaded from: https://googlechromelabs.github.io/chrome-for-testing/

Prefer the stable version and download the compatible browser and chromedriver.

The rest of the code continues to work.

source: https://stackoverflow.com/questions/45500606/set-chrome-browser-binary-through-chromedriver-in-python

--------------------------------------------------
How can I list the taints on Kubernetes nodes?
The [docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint) are great about explaining how to set a taint on a node, or remove one. And I can use `kubectl describe node` to get a verbose description of one node, including its taints. But what if I&#39;ve forgotten the name of the taint I created, or which nodes I set it on? Can I list all of my nodes, with any taints that exist on them?
||||||||||||||<!-- language-all: lang-bash -->

    kubectl get nodes -o json | jq '.items[].spec'

which will give the complete spec with node name, or:

    kubectl get nodes -o json | jq '.items[].spec.taints'

will produce the list of the taints per each node

--------------------------------------------------
How to write unitTest for methods using a stream as a parameter
I have class `ImportProvider` , and I want write unit test for Import method.

But this should be unit test, so I don&#39;t want to read from file to stream.
Any idea?

  

    public class ImportProvider : IImportProvider
    { 
         public bool Import(Stream stream)
         {
             //Do import
        
             return isImported;
         }
    }
        
    public interface IImportProvider
    {
          bool Import(Stream input);
    }

This is unit test:

    [TestMethod]
    public void ImportProvider_Test()
    {
        // Arrange           
        var importRepository = new Mock&lt;IImportRepository&gt;(); 
        var imp = new ImportProvider(importRepository.Object);
        //Do setup...

        // Act
        var test_Stream = ?????????????
        // This working but not option:
        //test_Stream = File.Open(&quot;C:/ExcelFile.xls&quot;, FileMode.Open, FileAccess.Read);
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }
||||||||||||||Use a MemoryStream. Not sure what your function expects, but to stuff a UTF-8 string into it for example:

    //Act
    using (var test_Stream = new MemoryStream(Encoding.UTF8.GetBytes("whatever")))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }

EDIT: If you need an Excel file, and you are unable to read files from disk, could you add an Excel file as an embedded resource in your test project? See [How to embed and access resources by using Visual C#][1]

You can then read as a stream like this:

    //Act
    using (var test_Stream = this.GetType().Assembly.GetManifestResourceStream("excelFileResource"))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }


  [1]: https://support.microsoft.com/en-us/kb/319292

--------------------------------------------------
Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
What is causing this build error:

```

- Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
- Plugin Repositories (could not resolve plugin artifact &#39;com.android.application:com.android.application.gradle.plugin:7.0.3&#39;)
  Searched in the following repositories:
    Gradle Central Plugin Repository
    Google
```

in `build.gradle` file

Expecting a successful android build
||||||||||||||In my case `settings.gradle` file was missing. You can create a file and place into project root folder.

**settings.gradle**:

    pluginManagement {
        repositories {
            gradlePluginPortal()
            google()
            mavenCentral()
        }
    }
    dependencyResolutionManagement {
        repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)
        repositories {
            google()
            mavenCentral()
        }
    }
    rootProject.name = "android-geocode"
    include ':app'

--------------------------------------------------
How do I use an API key/secret on Binance&#39;s TestNet?
Following the instructions here, https://docs.binance.org/smart-chain/wallet/arkane.html, I created a Binance SmartChain account with its &quot;0x&quot; prefixed wallet address.  I then added funds.  What I can&#39;t figure out is how I get a TestNet API key and secret so that I can test my Python API calls.  I create the client like so

	from binance.client import Client
	...
	auth_client = Client(key, b64secret)
     if account.testing:
     	auth_client.API_URL = &#39;https://testnet.binance.vision/api&#39;
 
How do I get an API key tied to my Binance SmartChain address?
||||||||||||||You have to create your API credentials from [here][1] and pass the testnet variable into the Client constructor. See the
[documentation][2].

```python
auth_client = Client(key, b64secret, testnet=True)
```

does the job.


  [1]: https://testnet.binance.vision/
  [2]: https://python-binance.readthedocs.io/en/latest/binance.html?highlight=client#binance.client.Client.__init__

--------------------------------------------------
org.gradle.kotlin.kotlin-dsl was not found
I am getting the following error while running the build

    FAILURE: Build failed with an exception.
    
    * Where:
    Build file &#39;/home/charming/mainframer/bigovlog_android/buildSrc/build.gradle.kts&#39; line: 4
    
    * What went wrong:
    Plugin [id: &#39;org.gradle.kotlin.kotlin-dsl&#39;, version: &#39;1.2.6&#39;] was not found in any of the following sources:
    
    - Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
    - Plugin Repositories (could not resolve plugin artifact &#39;org.gradle.kotlin.kotlin-dsl:org.gradle.kotlin.kotlin-dsl.gradle.plugin:1.2.6&#39;)
      Searched in the following repositories:
        Gradle Central Plugin Repository

my buildSrc/build.gradle.kts

    repositories {
        jcenter()
    }
    plugins {
        `kotlin-dsl`
        id(&quot;groovy&quot;)
    }
    dependencies{
        gradleApi()
        localGroovy()
    }

I tried everything but still not working
||||||||||||||Did you check that Android Studio wasn't running in Offline Mode? Take a look at `Preferences/Build, Execution, Deployment/Gradle/Global Gradle settings` and see if Offline Work is checked.

--------------------------------------------------
Typesetting New Functions in LaTeX
So, I just have a little question:

What is the &quot;best way&quot; to typeset new functions in LaTeX which aren&#39;t already included in the various packages?  Right now I&#39;m just using `\mbox` as my go-to method,  but I just was wondering if there was a more &quot;acceptable way of doing it (as with mbox, I have to make sure to include spaces around the text of the functions in order for it to not look too strange)

Here is an example:

    $y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$

which comes out looking like:

![$y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$][1]

Don&#39;t get me wrong... I think it looks fine, but I was just looking for some opinions (as far as best practices go).

  [1]: http://adamnbowen.com/images/error_function.jpg
||||||||||||||Use `\DeclareMathOperator` from package `amsmath`. For example,

```tex
\documentclass{article}
\usepackage{amsmath}
\DeclareMathOperator\erfi{Erfi}

\begin{document}
Consider $x + y + \erfi(t) = z$ for example.
\end{document}
```

produces

[![result][1]][1]

If you only need it once, you can also use `\operatorname`: you get the same output as above with

```tex
\documentclass{article}
\usepackage{amsmath}
\begin{document}
Consider $x + y + \operatorname{Erfi}(t) = z$ for example.
\end{document}
```

If you cannot use the `amsmath` package for some reason, you can manually do `\mathop{\mathrm{Erfi}}` like:

```
\documentclass{article}
\begin{document}
Consider $x + y + \mathop{\mathrm{Erfi}}(t) = z$ for example.
\end{document}
```

See the always-useful TeX FAQ, specifically [Defining a new log-like function in LaTeX](https://texfaq.org/FAQ-newfunction).

  [1]: https://i.stack.imgur.com/9GCor.png

--------------------------------------------------
C# change a string variable with List or Array
I have some static strings 

    static string   Robocopy_Mirror = &quot;[Robocopy_Mirror]&quot;; 
    static string   Robocopy_Copy = &quot;[Robocopy_Copy]&quot;;
    static string   Network_Path_1 = &quot;[Network_Path_1]&quot;;    // \\NAS\Sync\
    static string   Lokal_Path_1 = &quot;[Lokal_Path_1]&quot;;      // X:\Sync\

And I thought I could save some lines of code if I put them in a List and change the values in the List with a loop.

    List&lt;string&gt; variableListe = new List&lt;string&gt;()  
    {   
        Robocopy_Mirror , Robocopy_Copy , Network_Path_1, Lokal_Path_1, 
        File_Network_Sync_1, File_Lokal_Sync_1, File_Network_Sync_2, File_Lokal_Sync_2
    };


But I can&#39;t change the static variables. I guess the List object does not change the static variable? Is there a quick way to change it? 

    for (int i = 0; i &lt; variableListe.Count-1; i++)
    {
        variableListe[i] = AppConfig.ElementAt(configPathPosition);
    }
    Console.WriteLine(Robocopy_Mirror); 

    // prints  &quot;[Robocopy_Mirror]&quot; instead of like C:\robocopy


||||||||||||||The static variables store references to string objects in memory. The elements in the list also store references to the same string objects in memory, but _each element is it's own variable_ and those elements _do not store references to the static variables;_ they refer to the string objects directly. 

When you change an element in the list, you're changing the variable in the list to point to a new object in a new memory location. The static variables do not change and continue to refer to the same unchanged strings as they did before.


--------------------------------------------------
How do I run curl command from within a Kubernetes pod
I have the following questions:

1. I am logged into a Kubernetes pod using the following command:

        ./cluster/kubectl.sh exec my-nginx-0onux -c my-nginx -it bash

    The &#39;ip addr show&#39; command shows its assigned the ip of the pod. Since pod is a logical concept, I am assuming I am logged into a docker container and not a pod, In which case, the pod IP is same as docker container IP. Is that understanding correct?

2. from a Kubernetes node, I do `sudo docker ps` and then do the following:-

        sudo docker exec  71721cb14283 -it &#39;/bin/bash&#39;

    This doesn&#39;t work. Does someone know what I am doing wrong?

3. I want to access the nginx service I created, from within the pod using curl. How can I install curl within this pod or container to access the service from inside. I want to do this to understand the network connectivity.
||||||||||||||Here is how you get a curl command line within a kubernetes network to test and explore your internal REST endpoints.

To get a prompt of a busybox running inside the network, execute the following command. (A tip is to use one unique container per developer.)

```sh
kubectl run curl-<YOUR NAME> --image=radial/busyboxplus:curl -i --tty --rm
```

You may omit the --rm and keep the instance running for later re-usage. To reuse it later, type:

```sh
kubectl attach <POD ID> -c curl-<YOUR NAME> -i -t
```

Using the command `kubectl get pods` you can see all running POD's. The `<POD ID>` is something similar to `curl-yourname-944940652-fvj28`.

**EDIT:** Note that you need to login to google cloud from your terminal (once) before you can do this! Here is an example, make sure to put in your zone, cluster and project: 
```sh
gcloud container clusters get-credentials example-cluster --zone europe-west1-c --project example-148812
```

--------------------------------------------------
Execute CURL with kubectl
I am trying to execute `curl` command with `kubectl` like 

    kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;

Gives belob error

	OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused &quot;exec: \&quot;kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39;\&quot;: 
	stat kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;: no such file or directory&quot; 
    :unknown command terminated with exit code 126
I have tried to escape the quotes but no luck. Then I tried simple curl 

    kubectl exec -it POD_NAME curl http://localhost:8080/xyz

This gives proper output as excepted. Any help with this 

Update: 

But when I run interactive (`kubectl exec -it POD_NAME /bin/bash`) mode of container and then run the curl inside the container works like champ
||||||||||||||i think you need to do something like this:

```
kubectl exec POD_NAME curl "-X PUT http://localhost:8080/abc -H \"Content-Type: application/json\" -d '{\"name\":\"aaa\",\"no\":\"10\"}' "
```

what the error suggests is that its trying to interpret everything inside `""` as a single command, not as a command with parameters. so its essentially looking for an executable called that

--------------------------------------------------
Open in Safari with UIActivityViewController?
I&#39;m sharing a URL via UIActivityViewController. I&#39;d like to see &quot;Open in Safari&quot; or &quot;Open in browser&quot; appear on the share sheet, but it doesn&#39;t. Is there a way to make this happen?

Note: I am not interested in solutions that involve adding somebody else&#39;s library to my app. I want to understand how to do this, not just get it to happen.

||||||||||||||Yes, you could add your custom action to Share sheet in iOS


You would have to copy this class.

    class MyActivity: UIActivity {
        var _activityTitle: String
        var _activityImage: UIImage?
        var activityItems = [Any]()
        var action: ([Any]) -> Void
        
        init(title: String, image: UIImage?, performAction: @escaping ([Any]) -> Void) {
            _activityTitle = title
            _activityImage = image
            action = performAction
            super.init()
        }
        override var activityTitle: String? {
            return _activityTitle
        }
    
        override var activityImage: UIImage? {
            return _activityImage
        }
        override var activityType: UIActivity.ActivityType {
            return UIActivity.ActivityType(rawValue: "com.someUnique.identifier")
        }
    
        override class var activityCategory: UIActivity.Category {
            return .action
        }
        override func canPerform(withActivityItems activityItems: [Any]) -> Bool {
            return true
        }
        override func prepare(withActivityItems activityItems: [Any]) {
            self.activityItems = activityItems
        }
        override func perform() {
            action(activityItems)
            activityDidFinish(true)
        }
    }

Please go through the class you might need to change a few things.

This is how you use it.

        let customItem = MyActivity(title: "Open in Safari", image: UIImage(systemName: "safari")  ) { sharedItems in
            guard let url = sharedItems[0] as? URL else { return }
            UIApplication.shared.open(url)
        }

        let items = [URL(string: "https://www.apple.com")!]
        let ac = UIActivityViewController(activityItems: items, applicationActivities: [customItem])
        ac.excludedActivityTypes = [.postToFacebook]
        present(ac, animated: true)

I have done this for one action, and tested it, it works.

Similarly you could do it for other custom actions.

For more on it refer this link.
[Link To Detailed Post][1]


  [1]: https://www.hackingwithswift.com/articles/118/uiactivityviewcontroller-by-example


--------------------------------------------------
How to cache playwright-python contexts for testing?
I am doing some web scraping using [`playwright-python&gt;=1.41`][1], and have to launch the browser in a headed mode (e.g. `launch(headless=False)`.

For CI testing, I would like to somehow cache the headed interactions with Chromium, to enable offline testing:

- First invocation: uses Chromium to make real-world HTTP transactions
- Later invocations: uses Chromium, but all HTTP transactions read from a cache

How can this be done? I can&#39;t find any clear answers on how to do this.

  [1]: https://github.com/microsoft/playwright-python
||||||||||||||It might solve your problem using HAR-file recording:
1. Run the first test while [recording a HAR-file][1]
2. Storing the HAR-file as an artifact, in your repo or similar in your CI environment
3. Running test again [with recorded HAR-file][2]

Here is how to do that with `playwright==1.41.1` and `pytest-playwright==0.3.3`:

```python
import pathlib

import pytest
from playwright.sync_api import Browser, Playwright

CACHE_DIR = pathlib.Path(__file__).parent / "cache"


@pytest.fixture(name="example_har", scope="session")
def fixture_example_har(playwright: Playwright) -> pathlib.Path:
    har_file = CACHE_DIR / "example.har"
    with (
        playwright.chromium.launch(headless=False) as browser,
        browser.new_page() as page,
    ):
        page.route_from_har(har_file, url="*/**", update=True)
        page.goto("https://example.com/")
    return har_file


def test_caching(browser: Browser, example_har: pathlib.Path) -> None:
    with browser.new_context(offline=True) as context:
        page = context.new_page()
        page.route_from_har(example_har, url="*/**")
        page.goto("https://example.com/")
```

  [1]: https://playwright.dev/python/docs/mock#recording-a-har-file
  [2]: https://playwright.dev/python/docs/mock#replaying-from-har

--------------------------------------------------
Python: Using .format() on a Unicode-escaped string
I am using Python 2.6.5. My code requires the use of the &quot;more than or equal to&quot; sign. Here it goes:  

    &gt;&gt;&gt; s = u&#39;\u2265&#39;
    &gt;&gt;&gt; print s
    &gt;&gt;&gt; ≥
    &gt;&gt;&gt; print &quot;{0}&quot;.format(s)
    Traceback (most recent call last):
         File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; 
    UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\u2265&#39;
      in position 0: ordinal not in range(128)`  

Why do I get this error? Is there a right way to do this? I need to use the `.format()` function.

||||||||||||||Just make the second string also a unicode string

    >>> s = u'\u2265'
    >>> print s
    ≥
    >>> print "{0}".format(s)
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    UnicodeEncodeError: 'ascii' codec can't encode character u'\u2265' in position 0: ordinal not in range(128)
    >>> print u"{0}".format(s)
    ≥
    >>> 



--------------------------------------------------
Only Content controls are allowed directly in a content page that contains Content controls in ASP.NET
I have an application which has a master page and child pages. My application is working fine on local host (on my intranet). But as soon as I put it on a server that is on the internet, I get the error shown below after clicking on any menus.

&gt; Only Content controls are allowed directly in a content page that contains Content controls.

![screenshot][1]




  [1]: http://i.stack.imgur.com/b21sZ.png
||||||||||||||
Double and triple check your opening and closing Content tags throughout your child pages.

**Confirm that they** 

 - are in existence
 - are spelled correctly
 - have an ID
 - have runat="server"
 - have the correct ContentPlaceHolderID

--------------------------------------------------
Apollo Client is not reading variables passed in using useQuery hook
Having a weird issue passing variables into the useQuery hook.

The query:
```
const GET_USER_BY_ID= gql`
  query($id: ID!) {
    getUser(id: $id) {
      id
      fullName
      role
    }
  }
`;
```
Calling the query:
```
const DisplayUser: React.FC&lt;{ id: string }&gt; = ({ id }) =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID, {
    variables: { id },
  });

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Rendering the component:
```
&lt;DisplayUser id=&quot;5e404fa72b819d1410a3164c&quot; /&gt;
```

This yields the error: 
```
&quot;Argument \&quot;id\&quot; of required type \&quot;ID!\&quot; was provided the variable \&quot;$id\&quot; which was not provided a runtime value.&quot;
```

Calling the query from GraphQL Playground returns the expected result:
```
{
  &quot;data&quot;: {
    &quot;getUser&quot;: {
      &quot;id&quot;: &quot;5e404fa72b819d1410a3164c&quot;,
      &quot;fullName&quot;: &quot;Test 1&quot;,
      &quot;role&quot;: &quot;USER&quot;
    }
  }
}
```
And calling the query without a variable but instead hard-coding the id:
```
const GET_USER_BY_ID = gql`
  query {
    getUser(id: &quot;5e404fa72b819d1410a3164c&quot;) {
      id
      fullName
      role
    }
  }
`;

const DisplayUser: React.FC = () =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID);

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Also returns the expected result.

I have also attempted to test a similar query that takes `firstName: String!` as a parameter which also yields an error saying that the variable was not provided a runtime value. This query also works as expected when hard-coding a value in the query string.

This project was started today and uses `&quot;apollo-boost&quot;: &quot;^0.4.7&quot;`, `&quot;graphql&quot;: &quot;^14.6.0&quot;`, and `&quot;react-apollo&quot;: &quot;^3.1.3&quot;`.
||||||||||||||[Solved]

In reading through the stack trace I noticed the issue was referencing `graphql-query-complexity` which I was using for validationRules. I removed the validation rules and now everything works! Granted I don't have validation at the moment but at least I can work from here. Thanks to everyone who took the time to respond!

--------------------------------------------------
Why it is a StackOverFlow Exception?
Why following code throws `StackoverflowException`? 

    class Foo
    {
        Foo foo = new Foo();
    }
    class Program
    {
        static void Main(string[] args)
        {
            new Foo();
        }
    }
||||||||||||||In `Main` you create a new `Foo` object, invoking its constructor.
Inside the `Foo` constructor, you create a different `Foo` instance, again invoking the `Foo` constructor.

This leads to infinite recursion and a `StackOverflowException` being thrown.

--------------------------------------------------
Function to aggregate json
Assume I have a gcs bucket with json files with the following structure:

```
[
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeid&quot;: &quot;Y1&quot;,
    &quot;storeName&quot;: &quot;alibaba1&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.8/3.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y2&quot;,
     &quot;storeName&quot;: &quot;alibaba2&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.7/2.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y3&quot;,
     &quot;storeName&quot;: &quot;alibaba3&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;2.7/4.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y4&quot;,
     &quot;storeName&quot;: &quot;alibaba4&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;3.7/5.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  }
]
```

What I want to do is to aggregate the different values by summing ```a, b,c, d, f,g``` and taking the average of ```e``` to return one single ```json``` like

```
[
{
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;a&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;b&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;c&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;d&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;e&quot;: &quot;average over all first instance/average over all second instance&quot;,
    &quot;f&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;g&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
  }
]
``` 

Not that any of the values in ```*/*/*``` could be NaN and that the data in ```e``` could be a string ```data unvavailable```.

In have created this function 

```
def format_large_numbers_optimized(value):
    abs_values = np.abs(value)
    mask = abs_values &gt;= 1e6
    formatted_values = np.where(mask, 
                                np.char.add(np.round(value / 1e6, 2).astype(str), &quot;M&quot;), 
                                np.round(value, 2).astype(str))
    return formatted_values

def process_json_data_optimized(json_list):
    result = {}
    keys = set(json_list[0].keys()) - {&#39;Id&#39;, &#39;Name&#39;, &#39;storeid&#39;, &#39;storeName&#39;}
    for key in keys:
        result[key] = {&#39;values&#39;: []}
    for json_data in json_list:
        for key in keys:
            value = json_data.get(key, &#39;0&#39;)  
            result[key][&#39;values&#39;].append(value)
    for key in keys:
        all_values_processed = []
        for value in result[key][&#39;values&#39;]:
            if isinstance(value, str) and &#39;/&#39; in value:
                processed_values = [float(v) if v != &#39;data unavailable&#39; else 0 for v in value.split(&#39;/&#39;)]
            elif isinstance(value, float) or isinstance(value, int):
                processed_values = [value]
            else:
                processed_values = [0.0]  
            all_values_processed.append(processed_values)
        numeric_values = np.array(all_values_processed)
        if numeric_values.ndim == 1:
            numeric_values = numeric_values[:, np.newaxis]
        summed_values = np.sum(numeric_values, axis=0)
        formatted_summed_values = &#39;/&#39;.join(format_large_numbers_optimized(summed_values))
        result[key][&#39;summed&#39;] = formatted_summed_values
    processed_result = {key: data[&#39;summed&#39;] for key, data in result.items()}
    processed_result[&#39;Id&#39;] = json_list[0][&#39;Id&#39;]
    processed_result[&#39;Name&#39;] = json_list[0][&#39;Name&#39;]
    return processed_result
```

But it does not create what I expect. I am a at a total loss. Would really appreciate any help.
||||||||||||||Note that you are placing the values as lists `all_values_processed`.
Assuming that the `/` character is just a separator, and that what you want by replacing `all_values_processed.append(processed_values)` by `all_values_processed += processed_values`. Or even better you could just aggregate the values.

For instance you could have a function to aggregate like this

```lang-py
import math
def agg_func(value, initial):
  v_count, v_sum = initial
  if isinstance(value, str) and '/' in value:
    for v in value.split('/'):
      if v != 'data unavailable' :
         v = float(v)
         if not math.isnan(v):
           v_sum += v
           v_count += 1
  elif isinstance(value, float) or isinstance(value, int):
    if not math.isnan(value):
      v_sum += value
      v_count += 1
  return v_count, v_sum
```

A function that aggregate the given keys in the json

```lang-py
def agg_json(v_list, fields):
  state = {k: (0, 0) for k in fields}
  for item in v_list:
    for k in fields:
      if k in item:
        state[k] = agg(item[k], state[k])
  return state
```


Now
```lang-py
state = agg_json(json_list, ['a', 'b', 'c', 'd', 'e', 'f', 'g'])
```

will give you a dictionary with tuples containing the count and the sum for each field. To get your final answer you could do

```lang-py
result = {k: v[1] / v[0] if k == 'e' else v[1] for k, v in state.items()}
```

--------------------------------------------------
color text in divs with two colors using css only - tricky
OK, let me rewrite my question in another words so it looks clear and interesting: [jsFiddle][1]


  [1]: http://jsfiddle.net/xY6T3/1/

I need a pure css solution that colorizes the lines of text in the color depending whether the line is odd or even.

The example of code could be :

    &lt;div class=&quot;main&quot;&gt;
        &lt;div class=&quot;zipcode12345&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode23456&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode90033&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode11321&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;

Is it possible to make it with css? As you see [@ jsFiddle][1], it is not colorized as expected.

So, the main div is &quot;main&quot;.
The inner `div`s always have class names in format &quot;zipcodeXXXXX&quot;, as you see.
The number of zipcodeXXXXX is variable, the number of `myclass` is variable.
However, the odd lines should be always red and the even lines should be always blue.
Does pure css solution exist?

That would be kind of 

    .myclass:nth-child(2n+1){
     color:red;
    }
    .myclass:nth-child(2n){
     color:blue;
    }

if we could igonre `&quot;zipcodeXXXXX&quot;` divs, right?

Thank you.
||||||||||||||Simply apply different odd/even rules to the parent elements as well as the child elements:

<!-- language: lang-css -->

    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(odd),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(even) {
        color: red;
    }
    
    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(even),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(odd) {
        color: blue;
    }

[**JSFiddle demo**][1].


  [1]: http://jsfiddle.net/xY6T3/9/

--------------------------------------------------
How to populate columns in a table using JavaScript
I need to create a simple table using JavaScript based on an array with nested objects, which should have only two columns. In the first cell of the first column of the table, the Processor header is specified, after which the corresponding processor models are written to the lower cells. In the first cell of the second column, the Processor frequency header is indicated, after which the frequencies are written to the lower cells. I was able to generate code for this task, but it doesn&#39;t work correctly. Instead of writing keys to column cells after the first iteration, for some reason, it takes into account unnecessary objects. That&#39;s why you get an undefined value in the table headers and a re-duplication. Please tell me how to solve this problem. 

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    let processorFrequency = [
        {
            titleOne : &#39;Processor&#39;, values : [
                {name : &#39;80386LC (1988г.)&#39;},
                {name : &#39;80486DX4 (1994г.)&#39;},
                {name : &#39;Pentium MMX (1997г.)&#39;},
                {name : &#39;Pentium II (1998г.)&#39;},
                {name : &#39;Pentium III (1999г.)&#39;},
                {name : &#39;Pentium IV&#39;},
                {name : &#39;Athlon-Athlon XP&#39;},
                {name : &#39;Athlon 64&#39;},

            ]
        },
        {
            titleTwo : &#39;Processor frequency&#39;, values : [
                {name : &#39;33-60&#39;},
                {name : &#39;80-133&#39;},
                {name : &#39;160-233&#39;},
                {name : &#39;260-550&#39;},
                {name : &#39;300-1400&#39;},
                {name : &#39;1600-3800&#39;},
                {name : &#39;1400-3200&#39;},
                {name : &#39;2600-3800&#39;},
            ]
    },
    ]



    function Test(){
        let table = document.getElementsByTagName(&#39;table&#39;)[0];
        for (let i = 0; i &lt; processorFrequency.length; i++) {
            var pf = processorFrequency[i];
            let tableRow = document.createElement(&#39;tr&#39;);
            let tdOne = document.createElement(&#39;td&#39;);
            let tdTwo = document.createElement(&#39;td&#39;);
            let txtOne = document.createTextNode(pf.titleOne);
            let txtTwo = document.createTextNode(pf.titleTwo);
            
            tdOne.className = &#39;head&#39;;
            tdTwo.className = &#39;head&#39;;

            tdOne.appendChild(txtOne);
            tdTwo.appendChild(txtTwo);

            tableRow.appendChild(tdOne);
            tableRow.appendChild(tdTwo);
            table.appendChild(tableRow);

            var values = pf.values;
            for (let j = 0; j &lt; values.length; j++) {
                let value = values[j];
                let tableRow = document.createElement(&#39;tr&#39;);
                let td = document.createElement(&#39;td&#39;);
                let txt = document.createTextNode(value.name);
                td.appendChild(txt);
                tableRow.appendChild(td);
                table.appendChild(tableRow);
            }
        }
    } 

    Test();

&lt;!-- language: lang-css --&gt;

    table td, table th {
      border: 1px solid black;
      padding: 5px;
    }

&lt;!-- language: lang-html --&gt;

    &lt;table&gt;&lt;!-- Contents will be created via JavaScript --&gt;
    &lt;/table&gt;

&lt;!-- end snippet --&gt;


||||||||||||||Seems like the issue is that you are creating a new row for each processor AND frequency value, which results in extra rows being added to the table. The correct way should be create a single row for each processor, with two cells (one for the processor model and one for the processor frequency):

    let processorFrequency = [
        {
            titleOne: 'Processor', values: [
                { name: '80386LC (1988г.)' },
                { name: '80486DX4 (1994г.)' },
                { name: 'Pentium MMX (1997г.)' },
                { name: 'Pentium II (1998г.)' },
                { name: 'Pentium III (1999г.)' },
                { name: 'Pentium IV' },
                { name: 'Athlon-Athlon XP' },
                { name: 'Athlon 64' },
            ]
        },
        {
            titleTwo: 'Processor frequency', values: [
                { name: '33-60' },
                { name: '80-133' },
                { name: '160-233' },
                { name: '260-550' },
                { name: '300-1400' },
                { name: '1600-3800' },
                { name: '1400-3200' },
                { name: '2600-3800' },
            ]
        },
    ];
    
    function Test() {
        let table = document.getElementsByTagName('table')[0];
    
        for (let i = 0; i < processorFrequency[0].values.length; i++) {
            let tableRow = document.createElement('tr');
            
            // Processor Name Cell
            let tdOne = document.createElement('td');
            let txtOne = document.createTextNode(processorFrequency[0].values[i].name);
            tdOne.appendChild(txtOne);
            tableRow.appendChild(tdOne);
    
            // Processor Frequency Cell
            let tdTwo = document.createElement('td');
            let txtTwo = document.createTextNode(processorFrequency[1].values[i].name);
            tdTwo.appendChild(txtTwo);
            tableRow.appendChild(tdTwo);
    
            table.appendChild(tableRow);
        }
    }
    
    Test();

--------------------------------------------------
System.UnauthorizedAccessException: Access to the path &quot;...&quot; is denied
  I have C# wpf installation done with .net using click once installation. All works fine. Then I have the following code which is part of the installed program:

    String destinationPath = System.Windows.Forms.Application.StartupPath + &quot;\\&quot; + fileName;
    File.Copy(path, destinationPath, true);
    this.DialogResult = true;
    this.Close();

But I get this error:

&gt;System.UnauthorizedAccessException: Access to the path C:\user\pc\appdata\local\apps\2.0.......  is denied.
&gt;
&gt;at System.IO.File.InternalCopy(String sourceFileName, String destFileName, Boolean overwrite, Boolean checkHost)
&gt;       at System.IO.File.Copy(String sourceFileName, String destFileName, Boolean overwrite)

Is it a permission error or do I need to tweak something in my code?

What puzzles me is why the user is able to install the program using click once into that directory without any issues, but uploading a file to it doesn&#39;t work?
||||||||||||||When installing an application the installer usually asks for administrative privileges. If the user chooses "Yes" the program will run and have read and write access to a larger variety of paths than what a normal user has. If the case is such that the installer did not ask for administrative privileges, it might just be that ClickOnce automatically runs under some sort of elevated privileges.

I'd suggest you write to the local appdata folder instead, but if you feel you really want to write to the very same directory as your application you must first run your app with administrator privileges.

To make your application always ask for administrator privileges you can modify your app's manifest file and set the `requestedExecutionLevel` tag's `level` attribute to `requireAdministrator`:

    <requestedExecutionLevel level="requireAdministrator" uiAccess="false" />

You can read a bit more in [**How do I force my .NET application to run as administrator?**](https://stackoverflow.com/questions/2818179/how-do-i-force-my-net-application-to-run-as-administrator)

--------------------------------------------------
Create a NuGet package for .NET8 MAUI with Azure DevOps
I have created a `.NET8 MAUI Class Library` to use in MAUI projects. The repo is in `Azure DevOps` and I was trying to build and publish the package via NuGet.

For that, I wrote a YAML file

    trigger:
    - main
    
    pool:
      vmImage: ubuntu-latest
    
    steps:
    - task: UseDotNet@2
      displayName: &#39;Use dotnet 8&#39;
      inputs:
        version: &#39;8.0.x&#39;
    - task: CmdLine@2
      inputs:
        script: &#39;dotnet workload install maui&#39;
    - task: DotNetCoreCLI@2
      displayName: Restore packages
      inputs:
        command: &#39;restore&#39;
        feedsToUse: &#39;select&#39;
        vstsFeed: &#39;c800d0d7-e2af-4567-997f-de7cf7888e6c&#39;
    - task: DotNetCoreCLI@2
      displayName: Build project
      inputs:
        command: &#39;build&#39;
        projects: &#39;**/PSC.Maui.Components.BottomSheet.csproj&#39;
        arguments: &#39;--configuration $(buildConfiguration)&#39;

When the pipeline runs, I get this error

    Generating script.
    Script contents:
    dotnet workload install maui
    ========================== Starting Command Output ===========================
    /usr/bin/bash --noprofile --norc /home/vsts/work/_temp/42901c0d-f407-4f75-912b-f93132efa865.sh
    Workload ID maui isn&#39;t supported on this platform.
    
    ##[error]Bash exited with code &#39;1&#39;.
    Finishing: CmdLine


[![enter image description here][1]][1]

Then, I tried to create the NuGet package locally, but it was not recognized by the NuGet website when I uploaded it.

How can I change the pipeline?

  [1]: https://i.stack.imgur.com/loBv9.png
||||||||||||||.Net MAUI does not support Linux, therefore you can neither build to it or from it.  

See [here](https://learn.microsoft.com/en-us/dotnet/maui/supported-platforms?view=net-maui-8.0)  

--------------------------------------------------
How to specify multiple locators for Selenium web element using the FindBy and PageFactory mechanisms
I like to use `PageFactory` with `@FindBy` annotations in my automation framework to auto-locate elements in my page object classes. 

I have one WebElement for which I need to be able to specify a couple of different locators. I thought FindBys was my solution, but apparently, that is not how it works. It&#39;s the equivalent of `driver.findElement(option1).findelement.(option2)`. That&#39;s not what I need. I need something that will find an element by one or the other locators. If one doesn&#39;t work, then use the other locator. Is there a way to do this in Selenium using FindBy annotations?
||||||||||||||There is apparently a new feature in Selenium as of May this year -- the @FindAll annotation that does exactly what I need;

http://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/support/FindAll.html
http://selenium.10932.n7.nabble.com/Pull-Request-62-Add-a-FindAll-annotation-to-the-Java-Page-Factory-td24814.html

--------------------------------------------------
Store cout from function as string
I have a function that takes in a vector of integers and outputs them via `std::cout`. 

    #include &lt;iostream&gt;
    #include &lt;vector&gt;
    
    void final_sol(std::vector&lt;int&gt; list){
        for (int i ; i &lt; list.size() ; i++){
            std::cout &lt;&lt; list[i] &lt;&lt; &quot; &quot;;
        }
    }
    
    int main(){
        std::vector&lt;int&gt; list = {1, 2, 3, 4, 5};
        final_sol(list);
        return 0;
    }
However, from this point I would like to have a way to quickly obtain the outputs of `final_sol(vector)` as a string. One way to do this would be to modify the original function to also create the string. However, I am not interested in modifying `final_sol(vector)`. Is there another way I could store the outputs as a string?
||||||||||||||Provide overload:
```
void final_sol(std::ostream& out, const std::vector<int>& list){
    for (int i = 0; i < list.size() ; i++){
        out << list[i] << " ";
    }
}

void final_sol(const std::vector<int>& list){
    final_sol(std::cout, list);
}
```
This way you existing calling code will not be impacted - most probably this is what you want: not modifying function signature. Not what you described: not do not modifying implementation of final_sol.

Then you can do:
```cpp
std::ostringstream str;
final_sol(str, list);
auto s = str.str()
```


--------------------------------------------------
FastAPI runs api-calls in serial instead of parallel fashion
I have the following code:

```python
import time
from fastapi import FastAPI, Request
    
app = FastAPI()
    
@app.get(&quot;/ping&quot;)
async def ping(request: Request):
        print(&quot;Hello&quot;)
        time.sleep(5)
        print(&quot;bye&quot;)
        return {&quot;ping&quot;: &quot;pong!&quot;}
```
If I run my code on localhost - e.g., `http://localhost:8501/ping` - in different tabs of the same browser window, I get:
```
Hello
bye
Hello
bye
```
instead of:
```
Hello
Hello
bye
bye
```
I have read about using `httpx`, but still, I cannot have a true parallelization. What&#39;s the problem?
||||||||||||||As per [FastAPI's documentation][1]:

> When you declare a path operation function with normal `def` instead
> of `async def`, it is run in an external threadpool **that is then
> `await`ed**, instead of being called directly (as it would block the
> server).

also, as described [here][2]:

> If you are using a third party library that communicates with
> something (a database, an API, the file system, etc.) and doesn't have
> support for using `await`, (this is currently the case for most
> database libraries), then declare your path operation functions as
> normally, with just `def`.
> 
> If your application (somehow) doesn't have to communicate with
> anything else and wait for it to respond, use `async def`.
> 
> If you just don't know, use normal `def`.
> 
> **Note**: You can mix `def` and `async def` in your path operation functions as much as you need and define each one using the best
> option for you. FastAPI will do the right thing with them.
> 
> Anyway, in any of the cases above, FastAPI **will still work
> asynchronously** and be extremely fast.
> 
> But by following the steps above, it will be able to do some
> performance optimizations.



Thus, `def` endpoints (in the context of asynchronous programming, a function defined with just `def` is called *synchronous* function), in FastAPI, run in a separate thread from an external threadpool that is then `await`ed, and hence, FastAPI will still work *asynchronously*. In other words, the server will process requests to such endpoints *concurrently*. Whereas, `async def` endpoints run in the [`event loop`][3]&mdash;on the main (single) thread&mdash;that is, the server will also process requests to such endpoints *concurrently*/*asynchronously*, **as long as there is** an [`await`][4] call to non-blocking I/O-bound operations inside such `async def` endpoints/routes, such as *waiting* for (1) data from the client to be sent through the network, (2) contents of a file in the disk to be read, (3) a database operation to finish, etc., (have a look [here][5]). If, however, an endpoint defined with `async def` does not `await` for something inside, in order to give up time for other tasks in the `event loop` to run (e.g., requests to the same or other endpoints, background tasks, etc.), each request to such an endpoint will have to be completely finished (i.e., exit the endpoint), before returning control back to the `event loop` and allow other tasks to run. In other words, in such cases, the server will process requests *sequentially*. **Note** that the same concept not only applies to FastAPI endpoints, but also to [`StreamingResponse`'s generator function][6] (see [`StreamingResponse`][7] class implementation), as well as [`Background Tasks`][8] (see [`BackgroundTask`][9] class implementation); hence, after reading this answer to the end, you should be able to decide whether you should define a FastAPI endpoint, `StreamingResponse`'s generator, or background task function with `def` or `async def`. 

The keyword `await` (which works only within an `async def` function) passes function control back to the `event loop`. In other words, it suspends the execution of the surrounding [coroutine][10] (i.e., a coroutine object is the result of calling an `async def` function), and tells the `event loop` to let some other task run, until that `await`ed task is completed. **Note** that just because you may define a custom function with `async def` and then `await` it inside your `async def` endpoint, it doesn't mean that your code will work asynchronously, if that custom function contains, for example, calls to `time.sleep()`, CPU-bound tasks, non-async I/O libraries, or any other blocking call that is incompatible with asynchronous Python code. In FastAPI, for example, when using the `async` methods of [`UploadFile`][11], such as `await file.read()` and `await file.write()`, FastAPI/Starlette, behind the scenes, actually runs such *synchronous* [File objects' methods][12] in a separate thread from the external threadpool (using the `async` [`run_in_threadpool()`][13] function) and `await`s it; otherwise, such methods/operations would block the `event loop`&mdash;you can find out more by looking at the [implementation of the `UploadFile` class][14]. The number of worker threads of that external threadpool can be adjusted as required&mdash;please have a look at [this answer][15] for more details.

**Note**  that `async` does not mean *parallel*, but *concurrently*. Asynchronous code with [`async` and `await` is many times summarised as using coroutines][16]. **Coroutines** are collaborative (or [cooperatively multitasked][17]), meaning that "at any given time, a program with coroutines is running **only** one of its coroutines, and this running coroutine suspends its execution only when it explicitly requests to be suspended" (see [here][18] and [here][19] for more info on coroutines). As described in [this article][20]:

> Specifically, whenever execution of a currently-running coroutine
> reaches an `await` expression, the coroutine may be suspended, and
> another previously-suspended coroutine may resume execution if what it
> was suspended on has since returned a value. Suspension can also
> happen when an `async for` block requests the next value from an
> asynchronous iterator or when an `async with` block is entered or
> exited, as these operations use `await` under the hood.

If, however, a blocking I/O-bound or CPU-bound operation was directly executed/called inside an `async def` function/endpoint, it would **block the main thread**, and hence, the `event loop` (as the `event loop` runs in the main thread). Hence, a blocking operation such as `time.sleep()` in an `async def` endpoint would block the entire server (as in the code example provided in your question). Thus, if your endpoint is not going to make any `async` calls, you could declare it with normal `def` instead, in which case, FastAPI would run it in a separate thread from the external threadpool and `await` it, as explained earlier (more solutions are given in the following sections). Example:
```python
@app.get("/ping")
def ping(request: Request):
	#print(request.client)
	print("Hello")
	time.sleep(5)
	print("bye")
	return "pong"
```

Otherwise, if the functions that you had to execute inside the endpoint are `async` functions that you had to `await`, you should define your endpoint with `async def`. To demonstrate this, the example below uses the [`asyncio.sleep()`][21] function (from the [`asyncio`][22] library), which provides a non-blocking sleep operation. The `await asyncio.sleep()` method will suspend the execution of the surrounding coroutine (until the sleep operation is completed), thus allowing other tasks in the `event loop` to run. Similar examples are given [here][23] and [here][24] as well.
```python
import asyncio
 
@app.get("/ping")
async def ping(request: Request):
	#print(request.client)
	print("Hello")
	await asyncio.sleep(5)
	print("bye")
	return "pong"
```

**Both** the endpoints above will print out the specified messages to the screen in the same order as mentioned in your question&mdash;if two requests arrived at around the same time&mdash;that is:
```text
Hello
Hello
bye
bye
```

### Important Note
When you call your endpoint for the second (third, and so on) time, please remember to do that from **a tab that is isolated from the browser's main session**; otherwise, succeeding requests (i.e., coming after the first one) will be blocked by the browser (on **client side**), as the browser will be waiting for response from the server for the previous request before sending the next one. You can confirm that by using `print(request.client)` inside the endpoint, where you would see the `hostname` and `port` number being the same for all incoming requests&mdash;if requests were initiated from tabs opened in the same browser window/session)&mdash;and hence, those requests would be processed sequentially, because of the browser sending them sequentially in the first place. To **solve** this, you could either:
1. Reload the same tab (as is running), or
2. Open a new tab in an Incognito Window, or
3. Use a different browser/client to send the request, or
4. Use the `httpx` library to [make asynchronous HTTP requests][25], along with the [*awaitable*][26] [`asyncio.gather()`][27], which allows executing multiple asynchronous operations concurrently and then returns a list of results in the **same** order the awaitables (tasks) were passed to that function (have a look at [this answer][28] for more details).

   **Example**:
   ```python
   import httpx
   import asyncio

   URLS = ['http://127.0.0.1:8000/ping'] * 2

   async def send(url, client):
       return await client.get(url, timeout=10)

   async def main():
       async with httpx.AsyncClient() as client:
           tasks = [send(url, client) for url in URLS]
           responses = await asyncio.gather(*tasks)
           print(*[r.json() for r in responses], sep='\n')

   asyncio.run(main())
   ```
   In case you had to call different endpoints that may take different time to process a request, and you would like to print the response out on client side as soon as it is returned from the server&mdash;instead of waiting for `asyncio.gather()` to gather the results of all tasks and print them out in the same order the tasks were passed to the `send()` function&mdash;you could replace the `send()` function of the example above with the one shown below:
   ```
   async def send(url, client):
       res = await client.get(url, timeout=10)
       print(res.json())
       return res
   ```

`Async`/`await` and Blocking I/O-bound or CPU-bound Operations
--------------------------------------

If you are required to use `async def` (as you might need to `await` for coroutines inside your endpoint), but also have some _synchronous_ blocking I/O-bound or CPU-bound operation (long-running computation task) that will block the `event loop` (essentially, the entire server) and won't let other requests to go through, for example:
```python 
@app.post("/ping")
async def ping(file: UploadFile = File(...)):
    print("Hello")
	try:
		contents = await file.read()
		res = cpu_bound_task(contents)  # this will block the event loop
	finally:
		await file.close()
	print("bye")
    return "pong"
```

then:

1. You should check whether you could change your endpoint's definition to normal `def` instead of `async def`. For example, if the only method in your endpoint that has to be awaited is the one reading the file contents (as you mentioned in the comments section below), you could instead declare the type of the endpoint's parameter as `bytes` (i.e., `file: bytes = File()`) and thus, FastAPI would read the file for you and you would receive the contents as `bytes`. Hence, there would be no need to use `await file.read()`. Please note that the above approach should work for small files, as the enitre file contents would be stored into memory (see the [documentation on `File` Parameters][29]); and hence, if your system does not have enough RAM available to accommodate the accumulated data (if, for example, you have 8GB of RAM, you can’t load a 50GB file), your application may end up crashing. Alternatively, you could call the `.read()` method of the [`SpooledTemporaryFile`][30] directly (which can be accessed through the `.file` attribute of the `UploadFile` object), so that again you don't have to `await` the `.read()` method&mdash;and as you can now declare your endpoint with normal `def`, each request will run in a **separate thread** (example is given below). For more details on how to upload a `File`, as well how Starlette/FastAPI uses `SpooledTemporaryFile` behind the scenes, please have a look at [this answer][31] and [this answer][32].

   ```python 
   @app.post("/ping")
   def ping(file: UploadFile = File(...)):
       print("Hello")
	   try:
		   contents = file.file.read()
		   res = cpu_bound_task(contents)
	   finally:
		   file.file.close()
       print("bye")
       return "pong"
   ```

2. Use FastAPI's (Starlette's) [`run_in_threadpool()`][13] function from the `concurrency` module&mdash;as @tiangolo suggested [here][33]&mdash;which "will run the function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked" (see [here][34]). As described by @tiangolo [here][35], "`run_in_threadpool` is an `await`able function; the first parameter is a normal function, the following parameters are passed to that function directly. It supports both *sequence* arguments and *keyword* arguments".

   ```python
   from fastapi.concurrency import run_in_threadpool

   res = await run_in_threadpool(cpu_bound_task, contents)
   ```

3. Alternatively, use `asyncio`'s [`loop.run_in_executor()`][36]&mdash;after obtaining the running `event loop` using [`asyncio.get_running_loop()`][37]&mdash;to run the task, which, in this case, you can `await` for it to complete and return the result(s), before moving on to the next line of code. Passing `None` to the *executor* argument, the *default* executor will be used; which is a [`ThreadPoolExecutor`][38]:

   ```python
   import asyncio

   loop = asyncio.get_running_loop()
   res = await loop.run_in_executor(None, cpu_bound_task, contents)
   ```
   or, if you would like to [pass keyword arguments][39] instead, you could use a `lambda` expression (e.g., `lambda: cpu_bound_task(some_arg=contents)`), or, preferably, [`functools.partial()`][40], which is specifically recommended in the documentation for [`loop.run_in_executor()`][36]:
   ```python
   import asyncio
   from functools import partial

   loop = asyncio.get_running_loop()
   res = await loop.run_in_executor(None, partial(cpu_bound_task, some_arg=contents))
   ```

   In Python 3.9+, you could also use [`asyncio.to_thread()`][41] to asynchronously run a synchronous function in a separate thread&mdash;which, essentially, uses `await loop.run_in_executor(None, func_call)` under the hood, as can been seen in the [implementation of `asyncio.to_thread()`][42]. The `to_thread()` function takes the name of a blocking function to execute, as well as any arguments (`*args` and/or `**kwargs`) to the function, and then returns a coroutine that can be `await`ed. Example:
   ```
   import asyncio

   res = await asyncio.to_thread(cpu_bound_task, contents)
   ```
    
   **Note** that as explained in [**this answer**][15], passing `None` to the `executor` argument **does not** create a new `ThreadPoolExecutor` every time you call `await loop.run_in_executor(None, ...)`, but instead re-uses the *default* executor with the *default* number of worker threads (i.e., `min(32, os.cpu_count() + 4)`). Thus, depending on the requirements of your application, that number might be quite low. In that case, you should rather use a custom [`ThreadPoolExecutor`][38]. For instance:
   ```python
   import asyncio
   import concurrent.futures

   loop = asyncio.get_running_loop()
   with concurrent.futures.ThreadPoolExecutor() as pool:
	   res = await loop.run_in_executor(pool, cpu_bound_task, contents)
   ```
   I would strongly recommend having a look at the linked answer above to learn about the difference between using `run_in_threadpool()` and `run_in_executor()`, as well as how to create a re-usable custom `ThreadPoolExecutor` at the application startup, and adjust the number of maximum worker threads as needed.

4. `ThreadPoolExecutor` will successfully prevent the `event loop` from being blocked, but won't give you the **performance improvement** you would expect from running **code in parallel**; especially, when one needs to perform `CPU-bound` tasks, such as the ones described [here][43] (e.g., audio or image processing, machine learning, and so on). It is thus preferable to **run CPU-bound tasks in a separate process**&mdash;using [`ProcessPoolExecutor`][44], as shown below&mdash;which, again, you can integrate with `asyncio`, in order to `await` it to finish its work and return the result(s). As described [here][45], it is important to protect the entry point of the program to avoid recursive spawning of subprocesses, etc. Basically, your code must be under [`if __name__ == '__main__'`][46]. 

   ```python
   import concurrent.futures
   
   loop = asyncio.get_running_loop()
   with concurrent.futures.ProcessPoolExecutor() as pool:
       res = await loop.run_in_executor(pool, cpu_bound_task, contents) 
   ```
   Again, I'd suggest having a look at the linked answer earlier on how to create a re-usable `ProcessPoolExecutor` at the application startup. You might find [this answer][47] helpful as well.

5. Use **more [workers][48]** to take advantage of multi-core CPUs, in order to run multiple processes in parallel and be able to serve more requests. For example, `uvicorn main:app --workers 4` (if you are using [Gunicorn as a process manager with Uvicorn workers][49], please have a look at [**this answer**][50]). When using 1 worker, only one process is run. When using multiple workers, this will spawn multiple processes (all single threaded). Each process has a separate Global Interpreter Lock (GIL), as well as its own `event loop`, which runs in the main thread of each process and executes all tasks in its thread. That means, there is only one thread that can take a lock on the interpreter of each process; unless, of course, you employ additional threads, either outside or inside the `event loop`, e.g., when using a `ThreadPoolExecutor` with `loop.run_in_executor`, or defining endpoints/background tasks/`StreamingResponse`'s generator with normal `def` instead of `async def`, as well as when calling `UploadFile`'s methods (see the first two paragraphs of this answer for more details).

   **Note:** Each worker ["has its own things, variables and memory"][51]. This means that `global` variables/objects, etc., won't be shared across the processes/workers. In this case, you should consider using a database storage, or  Key-Value stores (Caches), as described [here][52] and [here][53]. Additionally, note that "if you are consuming a large amount of memory in your code, **each process** will consume an equivalent amount of memory".


6. If you need to perform **heavy background computation** and you don't necessarily need it to be run by the same process (for example, you don't need to share memory, variables, etc), you might benefit from using other bigger tools like [Celery][54], as described in [FastAPI's documentation][55].


  [1]: https://fastapi.tiangolo.com/async/#path-operation-functions
  [2]: https://fastapi.tiangolo.com/async/#concurrency-and-async-await
  [3]: https://docs.python.org/3/library/asyncio-eventloop.html
  [4]: https://stackoverflow.com/questions/38865050/is-await-in-python3-cooperative-multitasking
  [5]: https://fastapi.tiangolo.com/async/#asynchronous-code
  [6]: https://stackoverflow.com/a/75760884/17865804
  [7]: https://github.com/encode/starlette/blob/31164e346b9bd1ce17d968e1301c3bb2c23bb418/starlette/responses.py#L235
  [8]: https://fastapi.tiangolo.com/tutorial/background-tasks/
  [9]: https://github.com/encode/starlette/blob/33f46a13625bcca4b7520e33be299a23b2e2b26c/starlette/background.py#L15
  [10]: https://docs.python.org/3/library/asyncio-task.html#coroutines
  [11]: https://fastapi.tiangolo.com/tutorial/request-files/#uploadfile
  [12]: https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects
  [13]: https://github.com/encode/starlette/blob/b8ea367b4304a98653ec8ce9c794ad0ba6dcaf4b/starlette/concurrency.py#L35
  [14]: https://github.com/encode/starlette/blob/048643adc21e75b668567fc6bcdd3650b89044ea/starlette/datastructures.py#L426
  [15]: https://stackoverflow.com/a/77941425/17865804
  [16]: https://fastapi.tiangolo.com/async/#coroutines
  [17]: https://en.wikipedia.org/wiki/Cooperative_multitasking
  [18]: https://stackoverflow.com/questions/553704/what-is-a-coroutine
  [19]: https://stackoverflow.com/questions/1934715/difference-between-a-coroutine-and-a-thread
  [20]: https://jwodder.github.io/kbits/posts/pyasync-fundam/
  [21]: https://docs.python.org/3/library/asyncio-task.html#asyncio.sleep
  [22]: https://docs.python.org/3/library/asyncio.html
  [23]: https://docs.python.org/3/library/asyncio-task.html#coroutine
  [24]: https://stackoverflow.com/a/56730924
  [25]: https://www.python-httpx.org/async/#making-async-requests
  [26]: https://docs.python.org/3/library/asyncio-task.html#awaitables
  [27]: https://docs.python.org/3/library/asyncio-task.html#asyncio.gather
  [28]: https://stackoverflow.com/a/74239367/17865804
  [29]: https://fastapi.tiangolo.com/tutorial/request-files/#define-file-parameters
  [30]: https://docs.python.org/3/library/tempfile.html#tempfile.SpooledTemporaryFile
  [31]: https://stackoverflow.com/a/70657621/17865804
  [32]: https://stackoverflow.com/a/70667530/17865804
  [33]: https://github.com/tiangolo/fastapi/issues/1066#issuecomment-612940187
  [34]: https://bocadilloproject.github.io/guide/async.html#common-patterns
  [35]: https://gitter.im/tiangolo/fastapi?at=5ce550f675d9a575a625feb7
  [36]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor
  [37]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop
  [38]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
  [39]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio-pass-keywords
  [40]: https://docs.python.org/3/library/functools.html#functools.partial
  [41]: https://docs.python.org/3/library/asyncio-task.html#asyncio.to_thread
  [42]: https://github.com/python/cpython/blob/c5660ae96f2ab5732c68c301ce9a63009f432d93/Lib/asyncio/threads.py#L12
  [43]: https://fastapi.tiangolo.com/async/#is-concurrency-better-than-parallelism
  [44]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
  [45]: https://stackoverflow.com/q/15900366
  [46]: https://stackoverflow.com/questions/419163/what-does-if-name-main-do
  [47]: https://stackoverflow.com/a/77862153/17865804
  [48]: https://fastapi.tiangolo.com/deployment/server-workers/
  [49]: https://fastapi.tiangolo.com/deployment/server-workers/#gunicorn-with-uvicorn-workers
  [50]: https://stackoverflow.com/a/71613757/17865804
  [51]: https://fastapi.tiangolo.com/deployment/concepts/#memory-per-process
  [52]: https://stackoverflow.com/a/71537393/17865804
  [53]: https://stackoverflow.com/a/65699375/17865804
  [54]: https://docs.celeryq.dev/
  [55]: https://fastapi.tiangolo.com/tutorial/background-tasks/#caveat

--------------------------------------------------
Is it possible to access Svelte store from external js files?
I am wondering if i would be able to access my *Svelte* store values from a plain .js file.

I am trying to write functions returning a dynamic value based on a store value, to import them in any component.
But in a plain .js file I can&#39;t just access the store value with the $ sign..

Quick exemple of a basic function that uses a store value and could be used on multiple components: 

```js
//in .svelte

function add() {
    $counter = $counter + 1;
}
```

*EDIT: rephrasing a bit*

*EDIT:*
Found a solution but i don&#39;t really know if it&#39;s really optimized..

```js
//in .js file

import { get } from &quot;svelte/store&quot;;
import { counter } from &quot;./stores&quot;;

export function add() {
    var counterRef = get(counter);
    counter.set(counterRef + 1);
}
```
||||||||||||||In addition to rixo's answer, a better way to implement `add` is to use the store's `update` method:

```js
import { counter } from "./stores";

export function add() {
    counter.update(n => n + 1);
}
```

You could also create a [custom store](https://svelte.dev/tutorial/custom-stores) that implemented that logic.

--------------------------------------------------
Is CP437 decoding broken for control characters?
According to the [Wikipedia page for Code Page 437](https://en.wikipedia.org/wiki/Code_page_437) the byte values `\x01` through `\x1f` should decode to graphic characters, e.g. `b&#39;\x01&#39;` equates to ☺ `&#39;\u263A&#39;`. But that&#39;s not what `decode` produces:

    &gt;&gt;&gt; b&#39;\x01&#39;.decode(&#39;cp437&#39;)
    &#39;\x01&#39;

That was Python 3.6 but 2.7 does the same, for all 31 byte values.
||||||||||||||While there were graphics associated with the byte range `\x01` through `\x1f`, those graphics were only used in some contexts. In other contexts, those code points would be interpreted as control characters, as in ASCII. Quoting an [IBM page on CP437][1]:

> Code points X'01' through X'1F' and X'7F' may be controls or graphics depending on context. For displays the hexadecimal code in a memory-mapped 
video display buffer is a graphic. For printers the graphics context is established by a preceding control sequence in the data stream. There are two 
such control sequences: ESC X'5C' and ESC X'5E' named Print All Characters and Print Single Character respectively. In other situations the code 
points in question are used as controls.



Python's CP437 decoding is based on the [Unicode mappings on Unicode.org][2], which use the control character interpretation.

The [Unicode FAQ implies][3] that "The correct Unicode mappings for the special graphic characters (01-1F, 7F) of CP437 and other DOS-type code pages" should be available at https://www.unicode.org/Public/MAPPINGS, but digging down there only turns up the mappings with the control characters, and a [page][4] linking to several IBM websites. Digging through IBM's sites turns up ftp://ftp.software.ibm.com/software/globalization/gcoc/attachments/CP00437.txt, which gives graphical mappings for `\x01`-`\x1f` in terms of IBM's [GCGID system][5], but not in terms of Unicode.

I don't know if there actually *is* an official mapping, from either IBM or Unicode, that gives canonical Unicode mappings for `\x01`-`\x1f` in terms of the graphical interpretation of CP437.


  [1]: http://www-01.ibm.com/software/globalization/cp/cp00437.html
  [2]: ftp://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/PC/CP437.TXT
  [3]: http://unicode.org/faq/char_combmark.html#5
  [4]: http://www.unicode.org/Public/MAPPINGS/VENDORS/IBM/IBM_conversions.html
  [5]: https://www-01.ibm.com/software/globalization/gcgid/gcgid.html

--------------------------------------------------
Converting from Python-Polars to Rust-Polars
I have the following working Python polars code. I am learning Rust and am interested in converting Python to Rust.

```
df = df.with_columns(pl.concat([pl.col(base).slice(0, period).rolling_mean(period), pl.col(base).slice(period,None)]).alias(&#39;con&#39;))
```
How to convert the same in Rust? It might be very trivial, still not sure where I am going wrong.

```
let rolling_options = RollingOptions {
        window_size : Duration::parse(duration_str.as_str()),
        ..Default::default()
    };
let a = col(base).slice(0,period).rolling_mean(rolling_options);
let b = col(base).slice(period,lit(Null {}));

let temp_df = df.with_column(concat([a, b], UnionArgs::default()));
```
I keep getting the following error

&gt;mismatched types
expected enum `Expr`
   found enum `Result&lt;LazyFrame, PolarsError&gt;`

When i checked the data types of **a** and **b**, to my surprise they are **LazyFrame** and not **Expr**

[rolling_mean][1] as per the document returns **Expr** and so does [slice][2]. Not sure what I am missing.


  [1]: https://docs.rs/polars/latest/polars/prelude/enum.Expr.html#method.rolling_mean
  [2]: https://docs.rs/polars/latest/polars/prelude/enum.Expr.html#method.slice
||||||||||||||The result datatype you are getting is an enum meant to represent whether the operation was successful (returns a lazyFrame) or it failed (returns polarserror).

you should be able to 'uwnrap' the result

This link should cover the different ways to handle errors/results in rust:
https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html

--------------------------------------------------
Is Element.tagName always uppercase?
Reading at [MDN about Element.tagName][1] it states:

&gt;On HTML elements in DOM trees flagged as HTML documents, tagName returns the element name in the uppercase form.

My question is: is this trustable? Does IE (old and modern) behave as expected? Is this likely to change? or is it better to always work with `el.tagName.toLowerCase()`?


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element.tagName
||||||||||||||You don't have to `toLowerCase` or whatever, browsers do behave the same on this point (surprisingly huh?).

About the rationale, once I had discussion with a colleague who's very professional on W3C standards. One of his opinions is that using uppercase TAGNAME would be much easier to recognize them out of user content. That's quite persuasive for me.

-------------
**Edit:** As @adjenks says, XHTML doctype returns mixed-case tagName *if the document is served as `Content-Type: application/xhtml+xml`*. Test page: http://programming.enthuses.me/tag-node-case.php?doc=x

Technically, please read this spec for more info: http://www.w3.org/TR/REC-DOM-Level-1/level-one-core.html#ID-745549614

> Note that this (tagName) is **case-preserving in XML**, as are all of the operations of the DOM. The HTML DOM returns the tagName of an HTML element in the canonical uppercase form, regardless of the case in the source HTML document.

As of asker's question: this is trustable. Breaking change is not likely to happen in HTML spec.

--------------------------------------------------
How to validate more than one field of a Pydantic model?
I want to validate three model Fields of a Pydantic model. To do this, I am importing [`root_validator`][1] from pydantic, however I am getting the error below:
```py3
from pydantic import BaseModel, ValidationError, root_validator
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ImportError: cannot import name &#39;root_validator&#39; from &#39;pydantic&#39; (C:\Users\Lenovo\AppData\Local\Programs\Python\Python38-32\lib\site-packages\pydantic\__init__.py)
```

I tried this:
```python
@validator
def validate_all(cls, v, values, **kwargs):
    ...
```

I am inheriting my pydantic model from some common fields parent model. Values showing only parent class fields, but not my child class fields. For example:

```py3
class Parent(BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str
    
    @validator
    def validate_all(cls, v, values, **kwargs):
        #here values showing only (name and comment) but not address and phone.
        ...
```


  [1]: https://pydantic-docs.helpmanual.io/usage/validators/#root-validators
||||||||||||||To extend on the answer of `Rahul R`, this example shows in more detail how to use the `pydantic` validators.

This example contains all the necessary information to answer your question.

Note, that there is also the option to use a `@root_validator`, as mentioned by `Kentgrav`, see the example at the bottom of the post for more details.

```python
import pydantic

class Parent(pydantic.BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str

    # If you want to apply the Validator to the fields "name", "comments", "address", "phone"
    @pydantic.validator("name", "comments", "address", "phone")
    @classmethod
    def validate_all_fields_one_by_one(cls, field_value):
        # Do the validation instead of printing
        print(f"{cls}: Field value {field_value}")

        return field_value  # this is the value written to the class field

    # if you want to validate to content of "phone" using the other fields of the Parent and Child class
    @pydantic.validator("phone")
    @classmethod
    def validate_one_field_using_the_others(cls, field_value, values, field, config):
        parent_class_name = values["name"]
        parent_class_address = values["address"] # works because "address" is already validated once we validate "phone"
        # Do the validation instead of printing
        print(f"{field_value} is the {field.name} of {parent_class_name}")

        return field_value 

Customer(name="Peter", comments="Pydantic User", address="Home", phone="117")
```
**Output**
```cmd
<class '__main__.Customer'>: Field value Peter
<class '__main__.Customer'>: Field value Pydantic User
<class '__main__.Customer'>: Field value Home
<class '__main__.Customer'>: Field value 117
117 is the phone number of Peter
Customer(name='Peter', comments='Pydantic User', address='Home', phone='117')
```

To answer your question in more detail:

Add the fields to validate to the `@validator` decorator directly above the validation function.
- `@validator("name")` uses the field value of `"name"` (e.g. `"Peter"`) as input to the validation function. All fields of the class and its parent classes can be added to the `@validator` decorator.
- the validation function (`validate_all_fields_one_by_one`) then uses the field value as the second argument (`field_value`) for which to validate the input. The return value of the validation function is written to the class field. The signature of the validation function is `def validate_something(cls, field_value)` where the function and variable names can be chosen arbitrarily (but the first argument should be `cls`). According to Arjan (https://youtu.be/Vj-iU-8_xLs?t=329), also the `@classmethod` decorator should be added.


If the goal is to validate one field by using other (already validated) fields of the parent and child class, the full signature of the validation function is `def validate_something(cls, field_value, values, field, config)` (the argument names `values`,`field` and `config` **must** match) where the value of the fields can be accessed with the field name as key (e.g. `values["comments"]`).

**Edit1**: If you want to check only input values of a certain type, you could use the following structure:
```python
@validator("*") # validates all fields
def validate_if_float(cls, value):
    if isinstance(value, float):
        # do validation here
    return value
```

**Edit2**: Easier way to validate all fields together using `@root_validator`:
```python
import pydantic

class Parent(pydantic.BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str

    @pydantic.root_validator()
    @classmethod
    def validate_all_fields_at_the_same_time(cls, field_values):
        # Do the validation instead of printing
        print(f"{cls}: Field values are: {field_values}")
        assert field_values["name"] != "invalid_name", f"Name `{field_values['name']}` not allowed."
        return field_values
```

**Output**:

```python
Customer(name="valid_name", comments="", address="Street 7", phone="079")
<class '__main__.Customer'>: Field values are: {'name': 'valid_name', 'comments': '', 'address': 'Street 7', 'phone': '079'}
Customer(name='valid_name', comments='', address='Street 7', phone='079')
```

```python
Customer(name="invalid_name", comments="", address="Street 7", phone="079")
ValidationError: 1 validation error for Customer
__root__
  Name `invalid_name` not allowed. (type=assertion_error)
```

--------------------------------------------------
What is Python&#39;s bytes type actually used for?
Could somebody explain the general purpose of [the bytes type in Python 3](https://docs.python.org/3/library/stdtypes.html#bytes-objects), or give some examples where it is preferred over other data types? 

I see that the advantage of [bytearrays](https://docs.python.org/3/library/stdtypes.html#bytearray-objects) over strings is their mutability, but what about bytes? So far, the only situation where I actually needed it was sending and receiving data through sockets; is there something else? 
||||||||||||||Possible duplicate of [what is the difference between a string and a byte string][1]

In short, the bytes type is a sequence of bytes that have been encoded and are ready to be stored in memory/disk. There are many types of encodings (utf-8, utf-16, windows-1255), which all handle the bytes differently. The bytes object can be decoded into a str type.

The str type is a sequence of unicode characters. The str needs to be encoded to be stored, but is mutable and an abstraction of the bytes logic. 

There is a strong relationship between `str` and `bytes`. `bytes` can be decoded into a `str`, and `str`s can be encoded into bytes. 

You typically only have to use `bytes` when you encounter a string in the wild with a unique encoding, or when a library requires it. `str` , especially in python3, will handle the rest. 

More reading [here][2] and [here][3]



  [1]: https://stackoverflow.com/questions/6224052/what-is-the-difference-between-a-string-and-a-byte-string
  [2]: https://eli.thegreenplace.net/2012/01/30/the-bytesstr-dichotomy-in-python-3
  [3]: https://betterprogramming.pub/strings-unicode-and-bytes-in-python-3-everything-you-always-wanted-to-know-27dc02ff2686

--------------------------------------------------
Process terminated. Couldn&#39;t find a valid ICU package installed on the system in Asp.Net Core 3 - ubuntu
I am trying to run a Asp.Net Core 3 application in Ubuntu 19.10 thru terminal using `dotnet run` command but it does not seem to work. I get this error.

&gt; ```none
&gt; Process terminated. Couldn&#39;t find a valid ICU package installed on the system.
&gt; Set the configuration flag System.Globalization.Invariant to true if you want
&gt; to run with no globalization support.   
&gt;  at System.Environment.FailFast(System.String)   
&gt;  at System.Globalization.GlobalizationMode.GetGlobalizationInvariantMode()
&gt;  at System.Globalization.GlobalizationMode..cctor()   
&gt;  at System.Globalization.CultureData.CreateCultureWithInvariantData()   
&gt;  at System.Globalization.CultureData.get_Invariant()   
&gt;  at System.Globalization.CultureInfo..cctor()   
&gt;  at System.StringComparer..cctor()   
&gt;  at System.StringComparer.get_OrdinalIgnoreCase()   
&gt;  at Microsoft.Extensions.Configuration.ConfigurationProvider..ctor()   
&gt;  at Microsoft.Extensions.Configuration.EnvironmentVariables.EnvironmentVariablesConfigurationSource.Build(Microsoft.Extensions.Configuration.IConfigurationBuilder)
&gt;  at Microsoft.Extensions.Configuration.ConfigurationBuilder.Build()   
&gt;  at Microsoft.AspNetCore.Hosting.GenericWebHostBuilder..ctor(Microsoft.Extensions.Hosting.IHostBuilder)
&gt;  at Microsoft.Extensions.Hosting.GenericHostWebHostBuilderExtensions.ConfigureWebHost(Microsoft.Extensions.Hosting.IHostBuilder, System.Action&#39;1&lt;Microsoft.AspNetCore.Hosting.IWebHostBuilder&gt;)   
&gt;  at Microsoft.Extensions.Hosting.GenericHostBuilderExtensions.ConfigureWebHostDefaults(Microsoft.Extensions.Hosting.IHostBuilder, System.Action&#39;1&lt;Microsoft.AspNetCore.Hosting.IWebHostBuilder&gt;)   
&gt;  at WebApplication.Program.CreateHostBuilder(System.String[])   
&gt;  at WebApplication.Program.Main(System.String[])
&gt; ```

I installed the dotnet core sdk using the ubuntu store and after that I also installed Rider IDE.

The weird thing here is that when I run the app using Rider it runs fine, the only issue is using terminal dotnet core commands.

Does anybody know what might be the issue ?

The application is created using Rider. I don&#39;t think that this plays a role but just as a side fact.

I know there are also other ways to install dotnet core in ubuntu but since the sdk is available in the ubuntu story I thought it should work out of the box and of course its an easier choice.

Also tried this [one](https://stackoverflow.com/questions/58132275/ci-cannot-build-net-project-fails-with-couldnt-find-a-valid-icu-package-ins) but does not seem to work for me. Still the same issue happens after running the commands.

||||||||||||||The alternative solution as described in [Microsoft documentation][1] is to set environment variable before running your app 

    export DOTNET_SYSTEM_GLOBALIZATION_INVARIANT=1


  [1]: https://learn.microsoft.com/en-us/dotnet/core/run-time-config/globalization

--------------------------------------------------
Python: Should I save PyPi packages offline as a backup?
**My Python projects heavily depends on PyPi packages**.&lt;br&gt;
I want to make sure that: in any time in the future: the packages required by my apps will always be available online on PyPi.&lt;br&gt;
For example:-&lt;br&gt;
I found a project on Github that requires PyQt4.&lt;br&gt;
when I tried to run it on my Linux machine,&lt;br&gt;
it crashed on startup because it can&#39;t find PyQt4 package on PyPi.&lt;br&gt;
&gt; NB: I know that PyQt4 is deprecated

I searched a lot to find an archive for PyPi that still holds PyQt4 package, but I couldn&#39;t find them anywhere.&lt;br&gt;

so I had to rewrite that app to make it work on PyQt5.&lt;br&gt;
I only changed the code related to the UI (ie: PyQt4).&lt;br&gt;
other functions were still working.&lt;br&gt;

so the only problem with that app was that PyQt4 package was removed from PyPi.&lt;br&gt;
&lt;hr&gt;&lt;br&gt;
so, my question is: should I save a backup of the PyPi packages I use ?
||||||||||||||**TL;DR**

YES if you want availability... The next big question is **how** best to keep a backup version of the dependencies? There are some suggestions at the end of the answer.

**Long Verion:**

Your questions touches on the concept of "Availability" which one of the three pillars of Information Assurance (or Information Security). The other two pillars are Confidentiality and Integrity... The CIA triad.

PyPi packages are maintained by the owners of those packages, a project that depends on a package and list it as a dependency must take into account the possibility that the owner of the package will pull the package or a version of the package out of PyPi at any moment.

Important python packages with many dependencies usually are maintained by foundations or organisations that are more responsible with dealing with downstream dependants packages and projects. However keeping support for old packages is very costly and requires extra effort and usually maintainers sets a date for end of support, or publish a the package lifecycle where they state when a specific version will be removed from the public PyPi server.

Once that happens, the dependant have to update the code (as you did), or provide the original dependency via alternative means.

This topic is very important for procurement in Libraries, Universities, Labs, Companies, and Government Agencies where a software tool might have dependencies on other software packages (or ecosystem), and where "availability" should be addressed adequately. Addressing might mean ensuring high availability, but it could also mean living with the risk of losing availability of the packages... The choices you make for "security" of your project should be informed by a risk analysis.

Now to make sure that dependencies are always available... I quickly compiled the following list. Note that each option has pros and cons. You should evaluate these and other options based on your needs:

 1. Store the virtual environment along with the code. Once you create a virtual environment and install the packages you require for the project in that virtual environment, you can keep the virtual environment for posterity.
 2. Host your own PyPi instance (or mirror) and keep a copy of packages you depend upon hosted on it https://packaging.python.org/en/latest/guides/hosting-your-own-index/ 
 3. Use an "artifact management tool" such as Artifactory from https://jfrog.com/artifact-management/, where you can not only host python packages but also docker images, nmap packages, and other kinds of artifacts.
 4. Get the source code of all dependencies, and always build from source.

Let me know if you think of other ideas so that I add them to the list...

--------------------------------------------------
How to create a windowless application in C#?
I am new to C# and want to make a program that runs without a console/GUI, but can&#39;t figure out how. Is that even possible?

The only options I found were minimizing the window or hiding it AFTER start, but I want it to start without a window/visible in the task bar. I am aware of the fact that I (of course) won&#39;t be able to write to the console...
||||||||||||||What you're looking for is a windows service. You can create one, or at least I see the option to (in VS2022), when creating a new project.

Just create new project and search `Windows Service` and check the one that says `C#`. Pay attention though, one says VB.

Creating a window form and then hiding it.. unless you actually want to bring it up at some point, would be something I would advise against.

    using System;
    using System.Collections.Generic;
    using System.Linq;
    using System.ServiceProcess;
    using System.Text;
    using System.Threading.Tasks;
    
    namespace TestService
    {
        internal static class Program
        {
            /// <summary>
            /// The main entry point for the application.
            /// </summary>
            static void Main()
            {
                ServiceBase[] ServicesToRun;
                ServicesToRun = new ServiceBase[]
                {
                    new Service1()
                };
                ServiceBase.Run(ServicesToRun);
            }
        }
    }



--------------------------------------------------
How to draw tiled image with QT
I&#39;m writing interface with C++/Qt in QtCreator&#39;s designer. What element to chose to make as a rect with some background image?

And the second question: how to draw tiled image? I have and image with size (1&#215;50) and I want to render it for the parent width. Any ideas?

-----------------

    mTopMenuBg = QPixmap(&quot;images/top_menu_bg.png&quot;);
    mTopMenuBrush = QBrush(mTopMenuBg);
    mTopMenuBrush.setStyle(Qt::TexturePattern);
    mTopMenuBrush.setTexture(mTopMenuBg);
    
    ui-&gt;graphicsView-&gt;setBackgroundBrush(mTopMenuBrush);

&gt; QBrush: Incorrect use of
&gt; TexturePattern
||||||||||||||If you just want to show an image you can use [QImage][1].  To make a background with the image tiled construct a [QBrush][2] with the QImage.  Then, if you were using [QGraphicsScene][3] for example, you could set the bursh as the background brush.

Here is an example which fills the entire main window with the tiled image "document.png":

    int main(int argc, char *argv[]) {
    	QApplication app(argc, argv);
    	QMainWindow *mainWindow = new QMainWindow();
    
    	QGraphicsScene *scene = new QGraphicsScene(100, 100, 100, 100);
    	QGraphicsView *view = new QGraphicsView(scene);
    	mainWindow->setCentralWidget(view);
    
    	QImage *image = new QImage("document.png");
    	if(image->isNull()) {
    		std::cout << "Failed to load the image." <<std::endl;
    	} else {
    		QBrush *brush = new QBrush(*image);
    		view->setBackgroundBrush(*brush);
    	}
    
    	mainWindow->show();
    	return app.exec();
    }

The resulting app:  
![screen shot][4]


Alternatively, it seems that you could use [style sheets][5] with any widget and change the [background-image][6] property on the widget.  This has more integration with QtDesigner as you can set the style sheet and image in [QtDesigner][7].


  [1]: https://doc.qt.io/qt-6/qimage.html
  [2]: https://doc.qt.io/qt-6/qbrush.html
  [3]: https://doc.qt.io/qt-6/qgraphicsscene.html
  [4]: http://i.stack.imgur.com/1LK7W.jpg
  [5]: https://doc.qt.io/qt-6/stylesheet-reference.html
  [6]: https://doc.qt.io/qt-6/stylesheet-reference.html#background-image-prop
  [7]: https://doc.qt.io/qt-6/designer-stylesheet.html

--------------------------------------------------
Autofilter and set field using variable
I&#39;m using a serch criteria to get the number/index location of a column to use in the field section of the autofilter. Getting an error &quot;runtime err 1004: Autofilter method of range class failed&quot; not sure if it&#39;s possible. I can see in the degug the variable is holding the correct number

```

Private Sub cmdExtract1_Click()
Dim ws As Worksheet
    Dim lngLastRow As Long
    Dim rngData As Range
    Dim iColNumber As Integer
    
    
     Dim strSearch As String
    Dim aCell As Range
  
    Set ws = Worksheets(&quot;Detail Excel&quot;)
    ws.Activate


    &#39;Identify the last row and use that info to set up the Range
    With ws
    ws.Range(&quot;1:1&quot;).Select
        lngLastRow = ActiveSheet.Cells.Find(&quot;*&quot;, SearchOrder:=xlByRows, SearchDirection:=xlPrevious).Row

strSearch = &quot;Deleted App&quot;

    Set aCell = Sheet1.Rows(1).Find(What:=strSearch, LookIn:=xlValues, _
    LookAt:=xlWhole, SearchOrder:=xlByRows, SearchDirection:=xlNext, _
    MatchCase:=False, SearchFormat:=False)
    
     iColNumber = aCell.Column
         
    End With
    

&#39;Offer Date: include dates, remove blanks
Application.DisplayAlerts = False &#39;switching off the alert button
ws.Range(&quot;A1&quot; &amp; &quot;:y&quot; &amp; lngLastRow).AutoFilter Field:=iColNumber, Criteria1:=&quot;&quot;
ws.Range(&quot;A2&quot; &amp; &quot;:y&quot; &amp; lngLastRow).SpecialCells(xlCellTypeVisible).Delete
Application.DisplayAlerts = True &#39;switching on the alert button

On Error Resume Next
ws.ShowAllData
```




search variable then set the cell column number to a variable
||||||||||||||If you only want to delete rows with blanks you could do that without AutoFilter:

```
Private Sub cmdExtract1_Click()
    Dim ws As Worksheet, strSearch As String, m As Variant
  
    Set ws = Worksheets("Detail Excel")
    strSearch = "Deleted App"
    
    m = Application.match(strSearch, ws.rows(1), 0)
    
    If IsError(m) Then 'no header match?
        MsgBox "Header '" & strSearch & "' not found!", vbExclamation
        Exit Sub
    Else
        With ws.Range(ws.Cells(2, m), ws.Cells(rows.count, m).End(xlUp))
            On Error Resume Next 'ignore error in case no blanks
            .SpecialCells(xlCellTypeBlanks).EntireRow.Delete
            On Error GoTo 0      'stop ignoring errors
        End With
    End If

End Sub
```

EDIT: using autofilter

```
Private Sub cmdExtract1_Click()
    Dim ws As Worksheet, strSearch As String, m As Variant
    Dim lr As Long, lc As Long
  
    Set ws = Worksheets("Detail Excel")
    strSearch = "Deleted App"
    
    m = Application.match(strSearch, ws.rows(1), 0)
    
    If IsError(m) Then 'no header match?
        MsgBox "Header '" & strSearch & "' not found!", vbExclamation
        Exit Sub
    Else
        lr = LastOccupiedRow(ws)
        lc = ws.Cells(1, ws.Columns.count).End(xlToLeft).Column 'last header
        With ws.Range("A1", ws.Cells(lr, lc))
            .AutoFilter Field:=m, Criteria1:=""
            .Offset(1).SpecialCells(xlCellTypeVisible).EntireRow.Delete
        End With
        ws.ShowAllData
    End If
End Sub

Function LastOccupiedRow(ws As Worksheet) As Long
    Dim f As Range
    Set f = ws.Cells.Find("*", SearchOrder:=xlByRows, SearchDirection:=xlPrevious)
    If Not f Is Nothing Then LastOccupiedRow = f.row
End Function
```

--------------------------------------------------
How do I decrypt email from the href value
I am trying to decrypt email from the href value.
I encountered this problem while doing a web scraping task using python.

```html
&lt;a href=&quot;javascript:linkTo_UnCryptMailto(%27ocknvq%2CkphqBngjocpp0ej%27);&quot;&gt;
```

https://i.stack.imgur.com/yB8vo.png

 Whenever I click on *Email*, it directs me to Outlook mail application.From there I can get the email.However, it want the email without actually going to the mail application and in the console.

I have tried various decryption methods discussed on this website but it didn&#39;t work.Can someone give me a hint about what type of method for encryption is used?

||||||||||||||If you are looking to reverse engineer the encryption, you will need to go through the site's source and look for the JS function `linkTo_UnCryptMailto`.

More simply though, if you open dev tools (Ctrl + Shift + I) on chrome and click on the console, you should just be able to type `linkTo_UnCryptMailto(%27ocknvq%2CkphqBngjocpp0ej%27)` and view the result since the function appears to be global.

--------------------------------------------------
Show Bootstrap modal using (#myModal).modal(&#39;show&#39;) from Angular component
I have an Angular 8 project that is using bootstrap. I am trying to show the modal dialog inside my component using the `$(&#39;myModal&#39;).modal(&#39;show&#39;)` inside my component&#39;s *Typescript* file.

Here&#39;s my component&#39;s file:

    import {Component, OnInit} from &#39;@angular/core&#39;;
    import {Router} from &#39;@angular/router&#39;;
    // import * as $ from &#39;jquery&#39;;
    import * as bootstrap from &#39;bootstrap&#39;;
    
    @Component({
      selector: &#39;app-xyz&#39;,
      templateUrl: &#39;./xyz.component.html&#39;,
      styleUrls: [&#39;./xyz.css&#39;]
    })
    export class XyzComponent implements OnInit {
    
      constructor(private router: Router) {
      }
    
      ngOnInit() {
      }
    
      submit() {
        $(&#39;#confirm&#39;).modal(&#39;show&#39;);
      }
    
    }

Upon invoking the submit() funciton on click I get the following error: `ERROR TypeError: $(...).modal is not a function`

I installed bootstrap and jquery using `npm install bootstrap` --save and `npm install jquery --save`.

I even installed *ngx-bootstrap*.

However, when I uncomment the line importing *jQuery* I get a different error: `ERROR TypeError: jquery__WEBPACK_IMPORTED_MODULE_3__(...).modal is not a function`
||||||||||||||Check your angular.json file to make sure that the Jquery js file is included in your scripts array.

`"scripts": [
    "node_modules/jquery/dist/jquery.min.js",
]`

Then in your component TS file declare var $ instead of trying to import it:

`declare var $: any;`

That should allow you to trigger the modal via

`$('#confirm').modal();`

Also make sure that the HTML is correct. Example Modal:


    <div class="modal fade" id="confirm" tabindex="-1" role="dialog" data-backdrop="static" aria-labelledby="noDataModalCenterTitle" aria-hidden="true">
        <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
                <div class="modal-header" style="background-color:lightgrey">
                    <h5 class="modal-title" id="noDataModalLongTitle">No Data</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                    </button>
                </div>
                <div class="modal-body">
                    <i class="fas fa-check fa-4x mb-3 animated rotateIn"></i>
                    No Data Found. Please expand your search criteria and try again.
                </div>
                <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Ok</button>
                </div>
            </div>
        </div>
    </div>

Hope this helps!

--------------------------------------------------
cypher: Count distinct paths between two nodes disregarding link type
I&#39;m use cypher to count the number of paths between two nodes.  I have this query:

    MATCH p=(a)-[*1..2]-(b)
    where ID(a) = &quot;node1&quot;
    and ID(b) = &quot;node2&quot;
    return COUNT(p)

However, many of my nodes have multiple links to the same node with different relationship types.  I&#39;d like to ONLY count the distinct paths regardless of the relationship type.  

For example, the paths returned may be as follows:

    (node1)-[rel_type_a]-(node3)-[rel_type_b]-(node2) 
    (node1)-[rel_type_c]-(node3)-[rel_type_d]-(node2) 
    (node1)-[rel_type_e]-(node3)-[rel_type_f]-(node2) 
The query above counts this as 3 paths, but I only want to count this as a single path since all of the nodes are are the same, I&#39;m not interested in the relationship types.

Thanks in advance!

||||||||||||||I have found the following query that works:

    MATCH p=(a)-[*1..2]-(b)
    where ID(a) = "node1"
    and ID(b) = "node2"
    return count(distinct(nodes(p)))

If there is a better way to do this I'm all ears!

--------------------------------------------------
OffsetDateTime date object not getting stored in db the way date is set in object
I have a model which has startDateTime field which is storing DateTime in OffsetDateTime format.

```
startDateTime: 2023-07-25T04:40:46.143-08:00
```

However when we store the above object in our PostgreSQL db on GCP, it is getting stored in below format:  
`2023-07-25 12:40:46.143+00`

It looks like it is adjusting the above object to UTC time.  
But my requirement is, it sohuld store in the same format which is there in object and **should not adjust** to UTC time.

I explored the methods given [here](https://docs.oracle.com/javase/8/docs/api/java/time/OffsetDateTime.html), but none of the methods is satisfying my requirement.

Can someone please suggest if there is a way to acheive this. Any help would be appreciated.

Code I used and input is ```startDateTime: 2023-07-25T12:40:46.143Z```
and ``` &quot;timeZoneOffset&quot;: &quot;UTC-08:00&quot; ```. I am converting input to expected OffsetDateTime based on timezoneOffset value.
```
 val actualDateTime: OffsetDateTime = OffsetDateTime.parse(startDateTime)
            val zoneOffset: ZoneOffset = ZoneOffset.of(timeZoneOffset.replace(&quot;UTC&quot;, &quot;&quot;))
            val expectedDateTime: OffsetDateTime = actualDateTime.withOffsetSameInstant(zoneOffset)
            return expectedDateTime.toString()
```

DDL:
```
CREATE TABLE IF NOT EXISTS TRANSACTION
(
    id                uuid                     DEFAULT,
    start_date_time   TIMESTAMP WITH TIME ZONE NOT NULL,
    end_date_time     TIMESTAMP WITH TIME ZONE NOT NULL,
    currency          VARCHAR(5)               NOT NULL,
    country           VARCHAR(50)              NOT NULL,
    created_at        TIMESTAMP WITH TIME ZONE NOT NULL,
    created_by        VARCHAR (255)            NOT NULL,
    startDateTime     TIMESTAMP WITH TIME ZONE NOT NULL
)
```
||||||||||||||Your best practice is to think of time zone as a presentation attribute and the timestamp itself as an instant in time.  If your business need requires that you preserve the timezone of the input field, then you must store that in another field outside of the timestamp.

The type "timestamp with timezone" is used by postgresql only to interpret the timezone offset from an input string.  That timezone data is not preserved by the database.  The timestamp is always stored in UTC, and the time zone is immediately lost.

--------------------------------------------------
Chrome Devtools formatter for javascript proxy
I&#39;ve recently started using proxies in one of my projects.  The one downside of this has been that when inspecting the object in a debugger, it&#39;s now wrapped by the proxy [javascript proxy](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy).

[![enter image description here][1]][1]

Intead of seeing `[[Handler]],[[Target]],[[isRevoked]]` I would prefer to just see the object referenced by `[[Target]]`.

It&#39;s a minor inconvenience but I think that it could be solved with a [Chrome Devtools custom formatter](https://www.mattzeunert.com/2016/02/19/custom-chrome-devtools-object-formatters.html).

Seems like this would be fairly common, but I can&#39;t find any existing formatters.  Just wanted to double check that there wasn&#39;t already one out there before I go down the road of writing my own.


  [1]: https://i.stack.imgur.com/alW5J.png
||||||||||||||So it turns out this is quite difficult to achieve.  The first problem is that it's [impossible to identify a Proxy][1] without:

[A:][2] Adding a custom symbol to your proxy implementation (if you control the Proxy init code)

[B:][3] Overriding the `window.Proxy` prototype and using a Weakset to basically track every proxy init 

On top of that, there is no way to access to original `[[Target]]` object.  However, running `JSON.parse(JSON.stringify(obj))` does seems to work well for just `console.log` purposes.

Assuming you don't have control to modify the Proxy handler, this is what your solution would look like:

```
// track all proxies in weakset (allows GC)
const proxy_set = new WeakSet();
window.Proxy = new Proxy(Proxy, {
      construct(target, args) {
        const proxy = new target(args[0], args[1]);
        proxy_set.add(proxy);
        return proxy;
      },
});

window.devtoolsFormatters = [{
  header(obj: any) {
    try {
      if (!proxy_set.has(obj)) {
        return null;
      }
      return ['object', {object: JSON.parse(JSON.stringify(obj))}]; //hack... but seems to work
    } catch (e) {
      return null;
    }
},
  hasBody() {
      return false;
  },
}];
```

  [1]: https://stackoverflow.com/questions/36372611/how-to-test-if-an-object-is-a-proxy
  [2]: https://stackoverflow.com/a/37198132/800619
  [3]: https://stackoverflow.com/a/53463589/800619

--------------------------------------------------
How to select n columns from a matrix minimizing a given function
I must buy **one of each product**, but I can visit **no more then n shops**. Which n shops should I choose to spend the least amount of money? Products are not divisible, every shop have full inventory.

|           | Shop A | Shop B | Shop C |
| --------- | ------ | ------ | ------ |
| Product 1 | $10.00 | $12.00 | $9.99  |
| Product 2 | $8.50  | $9.99  | $7.99  |
| Product 3 | $15.00 | $14.50 | $16.99 |


So I need to  minimize
``df.min(axis=1).sum() ``, where df represents any combination of n columns.

Can I do better than check all the combinations by brute force? Greedy approach, or dynamic programming don&#39;t work here. Sorting columns also doesn&#39;t help, because a shop with half of its products prohibitively expensive and the other half almost free, can have a biggest total sum of its products, but still be the best candidate.
||||||||||||||This code solves the problem. Sadly I am not interested in the output itself, but in complexity. How many steps should I do when trying to simulate underlying algorithm on paper?


    import pulp
    
    # You could formulate this as in integer linear program with binary variables:
    problem = pulp.LpProblem("Product Purchase Problem", pulp.LpMinimize)
    
    # Define the decision variables
    stores = ["Store 1", "Store 2", "Store 3"]
    products = ["Product 1", "Product 2", "Product 3"]
    
    # Xij = 1 if product j is purchased at store i, else = 0;
    X = pulp.LpVariable.dicts("X", [(i, j) for i in stores for j in products], cat="Binary")
    # Si = 1 if store i is available, else = 0.
    S = pulp.LpVariable.dicts("S", stores, cat="Binary")
    
    # Define the objective function
    C = {
        ("Store 1", "Product 1"): 1,
        ("Store 1", "Product 2"): 3,
        ("Store 1", "Product 3"): 4,
        ("Store 2", "Product 1"): 3,
        ("Store 2", "Product 2"): 1,
        ("Store 2", "Product 3"): 3,
        ("Store 3", "Product 1"): 2,
        ("Store 3", "Product 2"): 3,
        ("Store 3", "Product 3"): 1,
    }
    
    # Minimize Σij CijXij, where Cij is the cost of purchasing product j from store i
    problem += pulp.lpSum([C[(i, j)] * X[(i, j)] for i in stores for j in products])
    
    # Subject to constraints
    for j in products:
    # Σi Xij = 1 for all products j (each product is purchased at some store)
        problem += pulp.lpSum([X[(i, j)] for i in stores]) == 1
        for i in stores:
    # Xij <= Si for all i,j (can only purchase at store i if store i is available)
            problem += pulp.lpSum(X[(i, j)]) <= S[i]
    
    # Subject to constraint Σi Si <= n (at most n stores available)
    problem += pulp.lpSum([S[i] for i in stores]) <= 2
    
    # Solve the problem
    problem.solve()
    
    # Print the results
    for v in problem.variables():
        print(v.name, "=", v.varValue)
    print("Total Cost =", pulp.value(problem.objective))

--------------------------------------------------
Django - Search Query Results Loading Incorrectly
On my search results page for some reason when a logged in user makes a search query in my gaming application all the results display instead of the query itself.

What needs to get fixed with what I currently have?

I’m building a gaming application where logged in users have access to play different games based on their rank in our application… so when a user makes a search for lets say the letter ``d`` all the unlocked games and locked games that have the letter ``d`` should display. I&#39;m currently getting that but the search query shows all results that don&#39;t inlcude letter ``d``. 


This is only happening when a user is logged in. When a user is logged out the search works correctly, a search query for ``d`` will show results of every game with the character ``d`` in it. 

Any help is gladly appreciated!

Thanks!

Below is my code. 

**models.py**

    class Game_Info(models.Model):
        id = models.IntegerField(primary_key=True, unique=True, blank=True, editable=False)
        game_title = models.CharField(max_length=100, null=True)
        game_rank = models.IntegerField(default=1)
        game_image = models.ImageField(default=&#39;default.png&#39;, upload_to=&#39;game_covers&#39;, null=True, blank=True)
    
    class User_Info(models.Model):
        id = models.IntegerField(primary_key=True, blank=True)
        image = models.ImageField(default=&#39;/profile_pics/default.png&#39;, upload_to=&#39;profile_pics&#39;, null=True, blank=True)
        user = models.OneToOneField(settings.AUTH_USER_MODEL,blank=True, null=True, on_delete=models.CASCADE)
        rank = models.IntegerField(default=1)    


**views.py**

    def is_valid_queryparam(param):
        return param != &#39;&#39; and param is not None
    
    def search_filter_view(request):
        user_profile_games_filter = Game_Info.objects.all()
        user_profile = User_Info.objects.all()
        title_query = request.POST.get(&#39;q&#39;)  
    
        if is_valid_queryparam(title_query):
            user_profile_games_filter = user_profile_games_filter.filter(game_title__icontains=title_query)
    
        if request.user.is_authenticated:
            user_profile = User_Info.objects.filter(user=request.user)
            user_profile_game_obj = User_Info.objects.get(user=request.user)
            user_profile_rank = int(user_profile_game_obj.rank)
    
    
            user_profile_games_filter = Game_Info.objects.annotate(
                user_unlocked_game=Case(
                    When(game_rank__lte=user_profile_rank, then=Value(True)),
                    default=Value(False),
                    output_field=BooleanField()
                )
            )
    
            context = {
                &#39;user_profile&#39;: user_profile,  
                &#39;user_profile_games_filter&#39;: user_profile_games_filter,
                &#39;title_query&#39; : title_query
            }
    
        else:
            context = {
                &#39;user_profile&#39;: user_profile,
                &#39;user_profile_games_filter&#39;: user_profile_games_filter,
                &#39;title_query&#39; : title_query
           }
    
        return render(request, &quot;search_results.html&quot;, context)


**search.html**

    &lt;h1&gt;Results for &amp;#34;{{ title_query }}&amp;#34;&lt;/h1&gt;
    
    
                {% for content in user_profile_games_filter %}
                    {% if content.user_unlocked_game %}
                            &lt;!-- unlocked games logic --&gt;
                            &lt;a class=&quot;game-tile-container&quot; href=&quot;{% url &#39;detail&#39; content.pk %}&quot;&gt;
                                &lt;li class=&quot;results_info&quot;&gt;
                                    &lt;img src= &quot;{{ content.game_image.url }}&quot;&gt; 
                                    &lt;span class=&quot;results_title&quot;&gt;{{ content.game_title }}&lt;/span&gt;
                                &lt;/li&gt;
                            &lt;/a&gt;
            
                            {% else %}
                            &lt;!-- locked games logic --&gt;
                            &lt;a class=&quot;game-tile-container&quot; href=&quot;{% url &#39;detail&#39; content.pk %}&quot;&gt;
                                &lt;li class=&quot;results_info&quot;&gt;
                                    &lt;div class=&quot;locked_game&quot;&gt;
                                        &lt;img class=&quot;lock-img&quot; src={% static &#39;images/treasure-chest-closed-alt.png&#39; %} /&gt;
                                        &lt;img src= &quot;{{ content.game_image.url }}&quot;&gt; 
                                        &lt;button class=&quot;level-up&quot;&gt;Reach level {{ content.game_rank }} to unlock&lt;/button&gt;
                                    &lt;/div&gt;
                                    &lt;span class=&quot;results_title&quot;&gt;{{ content.game_title }}&lt;/span&gt;
                
                                &lt;/li&gt;
                            &lt;/a&gt;
                    {% endif %}
                {% endfor %}


  

||||||||||||||Issue:

    user_profile_games_filter = Game_Info.objects.annotate(
                user_unlocked_game=Case(
                    When(game_rank__lte=user_profile_rank, then=Value(True)),
                    default=Value(False),
                    output_field=BooleanField()
                )
            )

**SOLUTION** for anyone new; found after a long search and talk in the comments!:

Create a new variable where you store a list of the ID's of the filtered objects, in this case is the variable `user_profile_games_filter`:

    NEW_VARIABLE = user_profile_games_filter.values_list('id', flat=True)

Next, with the filtered objects, assign it to the desired object, making sure that you filter the id using `id__in` and it should be equal to the list of IDs. No loop needed!

    user_profile_games_filter = Game_Info.objects.filter(id__in=NEW_VARIABLE).annotate(...)

--------------------------------------------------
How to extract the Vector from an OpenAI Embeddings Call?
I use nearly the same code as here in this Git Repo to get Embeddings from OpenAI:
https://gist.github.com/limcheekin/997de2ae0757cd46db796f162c3dd58c

    oai = OpenAI(
    # This is the default and can be omitted
    api_key=&quot;sk-.....&quot;,
    )

    def get_embedding(text_to_embed, openai):
       
        response = openai.embeddings.create(
            model= &quot;text-embedding-ada-002&quot;,
            input=[text_to_embed]
        )
        
        return response
    
    embedding_raw = get_embedding(text,oai)

According to the Git Repo the Vector should be in `response[&#39;data&#39;][0][&#39;embedding&#39;]`. But it isn&#39;t in my case.

When I print the response Variable, I got this:

    print(embedding_raw)

Output: 

    CreateEmbeddingResponse(data=[Embedding(embedding=[0.009792150929570198, -0.01779201813042164, 0.011846082285046577, -0.0036859565880149603, -0.0013213189085945487, 0.00037509595858864486,..... -0.0121011883020401, -0.015751168131828308], index=0, object=&#39;embedding&#39;)], model=&#39;text-embedding-ada-002&#39;, object=&#39;list&#39;, usage=Usage(prompt_tokens=360, total_tokens=360))


Sorry, I&#39;m new to python, but how can I access the vector data?
||||||||||||||Simply return just the embedding vector as follows:

    def get_embedding(text_to_embed, openai):
       
        response = openai.embeddings.create(
            model= "text-embedding-ada-002",
            input=[text_to_embed]
        )
        
        return response.data[0].embedding # Change this

    embedding_raw = get_embedding(text,oai)

--------------------------------------------------
React Redux Reducer: &#39;this.props.tasks.map is not a function&#39; error
I am making a React Redux example; however, I ran into an issue and get the error below:

&gt; TypeError: this.props.tasks.map is not a function
[Learn More]

I have tried many things and I cannot seem to understand why this is not working. I believe it is when the allReducers maps the tasks from the Tasks function. I have fixed this error back and forth but then it would complain it was undefined. I would fix that and loop back to this issue. Any help would be appreciated. Im sure I am making a simple mistake. Below are my following files

**App.js**
&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    import React from &#39;react&#39;;
    import TaskBoard from &quot;../containers/task-board&quot;;
    require(&#39;../../scss/style.scss&#39;);

    const App = () =&gt; (
        &lt;div&gt;
            &lt;h2&gt;Task List&lt;/h2&gt;
            &lt;hr /&gt;
            &lt;TaskBoard/&gt;
        &lt;/div&gt;
    );

    export default App;


&lt;!-- end snippet --&gt;

**index.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

        import {combineReducers} from &#39;redux&#39;;
        import {Tasks} from &#39;./reducer-tasks&#39;;
        const allReducers = combineReducers({
            tasks: Tasks
        });

        export default allReducers

&lt;!-- end snippet --&gt;

**task-board.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

        import React, {Component} from &#39;react&#39;;
        import {bindActionCreators} from &#39;redux&#39;;
        import {connect} from &#39;react-redux&#39;;
        import {deleteTaskAction} from &#39;../actions/ActionIndex&#39;;
        import {editTaskAction} from &#39;../actions/ActionIndex&#39;;
        class TaskBoard extends Component {
            renderList() {
                return this.props.tasks.map((task) =&gt; {
                    if(task.status == &quot;pending&quot;){
                        return (&lt;li key={task.id}&gt;
                            {task.id} {task.description}
                            &lt;button type=&quot;button&quot;&gt;Finish&lt;/button&gt;
                            &lt;button type=&quot;button&quot;&gt;Edit&lt;/button&gt;
                            &lt;button onClick={() =&gt; this.props.deleteTask(task)} type=&quot;button&quot;&gt;Delete&lt;/button&gt;
                        &lt;/li&gt;
                    );
                }
            });
        }
        render() {
            if (!this.props.tasks) {
                console.log(this.props.tasks);
                return (&lt;div&gt;You currently have no tasks, please first create one...&lt;/div&gt;);
            }
            return (
                &lt;div&gt;
                    {this.renderList()}
                &lt;/div&gt;
            );
        }
    }
        function mapStateToProps(state) {
            return {
                tasks: state.tasks
            };
        }
        function matchDispatchToProps(dispatch){
            return bindActionCreators(
            {
                deleteTask: deleteTaskAction,
                editTask: editTaskAction
            }, dispatch)
        }
        export default connect(mapStateToProps,matchDispatchToProps)(TaskBoard);

&lt;!-- end snippet --&gt;

**reducer-tasks.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    const initialState = {
    	tasks: [
            {
                id: 1,
                description: &quot;This is a task&quot;,
                status: &quot;pending&quot;
            },
            {
                id: 2,
                description: &quot;This is another task&quot;,
                status: &quot;pending&quot;
            },
            {
                id: 3,
                description: &quot;This is an easy task&quot;,
                status: &quot;pending&quot; 

            }
    	]
    }

    export function Tasks (state = initialState, action) {
        switch (action.type) {
            case &#39;ADD_TASK&#39;:
                return Object.assign({}, state, {
                	tasks: [
                		...state.tasks,
                		{
                			description: action.text,
                			status: action.status
                		}
                	]
                })
                break;

            case &#39;EDIT_TASK&#39;:
                return action.payload;
                break;

            case &#39;DELETE_TASK&#39;:
                return Object.assign({}, state, {
                	status: action.status
                })
                break;
        }

        return state;
    }

&lt;!-- end snippet --&gt;

**actionindex.js**
&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;


        export const addTaskAction = (task) =&gt; {
            return {
                type: &#39;ADD_TASK&#39;,
                text: &quot;Here is a sample description&quot;,
                status: &quot;pending&quot;
            }
        };
        export const deleteTaskAction = (task) =&gt; {
            return {
                type: &#39;DELETE_TASK&#39;,
                status: &quot;deleted&quot;
            }
        };
        export const editTaskAction = (task) =&gt; {
            return {
                type: &#39;EDIT_TASK&#39;,
                payload: task
            }
        };

&lt;!-- end snippet --&gt;


||||||||||||||It's because the function 'map' can only be used for arrays, not for objects.

If you print out this.props.tasks in the render function of task-board.js you'll see that it's an OBJECT which contains the tasks array, not the actual tasks array itself.

So to fix this it's quite easy, instead of:

        return this.props.tasks.map((task) => {

it's 

        return this.props.tasks.tasks.map((task) => {

Then it works

--------------------------------------------------
Google Chrome Extension - waiting until page loads
In my Google Chrome Extension, I have a [Content Script][1] (content.js) and a [Background Page][2] (background.html). I have context.js checking for a keyword that appears on the page. However, I want to wait until the page is fully loaded until I search the page, because the keyword may occur at the bottom of the page. 

See [Page action by content][3] sandwich example ([files][4]), this is basically what I am doing. If you load the extension you&#39;ll see the extension only works when the word &quot;sandwich&quot; appears at the top of the page.


  [1]: http://code.google.com/chrome/extensions/content_scripts.html
  [2]: http://code.google.com/chrome/extensions/background_pages.html
  [3]: http://code.google.com/chrome/extensions/samples.html
  [4]: http://src.chromium.org/viewvc/chrome/trunk/src/chrome/common/extensions/docs/examples/api/pageAction/pageaction_by_content/
||||||||||||||Try to add this to the "content_scripts" part of your manifest.json file.
```json
"run_at": "document_end"
```
https://developer.chrome.com/docs/extensions/mv3/content_scripts/

--------------------------------------------------
No matching constructor for initialization of &#39;v8::ScriptOrigin&#39; || candidate constructor (the implicit move constructor) not viable
I am using invoking a  class inside my project src c++ file as
```
ScriptOrigin script_origin(
        isolate_,
        script_name,
        Integer::New(isolate_, 0),                            // line offset
        Integer::New(isolate_, 0),                            // column offset
        False(isolate_),                                      // isCrossOrigin
        Local&lt;Integer&gt;(),                                     // scriptId
        Local&lt;Value&gt;(),                                       // sourceMapURL
        False(isolate_),                                      // isOpaque
        False(isolate_),                                      // isWASM
        su.IsNoModule() ? False(isolate_) : True(isolate_));
  ```
Tried hardcoding the Local&lt;Integer&gt;() as a random integer value also . Still getting the below error

    error: no matching constructor for initialization of &#39;v8::ScriptOrigin&#39;
          ScriptOrigin script_origin(
    note: candidate constructor (the implicit copy constructor) not viable: requires 1 argument, but 10 were provided 
    note: candidate constructor (the implicit move constructor) not viable: requires 1 argument, but 10 were provided


Am i invoking the class wrongly? i do not understand the mismatch arguments note


This is the source code of the class inside a v8 include file

```
 class V8_EXPORT ScriptOrigin {
 public:
  V8_INLINE ScriptOrigin(Isolate* isolate, Local&lt;Value&gt; resource_name,
                         int resource_line_offset = 0,
                         int resource_column_offset = 0,
                         bool resource_is_shared_cross_origin = false,
                         int script_id = -1,
                         Local&lt;Value&gt; source_map_url = Local&lt;Value&gt;(),
                         bool resource_is_opaque = false, bool is_wasm = false,
                         bool is_module = false,
                         Local&lt;Data&gt; host_defined_options = Local&lt;Data&gt;())
      : v8_isolate_(isolate),
        resource_name_(resource_name),
        resource_line_offset_(resource_line_offset),
        resource_column_offset_(resource_column_offset),
        options_(resource_is_shared_cross_origin, resource_is_opaque, is_wasm,
                 is_module),
        script_id_(script_id),
        source_map_url_(source_map_url),
        host_defined_options_(host_defined_options) {
    VerifyHostDefinedOptions();
  }
};
```


||||||||||||||Many of the parameters you're passing have the wrong type. For example, `Integer::New(isolate_, 0)` produces a `v8::Local<v8::Integer>`, not a C++ `int`. Similarly, `False(isolate_)` produces a `v8::Local<v8::Boolean>`, not a C++ `bool`.

This isn't really related to V8: whenever working with C++, you need to care about the types of your values.

The part where the compiler says ...
```none
note: candidate constructor (the implicit copy constructor) not 
viable: requires 1 argument, but 10 were provided 

note: candidate constructor (the implicit move constructor) not 
viable: requires 1 argument, but 10 were provided
```
... is just about that it _tried_ to match the 10 arguments you supplied with the copy constructor and move constructor (both of which expect 1 argument only), but that failed.

--------------------------------------------------
SSIS Excel Destination Editor closes unexpectedly
I&#39;m fairly new to SSIS, but what I&#39;m trying to do should be simple:

I have a Data Flow task that has an OLE DB Source feeding into an Excel Destination. The issue though, is I can&#39;t configure the Excel Destination correctly. I&#39;m able to connect my Excel connection manager, but when I hit the &quot;New...&quot; button next to the &quot;Name of the Excel sheet&quot; dropdown, the Excel Destination Editor window just closes instead of opening a different dialog. In the image below, I highlighted the button that&#39;s closing the window.

![SSIS Button](https://i.stack.imgur.com/BR2TV.png)

In general, I&#39;m following this guide [How to use SSIS to Export to Excel][1] (current step is just above the second to last image in the article).


  [1]: http://knowlton-group.com/using-ssis-to-export-data-to-excel/
||||||||||||||I think the issue lies with the Excel connection instead. Once I changed the output file to .xls instead of .xlsx and changed the connection to Excel 97-2003, I was able to create a new Excel Sheet for the file.

--------------------------------------------------
Is it possible to display toolbar options below textarea in Quilljs editor?
How to display toolbar below `textarea`.

My code: 
    

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    var quill = new Quill(&#39;#txtMessage&#39;, {
      theme: &#39;snow&#39;,
      modules: {
        toolbar: {
          container: [
            [&#39;bold&#39;, &#39;italic&#39;, &#39;underline&#39;],
            [{
              &#39;list&#39;: &#39;ordered&#39;
            }, {
              &#39;list&#39;: &#39;bullet&#39;
            }],
            [&#39;clean&#39;],
            [&#39;code-block&#39;],
            [{
              &#39;variables&#39;: [&#39;{Name}&#39;, &#39;{Email}&#39;]
            }],
          ],
          handlers: {
            &quot;variables&quot;: function(value) {
              if (value) {
                const cursorPosition = this.quill.getSelection().index;
                this.quill.insertText(cursorPosition, value);
                this.quill.setSelection(cursorPosition + value.length);
              }
            }
          }
        }
      }
    });

    // Variables
    const placeholderPickerItems = Array.prototype.slice.call(document.querySelectorAll(&#39;.ql-variables .ql-picker-item&#39;));
    placeholderPickerItems.forEach(item =&gt; item.textContent = item.dataset.value);
    document.querySelector(&#39;.ql-variables .ql-picker-label&#39;).innerHTML = &#39;Variables&#39; + document.querySelector(&#39;.ql-variables .ql-picker-label&#39;).innerHTML;

&lt;!-- language: lang-html --&gt;

    &lt;script src=&quot;//cdn.quilljs.com/1.3.6/quill.js&quot;&gt;&lt;/script&gt;
    &lt;link href=&quot;//cdn.quilljs.com/1.3.6/quill.snow.css&quot; rel=&quot;stylesheet&quot;/&gt;
    &lt;link href=&quot;//cdn.quilljs.com/1.3.6/quill.bubble.css&quot; rel=&quot;stylesheet&quot;/&gt;

    &lt;div id=&quot;txtMessage&quot;&gt;&lt;/div&gt;

&lt;!-- end snippet --&gt;

Output for the above code:
[![enter image description here][1]][1]

I want output as follows:
[![enter image description here][2]][2]
  [1]: https://i.stack.imgur.com/PQtJv.png
  [2]: https://i.stack.imgur.com/bQlmC.png

How to accomplish above result.
||||||||||||||I can't see why not use only css.

Something like this:

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    var quill = new Quill('#editor-container', {
      modules: {
        toolbar: [
          [{
            header: [1, 2, false]
          }],
          ['bold', 'italic', 'underline'],
          ['image', 'code-block']
        ]
      },
      placeholder: 'Compose an epic...',
      theme: 'snow' // or 'bubble'
    });

<!-- language: lang-css -->

    #editor-container {
      height: 375px;
    }

    .editor-wrapper {
      position: relative;
    }

    .ql-toolbar {
      position: absolute;
      bottom: 0;
      width: 100%;
      transform: translateY(100%);
    }

<!-- language: lang-html -->

    <script src="//cdn.quilljs.com/1.3.6/quill.js"></script>
    <link href="//cdn.quilljs.com/1.3.6/quill.snow.css" rel="stylesheet"/>
    <link href="//cdn.quilljs.com/1.3.6/quill.bubble.css" rel="stylesheet"/>
    <div class="editor-wrapper">
      <div id="editor-container">
      </div>
    </div>

<!-- end snippet -->

https://codepen.io/moshfeu/pen/wXwqmg

--------------------------------------------------
Can I install Visual Studio without Admin rights?
I use a machine where I don&#39;t have administrator rights. I&#39;ve been able to run programs without admin rights by extracting the program&#39;s .zip file to a directory I have created on my desktop. However, I can&#39;t find such a .zip file for Visual Studio.

Is there a way to install Visual Studio Community Edition without administrator rights?


||||||||||||||Practically no. Visual Studio (Express and above, excluding VS Code) consists of multiple components that must be installed as admin, and will be required for the app you're debugging to be available as system-wide component. It *might* be possible to use [ThinApp](https://www.vmware.com/products/thinapp.html) or its equivalent, but ThinApp can't even work with [VS 2010](https://www.vmware.com/support/thinapp4/doc/releasenotes_thinapp52.html) and it was by far the best of its class.

A (resource intensive) alternative to get VS on any PC will be packaging a VM with VS installed, either creating one yourself or get a [ready-made](https://developer.microsoft.com/en-us/windows/downloads/virtual-machines) ones. VirtuaBox is available as [portable fork](http://www.vbox.me/) if you can't even get Hyper-V tools installed. But this still require kernel drivers installation, which means at least one-time admin access. Depending on your internet connection & budget, it might be more practical to setup a VPS with VS installed, then remote there.

--------------------------------------------------
ggplot2: how to produce smaller points
I have a large dataset that I am plotting using a scatter plot. These points have a unique combination of x,y and therefore they don&#39;t overlap, but some of them are very close to each other therefore I&#39;m plotitng them with small size.  

1- How to produce smaller point symbols (smaller `size`) so that the areas are proportional. In this example, the last point does not have an area proportional to the `size`. I was expecting it 10 smaller than the middle one e.g.:

    df &lt;- data.frame(c1 = 1:3, c2 = c(1,1,1))
    ggplot(df) + geom_point(aes(x= c1, y = c2), size = c(1, 0.1, 0.01)) 

2- How does the `size` in ggplot2 matches the R graphics `cex` argument e.g.:  `plot(df$c2 ~ df$c1, cex = c(1, 0.1, 0.01))`. 
Thanks
||||||||||||||There is a `size = ` argument to `geom_point`, but you either specify a size for all points:

    + geom_point(size = 0.5)

Or you map the size to one of the columns in your data using `aes`:

    + geom_point(aes(size = c2))

In the latter case, you can control the range of sizes using `scale_size_continuous`. The default is min = 1, max = 6. To get _e.g._ min = 2, max = 8:

    + geom_point(aes(size = c2)) + scale_size_continuous(range = c(2, 8))

- Note that the "ggplot2 way" is to map data to geoms, not to assign values to each observation
- and no, size here is different to `cex`

--------------------------------------------------
Can&#39;t close Excel completely using win32com on Python
This is my code, and I found many answers for [VBA][1], .NET framework and is pretty strange. When I execute this, Excel closes.

    from win32com.client import DispatchEx
    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wbs.Close()
    excel.Quit()
    wbs = None
    excel = None # &lt;-- Excel Closes here

But when I do the following, it does not close.

    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wb = wbs.Open(&#39;D:\\Xaguar\\A1.xlsm&#39;)
    wb.Close(False)
    wbs.Close()
    excel.Quit()
    wb = None
    wbs = None
    excel = None  # &lt;-- NOT Closing !!!

I found some possible answer in Stack Overflow question *[Excel process remains open after interop; traditional method not working][2]*. The problem is that is not Python, and I don&#39;t find `Marshal.ReleaseComObject` and `GC`. I looked over all the demos on `...site-packages/win32com` and others.

Even it does not bother me if I can just get the PID and kill it.

I found a workaround in *[Kill process based on window name (win32)][3]*.

May be not the proper way, but a workround is:

    def close_excel_by_force(excel):
        import win32process
        import win32gui
        import win32api
        import win32con

        # Get the window&#39;s process id&#39;s
        hwnd = excel.Hwnd
        t, p = win32process.GetWindowThreadProcessId(hwnd)
        # Ask window nicely to close
        win32gui.PostMessage(hwnd, win32con.WM_CLOSE, 0, 0)
        # Allow some time for app to close
        time.sleep(10)
        # If the application didn&#39;t close, force close
        try:
            handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, p)
            if handle:
                win32api.TerminateProcess(handle, 0)
                win32api.CloseHandle(handle)
        except:
            pass

    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wb = wbs.Open(&#39;D:\\Xaguar\\A1.xlsm&#39;)
    wb.Close(False)
    wbs.Close()
    excel.Quit()
    wb = None
    wbs = None
    close_excel_by_force(excel) # &lt;--- YOU #@#$# DIEEEEE!! DIEEEE!!!

  [1]: http://en.wikipedia.org/wiki/Visual_Basic_for_Applications
  [2]: https://stackoverflow.com/questions/8977571/excel-process-remains-open-after-interop-traditional-method-not-working
  [3]: http://python.6.n6.nabble.com/Kill-process-based-on-window-name-win32-td1042063.html

||||||||||||||Try this:

    wbs.Close()
    excel.Quit()
    del excel # this line removed it from task manager in my case


--------------------------------------------------
Log4j2 createOnDemand=&quot;true&quot; does not allow creation of new file on a daily basis
`Log4j2 createOnDemand=&quot;true&quot;` does not allow creation of new file on a daily basis in-spite of using `RollingFile Appenders` with `TimeBasedTriggeringPolicy`.

Below is my `log4j2.xml` file.

I have two `appenders`, one is for all logs, another is for a custom purpose, which needs to be generated only on demand, but the `createOnDemand` is overriding the Rolling nature of the log and it is not allowing to create new log file for the custom log.

    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;Configuration status=&quot;WARN&quot;&gt;
    	&lt;Appenders&gt;
    		&lt;RollingFile name=&quot;App&quot; 
    				fileName=&quot;app.log&quot; 
    				filePattern=&quot;app.%d{yyyy-MM-dd}.log&quot;&gt;
    			&lt;PatternLayout pattern=&quot;[%t] %d{yyyy-MM-dd HH:mm:ss,SSS zzz} %-5p %l - %m%n&quot; /&gt;
    			&lt;Policies&gt;
    				&lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;
    			&lt;/Policies&gt;
    		&lt;/RollingFile&gt;
    		&lt;RollingFile name=&quot;custom&quot;
    				 fileName=&quot;appCustom.log&quot;
    				 filePattern=&quot;appCustom.%d{yyyy-MM-dd-HH-mm}.log&quot;
    				 createOnDemand=&quot;true&quot;&gt;
    		&lt;PatternLayout pattern=&quot;[%t] %d{yyyy-MM-dd HH:mm:ss,SSS zzz} %-5p %l - %m%n&quot; /&gt;
    		&lt;Policies&gt;
    			&lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;
    		&lt;/Policies&gt;
    		&lt;/RollingFile&gt;
    	&lt;/Appenders&gt;
    	&lt;Loggers&gt;
    		&lt;Logger name=&quot;AppLogger&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
    				&lt;AppenderRef ref=&quot;App&quot;/&gt;
    			&lt;/Logger&gt;
    		&lt;Logger name=&quot;customLogger&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
    			&lt;AppenderRef ref=&quot;custom&quot;/&gt;
    		&lt;/Logger&gt;
    		&lt;Root level=&quot;info&quot;&gt;
    				&lt;AppenderRef ref=&quot;file&quot; /&gt;
    		&lt;/Root&gt;
    	&lt;/Loggers&gt;
    &lt;/Configuration&gt;
||||||||||||||I have found the fix for the above issue.
This was an existing bug in lo4j2 which is fixed in the version - [2.13.1][1]

Below are the links :

https://issues.apache.org/jira/browse/LOG4J2-2759

https://logging.apache.org/log4j/2.x/changes-report.html#a2.13.3

I was using 2.11.0

Upgrading resolved my issue.


  [1]: https://blogs.apache.org/logging/entry/log4j-2-13-1-released

--------------------------------------------------
How to get the Slack DM channel ID of the Slack App
I have created a simple Slack App app where the only purpose is to send a message to a channel. I understand that there is the `conversations.list` API to list all public channels to get the correct ID. 

However, as a first step, I just want to send the message to the app channel itself. If I use the D... ID it works as expected. No invite by the channel is needed. But how do I get this ID? `conversations.list` only returns publich channels, but no the app channel itself.
||||||||||||||In Slack, there is no such thing as an app's channel. There is a DM channel between every user and your app/bot. In these terms, to send a DM message from your app/bot to the user, you need to know `ID` of this user and specify it as a `channel` argument of the `postMessage` API request.

--------------------------------------------------
How do I make a custom class that&#39;s serializable with dataclasses.asdict()?
I&#39;m trying to use a dataclass as a (more strongly typed) dictionary in my application, and found this strange behavior when using a custom type subclassing `list` within the dataclass. I&#39;m using Python 3.11.3 on Windows.

```python
from dataclasses import dataclass, asdict

class CustomFloatList(list):
    def __init__(self, args):
        for i, arg in enumerate(args):
            assert isinstance(arg, float), f&quot;Expected index {i} to be a float, but it&#39;s a {type(arg).__name__}&quot;

        super().__init__(args)

    @classmethod
    def from_list(cls, l: list[float]):
        return cls(l)

@dataclass
class Poc:
    x: CustomFloatList

p = Poc(x=CustomFloatList.from_list([3.0]))
print(p)  # Prints Poc(x=[3.0])
print(p.x)  # Prints [3.0]
print(asdict(p))  # Prints {&#39;x&#39;: []}
```

This does not occur if I use a regular list[float], but I&#39;m using a custom class here to enforce some runtime constraints.

How do I do this correctly?

I&#39;m open to just using `.__dict__` directly, but I thought `asdict()` was the more &quot;official&quot; way to handle this

A simple modification makes the code behave as expected, but is slightly less efficient:

```python
from dataclasses import dataclass, asdict

class CustomFloatList(list):
    def __init__(self, args):
        dup_args = list(args)
        for i, arg in enumerate(dup_args):
            assert isinstance(arg, float), f&quot;Expected index {i} to be a float, but it&#39;s a {type(arg).__name__}&quot;

        super().__init__(dup_args)

    @classmethod
    def from_list(cls, l: list[float]):
        return cls(l)

@dataclass
class Poc:
    x: CustomFloatList

p = Poc(x=CustomFloatList.from_list([3.0]))
print(p)
print(p.x)
print(asdict(p))
```
||||||||||||||If you look at the [source code of `asdict`](https://github.com/python/cpython/blob/d334122d2295a4863384676a3ce313a831b12335/Lib/dataclasses.py#L1364), you'll see that passes a generator expression that recursively calls itself on the elements of a list when it encounters a list:
```
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)
```



 But *your implementation depletes any iterator it gets in `__init__` before the `super` call*. 

Don't do that. You'll have to "cache" the values if you want to use the superclass constructor. Something like:

```
class CustomFloatList(list):
    def __init__(self, args):
        args = list(args)
        for i, arg in enumerate(args):
            assert isinstance(arg, float), f"Expected index {i} to be a float, but it's a {type(arg).__name__}"

        super().__init__(args)
```

Or perhaps:

```
class CustomFloatList(list):
    def __init__(self, args):
        super().__init__(args)
        for i, arg in enumerate(self):
            if not isinstance(arg, float):
                raise TypeError(f"Expected index {i} to be a float, but it's a {type(arg).__name__}")
```

--------------------------------------------------
How would I run PHP code when input box changes?
I&#39;m trying to do something with PHP when a text box changes. I can do this with JavaScript with onchange but that doesn&#39;t work with PHP.


I already have a PHP function in my already existing PHP code. When a text box value changes, I want it to run the function.

||||||||||||||The reason you can do it in JavaScript is because you're in the same scope...the client side. PHP is a server-side language, so it has no notion of what is happening on the client-side, unless you explicitly tell it.

To tell PHP to evaluate, and possible return the response of a function call, you have to pass it the value of the input using a network call, such as a fetch or ajax request.

Your question shows that you really don't understand PHP, and you need to learn the fundamentals of it. PHP does not run client-side, and JavaScript does not run server-side (again, unless you're using Node, in which case you wouldn't be using PHP).

--------------------------------------------------
Listener method using Spring and ActiveMQ throws &quot;Property name cannot be null&quot; warnings repeatedly
I&#39;ve attempted to implement ActiveMQ using Spring in two places. Both implementations have had this issue. Sending either an HTTP Request using postman or directly entering a message in the ActiveMQ console causes the following Error to be repeated infinitely:

```
2024-02-02T17:08:56.317-06:00 ERROR 2264 --- [ntContainer#0-1] c.j.a.config.JmsConfig$JMSErrorHandler   : Error in listener
```

```
org.springframework.jms.listener.adapter.ListenerExecutionFailedException: Listener method &#39;public void com.jackhodge.activemqlearning.consumer.component.MessageConsumer.messageListener(com.jackhodge.activemqlearning.model.SystemMessage)&#39; threw exception
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:118) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.onMessage(MessagingMessageListenerAdapter.java:84) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:783) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:741) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:719) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:333) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:270) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1258) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1248) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:1141) ~[spring-jms-6.1.3.jar:6.1.3]
  at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]
Caused by: java.lang.NullPointerException: Property name cannot be null
  at org.apache.activemq.command.ActiveMQMessage.getObjectProperty(ActiveMQMessage.java:575) ~[activemq-client-jakarta-5.18.3.jar:5.18.3]
  at org.apache.activemq.command.ActiveMQMessage.getStringProperty(ActiveMQMessage.java:683) ~[activemq-client-jakarta-5.18.3.jar:5.18.3]
  at org.springframework.jms.support.converter.MappingJackson2MessageConverter.getJavaTypeForMessage(MappingJackson2MessageConverter.java:456) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.support.converter.MappingJackson2MessageConverter.fromMessage(MappingJackson2MessageConverter.java:241) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener.extractMessage(AbstractAdaptableMessageListener.java:250) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter.extractPayload(AbstractAdaptableMessageListener.java:472) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter$LazyResolutionMessage.unwrapPayload(AbstractAdaptableMessageListener.java:539) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter$LazyResolutionMessage.getPayload(AbstractAdaptableMessageListener.java:521) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.annotation.support.PayloadMethodArgumentResolver.resolveArgument(PayloadMethodArgumentResolver.java:122) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:118) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:147) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:115) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:110) ~[spring-jms-6.1.3.jar:6.1.3]
  ... 10 common frames omitted
```

What stands out is `Error in listener [...] Property name cannot be null`.

The error still occurs when I don&#39;t do anything in my listener, and Logs/Breakpoints in the `messageListener` aren&#39;t sent nor activated.

Here&#39;s the simple app in which the error is occurring:

**JmsConfig**

```java
@Configuration
@EnableJms
public class JmsConfig {

    Logger logger = LoggerFactory.getLogger(JMSErrorHandler.class);
    
    @Bean
    public DefaultJmsListenerContainerFactory jmsListenerContainerFactory(
            ConnectionFactory connectionFactory,
            DefaultJmsListenerContainerFactoryConfigurer configurer,
            JMSErrorHandler defaultErrorHandler){
        DefaultJmsListenerContainerFactory jmsListenerContainerFactory = new DefaultJmsListenerContainerFactory();
    
        jmsListenerContainerFactory.setConnectionFactory(connectionFactory);
        jmsListenerContainerFactory.setConcurrency(&quot;1&quot;); // start w/ 5 consumers; auto-scale to 10 consumers as necessary
    
        jmsListenerContainerFactory.setErrorHandler(defaultErrorHandler);
        jmsListenerContainerFactory.setMessageConverter(this.jacksonJmsMessageConverter());
    
        configurer.configure(jmsListenerContainerFactory, connectionFactory);
        return jmsListenerContainerFactory;
    
    }
    
    
    @Service
    public class JMSErrorHandler implements ErrorHandler {
        @Override
        public void handleError(Throwable t) {
            logger.error(&quot;Error in listener &quot;, t);
        }
    }
    
    @Bean
    public MessageConverter jacksonJmsMessageConverter() {
        MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();
        converter.setTargetType(MessageType.TEXT);
        converter.setObjectMapper(new ObjectMapper());
        return converter;
    }

**PublishController**

```java
package com.jackhodge.activemqlearning.controller;

import com.jackhodge.activemqlearning.model.SystemMessage;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.jms.core.JmsTemplate;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RestController;

@RestControllerpublic class PublishController {// helperclass for sending/receiving messages
    // Spring JMS abstraction API: Distills and simplifies process; abstracts away boilerplate code
    private JmsTemplate jmsTemplate;

    @Autowired
    public PublishController(JmsTemplate jmsTemplate) {
        this.jmsTemplate = jmsTemplate;
    }

    // post method to trigger publishing of messages
    // Requests to here at sent to the Messaging Broker
    @PostMapping(&quot;/publishMessage&quot;)
    public ResponseEntity&lt;String&gt; publishMessage(@RequestBody SystemMessage systemMessage){
        try{
            jmsTemplate.convertAndSend(&quot;jackhodge-queue&quot;, systemMessage.toString());
            return new ResponseEntity&lt;&gt;(&quot;I, Jack Hodge, sent your message.&quot;, HttpStatus.OK);
        } catch (Exception e){
            return new ResponseEntity&lt;&gt;(e.getMessage(), HttpStatus.INTERNAL_SERVER_ERROR);
        }

    }
}
```

**SystemMessage**

```java
package com.jackhodge.activemqlearning.model;

import lombok.Setter;
import org.springframework.beans.factory.annotation.Autowired;
//import java.io.Serializable;

@Setter
public class SystemMessage {
    private String source;
    private String message;
    
    public SystemMessage(String source, String message) {
    
        this.source = source;
        this.message = message;
    }
    
    
    @Override
    public String toString() {
        return &quot;SystemMessage{&quot; +
                &quot;source=&#39;&quot; + source + &#39;\&#39;&#39; +
                &quot;, message=&#39;&quot; + message + &#39;\&#39;&#39; +
                &#39;}&#39;;
    }

}
```

**MessageConsumer**

```java
package com.jackhodge.activemqlearning.consumer.component;

import com.jackhodge.activemqlearning.model.SystemMessage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.jms.annotation.JmsListener;
import org.springframework.stereotype.Component;

@Componentpublic class MessageConsumer {

    public static final Logger LOGGER = LoggerFactory.getLogger(MessageConsumer.class);
    
    // Consumes from the Messaging broker
    @JmsListener(destination = &quot;jackhodge-queue&quot;)
    public void messageListener(SystemMessage systemMessage){
        LOGGER.info(&quot;Message Received {}&quot;, systemMessage);
    }

}
```

I&#39;m somewhat new to Swing and ActiveMQ and this problem has stumped me -- every avenue of breakpoints/logging/message sources as far as I&#39;m capable of has been tried. Thank you!

Sending this request via both Postman and the ActiveMQ console both resulted in the same Error regardless:

```
{     
    &quot;source&quot;:&quot;jeff bezo&quot;,     
    &quot;message&quot;:&quot;hello&quot; 
}
```
||||||||||||||I fixed this by setting setTypeIdPropertyName in my MessageConverter Bean:


    @Bean
    public MessageConverter jacksonJmsMessageConverter(){
            MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();
            converter.setTargetType(MessageType.TEXT);
            converter.setTypeIdPropertyName("_type");
            converter.setObjectMapper(new ObjectMapper());
            return converter;
        }




--------------------------------------------------
Start and kill background process within one Makefile recipe
Within one `make` recipe, I am trying to:
1. Run a server process in the background
2. Run a command that uses the server, in the foreground
3. Kill the background server process after foreground task completes

The below is where I am at (inspired by https://stackoverflow.com/a/30171236), but it doesn&#39;t quite work because `SERVER_PID` is currently empty (possibly related to this empty PID: https://stackoverflow.com/q/5768034).

```make
run-server:	## Run the server in the foreground.
	run server

run-kill-server:	## Run the server while using it.
	@$(MAKE) run-server&amp;
	export SERVER_PID=$! &amp;&amp; cmd-that-uses-server &amp;&amp; kill $(SERVER_PID)
```

https://stackoverflow.com/questions/7668311/makefile-run-processes-in-background is also related, except I believe it &quot;leaks&quot; the background processes, the recipes there don&#39;t try to `kill` spawned background processes
||||||||||||||> Within one make recipe, I am trying to:
> 
>  1.  Run a server process in the background
>  2.  Run a command that uses the server, in the foreground
>  3.  Kill the background server process after foreground task completes

But you're *not* doing all that in one recipe.  You're using two.

Bringing it all into one recipe would be a step in the right direction, though It will not in itself provide a complete solution.

> `SERVER_PID` is currently empty

Yes, because your recipe is setting *shell* variable `SERVER_PID`, but trying to reference a *`make`* variable of that name.  And also trying to reference a `make` variable named `!`, where you appear to want the shell variable of that name.  You need to escape your `$` by doubling it when you want to pass it through to the shell.

Additionally, each logical line of your recipe runs in a separate shell.  In the one where you define and later user `SERVER_PID`, no background process is ever run.

You probably want something more like this:
```
run-job:
        run server & export SERVER_PID=$$!; cmd-that-uses-server; kill $${SERVER_PID}
```

--------------------------------------------------
python while loop if all conditions are equal then do another random choice from list
This is my python code:
```python
import secrets
from time import sleep

ids = [{&#39;id&#39;: number} for number in range(1, 5+1)]

rand1 = secrets.choice(ids)
rand2 = secrets.choice(ids)
rand3 = secrets.choice(ids)

n = 0
while rand1[&#39;id&#39;] == rand2[&#39;id&#39;] == rand3[&#39;id&#39;]:
        n += 1
        print(&#39;Before&#39;)
        print(rand1[&#39;id&#39;], rand2[&#39;id&#39;], rand3[&#39;id&#39;])
        sleep(1)
        rand1 = secrets.choice(ids)
        rand2 = secrets.choice(ids)
        rand3 = secrets.choice(ids)
        print(&#39;After&#39;)
        print(rand1[&#39;id&#39;], rand2[&#39;id&#39;], rand3[&#39;id&#39;])
```
I&#39;m going to reach this:

&gt; do the while loop and choose a random id until none of the
&gt; rand1[&#39;id&#39;], rand2[&#39;id&#39;] and rand3[&#39;id&#39;] are equal.
&gt; 
&gt; Even two of them are equal, then do another for loop.
||||||||||||||Looping is not the right way to do this.  Just shuffle and deal:
```
import random

nums = list(range(1,5+1))
random.shuffle(nums)
ids = [{'id': n} for n in nums[:3]]
```

--------------------------------------------------
Avoiding duplicate tasks in celery broker
I want to create the following flow using celery configuration\api: 

 - Send TaskA(argB) Only if celery queue has no TaskA(argB) already pending

Is it possible? how?
||||||||||||||I cannot think of a way but to 

 1. Retrieve all executing and scheduled tasks via [`celery inspect`](http://docs.celeryproject.org/en/latest/userguide/workers.html#inspecting-workers)
    
 2. Iterate through them to see if your task is there.

check [this](https://stackoverflow.com/questions/5544629/retrieve-list-of-tasks-in-a-queue-in-celery) SO question to see how the first point is done.

good luck

--------------------------------------------------
How do I late-resolve * in .csproj files?
I have a `.csproj` file that looks like this:

````
&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;net8.0&lt;/TargetFramework&gt;
    &lt;LangVersion&gt;12.0&lt;/LangVersion&gt;
    &lt;DebugType&gt;portable&lt;/DebugType&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
	&lt;None Remove=&quot;*.dat&quot; /&gt;
    &lt;None Include=&quot;*.dat&quot; CopyToOutputDirectory=&quot;Always&quot; /&gt;
  &lt;/ItemGroup&gt;

  &lt;Target Name=&quot;PrecompileScript&quot; BeforeTargets=&quot;BeforeCompile&quot;&gt;
    &lt;Exec Command=&quot;dotnet run -c $(Configuration) --no-build --project ../DatGenerator/DatGenerator.csproj -directory $(ProjectDir)&quot; /&gt;
  &lt;/Target&gt;
&lt;/Project&gt;
````

The problem is `*.dat` is expanded too soon and doesn&#39;t actually pick up any files. How do I expand `*.dat` after the `&lt;Exec` directive that emits the `*.dat` runs? No, `-directory $(ProjectDir)/bin/$(Configuration)/$(TargetFramework)` isn&#39;t right. That doesn&#39;t work at all; see how there isn&#39;t a `&lt;OutputType&gt;exe&lt;/OutputType&gt;`. The copy build outputs built-in functionality needs to work.

Listing each `.dat` file manually is pretty bad. I need to pick up changes automatically here.
||||||||||||||To late-resolve a wildcard, move the item `Include` inside a target.

e.g. Change your code to

```C#
<Project Sdk="Microsoft.NET.Sdk">
  <PropertyGroup>
    <TargetFramework>net8.0</TargetFramework>
    <LangVersion>12.0</LangVersion>
    <DebugType>portable</DebugType>
  </PropertyGroup>

  <Target Name="PrecompileScript" BeforeTargets="BeforeBuild">
    <Exec Command="dotnet run -c $(Configuration) --no-build --project ../DatGenerator/DatGenerator.csproj -directory $(ProjectDir)" />
    <ItemGroup>
      <None Remove="*.dat" />
      <None Include="*.dat" CopyToOutputDirectory="PreserveNewest" />
    </ItemGroup>
  </Target>
</Project>
```

"[How MSBuild builds projects][1]" explains the evaluation and execution phases but, briefly:

 - When building a project, MSBuild has an evaluation phase followed by an executuion phase.
 - 'Top level' `PropertyGroup` and `ItemGroup` elements are evaluated in the evalution phase.
 - Target order is determined in the evaluation phase.
 - Targets are run in the execution phase.
 - `PropertyGroup` and `ItemGroup` elements within a target are evaluated when the target is run.

**Note** I updated the answer to change from using `BeforeCompile` to using `BeforeBuild`.

  [1]: https://learn.microsoft.com/en-us/visualstudio/msbuild/build-process-overview?view=vs-2022

--------------------------------------------------
react-google-maps/api DirectionsService keeps rerendering itself
I have written this code in react JS to using &quot;react-google-maps/api&quot; to calculate route between two points. Now my google map keeps rerendering itself until it gives &quot;DIRECTIONS_ROUTE: OVER_QUERY_LIMIT&quot; error. I don&#39;t know what&#39;s the issue. Help would be appreciated because I am a beginner in react and google-API and also I haven&#39;t found a lot of guides of google API in react.

Here is my code:

    import React from &quot;react&quot;;
    import {
      GoogleMap,
      useLoadScript,
      DirectionsService,
      DirectionsRenderer,
    } from &quot;@react-google-maps/api&quot;;
    
    const libraries = [&quot;places&quot;, &quot;directions&quot;];
    const mapContainerStyle = {
      width: &quot;100%&quot;,
      height: &quot;50vh&quot;,
    };
    const center = {
      lat: 31.582045,
      lng: 74.329376,
    };
    const options = {};
    
    const MainMaps = () =&gt; {
      const { isLoaded, loadError } = useLoadScript({
        googleMapsApiKey: &quot;********&quot;,
        libraries,
      });
    
      const [origin2, setOrigin2] = React.useState(&quot;lahore&quot;);
      const [destination2, setDestination2] = React.useState(&quot;gujranwala&quot;);
      const [response, setResponse] = React.useState(null);
    
      const directionsCallback = (response) =&gt; {
        console.log(response);
    
        if (response !== null) {
          if (response.status === &quot;OK&quot;) {
            setResponse(response);
          } else {
            console.log(&quot;response: &quot;, response);
          }
        }
      };
    
      const mapRef = React.useRef();
      const onMapLoad = React.useCallback((map) =&gt; {
        mapRef.current = map;
      }, []);
      if (loadError) return &quot;Error loading maps&quot;;
      if (!isLoaded) return &quot;loading maps&quot;;
    
      const DirectionsServiceOption = {
        destination: destination2,
        origin: origin2,
        travelMode: &quot;DRIVING&quot;,
      };
    
      return (
        &lt;div&gt;
          &lt;GoogleMap
            mapContainerStyle={mapContainerStyle}
            zoom={8}
            center={center}
            onLoad={onMapLoad}
          &gt;
            {response !== null &amp;&amp; (
              &lt;DirectionsRenderer
                options={{
                  directions: response,
                }}
              /&gt;
            )}
    
            &lt;DirectionsService
              options={DirectionsServiceOption}
              callback={directionsCallback}
            /&gt;
          &lt;/GoogleMap&gt;
        &lt;/div&gt;
      );
    };
    
    export default MainMaps;




||||||||||||||The rendering issue appears to be with the library itself. One alternative I can suggest is to instead use/load Google Maps API script instead of relying on 3rd party libraries. This way, you can just follow the [official documentation](https://developers.google.com/maps/documentation) provided by Google.

By loading the script, we can now follow their [Directions API documentation](https://developers.google.com/maps/documentation/javascript/directions):

Here is a sample app for your reference: https://stackblitz.com/edit/react-directions-64165413

`App.js`
```

    import React, { Component } from 'react';
    import { render } from 'react-dom';
    import Map from './components/map';
    import "./style.css";
    
    class App extends Component {
     
      render() {
        return (
           <Map 
            id="myMap"
            options={{
              center: { lat: 31.582045, lng: 74.329376 },
              zoom: 8
            }}
          />
        );
      }
    }
    
    export default App;

```

`map.js`
```

    import React, { Component } from "react";
    import { render } from "react-dom";
    
    class Map extends Component {
      constructor(props) {
        super(props);
        this.state = {
          map: "",
          origin: "",
          destination: ""
        };
        this.handleInputChange = this.handleInputChange.bind(this); 
        this.onSubmit = this.onSubmit.bind(this);
      }
    
      onScriptLoad() {
        this.state.map = new window.google.maps.Map(
          document.getElementById(this.props.id),
          this.props.options
        );
      }
    
      componentDidMount() {
        if (!window.google) {
          var s = document.createElement("script");
          s.type = "text/javascript";
          s.src = `https://maps.google.com/maps/api/js?key=YOUR_API_KEY`;
          var x = document.getElementsByTagName("script")[0];
          x.parentNode.insertBefore(s, x);
          // Below is important.
          //We cannot access google.maps until it's finished loading
          s.addEventListener("load", e => {
            this.onScriptLoad();
          });
        } else {
          this.onScriptLoad();
        }
      }
    
      onSubmit(event) {    
        this.calculateAndDisplayRoute();
        event.preventDefault();
      }
    
      calculateAndDisplayRoute() {
        var directionsService = new google.maps.DirectionsService();
        var directionsRenderer = new google.maps.DirectionsRenderer();
        directionsRenderer.setMap(this.state.map);
        directionsService.route(
          {
            origin: { query: this.state.origin },
            destination: { query: this.state.destination },
            travelMode: "DRIVING"
          },
          function(response, status) {
            if (status === "OK") {
              directionsRenderer.setDirections(response);
            } else {
              window.alert("Directions request failed due to " + status);
            }
          }
        );
        
      }
    
      handleInputChange(event) {
        const target = event.target;
        const value = target.type === "checkbox" ? target.checked : target.value;
        const name = target.name;
    
        this.setState({
          [name]: value
        });
      }
      addMarker(latLng) {
        var marker = new window.google.maps.Marker({
          position: { lat: -33.8569, lng: 151.2152 },
          map: this.state.map,
          title: "Hello Sydney!"
        });
        var marker = new google.maps.Marker({
          position: latLng,
          map: this.state.map
        });
      }
    
      render() {
        return (
          <div>
            <input
              id="origin"
              name="origin"
              value={this.state.origin}
              placeholder="Origin"
              onChange={this.handleInputChange}
            />
            <input
              id="destination"
              name="destination"
              value={this.state.destination}
              placeholder="Destination"
              onChange={this.handleInputChange}
            />
            <button id="submit" onClick={this.onSubmit}>
              Search
            </button>
            <div className="map" id={this.props.id} />
          </div>
        );
      }
    }
    
    export default Map;

```

--------------------------------------------------
At what point does binary search become more efficient than sequential search?
I&#39;ve been learning a lot about algorithms lately, and the binary searched is hailed for its efficiency in finding an item in large amounts of **sorted** data. But what if the data is not sorted to begin with? at what points does a binary search provide an efficiency boost against sequential search, with binary search having to sort the given array first off THEN search. I&#39;m interested in seeing at what points binary search passes over sequential search, if anyone has tested this before i would love to see some results.

Given an array foo[BUFF] with 14 elements

    1 3 6 3 1 87 56 -2 4 61 4 9 81 7

I would assume a sequential sort would be more efficient to find a given number, let&#39;s say... 3, because binary search would need to first sort the array **THEN** search for the number 3. BUT:

Given an array bar[BUFF] with one thousand elements held

    1 2 4 9 -2 3 8 9 4 12 4 56 //continued

A call to sort then binary search should in theory be more efficiently if i am not mistaken.


||||||||||||||In an unsorted array where no information is known, you are going to have to do linear time search.

Linear time search checks each element once, so it's complexity is `O(n)`. Comparing that to sorting. Sorting algorithms which must check each element more than once and have a complexity of `O(n * log n)`. So to even get it sorted is slower than a sequential search. Even though binary search is `O(log n)` it's pretty useless when you just have arbitrarily ordered data.

If your going to search for stuff multiple times though, consider sorting first as it'll increase your efficiency in the long run.

--------------------------------------------------
GPG error: http://archive.debian.org lenny/updates Release: The following signatures were invalid: KEYEXPIRED 1356982504
I am getting following error while update my source lists

    $ sudo apt-get update
    
    Reading package lists... Done
    
    W: GPG error: http://archive.debian.org lenny/updates Release: The following signatures were invalid: KEYEXPIRED 1356982504
    
    W: You may want to run apt-get update to correct these problems



**How to resolve this issue?**

||||||||||||||To find any expired repository keys and their IDs, use apt-key as follows:

      apt-key list | grep expired

You will get a result similar to the following:

      pub   4096R/BE1DB1F1 2011-03-29 [expired: 2014-03-28]

The key ID is the bit after the / i.e. BE1DB1F1 in this case.

To update the key, run

      sudo apt-key adv --recv-keys --keyserver keys.gnupg.net BE1DB1F1



--------------------------------------------------
Select a Dictionary&lt;T1, T2&gt; with LINQ
I have used the &quot;select&quot; keyword and extension method to return an `IEnumerable&lt;T&gt;` with LINQ, but I have a need to return a generic `Dictionary&lt;T1, T2&gt;` and can&#39;t figure it out.  The example I learned this from used something in a form similar to the following: 

    IEnumerable&lt;T&gt; coll = from x in y 
        select new SomeClass{ prop1 = value1, prop2 = value2 };

I&#39;ve also done the same thing with extension methods.  I assumed that since the items in a  `Dictionary&lt;T1, T2&gt;` can be iterated as `KeyValuePair&lt;T1, T2&gt;` that I could just replace &quot;SomeClass&quot; in the above example with &quot;`new KeyValuePair&lt;T1, T2&gt; { ...`&quot;, but that didn&#39;t work (Key and Value were marked as readonly, so I could not compile this code).

Is this possible, or do I need to do this in multiple steps?

Thanks.


||||||||||||||The extensions methods also provide a [ToDictionary][1] extension.  It is fairly simple to use, the general usage is passing a lambda selector for the key and getting the object as the value, but you can pass a lambda selector for both key and value.

    class SomeObject
    {
        public int ID { get; set; }
        public string Name { get; set; }
    }

    SomeObject[] objects = new SomeObject[]
    {
        new SomeObject { ID = 1, Name = "Hello" },
        new SomeObject { ID = 2, Name = "World" }
    };

    Dictionary<int, string> objectDictionary = 
        objects.ToDictionary(
            o => o.ID, 
            o => o.Name);

Then `objectDictionary[1]` Would contain the value "Hello"

  [1]: http://msdn.microsoft.com/en-us/library/system.linq.enumerable.todictionary.aspx

--------------------------------------------------
How do I get this code to accommodate any given number by only using boolean logic (no conditionals/ functions)
Here is the problem:

```
number = 1101
#You may modify the lines of code above, but don&#39;t move them!
#When you Submit your code, we&#39;ll change these lines to
#assign different values to the variables.
#
#The number above represents a binary number. It will always
#be up to eight digits, and all eight digits will always be
#either 1 or 0.
#
#The string gives the binary representation of a number. In
#binary, each digit of that string corresponds to a power of
#2. The far left digit represents 128, then 64, then 32, then
#16, then 8, then 4, then 2, and then finally 1 at the far right.
#
#So, to convert the number to a decimal number, you want to (for
#example) add 128 to the total if the first digit is 1, 64 if the
#second digit is 1, 32 if the third digit is 1, etc.
#
#For example, 00001101 is the number 13: there is a 0 in the 128s
#place, 64s place, 32s place, 16s place, and 2s place. There are
#1s in the 8s, 4s, and 1s place. 8 + 4 + 1 = 13.
#
#Note that although we use &#39;if&#39; a lot to describe this problem,
#this can be done entirely boolean logic and numerical comparisons.
#
#Print the number that results from this conversion.
```


---



Here is my code

```
##Add your code here!

number_str = str(number) # &quot;1101&quot;
first_num = int(number_str[-1]) * 1
#print(&quot;first num:&quot;, first_num)
second_num = int(number_str[-2]) * 2
#print(&quot;second num:&quot;, second_num) 
third_num = int(number_str[-3]) * 4
#print(&quot;Third num:&quot;, third_num)
fourth_num = int(number_str[-4]) * 8
#print(&quot;fourth num:&quot;, fourth_num)
fifth_num = int(number_str[-5]) * 16
sixt_num = int(number_str[-6]) * 32
seventh_num = int(number_str[-7]) * 64
decimal = first_num + second_num + third_num + fourth_num + fifth_num + sixt_num + seventh_num
print(decimal)
```
The error I got was:
We found a few things wrong with your code. The first one is shown below, and the rest can be found in full_results.txt in the dropdown in the top left:
We tested your code with number = 1010111. We expected your code to print this:
87
However, it printed this:
7

------------------------
I understand that I hard-coded this problem to accommodate 4 digits. I would like this to work for any given numbers without throwing an IndexError: string index out of range.

I appreciate your help.
||||||||||||||I don't like doing people's homework for them, but in this case I think the example says more than an explanation.

You do this conversion one character at a time, from left to right.  At each step, you shift the result left by one, and if the digit is '1', you add it in.
```
number = 1011
decimal = 0
for c in str(number):
    decimal = decimal * 2 + (c=='1')
print(decimal)
```
If that's too clever, replace `(c=='1')` with `int(c)`.

--------------------------------------------------
How can I calculate a hash for a filesystem-directory using Python?
I&#39;m using this code to calculate hash value for a file: 

    m = hashlib.md5()
	with open(&quot;calculator.pdf&quot;, &#39;rb&#39;) as fh:
		while True:
			data = fh.read(8192)
			if not data:
				break
			m.update(data)
		hash_value = m.hexdigest()
		
		print  hash_value

when I tried it on a folder &quot;folder&quot;I got 

    IOError: [Errno 13] Permission denied: folder


How could I calculate the hash value for a folder ?
||||||||||||||This [Recipe][1] provides a nice function to do what you are asking. I've modified it to use the MD5 hash, instead of the SHA1, as your original question asks

	def GetHashofDirs(directory, verbose=0):
	  import hashlib, os
	  SHAhash = hashlib.md5()
	  if not os.path.exists (directory):
		return -1
		
	  try:
		for root, dirs, files in os.walk(directory):
		  for names in files:
			if verbose == 1:
			  print 'Hashing', names
			filepath = os.path.join(root,names)
			try:
			  f1 = open(filepath, 'rb')
			except:
			  # You can't open the file for some reason
			  f1.close()
			  continue

		    while 1:
    		  # Read file in as little chunks
	    	  buf = f1.read(4096)
		      if not buf : break
    		  SHAhash.update(hashlib.md5(buf).hexdigest())
			f1.close()

	  except:
		import traceback
		# Print the stack traceback
		traceback.print_exc()
		return -2

	  return SHAhash.hexdigest()


You can use it like this:

    print GetHashofDirs('folder_to_hash', 1)

The output looks like this, as it hashes each file:

<!-- language: lang-none -->

    ...
    Hashing file1.cache
    Hashing text.txt
    Hashing library.dll
    Hashing vsfile.pdb
    Hashing prog.cs
    5be45c5a67810b53146eaddcae08a809

The returned value from this function call comes back as the hash. In this case, `5be45c5a67810b53146eaddcae08a809`

  [1]: http://code.activestate.com/recipes/576973-getting-the-sha-1-or-md5-hash-of-a-directory/

--------------------------------------------------
Browser-side JS: File System API vs File System Access API?
There was a File System API but shown as deprecated now:&lt;br/&gt; https://developer.mozilla.org/en-US/docs/Web/API/Window/requestFileSystem

There is now another, File System Access API:&lt;br/&gt; https://developer.mozilla.org/en-US/docs/Web/API/File_System_Access_API

What happened to the old API and why was it discontinued? Should the new File System Access API be stable in all common browsers?
||||||||||||||It turned out that File System Access API is not deprecated, it's just not standardised (May 2021); the deprecated one is the function `window.requestFileSystem`; the same function on Chromium-based browsers is `window.webkitRequestFileSystem`.

`File System API` is for creating a virtual drive (temporary or persistent) for each website when using browser-based db (IndexedDB) is not necessary especially for the purpose of storing files.<br/>
https://developer.mozilla.org/en-US/docs/Web/API/FileSystem

`File System Access API` is different, it is for accessing the real file system of the OS. This API is now standardised and available on Chromium-based browsers (May 2021). Firefox has not yet adapted this API.<br/>
https://developer.mozilla.org/en-US/docs/Web/API/File_System_Access_API

Status of these APIs: https://developer.mozilla.org/en-US/docs/Web/API


--------------------------------------------------
How to simulate a loop&#39;s &#39;break&#39; statement inside an array-iterating, custom implemented, &#39;forEach&#39; function/method?
What is the best way to implement a simulation of a loop&#39;s `break` statement, when one does iterate through a user/engine defined function?

    forEach([0, 1, 2, 3, 4], function (n) {
      console.log(n);
      
      if (n === 2) {
        break;
      }
    });

I&#39;ve thought of implementing `forEach` in a way that would break when the function returns `false`. But I would like to hear thoughts on how that is normally done.

||||||||||||||`return`ing `false` is the most common way to do it. That's what jQuery's iterator function [`.each()`][1] does:

> We can break the $.each() loop at a particular iteration by making the
> callback function return **false**. Returning non-false is the same as a
> continue statement in a for loop; it will skip immediately to the next
> iteration.

And its *very* simplified implementation:

    each: function( object, callback ) {
      var i = 0, length = object.length,
      for ( var value = object[0]; 
            i < length && callback.call( value, i, value ) !== false; // break if false is returned by the callback 
            value = object[++i] ) {}
      return object;
    }

  [1]: http://api.jquery.com/jQuery.each/

--------------------------------------------------
Understanding OpenMP shortcomings regarding fork
***I wish to understand what do they mean here. Why would this program &quot;hang&quot;?***

From https://bisqwit.iki.fi/story/howto/openmp/  

&gt; OpenMP and `fork()` It is worth mentioning that using OpenMP in a
&gt; program that calls `fork()` requires special consideration. This
&gt; problem only affects GCC; ICC is not affected.   If your program
&gt; intends to become a background process using `daemonize()` or other
&gt; similar means, you must not use the OpenMP features before the fork.
&gt; After OpenMP features are utilized, a fork is only allowed if the
&gt; child process does not use OpenMP features, or it does so as a
&gt; completely new process (such as after `exec()`).
&gt; 
&gt; This is an example of an erroneous program:
&gt; 
&gt;     #include &lt;stdio.h&gt;   
&gt;     #include &lt;sys/wait.h&gt;   
&gt;     #include &lt;unistd.h&gt;
&gt;     
&gt;     void a(){
&gt;         #pragma omp parallel num_threads(2)
&gt;         {
&gt;             puts(&quot;para_a&quot;); // output twice
&gt;         }
&gt;         puts(&quot;a ended&quot;); // output once   
&gt;     }
&gt;        
&gt;     void b(){
&gt;         #pragma omp parallel num_threads(2)
&gt;         {
&gt;             puts(&quot;para_b&quot;);
&gt;         }
&gt;         puts(&quot;b ended&quot;);   
&gt;     }
&gt;     
&gt;     int main(){    
&gt;         a();   // Invokes OpenMP features (parent process)   
&gt;         int p = fork();    
&gt;         if(!p){
&gt;             b(); // ERROR: Uses OpenMP again, but in child process
&gt;             _exit(0);    
&gt;         }    
&gt;         wait(NULL);    
&gt;         return 0;   
&gt;     }
&gt; 
&gt; When run, this program hangs, never reaching the line that outputs &quot;b
&gt; ended&quot;. There is currently no workaround as the libgomp API does not
&gt; specify functions that can be used to prepare for a call to `fork()`.


||||||||||||||The code as posted violates the POSIX standard.

The [POSIX `fork()` standard states][1]:

> A process shall be created with a single thread. If a multi-threaded
> process calls fork(), the new process shall contain a replica of the
> calling thread and its entire address space, possibly including the
> states of mutexes and other resources. **Consequently, to avoid
> errors, the child process may only execute async-signal-safe
> operations until such time as one of the `exec` functions is called.**

Running OMP-parallelized code is clearly violating the above restriction.
  [1]: http://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html

--------------------------------------------------
How to replace reference to a file with its contents?
Supose that I have a directory containing among others two markdown files 

```
a note.md
another one.md
```

with the `a note.md` containing

```
Here is a description of an idea.

![[another one.md]]
```

and the `another one.md` containing

```
Here is another idea related to it.
```

I am looking for a command in bash that would 

1. take `a note.md`, 
2. replace that the reference `![[another one.md]]` in the `a note.md` with the actual contents of the `another one.md`, and 
3. return the result (so that I could pipe it to Pandoc).

The output in this example would contain

```
Here is a description of an idea.

Here is another idea related to it.
```

---

Why? Obsidian markdown note-taking app allows [embedding file contents](https://help.obsidian.md/Linking+notes+and+files/Embed+files#Embed+a+note+in+another+note) into markdown files  using `![[]]` as described above. However, when converting such files using Pandoc, the references are treated as text. So I am looking for a way to add the embedded content prior to Pandoc conversion.

||||||||||||||If you can use perl :

```
perl -i -pe 's/!\[\[(.*?)]]/`cat "$1"`/eg' "a note.md"
```

`-i` option changes "a note.md", so you may want to do a backup before running the command.

@jhnc provided a much safer version :

```
perl -0777pe 's/!\[\[([^]]+)]]/ -f $1 && `cat \Q$1` =~ s#^\s*(.*?)\s*$#$1#sr || $& /eg' 'a note.md' > 'result.md'
```

--------------------------------------------------
Regex to match key with optional quotes and optional separator
I&#39;m trying to build a regex where it should be able to get all the values of the key mentioned in the log satisfying the following conditions

 1. It can be either in single quote, double quote or no quote.
 2. Key will be either followed by `: - `
 3. Value can either be in single quote, double quote or no quote.

Here is the example I have tried out.

    [&#39;&quot;](?:accountNumber|subNo)[&#39;&quot;][:-]\s*[&#39;&quot;](\d+)[&#39;&quot;]

This regex is finding out the `accountNumber` in the second part of the log

    23:22:12.127 DEBUG Getting Service Details for: accountNumber-&quot;525012078&quot;, subNo 5488870689 
    23:22:12.403 INFO  /subscriptions, [{&quot;accountNumber&quot;:&quot;1233&quot;,&quot;subNo&quot;:&quot;123&quot;,&quot;type&quot;:...}}] 

Qn: Need to find out the `accountNumber` or `subNo` from the log, in both JSON format and statement log.
||||||||||||||You can use
```none
(['"]|)\b(?:accountNumber|subNo)\1[:-]\s*(['"])(\d+)\2
```
See the [regex demo][1].

See the [Java code][2]:
```java
String s = "23:22:12.127 DEBUG Getting Service Details for: accountNumber-\"525012078\", subNo 5488870689 \n23:22:12.403 INFO  /subscriptions, [{\"accountNumber\":\"1233\",\"subscriptionNo\":\"123\",\"type\":...}}] ";
Pattern pattern = Pattern.compile("(['\"]|)\\b(?:accountNumber|subNo)\\1[:-]\\s*(['\"])(\\d+)\\2");
Matcher matcher = pattern.matcher(s);
while (matcher.find()){
	System.out.println(matcher.group(3)); 
} 
```
*Details*:

 - `(['"]|)\b(?:accountNumber|subNo)` - either `"` or `'` captured into Group 1  or an empty space + word boundary + `accountNumber` or `subNo`
 - `\1` - Same value as in Group 1
 - `[:-]` - a `:` or `-`
 - `\s*` - zero or more whitespaces
 - `(['"])` - Group 2: a `'` or `"` char
 - `(\d+)` - Group 3 (the result): one or more digits
 - `\2` - same value as in Group 2.

  [1]: https://regex101.com/r/QNQ3NX/2
  [2]: https://ideone.com/oGhQxI

--------------------------------------------------
Check the total number of parameters in a PyTorch model
How do I count the total number of parameters in a PyTorch model? Something similar to `model.count_params()` in Keras.
||||||||||||||PyTorch doesn't have a function to calculate the total number of parameters as Keras does, but it's possible to sum the number of elements for every parameter group:

    pytorch_total_params = sum(p.numel() for p in model.parameters())

If you want to calculate only the _trainable_ parameters:

    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

---
_Answer inspired by [this answer](https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9) on PyTorch Forums_.

--------------------------------------------------
Pylint warn the usage of print statement
I am using `pylint_django` for my django project. And I want to disable print statement usage or warn about it at least. Because I am using custom logger class. But there is no any warn about usage of print.


```[MASTER]

extension-pkg-whitelist=

ignore=CVS

ignore-patterns=

jobs=1


limit-inference-results=100

load-plugins=

persistent=yes

suggestion-mode=yes

unsafe-load-any-extension=no


[MESSAGES CONTROL]
confidence=

disable=missing-docstring,
        invalid-name,
        astroid-error,
        protected-access,
        broad-except

enable=c-extension-no-member, print-statement


[REPORTS]
evaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)

output-format=text

reports=no

score=yes


[REFACTORING]

max-nested-blocks=5

never-returning-functions=sys.exit


[LOGGING]

logging-format-style=old

logging-modules=logging

....
```

How can i solve this issue?

VsCode settings.json
```
{
  &quot;python.linting.pylintEnabled&quot;: true,
  &quot;python.linting.enabled&quot;: true,
  &quot;python.linting.flake8Enabled&quot;: false,
  &quot;python.linting.prospectorEnabled&quot;: false,
  &quot;python.linting.pylintArgs&quot;: [
    &quot;--load-plugins=pylint_django&quot;,
    &quot;--rcfile=.pylintrc&quot;,
    &quot;--enable=print-statement&quot;
  ]
}
```
||||||||||||||You can do that using [the deprecated checkers ``bad-functions`` options](https://pylint.pycqa.org/en/latest/user_guide/configuration/all-options.html#deprecated-builtins-options):

    [tool.pylint]
    bad-functions = ["map", "filter", "print"]

--------------------------------------------------
How to convert Top-and-Bottom 3d video to side-by-side 3d video with FFmpeg
I have a top-and-bottom 3d video, and i want to look at it with Gear VR, but Gear VR only support side-by-side video, so i need to convert it to side-by-side, while i don&#39;t know how to use ffmpeg to achieve it,does anyone knows ? thanks very much.

||||||||||||||See [stereo3d](http://ffmpeg.org/ffmpeg-filters.html#stereo3d) filter documentation:

`ffmpeg -i top-and-bottom.mov -vf stereo3d=abl:sbsl -c:a copy side-by-side.mov`

--------------------------------------------------
Docker: How to solve the public key error in ubuntu while installing docker
I am getting the below error message when running the below command for installing docker and kubernetes in Ubuntu server. 


    root@master:/home/ubuntu# add-apt-repository \
    &gt;   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
    &gt;   $(lsb_release -cs) \
    &gt;   stable&quot;
    Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease
    Get:2 https://download.docker.com/linux/ubuntu bionic InRelease [64.4 kB]
    Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease
    Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease
    Hit:5 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease
    **Err:2 https://download.docker.com/linux/ubuntu bionic InRelease
      The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY 7EA0A9C3F273FCD8**
    Reading package lists... Done
    W: GPG error: https://download.docker.com/linux/ubuntu bionic InRelease: The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY 7EA0A9C3F273FCD8
    **E: The repository &#39;https://download.docker.com/linux/ubuntu bionic InRelease&#39; is not signed.**
    N: Updating from such a repository can&#39;t be done securely, and is therefore disabled by default.
    N: See apt-secure(8) manpage for repository creation and user configuration details.
    root@master:/home/ubuntu#




I have also ran the below command but no luck

    root@master:/# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 7EA0A9C3F273FCD8
    Executing: /tmp/apt-key-gpghome.rDOuMCVLF2/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 7EA0A9C3F273FCD8
    gpg: keyserver receive failed: No keyserver available


||||||||||||||# EDIT: This answer apparently does not work any more

Run this to add the correct key:

    # Does not work any more
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

Source: https://docs.docker.com/install/linux/docker-ce/ubuntu/

--------------------------------------------------
Create an arc between two points in matplotlib
I am trying to recreate the chart below using matplotlib:
[![enter image description here][1]][1]

I have most of it done but, I just cant figure out how to create the arcs between the years:

    import matplotlib.pyplot as plt
    from scipy.interpolate import interp1d
    import numpy as np
    import pandas as pd
    
    colors = [&quot;#CC5A43&quot;,&quot;#2C324F&quot;,&quot;#5375D4&quot;,]
    
    data = {
        &quot;year&quot;: [2004, 2022, 2004, 2022, 2004, 2022],
        &quot;countries&quot; : [ &quot;Denmark&quot;, &quot;Denmark&quot;, &quot;Norway&quot;, &quot;Norway&quot;,&quot;Sweden&quot;, &quot;Sweden&quot;,],
        &quot;sites&quot;: [4,10,5,8,13,15]
    }
    df= pd.DataFrame(data)
    df = df.sort_values([ &#39;year&#39;], ascending=True ).reset_index(drop=True)
    df[&#39;ctry_code&#39;] = df.countries.astype(str).str[:2].astype(str).str.upper()
    df[&#39;year_lbl&#39;] =&quot;&#39;&quot;+df[&#39;year&#39;].astype(str).str[-2:].astype(str)
    sites = df.sites
    lbl1 = df.year_lbl
    
    
    fig, ax = plt.subplots( figsize=(6,6),sharex=True, sharey=True, facecolor = &quot;#FFFFFF&quot;, zorder= 1)
    
    
    ax.scatter(sites, sites, s= 340, c= colors*2 , zorder = 1)
    ax.set_xlim(0, sites.max()+3)
    ax.set_ylim(0, sites.max()+3)
    ax.axline([ax.get_xlim()[0], ax.get_ylim()[0]], [ax.get_xlim()[1], ax.get_ylim()[1]], zorder = 0, color =&quot;#DBDEE0&quot; )
    
    for i, l1 in zip(range(0,6), lbl1) :
        ax.annotate(l1, (sites[i], sites[i]), color = &quot;w&quot;,va= &quot;center&quot;, ha = &quot;center&quot;)
    
 
    ax.set_axis_off()

Which gives me this:
[![enter image description here][2]][2]

I have tried both [mpatches.arc][3] and [patches and path][4] but cant make it work.


  [1]: https://i.stack.imgur.com/P9NGe.png
  [2]: https://i.stack.imgur.com/UaONB.png
  [3]: https://stackoverflow.com/questions/30642391/how-to-draw-a-filled-arc-in-matplotlib
  [4]: https://stackoverflow.com/questions/50346166/draw-an-arc-as-polygon-using-start-end-center-and-radius-using-python-matplotl
||||||||||||||# Semicircle arc between two points
To draw a semicircle between two points:
- the center of the two points will be the center of the circle
- for a circular arc, `width` and `height` both need to be set to the diameter; that diameter is the distance between the two points (square root of sum of squares of the x and y differences)
- the starting angle can be calculated by the arc tangent of the vector from one point to the other
- the final angle will be 180º further

Encapsulated in a function, together with a little test:
```python
import matplotlib.pyplot as plt
from matplotlib. Patches import Arc
import numpy as np

def draw_semicircle(x1, y1, x2, y2, color='black', lw=1, ax=None):
    '''
    draw a semicircle between the points x1,y1 and x2,y2
    the semicircle is drawn to the left of the segment
    '''
    ax = ax or plt.gca()
    # ax. Scatter([x1, x2], [y1, y2], s=100, c=color)
    startangle = np.degrees(np.arctan2(y2 - y1, x2 - x1))
    diameter = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)  # Euclidian distance
    ax.add_patch(Arc(((x1 + x2) / 2, (y1 + y2) / 2), diameter, diameter, theta1=startangle, theta2=startangle + 180,
                     edgecolor=color, facecolor='none', lw=lw, zorder=0))

angle = np.linspace(0, 38, 80)
x = angle * np.cos(angle)
y = - angle * np.sin(angle)
fig, ax = plt.subplots()
for x1, y1, x2, y2 in zip(x[:-1], y[:-1], x[1:], y[1:]):
    draw_semicircle(x1, y1, x2, y2, color='fuchsia', lw=2)

ax.set_aspect('equal')  # show circles without deformation
ax.autoscale_view()  # fit the arc into the data limits
ax. Axis('off')
plt.show()
```
[![matplotlib semicircles between two points][1]][1]


# Specific code for the data in the question
Here is an adaption of the code for your case (180º arc on a 45º line).  The text can be positioned using the x coordinate of the first and the y coordinate of the second point.
```python
import matplotlib.pyplot as plt
from matplotlib. Patches import Arc
import pandas as pd
import math

colors = ["#CC5A43", "#2C324F", "#5375D4"]
data = {
    "year": [2004, 2022, 2004, 2022, 2004, 2022],
    "countries": ["Denmark", "Denmark", "Norway", "Norway", "Sweden", "Sweden"],
    "sites": [4, 10, 5, 8, 13, 15]
}
df = pd.DataFrame(data)
df = df.sort_values(['year'], ascending=True).reset_index(drop=True)
df['ctry_code'] = df.countries.astype(str).str[:2].astype(str).str.upper()
df['year_lbl'] = "'" + df['year'].astype(str).str[-2:].astype(str)
sites = df.sites
lbl1 = df.year_lbl
countries = df.ctry_code

fig, ax = plt.subplots(figsize=(6, 6), sharex=True, sharey=True, facecolor="#FFFFFF", zorder=1)

ax. Scatter(sites, sites, s=340, c=colors * 2, zorder=1)
ax.set_xlim(0, sites.max() + 3)
ax.set_ylim(0, sites.max() + 3)
ax.set_aspect('equal')
ax.axline([ax.get_xlim()[0], ax.get_ylim()[0]], [ax.get_xlim()[1], ax.get_ylim()[1]], zorder=0, color="#DBDEE0")

for site, l1 in zip(sites, lbl1):
    ax.annotate(l1, (site, site), color="w", va="center", ha="center")

for x1, x2, color, country in zip(sites[:len(sites) // 2], sites[len(sites) // 2:], colors, countries):
    center = (x1 + x2) / 2
    diameter = math.sqrt((x2 - x1) ** 2 + (x2 - x1) ** 2)  # Euclidian distance
    ax.add_patch(Arc((center, center), diameter, diameter, theta1=45, theta2=225,
                     edgecolor=color, facecolor='none', lw=2))
    ax.annotate(country, (x1, x2), color=color, va="center", ha="center",
                bbox=dict(boxstyle="round, pad=0.5", facecolor="aliceblue", edgecolor=color, lw=2))
ax.set_axis_off()
plt.show()
``` 
[![matplotlib arcs between points][2]][2]


  [1]: https://i.stack.imgur.com/GY9Hl.png
  [2]: https://i.stack.imgur.com/A5Spr.png

--------------------------------------------------
TypeScript error on context value type mismatch
I am implementing React context using TypeScript and I seem to be getting and error on the `value` prop of my context.

Here is what the error says:
```
Type &#39;{ createReferral: (referralData: { actionStepId: number; appTypeId: string; comment: string; createdDate: Date; createdBy: string; createdByDisplayName: string; currentAssigneeDisplayName: string; ... 4 more ...; submissionId: string; }) =&gt; void; getCurrentUser: () =&gt; AccountInfo; }&#39; is not assignable to type &#39;SelectedContextType&#39;.
```

So it looks like there is a mismatch between my expected context value types and the actual value types.

I declare my context as follows:
```
interface SelectedContextType {
  createReferral: (referralData: {
    actionStepId: number
    appTypeId: string
    comment: string
    createdDate: Date
    createdBy: string
    createdByDisplayName: string
    currentAssigneeDisplayName: string
    currentAssigneeGRNID: string
    reasonTypeId: number | string
    referralTypeId: number | string
    statusTypeId: number
    submissionId: string
  }) =&gt; void
  getCurrentUser: () =&gt; { name: string; username: string }
}

export const AppContext = createContext&lt;SelectedContextType | undefined&gt;(
  undefined
)
```

and I declare these functions in my context:
```
const AppProvider = ({ children, msalContext }: AppProps) =&gt; {
  function getCurrentUser() {
    return msalContext.accounts[0]
  }

  function createReferral(referralData: {
    actionStepId: number
    appTypeId: string
    comment: string
    createdDate: Date
    createdBy: string
    createdByDisplayName: string
    currentAssigneeDisplayName: string
    currentAssigneeGRNID: string
    reasonTypeId: number | string
    referralTypeId: number | string
    statusTypeId: number
    submissionId: string
  }) {
    referralAxios
      .post(&#39;/api/ReferralMasters&#39;, {
        actionStepID: referralData.actionStepId,
        appTypeID: referralData.appTypeId,
        comment: referralData.comment,
        createdDate: referralData.createdDate,
        createdBy: referralData.createdBy,
        createdByDisplayName: referralData.createdByDisplayName,
        currentAssigneeDisplayName: referralData.currentAssigneeDisplayName,
        currentAssigneeGRNID: referralData.currentAssigneeGRNID,
        reasonTypeID: referralData.reasonTypeId,
        referralTypeID: referralData.referralTypeId,
        statusTypeID: referralData.statusTypeId,
        submissionID: Number(referralData.submissionId),
      })
      .then(function (response) {
        console.log(response)
        if (response.data.actionStepID === 2) {
          sendReferralEmail(response.data)
        }
      })
      .catch(function (error) {
        console.log(error, &#39;testError&#39;)
      })
  }

  const appContext = { createReferral, getCurrentUser }
  return (
    &lt;AppContext.Provider value={appContext}&gt;{children}&lt;/AppContext.Provider&gt;
  )
}

export default withMsal(AppProvider)
```

I am not sure what exactly the error is saying. I see it referencing `AccountInfo` which does not exist as a type anywhere in my app.


The `msalContext.accounts[0]` is an object that looks like:
```
authorityType: &quot;MSSTS&quot;
environment: &quot;login.windows.net&quot;
homeAccountId: &quot;89887r89e&quot;
idTokenClaims: {aud: &#39;1799}
localAccountId: &quot;60000&quot;
name: &quot;User&quot;
username: &quot;User@STAR.COM&quot;
```

example: [example][1]


  [1]: https://www.typescriptlang.org/play?#code/JYWwDg9gTgLgBAJQKYEMDGMA0cDec1SoxIDCEAdsQB7wC+cAZlBCHAESHoxsBQoksXHACSAWQDOKADZlKSGtgDuwGAAsJ0uPSYs4AcgACKAF4BXQgHoQkqQFpOGPTz5yoDdEjgBBMGAAKzGDiuDxw+KrAUgAmhOQAXIioGAB0yFwAchBRSKFw1tKy1DAJYjaF8jA8tM7Aru5ongDKSFJIGEhR5TQAKgCeYJ44uWEEREjIDEhQUNIJABSEk9PSACIoMCgJQ2E7O1zAFI3EYMJRCeSmIABGU8O7cCi+fQOnCeIwULUA5ne7aCwgJCUN4fb6-HajdYdNbEBIwnL3CGcYhRABCvRBn3IP0RI2RHXRK2A4jAUhQvXSKEBmLBuPw5liMC84nEwC+5CQSCJJLJFKpSBp2PBIwZQKZLLZHKQAHEEOlhCtBTjcZxxBRnkhXnALtcpnAAD5wd5Y5WIxZTGZSDVanU3KAGo2goV097rUzia1nbWXO3Co2mK4gYmsiha420na0ACUcAAvAA+OAANwgwCidy+SBgJFFlAAquIpvMYwmhOR+UqANxwd1TcvUx0mrRVZzyATwf7kd7eXxdeCx-D4vsAHmarXanQoRQ1DtM5GyDFqHXjczuc4XS-TUecne7Pn8zCTab1A7meDQEWisWw+RkU4qWgS+4CECCJcT20Yc4wB3IcEz2a5jABZTHMMafiMFBqq0yRSBAXxzLefbJOg-xzjA4gANoAAwALrbrshAwOYf5IfeNAoWgaGUFheG5NUdwMN+MC-oOYwTBa0gLEgSyWjCmwhHS+yHMcNo+rcQlPP0mpeuGzq4v8ICAsCjYRoikIovCcJQn6GkEhiqnyep+Jor03KkuSlINnJpr3GgQHMqy7KcuZvJWQKhm2X8DkSs5MpygqSp+qq6rSWJur2oaNnBTxnFWmFXq2nqUVOl5OyusRHoJec4lQH64gBkGEqhrJqV3NGgm4hYFhwMkdXlc4fxQfAjxgH2cZCHpHHLFI2AATm0xiiB9rVGEREkXAq67MO+7IS+R7ZPaSbSKYSCxjgrV9rQ8Y4BekQxECtDDhYs3kTAyTzceUDxrk260EAA
||||||||||||||[the AccountInfo.name value is nullable](https://github.com/AzureAD/microsoft-authentication-library-for-js/blob/e7a55511680aec602310f63db0bb5b7d2b07fab3/lib/msal-common/src/account/AccountInfo.ts#L20)

just make it nullable either in your code:
```ts
  getCurrentUser: () => { name?: string; username: string }
```

--------------------------------------------------
Allow multiple CORS domain in express js
How do I allow multiple domains for CORS in express in a simplified way.

I have

     cors: {
            origin: &quot;www.one.com&quot;;
        }
    
        app.all(&#39;*&#39;, function(req, res, next) {
                res.header(&quot;Access-Control-Allow-Origin&quot;, cors.origin);
                res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept&quot;);
                next();
            });

This works when there is only one domain mentioned in `origin`

But if I want to have `origin` as an array of domains and I want to allow CORS for all the domains in the origin array, I would have something like this - 

    cors: {
                origin: [&quot;www.one.com&quot;,&quot;www.two.com&quot;,&quot;www.three.com&quot;];
            }
    
But then the problem is this below code would not work - 

    app.all(&#39;*&#39;, function(req, res, next) {
                    res.header(&quot;Access-Control-Allow-Origin&quot;, cors.origin);
                    res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept&quot;);
                    next();
                });

How do I make `res.header` take an array of domains via `cors.origin` ?
||||||||||||||The value of `Access-Control-Allow-Origin` must be a string, not a list. So to make it dynamic you need to get the requesting origin from the `Origin` HTTP request header, check it against your array of authorized origins. If it's present, then add that origin as the value of the `Access-Control-Allow-Origin` header; otherwise, use a default value, which would prohibit unauthorized domains from accessing the API.

There is no native implementation for this. You can do it yourself using the code below. 

    cors: {
      origin: ["www.one.com","www.two.com","www.three.com"],
      default: "www.one.com"
    }

    app.all('*', function(req, res, next) {
      const origin = cors.origin.includes(req.header('origin').toLowerCase()) ? req.headers.origin : cors.default;
      res.header("Access-Control-Allow-Origin", origin);
      res.header("Access-Control-Allow-Headers", "Origin, X-Requested-With, Content-Type, Accept");
      next();
    });


--------------------------------------------------
Python 3.Kivy. Is there any way to limit entered text in TextInput widget?
I&#39;m writing kivy app and resently I faced with a  problem of unlimited inputing text in TextInput widget. Is there any solution to this problem?
||||||||||||||A possible solution is to create a new property and overwrite the insert_text method:

    
    from kivy.app import App
    from kivy.uix.textinput import TextInput
    from kivy.properties import NumericProperty
    
    
    class MyTextInput(TextInput):
        max_characters = NumericProperty(0)
        def insert_text(self, substring, from_undo=False):
            if len(self.text) > self.max_characters and self.max_characters > 0:
                substring = ""
            TextInput.insert_text(self, substring, from_undo)
    
    class MyApp(App):
        def build(self):
            return MyTextInput(max_characters=4)
    
    
    if __name__ == '__main__':
        MyApp().run()

--------------------------------------------------
Generating random numbers over a range in Go
All the integer functions in [`math/rand`](http://golang.org/pkg/math/rand/#Int) generate non-negative numbers.

    rand.Int() int              // [0, MaxInt]
    rand.Int31() int32          // [0, MaxInt32]
    rand.Int31n(n int32) int32  // [0, n)
    rand.Int63() int64          // [0, MaxInt64]
    rand.Int63n(n int64) int64  // [0, n)
    rand.Intn(n int) int        // [0, n)

I would like to generate random numbers in the range **[-m, n)**. In other words, I would like to generate a mix of positive and negative numbers.
||||||||||||||I found this example at [Go Cookbook](http://golangcookbook.blogspot.com/2012/11/generate-random-number-in-given-range.html), which is equivalent to `rand.Range(min, max int)` (if that function existed):

```go
rand.Intn(max - min) + min
```

--------------------------------------------------
C#: Create a virtual drive in Computer
Is there any way to create a virtual drive in &quot;(My) Computer&quot; and manipulate it, somewhat like JungleDisk does it?

It probably does something like:

    override OnRead(object sender, Event e) {
        ShowFilesFromAmazon();
    }

Are there any API:s for this? Maybe to write to an XML-file or a database, instead of a real drive.

---

The [Dokan Library][1] seems to be the answer that mostly corresponds with my question, even though [System.IO.IsolatedStorage][2] seems to be the most standardized and most Microsoft-environment adapted.


  [1]: http://dokan-dev.net/en/
  [2]: http://msdn.microsoft.com/en-us/library/system.io.isolatedstorage.aspx
||||||||||||||You can use the <a href="https://dokan-dev.github.io/">Dokan library</a> to create a virtual drive. There is a .Net wrapper for interfacing with C#.

--------------------------------------------------
In Excel how could I change font colour according to content of a cell
I have a list of playing cards with each card in its own cell e.g.
3h
2s
Kd
Ah
Jc....
To help with visualisation I wanted to change the font colour of the hearts and diamonds to red.
Of course manually would be an option, but very tedious

I have tried (unsuccessfully) to write a vba script to do a search and replace using
https://stackoverflow.com/questions/17684155/excel-vba-find-and-replace as an example.
However as I am NOT changing the text it just looped continually. I also could not make the search format specific.

```
Sub Main()
Dim c As Range
Dim redCell As String

With Worksheets(1).Range(&quot;A1:A52&quot;)
Application.FindFormat.Clear
Application.FindFormat.Font.Color = rgbBlack &#39;Automatic was a problem
    Set c = .Find(&quot;h&quot;, LookIn:=xlValues, Searchformat:=True)
    If Not c Is Nothing Then
        redCell = c.Address
        Do
            Range(redCell).Font.Color = -16776961
            Set c = .FindNext(c)
        Loop While Not c Is Nothing
    End If
End With
End Sub
```

I also tried to use conditional formatting but could not find a criteria that worked.

    =OR(FIND(&quot;h&quot;,A1)=2,FIND(&quot;d&quot;,A1)=2) 

generates a #VALUE! error
Any pointers would be gratefully received.

||||||||||||||You can use this formula for your format condition:

    =OR(ISNUMBER(FIND("h",A1)),ISNUMBER(FIND("d",A1)))

Your attempt returns an error as `FIND("d",A1)` returns an error (no d found in A1). By using `ISNUMBER` `FIND("d",A1)` will return `false` instead of an error.


--------------------------------------------------
vs code not opening up in windows
whenever i try to open my vs code editor, nothing happens it doesn&#39;t launch and even there are no errors..!! And i am confused what&#39;s wrong here in my vs code. Please anyone help me fix it..!!

Below are the verbose command i typed in the terminal..

```
C:\Users\Avinash&gt;code . --verbose

[main 2020-05-10T05:17:56.317Z] Error: UNKNOWN: unknown error, mkdir
[main 2020-05-10T05:17:56.318Z] Lifecycle#kill()
[main 2020-05-10T05:17:56.320Z] [File Watcher (node.js)] Error: UNKNOWN: unknown error, stat &#39;c:\Users\Avinash Maurya\AppData\Roaming\Code\User&#39;
```
||||||||||||||no need of Unistalling, just go to your vscode-setup and reinstall it. (by this procedure all of your's settings, files , extensions etc.. will be restored as it is.)

--------------------------------------------------
How to copy a file from one folder to another using VBScript
How can I copy a file from one folder to another using VBScript?

I had tried this below one from the information provide on the internet:

    dim filesys
    
    set filesys=CreateObject(&quot;Scripting.FileSystemObject&quot;)
    
    If filesys.FileExists(&quot;c:\sourcefolder\anyfile.txt&quot;) Then
    
    filesys.CopyFile &quot;c:\sourcefolder\anyfile.txt&quot;, &quot;c:\destfolder\&quot;

When I execute this, I get a &#39;permission denied&#39; error. 
||||||||||||||Try this.  It will check to see if the file already exists in the destination folder, and if it does will check if the file is read-only.  If the file is read-only it will change it to read-write, replace the file, and make it read-only again.

    Const DestinationFile = "c:\destfolder\anyfile.txt"
    Const SourceFile = "c:\sourcefolder\anyfile.txt"
    
    Set fso = CreateObject("Scripting.FileSystemObject")
    	'Check to see if the file already exists in the destination folder
    	If fso.FileExists(DestinationFile) Then
    		'Check to see if the file is read-only
    		If Not fso.GetFile(DestinationFile).Attributes And 1 Then 
    			'The file exists and is not read-only.  Safe to replace the file.
    			fso.CopyFile SourceFile, "C:\destfolder\", True
    		Else 
    			'The file exists and is read-only.
    			'Remove the read-only attribute
    			fso.GetFile(DestinationFile).Attributes = fso.GetFile(DestinationFile).Attributes - 1
    			'Replace the file
    			fso.CopyFile SourceFile, "C:\destfolder\", True
    			'Reapply the read-only attribute
    			fso.GetFile(DestinationFile).Attributes = fso.GetFile(DestinationFile).Attributes + 1
    		End If
    	Else
    		'The file does not exist in the destination folder.  Safe to copy file to this folder.
    		fso.CopyFile SourceFile, "C:\destfolder\", True
    	End If
    Set fso = Nothing

--------------------------------------------------
How does Spring auto convert objects to json for @RestController
I&#39;m looking at code in which I&#39;m assuming spring decides to use Jackson behind the scenes to auto convert an object to json for a @RestController
```
@RestController 
@RequestMapping(&quot;/api&quot;)
public class ApiController {

    private RoomServices roomServices;

    @Autowired
    public ApiController(RoomServices roomServices) {
        this.roomServices = roomServices;
    }

    @GetMapping(&quot;/rooms&quot;)
    public List&lt;Room&gt; getAllRooms() {
        return this.roomServices.getAllRooms();
    }
}
```
The Room class is just a plain java class with some fields, getters/setters. There is no Jackson or any other explicit serialization going on in the code. Although this does return json when checking the url. I tried looking through the spring documentation but I&#39;m not quite sure what I&#39;m looking for. What is the name for this process in spring / how does it work? I tried with just @Controller and it broke. Is this functionality coming from @RestController? 
||||||||||||||If you are using [Spring Boot Starter Web][1], you can see that it's using [Spring Boot Starter JSON][2] through the compile dependencies, and Jackson is the dependency of the Start JSON library. So, you're assumption is right (Spring is using Jackson for JSON conversion by default)

Spring use it's AOP mechanism to intercept the mapping methods in `@Controller` (you can see that [`@RestController`][3] is actually a `@Controller` with `@ResponseBody`), spring create a proxy object (using JDK proxy or through cglib) for the class that annotated with `@Controller`.

When the request flow is processing, the program who really call the mapping method will be lead to the proxy first, the proxy will invoke the real `@Controller` object's method and convert it's returning value to JSON String using Jackson Library (if the method is annotated with `@ResponseBody`) and then return the JSON String back to the calling program.


  [1]: https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-web/2.4.1
  [2]: https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-json/2.4.1
  [3]: https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/bind/annotation/RestController.html

--------------------------------------------------
mount: unknown filesystem type &#39;vmhgsf&#39;
I&#39;m trying to mount my Windows shared folder in CentOS using command: 

    ~mount -t vmhgfs .host:/shared-folder /var/www/html/

Unfortunately I get :

    ~mount: unknown filesystem type &#39;vmhgfs&#39;

error. I tried to use:

    ~/usr/bin/vmhgfs-fuse /mnt

but mountpoint is not empty...

Is there any way to mount this folder on VMware player?
||||||||||||||Cyb

Try this:

    vmhgfs-fuse .host:/shared-folder /var/www/html/
you might need to use **sudo** on this

--------------------------------------------------
Why do we require “requires requires”?
One of the corners of C++20 constraints is that there are certain situations in which you have to write `requires requires`. For instance, this example from [\[expr.prim.req\]/3](http://eel.is/c++draft/expr#prim.req-3):

&gt; A _requires-expression_ can also be used in a _requires-clause_ ([temp]) as a way of writing ad hoc constraints on template arguments such as the one below:
&gt;
&gt; 
    template&lt;typename T&gt;
      requires requires (T x) { x + x; }
        T add(T a, T b) { return a + b; }

&gt; The first requires introduces the _requires-clause_, and the second introduces the _requires-expression_. 

What is the technical reason behind needing that second `requires` keyword? Why can&#39;t we just allow writing:

    template&lt;typename T&gt;
      requires (T x) { x + x; }
        T add(T a, T b) { return a + b; }

&lt;sub&gt;(Note: please don&#39;t answer that the grammar `requires` it)&lt;/sub&gt;
||||||||||||||It is because the grammar requires it. It does.

A `requires` constraint does not *have to* use a `requires` expression. It can use any more-or-less arbitrary boolean constant expression. Therefore, `requires (foo)` must be a legitimate `requires` constraint.

A `requires` *expression* (that thing that tests whether certain things follow certain constraints) is a distinct construct; it's just introduced by the same keyword. `requires (foo f)` would be the beginning of a valid `requires` expression.

What you want is that if you use `requires` in a place that accepts constraints, you should be able to make a "constraint+expression" out of the `requires` clause.

So here's the question: if you put `requires (foo)` into a place that is appropriate for a requires constraint... how far does the parser have to go before it can realize that this is a requires *constraint* rather than a constraint+expression the way you want it to be?

Consider this:

````
void bar() requires (foo)
{
  //stuff
}
````

If `foo` is a type, then `(foo)` is a parameter list of a requires expression, and everything in the `{}` is not the body of the function but the body of that `requires` expression. Otherwise, `foo` is an expression in a `requires` clause.

Well, you could say that the compiler should just figure out what `foo` is first. But C++ *really* doesn't like it when the basic act of parsing a sequence of tokens requires that the compiler figure out what those identifiers mean before it can make sense of the tokens. Yes, C++ is context-sensitive, so this does happen. But the committee prefers to avoid it where possible.

So yes, it's grammar.

--------------------------------------------------
Jquery clone row and its all elements with different id
HTML Table whose 2nd row which I want to clone is&lt;br/&gt;

    &lt;table id=&quot;tblDoc&quot; class=&quot;doc-Table&quot;&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;label&gt;Document Description&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Custom&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;File Type&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Ref&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Document&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr id=&quot;uploadrow_0&quot;&gt;
        &lt;td&gt;
            &lt;asp:DropDownList ID=&quot;ddlDocumentDescription_0&quot; runat=&quot;server&quot;&gt;&lt;/asp:DropDownList&gt;
        &lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;txtCustomFileName_0&quot; type=&quot;text&quot; class=&quot;upload-TextBoxes&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;select id=&quot;ddlFileType_0&quot; class=&quot;upload-Dropdowns&quot;&gt;
                &lt;option value=&quot;0&quot;&gt;--Select--&lt;/option&gt;
                &lt;option value=&quot;1&quot;&gt;A&lt;/option&gt;
                &lt;option value=&quot;2&quot;&gt;B&lt;/option&gt;
            &lt;/select&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;txtReferenceNo_0&quot; type=&quot;text&quot; class=&quot;upload-TextBoxes&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;fileDocument_0&quot; class=&quot;file-upload&quot; type=&quot;file&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
    
&lt;div&gt;
    &lt;span id=&quot;addAnother&quot; class=&quot;add-another&quot;&gt;+ Add Another&lt;/span&gt;
&lt;/div&gt;


I want to make a copy of second row each time on add another button.So I  have used  &lt;br/&gt;

    $(document).ready(function () {
        $(&quot;#addAnother&quot;).click(function () {
            addAnotherRow();
        });
    });

    function addAnotherRow() {
        var row = $(&quot;#tblDoc tr:nth-child(2)&quot;).clone();
        $(&#39;#tblDoc&#39;).append(row);
    }

When I clone it give same id for second row.&lt;br/&gt;&lt;br/&gt;
I want second row with id:&lt;br/&gt;
1 - uploadrow_1&lt;br/&gt;
2 - ddlDocumentDescription_1 (Its a asp.net control so id will not look like this)&lt;br/&gt;
3 - txtCustomFileName_1&lt;br/&gt;
4 - ddlFileType_1&lt;br/&gt;
5 - txtReferenceNo_1&lt;br/&gt;
6 - fileDocument_1&lt;br/&gt;
and so on.&lt;br/&gt;&lt;br/&gt;
Thanks in advance for any help.


||||||||||||||http://jsfiddle.net/y7q6x4so/3/

Select the last row and add id incrementing by one all the time.    

        function addAnotherRow() {
            var row = $("#tblDoc tr").last().clone();
            var oldId = Number(row.attr('id').slice(-1));
            var id = 1 + oldId;
            
            
            row.attr('id', 'uploadrow_' + id );
            row.find('#txtCustomFileName_' + oldId).attr('id', 'txtCustomFileName_' + id);
            row.find('#ddlDocumentDescription_' + oldId).attr('id', 'ddlDocumentDescription_' + id);
            row.find('#ddlFileType_' + oldId).attr('id', 'ddlFileType_' + id);
            row.find('#txtReferenceNo_' + oldId).attr('id', 'txtReferenceNo_' + id);
            row.find('#fileDocument_' + oldId).attr('id', 'fileDocument_' + id);
            
            $('#tblDoc').append(row);
        }

![enter image description here][1]


  [1]: http://i.stack.imgur.com/zx5a2.png

--------------------------------------------------
Trying to create edge list (weighted) to create adjacency list
I am storing the open coords as two attributes in one list:
```
self.x, self.y = []
```
My attempt at an edge list (lifted from stack overflow lol):
```
edge = []
        for i in self.x, self.y:
            for j in i[1]:
                edge.append([i[0], j])
                for i in edge:
                    print(i)
```

Whenever I try this the error:
```
for j in i[1]:
TypeError: &#39;int&#39; object is not subscriptable
```
comes up.

I&#39;m guessing this is because it&#39;s a tuple? I am trying to create an adjacency list with weighted edges of the distance between the coords, but I haven&#39;t thought about adding the weights yet. 

The coords, when added to a big list, look like this:
```
[[[0, 0], [1, 0], [2, 0].....]]
```
But when I insert them into the class, I do it separately.

On another note, can I store the all coords into one attribute like this effectively? Or would the attribute overwrite each time and only do one coord??



I was expecting, or hoped, that it would create edges between nodes to then create an adjacency list (of which I have no clue how to code either). With the completed graph, I aim to create an a* algorithm..

Sorry, if this may be obvious but I haven&#39;t coded properly in a very long time. I am aware it is kinda messy.

Thank you.
||||||||||||||Something like this?

~~~python
import itertools
import math

class Graph:
    def __init__(self):
        self.coords = []  # ← array of coords tuples (x, y)
        self.adjacency_list = (
            {}
        )  # ^ dictionary (hash lookup structure of 'key': 'value' pairs):
        #      where each key is a tuple (x, y)
        #      and each value is a list of (neighbor, weight) tuples.

    def add_coord(self, x, y):
        self.coords.append((x, y))

    def calculate_distance(self, coord1, coord2):
        x1, y1 = coord1
        x2, y2 = coord2
        return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)

    def build_adjacency_list(self):
        for coord1 in self.coords:
            self.adjacency_list[coord1] = []
            for coord2 in self.coords:
                if coord1 != coord2:
                    distance = self.calculate_distance(coord1, coord2)
                    self.adjacency_list[coord1].append((coord2, distance))


# Demo adjacency list
g = Graph()
for (x, y) in list(itertools.product(range(3), range(3))):
    print(x, y)
    g.add_coord(x, y)
g.build_adjacency_list()

# Print out the adjacency list
for coord, neighbors in g.adjacency_list.items():
    print(f"\n{coord}: \n{neighbors}")
~~~

which prints:

~~~none
(0, 0): 
[((0, 1), 1.0), ((0, 2), 2.0), ((1, 0), 1.0), ((1, 1), 1.4142135623730951), ((1, 2), 2.23606797749979), ((2, 0), 2.0), ((2, 1), 2.23606797749979), ((2, 2), 2.8284271247461903)]

(0, 1): 
[((0, 0), 1.0), ((0, 2), 1.0), ((1, 0), 1.4142135623730951), ((1, 1), 1.0), ((1, 2), 1.4142135623730951), ((2, 0), 2.23606797749979), ((2, 1), 2.0), ((2, 2), 2.23606797749979)]

(0, 2): 
[((0, 0), 2.0), ((0, 1), 1.0), ((1, 0), 2.23606797749979), ((1, 1), 1.4142135623730951), ((1, 2), 1.0), ((2, 0), 2.8284271247461903), ((2, 1), 2.23606797749979), ((2, 2), 2.0)]

(1, 0): 
[((0, 0), 1.0), ((0, 1), 1.4142135623730951), ((0, 2), 2.23606797749979), ((1, 1), 1.0), ((1, 2), 2.0), ((2, 0), 1.0), ((2, 1), 1.4142135623730951), ((2, 2), 2.23606797749979)]

(1, 1): 
[((0, 0), 1.4142135623730951), ((0, 1), 1.0), ((0, 2), 1.4142135623730951), ((1, 0), 1.0), ((1, 2), 1.0), ((2, 0), 1.4142135623730951), ((2, 1), 1.0), ((2, 2), 1.4142135623730951)]

(1, 2): 
[((0, 0), 2.23606797749979), ((0, 1), 1.4142135623730951), ((0, 2), 1.0), ((1, 0), 2.0), ((1, 1), 1.0), ((2, 0), 2.23606797749979), ((2, 1), 1.4142135623730951), ((2, 2), 1.0)]

(2, 0): 
[((0, 0), 2.0), ((0, 1), 2.23606797749979), ((0, 2), 2.8284271247461903), ((1, 0), 1.0), ((1, 1), 1.4142135623730951), ((1, 2), 2.23606797749979), ((2, 1), 1.0), ((2, 2), 2.0)]

(2, 1): 
[((0, 0), 2.23606797749979), ((0, 1), 2.0), ((0, 2), 2.23606797749979), ((1, 0), 1.4142135623730951), ((1, 1), 1.0), ((1, 2), 1.4142135623730951), ((2, 0), 1.0), ((2, 2), 1.0)]

(2, 2): 
[((0, 0), 2.8284271247461903), ((0, 1), 2.23606797749979), ((0, 2), 2.0), ((1, 0), 2.23606797749979), ((1, 1), 1.4142135623730951), ((1, 2), 1.0), ((2, 0), 2.0), ((2, 1), 1.0)]
~~~

`itertools.product(range(3), range(3))` produces: 
~~~none
0 0
0 1
0 2
1 0
1 1
1 2
2 0
2 1
2 2
~~~

In calling the method to build the adjacency list, all coordinates-tuple combinatorial (cartesian product, specifically) pairwise (excluding self-pairs...) connections were added to the dictionary for each individual coordinates tuple. This is what is printed above as the output. Below this is visualized.

<hr>

## Visualize the adjacency list as a network graph

~~~python
import networkx as nx
from pyvis.network import Network

# Create a NetworkX graph
G_nx = nx.Graph()

# Add edges to the NetworkX graph
for coord, neighbors in g.adjacency_list.items():
    for neighbor, weight in neighbors:
        G_nx.add_edge(coord, neighbor, weight=weight)

# Create a Pyvis network
net = Network(
    notebook=True,
    cdn_resources="remote",
    width="100%",
    bgcolor="white",
    font_color="red",
)
net.repulsion()

# Add nodes to the Pyvis network
for coord in g.adjacency_list.keys():
    net.add_node(str(coord), size=5) # *

# Add edges to the Pyvis network
for coord, neighbors in g.adjacency_list.items():
    for neighbor, weight in neighbors:
        net.add_edge(str(coord), str(neighbor), weight=weight)


for edge in net.edges:
    source, target = edge["from"], edge["to"]
    weight = G_nx[eval(source)][eval(target)]["weight"]
    edge["label"] = str(round(weight, 2))


net.show("example.html")
~~~

><sup>* _**Note**_: The size of nodes is ok to explicitly set here because all nodes have the same number of edges anyways. By default, they come out in the visualized graph a bit too big IMO.</sup>

[![Networkx+pyvis graph of adjacency list][1]][1]

And also, visualizing a slightly different pyvis+networkx graph, where the calculated distances (used to represent 'weights' in the example here) have an influence (e.g., by instead setting `net.barnes_hut()`):

[![Same network graph different 'physics' setting][2]][2]


  [1]: https://i.stack.imgur.com/Bt9Sn.png
  [2]: https://i.stack.imgur.com/Fq17m.gif

--------------------------------------------------
ckeditor&#39;s popup input fields dont work when used with bootstrap 5 modal (ckeditor 4)
I have come across an error while using ckeditor in bootstrap 5 modal and it looks like it&#39;s a very known error and many have given solution for it for different bootstrap versions but i am not able to figure out one for bootstrap 5, please have a look.

Here is the problem with solution:- https://stackoverflow.com/a/31679096

Other similar problems:-

https://stackoverflow.com/questions/19570661/ckeditor-plugin-text-fields-not-editable

https://stackoverflow.com/questions/14420300/bootstrap-with-ckeditor-equals-problems/18554395#18554395

Mainly what would be the alternative of below line for bootstrap 5. $.fn.modal.Constructor.prototype.enforceFocus

If I search for it in bootstrap 4 js file I&#39;m able to find fn.modal.Constructor in there but not in bootstrap 5. Please if someone can recreate the verified solution in the above link according to bootstrap 5 it would be very appreciated. Thank you for your time.

[image describing problem][1]

Also few notes:- 
1. All the other input types like checkboxes and dropdown works but not just text field.

2. I have also tried removing tabindex=&quot;-1&quot; from bootstrap modal code but the problem remains.


  [1]: https://i.stack.imgur.com/X04EL.png


||||||||||||||Thanks for this. Saved me a lot of head scratching. As of Bootstrap 5.3, this requires a small tweak:

```
bootstrap.Modal.prototype._initializeFocusTrap = function () { return { activate: function () { }, deactivate: function () { } } };
```

--------------------------------------------------
Getting NameResolutionError: Failed to resolve &#39;oauth2.googleapis.com&#39; trying to upload file to Google Cloud Storage via app on local k8s cluster
I have Kubernetes deployment with the following config

```
resource &quot;kubernetes_deployment&quot; &quot;batch-producer&quot; {
  metadata {
    name = var.app-name
    namespace = var.k8s-namespace.metadata[0].name
    labels = {
      app = var.app-name
    }
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        app = var.app-name
      }
    }

    template {
      metadata {
        labels = {
          app = var.app-name
        }
      }

      spec {
        container {
          name  = var.app-name
          image = var.docker-image

          image_pull_policy = &quot;Never&quot;

          port { container_port = 80 }
          port { container_port = 443 }

          command = [
            &quot;sh&quot;,
            &quot;-exc&quot;,
            &lt;&lt;-EOT
            mkdir /secrets
            echo ${var.storage-sa-key} | base64 --decode &gt; ./secrets/gcp_creds.json
            python ./run.py
            
            EOT
            ,
            &quot;&quot;
          ]

          env_from {
            config_map_ref {
              name = &quot;batch-producer-config&quot;
            }
          }

          env {
            name = &quot;GOOGLE_APPLICATION_CREDENTIALS&quot;
            value = &quot;/secrets/gcp_creds.json&quot;
          }
        }
      }
    }
  }
}
```

And app should just write file to GCS
```
class GCSWriter(AbstractWriter):
    def __init__(self, properties: dict):
        self.storage_client = storage.Client() \
            .from_service_account_json(json_credentials_path=os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;])
        self.bucket_name = properties.get(&quot;bucket_name&quot;)
        self.logger = get_logger()

    def write(self, source_path):
        # time.sleep(100)
        bucket = self.storage_client.bucket(self.bucket_name)
        blob = bucket.blob(os.path.join(
            &quot;incomes_data_source&quot;,
            dt.today().strftime(&#39;%Y/%m/%d&#39;),
            os.path.split(source_path)[1]))
        blob.upload_from_filename(source_path)
        self.logger.info(&quot;File &#39;%s&#39; was uploaded to GCS successfully&quot;, source_path)
```
App deploys, but I&#39;m getting the following error after some time:
```
HTTPSConnectionPool(host=&#39;oauth2.googleapis.com&#39;, port=443): Max retries exceeded with url: /token (Caused by NameResolutionError(&quot;&lt;urllib3.connection.HTTPSConnection object at 0x7f993b606bd0&gt;: Failed to resolve &#39;oauth2.googleapis.com&#39; ([Errno -3] Temporary failure in name resolution)&quot;))
```


I tried to ping google.com or download random file via curl from pod - success.
I also tried to use existing access-key.json running same container just via docker - it also works and I can see uploaded file in GCS.
Looking for a clue how I can resolve this.
||||||||||||||Thanks to Vasilii Angapov comment I found this issue - https://github.com/docker/for-mac/issues/7110.

I didn't dive deep into root cause, I just followed recommendation for downgrading to CoreDNS-1.10.0.

```
kubectl edit deployment/coredns -n kube-system
```
Changed version from 1.11.1 to 1.10.0 and wait for deployment to restart.

Everything works after that in my setup (Docker Desktop v4.27.1, Kubernetes v1.29.1)

I also tried to launch same configuration via Rancher Desktop with Kubernetes v1.28.n, it also works well.

--------------------------------------------------
How to generate time based UUIDs?
I want to generate time-based universally unique identifier (UUID) in Java. 

The method [`java.util.UUID.randomUUID()`][1] generates a [UUID Version 4][2] where 122 of the 128 bits are from a [cryptographically-strong][3] random number generator. 

How to generate a [Version 1][4] (time based) UUID ? Is there a separate library for that or is it some how provided in the Java 7 API and I am missing it.


  [1]: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/UUID.html#randomUUID()
  [2]: https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_(random)
  [3]: https://en.wikipedia.org/wiki/Strong_cryptography#Cryptographically_strong_algorithms
  [4]: https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_1_(date-time_and_MAC_address)
||||||||||||||FasterXML Java Uuid Generator (JUG)

https://github.com/cowtowncoder/java-uuid-generator

    UUID uuid = Generators.timeBasedGenerator().generate();

--------------------------------------------------
Oracle with PHP on Docker
I&#39;m trying to install an Oracle database drive for my Laravel application. I&#39;m using Laravel Sail to provide Docker.

The problem is that the Oracle driver can&#39;t build. This message occurs:

```
/usr/bin/ld: skipping incompatible /opt/oracle/instantclient_19_14/libclntsh.so when searching for -lclntsh
/usr/bin/ld: cannot find -lclntsh
/usr/bin/ld: skipping incompatible /opt/oracle/instantclient_19_14/libclntsh.so when searching for -lclntsh
collect2: error: ld returned 1 exit status
make: *** [Makefile:227: oci8.la] Error 1
ERROR: `make&#39; failed
```

My dockerfile: https://pastebin.com/RTPWt1XK

I&#39;m using MacBook Pro (v. 12 with M1)
||||||||||||||Using the **Instant Client for Linux ARM** ([instantclient-basic-linux.arm64-19.10.0.0.0][1]) and **PHP 8.2**

This `dockerfile` works for me:

```
FROM ubuntu:22.04

LABEL maintainer="Taylor Otwell"

ARG WWWGROUP
ARG NODE_VERSION=18
ARG POSTGRES_VERSION=14

WORKDIR /var/www/html

ENV DEBIAN_FRONTEND noninteractive
ENV TZ=UTC

RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

RUN apt-get update \
    && apt-get install -y gnupg gosu curl wget ca-certificates zip unzip git supervisor sqlite3 libcap2-bin libpng-dev python2 dnsutils \
    && curl -sS 'https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x14aa40ec0831756756d7f66c4f4ea0aae5267a6c' | gpg --dearmor | tee /etc/apt/keyrings/ppa_ondrej_php.gpg > /dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/ppa_ondrej_php.gpg] https://ppa.launchpadcontent.net/ondrej/php/ubuntu jammy main" > /etc/apt/sources.list.d/ppa_ondrej_php.list \
    && apt-get update \
    && apt-get install -y php8.2-cli php8.2-dev \
       php8.2-pgsql php8.2-sqlite3 php8.2-gd \
       php8.2-curl \
       php8.2-imap php8.2-mysql php8.2-mbstring \
       php8.2-xml php8.2-zip php8.2-bcmath php8.2-soap \
       php8.2-intl php8.2-readline \
       php8.2-ldap \
       php8.2-msgpack php8.2-igbinary php8.2-redis php8.2-swoole \
       php8.2-memcached php8.2-pcov php8.2-xdebug \
    && php -r "readfile('https://getcomposer.org/installer');" | php -- --install-dir=/usr/bin/ --filename=composer \
    && curl -sLS https://deb.nodesource.com/setup_$NODE_VERSION.x | bash - \
    && apt-get install -y nodejs \
    && npm install -g npm \
    && curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | tee /etc/apt/keyrings/yarn.gpg >/dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/yarn.gpg] https://dl.yarnpkg.com/debian/ stable main" > /etc/apt/sources.list.d/yarn.list \
    && curl -sS https://www.postgresql.org/media/keys/ACCC4CF8.asc | gpg --dearmor | tee /etc/apt/keyrings/pgdg.gpg >/dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/pgdg.gpg] http://apt.postgresql.org/pub/repos/apt jammy-pgdg main" > /etc/apt/sources.list.d/pgdg.list \
    && apt-get update \
    && apt-get install -y yarn \
    && apt-get install -y mysql-client \
    && apt-get install -y postgresql-client-$POSTGRES_VERSION \
    && apt-get -y autoremove \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

ENV LD_LIBRARY_PATH="/opt/oracle/instantclient_19_10/"
ENV ORACLE_HOME="/opt/oracle/instantclient_19_10/"
ENV OCI_HOME="/opt/oracle/instantclient_19_10/"
ENV OCI_LIB_DIR="/opt/oracle/instantclient_19_10/"
ENV OCI_INCLUDE_DIR="/opt/oracle/instantclient_19_10/sdk/include"
ENV OCI_VERSION=19

# Download Oracle
RUN mkdir /opt/oracle \
    && cd /opt/oracle \
    && wget https://download.oracle.com/otn_software/linux/instantclient/191000/instantclient-basic-linux.arm64-19.10.0.0.0dbru.zip \
    && wget https://download.oracle.com/otn_software/linux/instantclient/191000/instantclient-sdk-linux.arm64-19.10.0.0.0dbru.zip \
    && unzip /opt/oracle/instantclient-basic-linux.arm64-19.10.0.0.0dbru.zip -d /opt/oracle \
    && unzip /opt/oracle/instantclient-sdk-linux.arm64-19.10.0.0.0dbru.zip -d /opt/oracle \
    && rm -rf /opt/oracle/*.zip \
    && echo /opt/oracle/instantclient_19_10 > /etc/ld.so.conf.d/oracle-instantclient.conf \
    && ldconfig

# Configure Oracle
RUN apt-get update \
    && apt-get install -y \
      php-dev \
      php-pear \
      build-essential \
      libaio1 \
      libaio-dev \
      freetds-dev
RUN pecl channel-update pecl.php.net \
    && echo 'instantclient,/opt/oracle/instantclient_19_10' | pecl install oci8 \
    && echo extension=oci8.so >> /etc/php/8.2/cli/php.ini \
    && echo "extension=oci8.so" >> /etc/php/8.2/mods-available/oci8.ini

RUN setcap "cap_net_bind_service=+ep" /usr/bin/php8.2

RUN groupadd --force -g $WWWGROUP sail
RUN useradd -ms /bin/bash --no-user-group -g $WWWGROUP -u 1337 sail

COPY start-container /usr/local/bin/start-container
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf
COPY php.ini /etc/php/8.2/cli/conf.d/99-sail.ini
RUN chmod +x /usr/local/bin/start-container

EXPOSE 8000

ENTRYPOINT ["start-container"]

```


  [1]: https://www.oracle.com/database/technologies/instant-client/linux-arm-aarch64-downloads.html

--------------------------------------------------
Combining filters between multiple columns
How to sum filter using conditions in multiple columns

        A      B          C        D
      Ben      1         Tom       1
      Joe      3         Ben       4
      Tom      2         Ben       1

I want to get the sum of B,D where A,C does not equal Joe...basically get everyone&#39;s hours except Joes.

update: f you on the negative score...the system does not let me preview the question before posting...so forced to edit on the fly.



This seems so simple but I have been racking my brain trying to get it to work...maybe an array?


UPDATE:  it was as simple as a SUMIF! I&#39;ve used sumif many times lol.  Thanks to googlesheetsguy
||||||||||||||Here's a possbile solution

```cpp
=INDEX(QUERY(WRAPROWS(TOCOL(A2:D4),2),"select sum(Col2) where Col1 <> 'Joe'"),2)
```

[![enter image description here][1]][1]

If you only have two ranges, you can also use:

```cpp
=SUM(FILTER({B2:B4;D2:D4},"Joe"<>{A2:A4;C2:C4}))
```

[![enter image description here][2]][2]

But if you have a lot it becomes unpractical.

  [1]: https://i.stack.imgur.com/iy9Ex.png
  [2]: https://i.stack.imgur.com/U7BRf.png

--------------------------------------------------
Laravel and ngrok: url domain is not correct for routes and assets
My setup:

- Homestead on Mac OSX with multiple sites configured
- I have one site setup using domfit.test as the local domain (auto mapped using hostsupdater)

My problem:

If I `vagrant ssh`, and then `share domfit.test` I get a random generated ngrok url as you&#39;d expect (http://whatever.ngrok.io), however when I access this URL all my resources / routes are being prefixed with `http://domfit.test/` (http://domfit.test/login for instance)

I&#39;ve tried the following:

- Setting APP_URL as the ngrok URL
- `php artisan config:clear`
- `php artisan cache:clear`
- `{{ url(&#39;login&#39;) }}`
- `{{ route(&#39;login&#39;) }}`

My understanding is that `url()` should return the actual URL that the browser requested (rather than using `APP_URL`) but it always returns `domfit.test`.

If I rename my site in `Homestead.yaml` (for example to `newdomfit.test`) and re-provision then this is the domain that `url()` and `route()` uses, regardless of my `APP_URL`. So the `Homestead.yaml` seems to be forcing that domain. Which begs the question - how are you meant to actually use the share functionality?

I&#39;m new to Laravel so I am not sure if all of this is expected behavior and I am misunderstanding something? 

I just want my links and resources in templates to work for local (`domfit.test`), shared (`ngrok`) and eventually production with the same piece of code. My worry is I will have to change all of my `route()` or `url()` references when I attempt to put this website live.

**EDIT BELOW**

OK I&#39;ve just tried again. Changed `APP_URL` for `ngrok`:

Searched my entire codebase for `domfit.test`, and only some random session files seem to have references:

code/domfit/storage/framework/sessions/

    APP_NAME=DomFit
    APP_VERSION=0.01
    APP_ENV=local
    APP_KEY=XXXX
    APP_DEBUG=true
    APP_URL=http://04b7beec.ngrok.io

Then in my Controller I have it doing this for some simple debugging:

    echo(url(&#39;/login&#39;));
    echo(route(&#39;login&#39;));
    echo($_SERVER[&#39;HTTP_HOST&#39;]);
    echo($_SERVER[&#39;HTTP_X_ORIGINAL_HOST&#39;]);

If I use the `ngrok` URL the output I get is:

    http://domfit.test/login
    http://domfit.test/login
    domfit.test
    04b7beec.ngrok.io

I don&#39;t understand how `$_SERVER[&#39;HTTP_HOST&#39;]` is returning the wrong url?

It looks like it could be related to this: https://github.com/laravel/valet/issues/342

**ANOTHER EDIT**

It looks like it has to do with Homestead&#39;s `share` command:

    function share() {
    if [[ &quot;$1&quot; ]]
    then
        ngrok http ${@:2} -host-header=&quot;$1&quot; 80
    else
        echo &quot;Error: missing required parameters.&quot;
        echo &quot;Usage: &quot;
        echo &quot;  share domain&quot;
        echo &quot;Invocation with extra params passed directly to ngrok&quot;
        echo &quot;  share domain -region=eu -subdomain=test1234&quot;
    fi
}

Which passes the option `-host-header` to `ngrok` which according to their documentation:

&gt; Some application servers like WAMP, MAMP and pow use the Host header for determining which development site to display. For this reason, ngrok can rewrite your requests with a modified Host header. Use the -host-header switch to rewrite incoming HTTP requests.

If I use `ngrok` without it, then the website that gets displayed is a different one (because I have multiple sites configured in Homestead) - so I&#39;m still not sure how to get around this. For the time being I could disable the other sites as I&#39;m not actively developing those.
||||||||||||||### Update for ngrok 3.0+

ngrok 3.0 stopped using the `X-Original-Host` header and started using the `X-Forwarded-Host` header.

Therefore, if using ngrok 3.0+ with TrustedProxies set to trust all proxies (`protected $proxies = '*';`), then there should be nothing else that needs to change.

However, if not using TrustedProxies, all the below information is still relevant, just replace any references of `HTTP_X_ORIGINAL_HOST` with `HTTP_X_FORWARDED_HOST`.

### For ngrok < 3.0

Even though you're going to the ngrok url, the host header in the request is still set as the name of your site. Laravel uses the host header to build the absolute url for links, assets, etc. ngrok includes the ngrok url in the `X-Original-Host` header, but Laravel doesn't know anything about that.

There are two basic solutions to the issue:

1. update the request with the proper server and header values, or
2. use the `forceRootUrl()` method to ignore the server and header values.

---

**TrustedProxies and Forwarded Host**

If you're using TrustedProxies (default in Laravel >= 5.5), and you have it configured to trust all proxies (`protected $proxies = '*';`), you can set the `X-Forwarded-Host` header to the `X-Original-Host` header. Laravel will then use the value in the `X-Forwarded-Host` header to build all absolute urls.

You can do this at the web server level. For example, if you're using apache, you can add this to your `public/.htaccess` file:

    # Handle ngrok X-Original-Host Header
    RewriteCond %{HTTP:X-Original-Host} \.ngrok\.io$ [NC]
    RewriteRule .* - [E=HTTP_X_FORWARDED_HOST:%{HTTP:X-Original-Host}]

If you prefer to handle this in your application instead of the web server, you will need to update the Laravel request. There are plenty of places you could choose to do this, but one example would be in your `AppServiceProvider::boot()` method:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $request->server->set('HTTP_X_FORWARDED_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
            $request->headers->set('X_FORWARDED_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

---

**Not Using TrustedProxies**

If you're not using TrustedProxies, you can't use the `.htaccess` method. However, you can still update the server and headers values in your application. In this case, you'd need to overwrite the Host header:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $request->server->set('HTTP_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
            $request->headers->set('HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

---

**Using `forceRootUrl()`**

If you don't want to modify any headers or the Laravel request, you can simply tell the URL generator what root url to use. The URL generator has a `forceRootUrl()` method that you can use to tell it to use a specific value instead of looking at the request. Again, in your `AppServiceProvider::boot()` method:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $this->app['url']->forceRootUrl($request->server->get('HTTP_X_FORWARDED_PROTO').'://'.$request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

--------------------------------------------------
Switching between GCC and Clang/LLVM using CMake
I have a number of projects built using CMake and I&#39;d like to be able to easily switch between using GCC or Clang/LLVM to compile them. I believe (please correct me if I&#39;m mistaken!) that to use Clang I need to set the following:

        SET (CMAKE_C_COMPILER             &quot;/usr/bin/clang&quot;)
        SET (CMAKE_C_FLAGS                &quot;-Wall -std=c99&quot;)
        SET (CMAKE_C_FLAGS_DEBUG          &quot;-g&quot;)
        SET (CMAKE_C_FLAGS_MINSIZEREL     &quot;-Os -DNDEBUG&quot;)
        SET (CMAKE_C_FLAGS_RELEASE        &quot;-O4 -DNDEBUG&quot;)
        SET (CMAKE_C_FLAGS_RELWITHDEBINFO &quot;-O2 -g&quot;)
        
        SET (CMAKE_CXX_COMPILER             &quot;/usr/bin/clang++&quot;)
        SET (CMAKE_CXX_FLAGS                &quot;-Wall&quot;)
        SET (CMAKE_CXX_FLAGS_DEBUG          &quot;-g&quot;)
        SET (CMAKE_CXX_FLAGS_MINSIZEREL     &quot;-Os -DNDEBUG&quot;)
        SET (CMAKE_CXX_FLAGS_RELEASE        &quot;-O4 -DNDEBUG&quot;)
        SET (CMAKE_CXX_FLAGS_RELWITHDEBINFO &quot;-O2 -g&quot;)
        
        SET (CMAKE_AR      &quot;/usr/bin/llvm-ar&quot;)
        SET (CMAKE_LINKER  &quot;/usr/bin/llvm-ld&quot;)
        SET (CMAKE_NM      &quot;/usr/bin/llvm-nm&quot;)
        SET (CMAKE_OBJDUMP &quot;/usr/bin/llvm-objdump&quot;)
        SET (CMAKE_RANLIB  &quot;/usr/bin/llvm-ranlib&quot;)

Is there an easy way of switching between these and the default GCC variables, preferably as a system-wide change rather than project specific (i.e. not just adding them into a project&#39;s CMakeLists.txt)?

Also, is it necessary to use the `llvm-*` programs rather than the system defaults when compiling using clang instead of gcc? What&#39;s the difference?
||||||||||||||CMake honors the environment variables `CC` and `CXX` upon detecting the C and C++ compiler to use:

    $ export CC=/usr/bin/clang
    $ export CXX=/usr/bin/clang++
    $ cmake ..
    -- The C compiler identification is Clang
    -- The CXX compiler identification is Clang

The compiler specific flags can be overridden by putting them into a make override file and pointing the [`CMAKE_USER_MAKE_RULES_OVERRIDE`][1] variable to it. Create a file `~/ClangOverrides.txt` with the following contents:

    SET (CMAKE_C_FLAGS_INIT                "-Wall -std=c11")
    SET (CMAKE_C_FLAGS_DEBUG_INIT          "-g")
    SET (CMAKE_C_FLAGS_MINSIZEREL_INIT     "-Os -DNDEBUG")
    SET (CMAKE_C_FLAGS_RELEASE_INIT        "-O3 -DNDEBUG")
    SET (CMAKE_C_FLAGS_RELWITHDEBINFO_INIT "-O2 -g")
    
    SET (CMAKE_CXX_FLAGS_INIT                "-Wall -std=c++17")
    SET (CMAKE_CXX_FLAGS_DEBUG_INIT          "-g")
    SET (CMAKE_CXX_FLAGS_MINSIZEREL_INIT     "-Os -DNDEBUG")
    SET (CMAKE_CXX_FLAGS_RELEASE_INIT        "-O3 -DNDEBUG")
    SET (CMAKE_CXX_FLAGS_RELWITHDEBINFO_INIT "-O2 -g")

The suffix `_INIT` will make CMake initialize the corresponding `*_FLAGS` variable with the given value. Then invoke `cmake` in the following way:

    $ cmake -DCMAKE_USER_MAKE_RULES_OVERRIDE=~/ClangOverrides.txt ..

Finally to force the use of the LLVM binutils, set the internal variable `_CMAKE_TOOLCHAIN_PREFIX`. This variable is honored by the `CMakeFindBinUtils` module:

    $ cmake -D_CMAKE_TOOLCHAIN_PREFIX=llvm- ..

Setting `_CMAKE_TOOLCHAIN_LOCATION` is no longer necessary for CMake version 3.9 or newer.

Putting this all together you can write a shell wrapper which sets up the environment variables `CC` and `CXX` and then invokes `cmake` with the mentioned variable overrides. 

Also see this [CMake FAQ][2] on make override files.


  [1]: https://cmake.org/cmake/help/latest/variable/CMAKE_USER_MAKE_RULES_OVERRIDE.html
  [2]: https://gitlab.kitware.com/cmake/community/-/wikis/FAQ#make-override-files

--------------------------------------------------
How to put each element of array into another array in same order
I have first array:

    Array
    (
        [0] =&gt; generala value 1
        [1] =&gt; specificatii value 1
    )

and second array is:

 

       Array
    (
        [0] =&gt; Array
            (
                [title] =&gt; generala title 1
                [atribute_cat_id] =&gt; 1
                [product_id] =&gt; 98
            )
    
        [1] =&gt; Array
            (
                [title] =&gt; specificatii title 1
                [atribute_cat_id] =&gt; 2
                [product_id] =&gt; 98
            )    
    )

I want to get this array form:

    Array
    (
        [0] =&gt; Array
            (
                [title] =&gt; generala title 1
                [atribute_cat_id] =&gt; 1
                [product_id] =&gt; 98
                [value] =&gt; generala value 1
            )
    
        [1] =&gt; Array
            (
                [title] =&gt; specificatii title 1
                [atribute_cat_id] =&gt; 2
                [product_id] =&gt; 98
                [value] =&gt; specificatii value 1
            )
    
    )

I tried with array_merge but this method put all elements from first array to each element from second array! 
Every time both arrays have same number of elements! 
Any ideea?
Thank you!
||||||||||||||Loop over one of the arrays, using the indexes to access the corresponding element in the other array.

```
foreach ($first_array AS $i => $value) {
    $second_array[$i]['value'] = $value;
}
```


--------------------------------------------------
A* algorithm only exploring a few nodes before stopping - without reaching goal node
I am attempting to implement the A* algorithm but it only goes to three nodes before just stopping completely.

This is the algorithm code:
```
def AStar(start_node, end_node):
    openSet = PriorityQueue()
    openSet.enequeue(0, start_node)

    infinity = float(&quot;inf&quot;)

    gCost = {}
    fCost = {}
    cameFrom = {}

    for node in graph:
        gCost[node] = infinity
        fCost[node] = infinity
    gCost[start_node] = 0
    fCost[start_node] = heuristic(start_node, end_node)

    while not openSet.isEmpty():
        current = openSet.dequeue()  # Doesn&#39;t work yet

        if current == end_node:
            RetracePath(cameFrom, end_node)

        for neighbour in find_neighbors(start_node, graph):
            tempGCost = gCost[current] + 1

            if tempGCost &lt; gCost[neighbour]:
                cameFrom[neighbour] = current
                gCost[neighbour] = tempGCost
                fCost[neighbour] = tempGCost + heuristic(neighbour, end_node)

                if not openSet.contains(neighbour):
                    openSet.enequeue(fCost[neighbour], neighbour)

        print(f&quot;Came from: {cameFrom}\nCurrent: {current}&quot;)
    return False
```

And this is the code that finds the adjacent nodes:
```

def find_neighbors(node, graph):
    x, y = node
    neighbors = []

    right_neighbor = (x + 1, y)
    left_neighbor = (x - 1, y)
    lower_neighbor = (x, y + 1)
    upper_neighbor = (x, y - 1)

    if right_neighbor in graph:
        neighbors.append(right_neighbor)
    if left_neighbor in graph:
        neighbors.append(left_neighbor)
    if lower_neighbor in graph:
        neighbors.append(lower_neighbor)
    if upper_neighbor in graph:
        neighbors.append(upper_neighbor)
```

And this is an example of what is being outputted:
```
Enemy coords: (6, 2)
Player coords: 10, 2
Enemy neighbours: [(7, 2), (6, 3)]
Priority Queue: [[0, (6, 2)]]
Priority Queue: [[4, (7, 2)]]
Priority Queue: [[4, (7, 2)], [6, (6, 3)]]
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (6, 2)
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (7, 2)
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (6, 3)
```
Before it just stops the code without reaching the goal coordinates (player coords)

lmk if you have any questions about the code,
thank you.


||||||||||||||Your code check only the neighbors of the start node, when you call `find_neighbors` you always use `start_node` you should use `current` instead : 

```python
for neighbour in find_neighbors(current, graph):
    # your code
```

--------------------------------------------------
Why isn&#39;t my script printing all my results to the file?
I have the simple code below that loops through a dataframe and prints the results to the screen and also to a file.

My nag issue is however, it prints all the data to the screen just perfectly, but the file is only getting the last end of the data.

Here is my code:

    for star in Constellation_data(starDf.values.tolist()):
        print(star)
        sourceFile = open(&#39;stars.txt&#39;, &#39;w&#39;)
        print(star, file = sourceFile)
        sourceFile.close()

I open the file, then print to it, then close.  So I not sure why it doesn&#39;t contain all the data like the screen has.

Thanks!
||||||||||||||"w" deletes the existing file so for each iteration of the loop, you delete any previous content written. The normal way to handle this issue is to open the file once before the loop

    with open('stars.txt', 'w') as sourceFile:
        for star in Constellation_data(starDf.values.tolist()):
            print(star)
            print(star, file = sourceFile)

Note the `with` clause - it will automatically close the file when done.

If there is a reason why you want to close the file on each write (perhaps another file is reading it or you want to save state more often), then you can use append mode. I've added code to delete the old file and then append on each loop. The first append will create the file.

    if os.path.exists('stars.txt'):
        os.remove('stars.txt')
    for star in Constellation_data(starDf.values.tolist()):
        with open('stars.txt', 'a') as sourceFile:
            print(star)
            print(star, file = sourceFile)


--------------------------------------------------
DI resolves service by Interface of object and not by actual type
Let assume that we have following code

We have commands that all implement same interface
``` 
public interface ICommand {}
```
and that we have Command handlers that implement following interface 

``` 
public interface ICommandHandler&lt;T&gt; where T: ICommand {}
```

Actual implementations of command handlers are registered to DI. 

Now, if we have method that builds Commands based on some condition

```
public ICommand BuildCommand()
{
   if(someCondition) return new CommandA();
   else return new CommandB();
}
```

and we use it in code like this 
``` c#
public class SomeClass
{
  IServiceProvider _serviceProvder; 
  public void method_1()
  {
    ICommand command = BuildCommand();
    HandleCommand(command);
  }

  public void HandleCommand&lt;T&gt;(T command) 
  {
    var handler = _serviceProvider.GetRequiredService&lt;ICommandHandler&lt;T&gt;&gt;();
    handler.Handle();
  }
```
if will throw an error stating that it cannot resolve service 
``` ICommandHandler&lt;ICommand&gt; ```

I would expect DI to resolve by actual type (either CommandA or CommandB). How to get past this? 
||||||||||||||> I would expect DI to resolve by actual type (either CommandA or CommandB). How to get past this?

It can't resolve this, because of the following reasons:

* At compile time, you are supplying `ICommandHandler<ICommand>` to the `GetRequiredService<T>` method; it is given no runtime information that would allow it to spot anything different.
* When asked to resolve `ICommandHandler<ICommand>`, the container can't return anything else, because `ICommandHandler<ICommand>` is a different type to `ICommandHandler<CommandA>`. And even if it could, the request would even be ambiguous, because it could result in either an `ICommandHandler<CommandA>` *or* an `ICommandHandler<CommandB>`. Which one should it return?
* It's impossible to cast an `ICommandHandler<ICommand>` to `ICommandHandler<CommandA>` or vise versa in .NET, unless you make `ICommandHandler<T>` [variant][1] (i.e. you need to mark `T` with either `in` or `out`).

The solution here is to resort to using Reflection. For instance:

``` c#
public void HandleCommand(ICommand command) 
{
    Type handlerType = typeof(ICommandHandler<>).MakeGenericType(command.GetType());

    dynamic handler = _serviceProvider.GetRequiredService(handlerType);

    handler.Handle((dynamic)command);
}
```

In this example I'm using the `dynamic` keyword for simplicity. There are pros and cons to using this keyword. Obvious downside is of course loss of compile-time support. This is, IMO, not a big issue, because you would typically only have a single place in the application that calls command handlers using reflection, and that code can easily be tested. A more important downside, however, is that it will fail if a resolved handler implementation is internal - even when the `ICommandHandler<T>` is defined as `public`.

So instead of using dynamic, you can also invoke the method using the Reflection API:

```
public void HandleCommand(ICommand command) 
{
    Type handlerType = typeof(ICommandHandler<>).MakeGenericType(command.GetType());

    object handler = _serviceProvider.GetRequiredService(handlerType);

    MethodInfo method = handlerType.GetMethod("Handle");

    try
    {
        method.Invoke(handler, new object[] { command });
    }
    catch (TargetInvocationException ex)
    {
        ExceptionDispatchInfo.Capture(ex.InnerException).Throw();
    }
}
```


  [1]: https://learn.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/covariance-contravariance/variance-in-generic-interfaces

--------------------------------------------------
Generate P random N-dimensional points from list of ALL possible pairwise distances
I would like to generate random N-dimensional points with the constraint of having precise Euclidean distances (known) between each other.

Number of points `P = 100`
Number of dimensions of the hyperspace `N = 512`

Consequently, the possible number of pairwise distances is given by the formula `L = P*(P-1)/2`.
If `P = 100`, then `L = 4950`.

Let&#39;s say I have a list of 4950 distance values, where each value refers to a precise point-point combination.

Is it possible to implement this using numpy?

It is trivial to do it when considering pairs of points (`P = 2`) as `L = 1`, but I&#39;m trying to figure out if it can it be generalized to higher values of `P`?

This is my implementation for `P = 2`, considering `set_dist` as the desired distance value.

```
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances

N = 512

set_dist = 5.

point_0 = np.random.rand(N).reshape(1, -1)
point_1 = np.random.rand(N).reshape(1, -1)
rand_dist = euclidean_distances(point_0, point_1)
point_0 = point_0 * set_dist / rand_dist
point_1 = point_1 * set_dist / rand_dist
```
||||||||||||||With more spatial dimensions than points (i.e. *N* > *P*) this should be possible, if the distances are valid, which in particular means they have to satisfy the triangle inequality.

Let's take *N* = 3 for intuition. The first point you can pick anywhere. The distance between first and second defined a sphere around the first. The second has to lie somewhere on that sphere. The third has two distances to points you already placed. It has to lie on the intersection of the two corresponding spheres, which is a circle. A potential fourth point would lie on one of two points where three spheres intersect. For a fifth point you'd run out of dimensions, and rounding errors might make it difficult to satisfy all requirements simultaneously even if the distances originate from a real 3d configuration. That's why *N* > *P* is useful as you likely avoid this headache.

In terms of implementation, the above suggests that you would need to uniformly sample from hyperspheres of decreasing dimensions. You'd also have to inspect hyperspheres, and translate from the sample space to the actual positions. I don't know what rolls numpy has to offer to help with any of this.

Personally I'd also explore a different approach: generate the whole configuration in a simple well-defined position and orientation, then apply a random isometry (rotation, translation, perhaps reflection) to it. You would place the first point in the origin. The second point goes on the positive <i>x</i><sub>1</sub>-axis. The third on the <i>x</i><sub>1</sub>-<i>x</i><sub>2</sub>-plane with positive <i>x</i><sub>2</sub>-coordinate, and so on. So the number of zeros in the coordinate vector decreases by one for each point, and the newest coordinate is always positive. This should in general give you uniquely determined coordinates, shifting the whole randomisation to an operation on the complete configuration.

I haven't yet read the literature but I guess randomised sampling of isometries should have been discussed somewhere. But perhaps just applying a sequence of random operations, like some rotations around specific axes, will already make the result random enough? Depends on your requirements.

--------------------------------------------------
When I append items to a 2d list, it doesn&#39;t add to the same list
```
class PriorityQueue:
    def __init__(self):
        self.q = []

    def enqueue(self, priority, item):
        self.q.append([priority, item])
        self.q = sorted(self.q)
        return self.q


x = PriorityQueue()
print(x.enqueue(3, &quot;Potato&quot;))

y = PriorityQueue()
print(y.enqueue(1, &quot;Egg&quot;))
```

I&#39;m trying to do a priority list but it won&#39;t sort.

output;
```
[[3, &#39;Potato&#39;]]
[[1, &#39;Egg&#39;]]
```
How do I fix this?
||||||||||||||You are always creating a new `PriorityQueue`, I guess you want to have one queue:

    class PriorityQueue:
    
        def __init__(self):
            self.q = []
    
        def enqueue(self, priority, item):
            self.q.append([priority, item])
            self.q = sorted(self.q)
            return self.q
    
    
    y = PriorityQueue()
    print(y.enqueue(3, "Egg"))
    print(y.enqueue(4, "Potato"))
    print(y.enqueue(2, "Chesse"))
    print(y.enqueue(1, "Cake"))

Out:

    [[3, 'Egg']]
    [[3, 'Egg'], [4, 'Potato']]
    [[2, 'Cheese'], [3, 'Egg'], [4, 'Potato']]
    [[1, 'Cake'], [2, 'Cheese'], [3, 'Egg'], [4, 'Potato']]

--------------------------------------------------
Github workflow does not read variables from environments
Following is my simple github workflow. It is intended to print an environment variable. 
```
name: verify

on:
  workflow_dispatch:

jobs:
  read_env_variables:
    environment: build 
    runs-on: [ self-hosted, onprem_dae, docker ]
    steps:
      - name: cat on branch file
        run: |
          echo ${{ env.SOME_VARIABLE }}
```

I have created an environment named &quot;build&quot;. In this environment, I have an environment variable named `SOME_VARIABLE` set to *xyz*. 

When the workflow is triggered, I expected to echo value *xyz* but actual value is &quot;&quot;. Is there something missing? 

||||||||||||||Your issue here is related to the syntax.

To use the `${{ env.SOME_VARIABLE }}` syntax, you need to set an env variable at the workflow, job or step level.

**Here is an example:**

```yaml
name: Environment Workflow

on:
  workflow_dispatch:

env:
  WORKFLOW_VARIABLE: WORKFLOW

jobs:

  job1:
    runs-on: ubuntu-latest
    env:
      JOB_VARIABLE: JOB
    steps:
      - name: Run Commands with various variables
        if: ${{ env.WORKFLOW_VARIABLE == 'WORKFLOW' }}
        env:
          STEP_VARIABLE: STEP
        run: |
          echo "Hello World"
          echo "This is the $WORKFLOW_VARIABLE environment variable"
          echo "This is the $JOB_VARIABLE environment variable"
          echo "This is the $STEP_VARIABLE environment variable"
```

* * *

Now, if you want to use the **environment secrets for deployment**, [as explained here on the Github Documentation][1], the syntax would be different using the `job_id.environment` [as you are already using following this doc][2].

Here is an example:

```yaml
  job4:
    runs-on: ubuntu-latest
    environment: build
    steps:
      - name: Show repo env secret
        run: |
          echo ${{ secrets.REPO_ENV_SECRET }}
```

Note that **this variable is a secret**, therefore you won't be able to see it through an echo command on the step (it will show `***`)

* * *

Here is the workflow I used to validate all this implementation if you want to take a look:
- [workflow yaml file][3]
- [workflow run][4]


  [1]: https://docs.github.com/en/actions/deployment/targeting-different-environments/using-environments-for-deployment
  [2]: https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions#jobsjob_idenvironment
  [3]: https://github.com/GuillaumeFalourd/poc-github-actions/blob/main/.github/workflows/10-environment-workflow.yml
  [4]: https://github.com/GuillaumeFalourd/poc-github-actions/actions/runs/7297762218

--------------------------------------------------
How do I check the type of widget in GTK+3.0?
I saw [this][1] post but it was for Python so that doesn&#39;t help me too much. I&#39;m programming in C++, working on a code-base that I didn&#39;t write. I see some checks like `GTK_IS_ENTRY` and `GTK_IS_COMBO_BOX`, but I&#39;m not sure where this person found these or what other `GTK_IS_...` there are. Is there a reference to these somewhere? I searched online and also on the Gtk/GLib websites but I couldn&#39;t find anything. Thanks!


  [1]: https://stackoverflow.com/questions/60112777/find-type-of-gtk-widgets
||||||||||||||The type checks macros are typically part of the API contract for a GObject, and they are [conventionally provided by the library][1], so they don't end up in the documentation. All they do is call [`G_TYPE_CHECK_INSTANCE_TYPE`][2] with the given GType macro, like `GTK_TYPE_ENTRY` or `GTK_TYPE_COMBO_BOX`.


[1]: https://developer-old.gnome.org/gobject/stable/gtype-conventions.html
[2]: https://developer-old.gnome.org/gobject/stable/gobject-Type-Information.html#G-TYPE-CHECK-INSTANCE-TYPE:CAPS

--------------------------------------------------
Match all characters between two commas or between ,&quot; and &quot;, with regex powershell
Using powershell regex I would like it to find the first match between two commas or between ,&quot; and &quot;,

Example:

```
&quot;0x00000000&quot;,&quot;What do you want to eat? fish, meat or\n eggs?&quot;,&quot;&quot;
&quot;0x00030002&quot;,&quot;What do you want to eat?&quot;,&quot;&quot;
0x00030002,What do you want to eat?,
```

I want it to become:

```
What do you want to eat? fish, meat or eggs?
What do you want to eat?
What do you want to eat?
```




I tried this code but it doesn&#39;t behave correctly:

`(?&lt;=,&quot;|\?&lt;=,).*(?=&quot;,.*?|\?=,.*?)`

||||||||||||||Rather than using a regular expression for this (which has some pitfalls), I would use the native [`ConvertFrom-Csv` cmdlet](https://learn.microsoft.com/powershell/module/microsoft.powershell.utility/convertfrom-csv) for this:

    $List = @'
    "0x00000000","What do you want to eat? fish, meat or eggs?",""
    '"0x00030002","What do you want to eat?",""
    0x00000000,What do you want to eat? fish, meat or eggs?,
    0x00030002,What do you want to eat?,
    "0x00000000","What do you want to eat? ""fish"", ""meat"" or ""eggs?"""
    '@ -Split '\r?\n'

    $List | ConvertFrom-Csv -Header Hex, String, Rest | Select-Object -Expand String

    What do you want to eat? fish, meat or eggs?
    What do you want to eat?
    What do you want to eat? fish
    What do you want to eat?
    What do you want to eat? "fish", "meat" or "eggs?"

--------------------------------------------------
Why is Rails validator not using normalized value?
My model has a decimal amount attribute.

```
create_table :foos do |t|
  t.decimal :amount
end

class Foo &lt; ApplicationRecord
end
```

I always want the amount to be negative, so I add a normalisation:

```
class Foo &lt; ApplicationRecord
  normalizes :amount, with: -&gt; amount { - amount.abs }
end
```

This seems to work perfectly.

Now, to be safe, I add a validation:

```
class Foo &lt; ApplicationRecord
  normalizes :amount, with: -&gt; amount { - amount.abs }
  validates :amount, numericality: {less_than: 0}
end
```

Now when I set the amount to a positive value, although the normalisation converts it to a negative value, the validator seems to think the value is still positive and adds a validation error.

```
foo = Foo.new amount: 4
foo.amount  # =&gt; -4
foo.valid?  # =&gt; false
foo.errors  # =&gt; #&lt;ActiveModel::Error attribute=amount, type=less_than, options={:value=&gt;4, :count=&gt;0}&gt;
```

According to the tests for `normalizes`, [normalisation happens before validation](https://github.com/rails/rails/blob/0add5dba834f2f1b84fcf1bd1b758545b325fb73/activerecord/test/cases/normalized_attribute_test.rb#L35).

How can I get this to work?
||||||||||||||Numericality validator seems to be specifically using raw value for validation:  
*https://github.com/rails/rails/blob/v7.1.3/activemodel/lib/active_model/validations/numericality.rb#L129*

```rb
if record.respond_to?(came_from_user)
  if record.public_send(came_from_user)
    raw_value = record.public_send(:"#{attr_name}_before_type_cast")
```

Don't know if that is another bug or intentional. You could write your own validation to bypass this problem:

```rb
validate do
  errors.add(:amount, :less_than, value: amount, count: 0) unless amount.negative?
end
```

--------------------------------------------------
add recyclerview and cardview dependencies to gradle module
i want to add recyclerview and cardview dependencies to gradle module but it keeps giving error : the library should not use different version(25) then compile sdk version(26) ... i have latest updated android studio sdk version 26... here is code:apply plugin: &#39;com.android.application&#39;

    android {
        compileSdkVersion 26
        buildToolsVersion &quot;26.0.0&quot;
        defaultConfig {
            applicationId &quot;com.dpl_it.m.hamzam.widgets&quot;
            minSdkVersion 15
            targetSdkVersion 26
            versionCode 1
            versionName &quot;1.0&quot;
            testInstrumentationRunner &quot;android.support.test.runner.AndroidJUnitRunner&quot;
        }
        buildTypes {
            release {
                minifyEnabled false
                proguardFiles getDefaultProguardFile(&#39;proguard-android.txt&#39;), &#39;proguard-rules.pro&#39;
            }
        }
    }
    
    dependencies {
        compile fileTree(include: [&#39;*.jar&#39;], dir: &#39;libs&#39;)
        androidTestCompile(&#39;com.android.support.test.espresso:espresso-core:2.2.2&#39;, {
            exclude group: &#39;com.android.support&#39;, module: &#39;support-annotations&#39;
        })
        compile &#39;com.android.support:appcompat-v7:26.0.0&#39;
        compile &#39;com.android.support.constraint:constraint-layout:1.0.2&#39;
        testCompile &#39;junit:junit:4.12&#39;
        compile &#39;com.android.support:cardview-v7:25.4.0&#39;
        compile &#39;com.android.support:recyclerview-v7:25.4.0&#39;
    
    }


||||||||||||||Your dependency should be

    compile 'com.android.support:cardview-v7:26.0.0-beta2'
    compile 'com.android.support:recyclerview-v7:26.0.0-beta2'

but the support library for SDK 26 is in beta. See here for the recent notes

https://developer.android.com/topic/libraries/support-library/revisions.html

--------------------------------------------------
IntelliJ System.out.println() - Cannot resolve method println(java.lang.String)
I am using IntelliJ IDEA, learning Java. All went well until yesterday, when the mentioned error occurred.

I didn&#39;t make any changes. I was looking for the solution the following ways:

1. reboot the pc
2. restart IntelliJ.
3. delete the project directory and use another one (both on desktop) 

nothing helps. buy running simple hello world method. It keeps showing this error:

[![screenshot][1]][1]

Is there someone able to help me?


  [1]: http://i.stack.imgur.com/yyvkp.jpg
||||||||||||||ok, is solved.

file -> invalidated caches / Restart

--------------------------------------------------
CSS: Control space between bullet and &lt;li&gt;
I&#39;d like to control how much horizontal space a bullet pushes its `&lt;li&gt;` to the right in an `&lt;ol&gt;` or `&lt;ul&gt;`.

That is, instead of always having

    *  Some list text goes
       here.

I&#39;d like to be able to change that to be

    *         Some list text goes
              here.

or
 
    *Some list text goes
     here.

I looked around but could only find instructions for shifting the entire block left or right, for example, http://www.alistapart.com/articles/taminglists/
||||||||||||||Put its content in a `span` which is relatively positioned, then you can control the space by the `left` property of the `span`.

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-css -->

    li span {
      position: relative;
      left: -10px;
    }

<!-- language: lang-html -->

    <ul>
      <li><span>item 1</span></li>
      <li><span>item 2</span></li>
      <li><span>item 3</span></li>
    </ul>

<!-- end snippet -->



--------------------------------------------------
Is it possible for a client to receive an http response but the server not be certain that it did?
Is there every a case, with HTTP or HTTPS, where a server sends an HTTP response to a client, the client gets the response in full, but the server cannot be certain that the client got the response in full, for example if the final ACK or FIN message from the client wasn&#39;t received by the server?

And, if so, what are the conditions under which this might occur?

I looked through several RFCs and googled around, but couldn&#39;t find any relevant answers.
||||||||||||||Let's ask slightly different questions:

* Q1: Is it possible for the server to determine its response was successfully sent?
  
  A: Yes.

* Q2: Is it possible for the server to detect an error occurred sending its response?

  A: Yes.

* Q3: What happens if the TCP/IP connection abnormally terminates before a message is completely received by the client?

  A: Both the client and the server get a RST.

The answer to Q1 and Q2 is "Yes" on either/both of two different levels:

  * [TCP/IP connection level](https://accedian.com/blog/close-tcp-sessions-diagnose-disconnections): the connection is closed gracefully ... or not.

  * [HTTP "Persistent Connection" (RFC 2616)](https://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html): provides additional Application Layer error reporting capabilities.


--------------------------------------------------
Shadcn UI installation breaks Tailwind CSS
Shadcn UI (https://ui.shadcn.com/) was working fine until I just for a couple weeks until yesterday, when I ran my NextJS app in my local host and none of the tailwind was working. To debug the issue, I created a blank NextJS 13 app in a completely new file location, and everything worked fine; tailwind was working on the default nextJS 13 page. I then ran

```
npx shadcn-ui init
```

without installing any of the components. Which did not spit out any errors, but then none of the tailwind styling worked anymore.

my tailwind.config.js after instillation:

```
/** @type {import(&#39;tailwindcss&#39;).Config} */
module.exports = {
  darkMode: [&quot;class&quot;],
  content: [
    &#39;./pages/**/*.{ts,tsx}&#39;,
    &#39;./components/**/*.{ts,tsx}&#39;,
    &#39;./app/**/*.{ts,tsx}&#39;,
	],
  theme: {
    container: {
      center: true,
      padding: &quot;2rem&quot;,
      screens: {
        &quot;2xl&quot;: &quot;1400px&quot;,
      },
    },
    extend: {
      colors: {
        border: &quot;hsl(var(--border))&quot;,
        input: &quot;hsl(var(--input))&quot;,
        ring: &quot;hsl(var(--ring))&quot;,
        background: &quot;hsl(var(--background))&quot;,
        foreground: &quot;hsl(var(--foreground))&quot;,
        primary: {
          DEFAULT: &quot;hsl(var(--primary))&quot;,
          foreground: &quot;hsl(var(--primary-foreground))&quot;,
        },
        secondary: {
          DEFAULT: &quot;hsl(var(--secondary))&quot;,
          foreground: &quot;hsl(var(--secondary-foreground))&quot;,
        },
        destructive: {
          DEFAULT: &quot;hsl(var(--destructive))&quot;,
          foreground: &quot;hsl(var(--destructive-foreground))&quot;,
        },
        muted: {
          DEFAULT: &quot;hsl(var(--muted))&quot;,
          foreground: &quot;hsl(var(--muted-foreground))&quot;,
        },
        accent: {
          DEFAULT: &quot;hsl(var(--accent))&quot;,
          foreground: &quot;hsl(var(--accent-foreground))&quot;,
        },
        popover: {
          DEFAULT: &quot;hsl(var(--popover))&quot;,
          foreground: &quot;hsl(var(--popover-foreground))&quot;,
        },
        card: {
          DEFAULT: &quot;hsl(var(--card))&quot;,
          foreground: &quot;hsl(var(--card-foreground))&quot;,
        },
      },
      borderRadius: {
        lg: &quot;var(--radius)&quot;,
        md: &quot;calc(var(--radius) - 2px)&quot;,
        sm: &quot;calc(var(--radius) - 4px)&quot;,
      },
      keyframes: {
        &quot;accordion-down&quot;: {
          from: { height: 0 },
          to: { height: &quot;var(--radix-accordion-content-height)&quot; },
        },
        &quot;accordion-up&quot;: {
          from: { height: &quot;var(--radix-accordion-content-height)&quot; },
          to: { height: 0 },
        },
      },
      animation: {
        &quot;accordion-down&quot;: &quot;accordion-down 0.2s ease-out&quot;,
        &quot;accordion-up&quot;: &quot;accordion-up 0.2s ease-out&quot;,
      },
    },
  },
  plugins: [require(&quot;tailwindcss-animate&quot;)],
}
```

my utils.ts after instillation

```
import { ClassValue, clsx } from &quot;clsx&quot;
import { twMerge } from &quot;tailwind-merge&quot;
 
export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
```

[the default page after instillation](https://i.stack.imgur.com/Ta8jG.png)

EDIT: after some testing, the issue seems to be coming from the globals.css and tailwind.config.js, still not sure what about them though.
||||||||||||||In my case the Shadcn components did not find the styles exported by tailwind.
As answered by @moyindavid the problem is in tailwind.config.

Solution:
Add the '@' folder to the exports of the tailwind attributes.

```
module.exports = {
   darkMode: ["class"],
   content: [
     './pages/**/*.{ts,tsx}',
     './components/**/*.{ts,tsx}',
     './app/**/*.{ts,tsx}',
     './@/**/*.{ts,tsx}', // <- HERE
     ],
```

--------------------------------------------------
How to disable/enable the row in table with the same button?
In my table I have one buttons . In the table I want to disable/enable the entire row with the help of the same button. the button default is enable.
I want to each row all can enable/disable when click the button of the row .
When I click on &#39;enable&#39; button the entire row color will change to red and the button value change to &#39;disable&#39;. 
 click again to the &#39;disable&#39; button , thenthe entire row color recovery and the button value change to &#39;enable&#39;. 
How to do it . Help needed.

  part of  my code : 
[jsfiddle][1]


  [1]: https://jsfiddle.net/rZqLX/1/
    
    &lt;table &gt;
    &lt;tr&gt;
        &lt;th&gt;Value1&lt;/td&gt;
        &lt;th&gt;Value2&lt;/td&gt;
        &lt;th&gt;
            &lt;input type=&quot;button&quot; value=&quot;enable&quot; /&gt;
        &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Value3&lt;/td&gt;
        &lt;th&gt;Value4&lt;/td&gt;
        &lt;th&gt;
            &lt;input type=&quot;button&quot; value=&quot;enable&quot; /&gt;
        &lt;/th&gt;
    &lt;/tr&gt;
    &lt;/table&gt;


||||||||||||||You could change the value of the button and then check if it's 'enable' or 'disable'

Here's the code:

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    $('td input[type="button"]').on('click', function() {
        $(this).val((_, val) => val == "enable" ? "disable" : "enable");
        $(this).closest('tr').toggleClass('selected');
    });

<!-- language: lang-css -->

    .selected {
        background-color:red;
    }
    table {
        padding:0px;
        border-collapse: collapse;
    }

<!-- language: lang-html -->

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
    <table>
        <tr>
            <td>Value1</td>
            <td>Value2</td>
            <td>
                <input type="button" value="enable" />
            </td>
        </tr>
        <tr>
            <td>Value3</td>
            <td>Value4</td>
            <td>
                <input type="button" value="enable" />
            </td>
        </tr>
    </table>

<!-- end snippet -->



--------------------------------------------------
MySql error: incompatible with sql_mode=only_full_group_by
I&#39;ve inherited a CodeIgniter query that generates this SQL

EXAMPLE:

    SELECT `users`.`id`, `users`.`username`, `users`.`email`, `users`.`photo`, `users`.`rating`
    FROM `pool_details`
    JOIN `users` ON `users`.`id` = `pool_details`.`captain_id`
    WHERE `pool_details`.`pool_type` =0
    AND `pool_details`.`pool_close` &gt; &#39;2020-01-02 18:39:42&#39;
    GROUP BY `pool_details`.`captain_id`
    ORDER BY `pool_details`.`members_count` DESC
    LIMIT 20

&gt; ERROR - 2020-01-02 18:39:42 --&gt; Query error: Expression #1 of ORDER BY
&gt; clause is not in GROUP BY clause and contains nonaggregated column
&gt; &#39;pool_details.members_count&#39; which is not functionally
&gt; dependent on columns in GROUP BY clause; this is incompatible with
&gt; sql_mode=only_full_group_by - Invalid query:

Here is the same data with some &quot;extra columns&quot; and the offending clauses removed:

    SELECT users.id, users.username, users.email,users.rating,pool_details.members_count,pool_details.pool_type,pool_details.pool_close
    FROM  pool_details
    JOIN  users ON users.id = pool_details.captain_id
    WHERE pool_details.pool_type = 0 AND pool_details.pool_close &gt; &#39;2020-01-02 18:39:42&#39;
    ORDER BY pool_details.members_count DESC;
    //GROUP BY `pool_details`.`captain_id`
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    | id | username | email                    | rating | members_count | pool_type | pool_close          |
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         | 2020-01-04 03:00:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         | 2020-01-03 23:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         
    ...
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 00:00:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 00:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-04 21:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 03:00:00 |
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    28 rows in set (0.00 sec)

What I want is to:

  1. Show User&#39;s username, email and rating
  2. Order by &quot;members_count&quot;
  3. Show the user only *once*

For example:

    SELECT DISTINCT users.id, users.username, users.email,users.rating
    FROM  pool_details
    JOIN  users ON users.id = pool_details.captain_id
    WHERE pool_details.pool_type = 0 AND pool_details.pool_close &gt; &#39;2020-01-02 18:39:42&#39;;
    +----+----------+--------------------------+--------+
    | id | username | email                    | rating |
    +----+----------+--------------------------+--------+
    |  5 | wheel    | wheel@boxpik.com         | NULL   |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |
    +----+----------+--------------------------+--------+
    2 rows in set (0.00 sec)
    &lt;= This shows the users individually ... but it&#39;s *NOT* ordered by &quot;members_count&quot;.

Q: Is there any combination of &quot;GROUP BY&quot; and/or &quot;DISTINCT&quot; that I can use with mySql 5.7 that will give me the result set I need?


||||||||||||||Other than the `ORDER BY`, you seem to just want `EXISTS`.  That said, you can use another subquery in the `ORDER BY`:

    SELECT u.*
    FROM users u
    WHERE EXISTS (SELECT 1
                  FROM pool_details pd
                  WHERE pd.captain_id = u.id AND
                        pd.pool_type = 0 AND
                        pd.pool_close > '2020-01-02 18:39:42'
                 )
    ORDER BY (SELECT MAX(pd2.members_count)
              FROM pool_details pd2
              WHERE pd2.captain_id = u.id AND
                    pd2.pool_type = 0 AND
                    pd2.pool_close > '2020-01-02 18:39:42'
             ) DESC
    LIMIT 20

EDIT:

You can also write this as:

    SELECT u.*,
           (SELECT MAX(pd2.members_count)
            FROM pool_details pd2
            WHERE pd2.captain_id = u.id AND
                  pd2.pool_type = 0 AND
                  pd2.pool_close > '2020-01-02 18:39:42'
          ) as max_members_count
    FROM users u
    HAVING max_member_count IS NOT NULL
    ORDER BY max_member_count DESC
    LIMIT 20

Or:

    SELECT u.*
    FROM users u JOIN
         (SELECT pd2.captain_id, MAX(pd2.members_count) as max_member_count
          FROM pool_details pd2
          WHERE pd2.pool_type = 0 AND
                pd2.pool_close > '2020-01-02 18:39:42'
         ) pd
         ON pd.captain_id = u.id
    ORDER BY max_member_count DESC
    LIMIT 20l


--------------------------------------------------
Using a java class to create a database
I&#39;m working in an application that uses servlets and mysql.

I&#39;d like to create a .jar file able to create the database that the application will be using. This will only be done once, in order to create the db.

I&#39;ve no problem in getting to access to a database, doing something like this:

    Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance();
    Connection conexion = (Connection)DriverManager.getConnection(&quot;jdbc:mysql://localhost/test&quot;,&quot;admin&quot;,&quot;admin&quot;);
    if (!conexion.isClosed())
    {	
       Statement st = (Statement) conexion.createStatement();
       ResultSet rs = st.executeQuery(&quot;select * from table_name&quot; );
    }
    conexion.close();

This is ok, but what I need to do is to create a new database (and its tables) from a java class, is that possible?

I&#39;m trying this:		

    Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance();
    Connection conexion = (Connection)DriverManager.getConnection(&quot;jdbc:mysql://localhost/mysql&quot;,&quot;admin&quot;,&quot;admin&quot;);
		
    Statement st = (Statement) conexion.createStatement();       
    st.executeUpdate(&quot;CREATE DATABASE hrapp&quot;);

but I&#39;m getting the following error:

    Exception in thread &quot;main&quot; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Access denied for user &#39;admin&#39;@&#39;localhost&#39; to database &#39;hrapp&#39;
    	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    	at java.lang.reflect.Constructor.newInstance(Unknown Source)
    	at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    	at com.mysql.jdbc.Util.getInstance(Util.java:381)
    	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1030)
    	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
    	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3491)
    	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3423)
    	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1936)
    	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2060)
    	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2536)
    	at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1564)
    	at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1485)
    	at BaseDatosSetup.BaseDatosSetup.main(BaseDatosSetup.java:18)

I solved it by granting the create action to the user. I don&#39;t know why, I was doing it as an administrator.
||||||||||||||W3CSchools.com -- [SQL CREATE DATABASE Statement][1]. You wouldn't use `executeQuery` though. Instead use `executeUpdate`.

[Here][2] is a simple example.

As mentioned by other users, you probably don't want to be creating databases from your code. It just isn't good practice.


  [1]: http://www.w3schools.com/SQl/sql_create_db.asp
  [2]: http://www.java2s.com/Code/Java/Database-SQL-JDBC/CreateDatabaseforMySQL.htm

--------------------------------------------------
Server Error in &#39;/&#39; Application. Object reference not set to an instance of an object
I&#39;ve been trying to figure it out but not getting it.

**Description:** An unhandled exception occurred during the execution of the current web request. Please review the stack trace for more information about the error and where it originated in the code.

**Exception Details:**

    System.NullReferenceException: Object reference not set to an instance of an object.

**Source Error:**

An unhandled exception was generated during the execution of the current web request. Information regarding the origin and location of the exception can be identified using the exception stack trace below.

**Stack Trace:**


    [NullReferenceException: Object reference not set to an instance of an object.]
       Microsoft.WebTools.BrowserLink.Runtime.Tracing.PageInspectorHttpModule.OnPreRequestHandlerExecute(Object sender, EventArgs e) +662
       System.Web.SyncEventExecutionStep.System.Web.HttpApplication.IExecutionStep.Execute() +141
       System.Web.HttpApplication.ExecuteStepImpl(IExecutionStep step) +74
       System.Web.HttpApplication.ExecuteStep(IExecutionStep step, Boolean&amp; completedSynchronously) +92
||||||||||||||If you are using VS2019, make sure to uncheck Enable Browser Link
[VSCode EnableBroswerLink][1]


  [1]: https://i.stack.imgur.com/xlU6n.png

--------------------------------------------------
How to create a PDF/A from command line with Libre Office Draw in headless mode?
LibreOffice Draw allows you to open a non PDF/A file and export this a PDF/A-1b or PDF/A-2b file.

[![export as PDF][1]][1]

The same is possible from the command line by calling on macOS

```bash
/Applications/LibreOffice.app/Contents/MacOS/soffice --headless \
        --convert-to pdf:draw_pdf_Export \
        --outdir ./pdfout \
        ./input-non-pdfa.pdf
```

or an a Linux simply

```bash
libreoffice --headless \
        --convert-to pdf:draw_pdf_Export \
        --outdir ./pdfout \
        ./input-non-pdfa.pdf
```

On the command line it is possible to tell the `convert-to` to create a pdf and use LibreOffice Draw to do this by telling `--convert-to pdf:draw_pdf_Export`. 

Is there also a way to tell LibreOffice to produce a PDF/A document in **headless** mode?

  [1]: https://i.stack.imgur.com/mpSA6.png
||||||||||||||For PDF/A-1(means `PDF/A-1b`?):
```
soffice --headless --convert-to pdf:"writer_pdf_Export:SelectPdfVersion=1" --outdir outdir input.pdf
```
Change the value from `1` to `2` for PDF/A-2, here is the Libreoffice source code [Common.xcs](https://github.com/LibreOffice/core/blob/d4f5299fd2806d8f5dcd467742effeaa0dee8863/officecfg/registry/schema/org/openoffice/Office/Common.xcs#L5417-L5449), [pdfexport.cxx](https://github.com/LibreOffice/core/blob/e83b5f6a015269ed7e5407a8440c0fc99fcfa397/filter/source/pdf/pdfexport.cxx#L590-L623) and [pdffilter.cxx](https://github.com/LibreOffice/core/blob/bdbb5d0389642c0d445b5779fe2a18fda3e4a4d4/filter/source/pdf/pdffilter.cxx#L85).

- (Maybe outdated) [API/Tutorials/PDF export - Apache OpenOffice Wiki](https://wiki.openoffice.org/wiki/API/Tutorials/PDF_export)
- [Python Guide - PDF export filter data - The Document Foundation Wiki](https://wiki.documentfoundation.org/Macros/Python_Guide/PDF_export_filter_data)
- [excel->pdf変換 command のdpi設定 - Ask LibreOffice](https://ask.libreoffice.org/ja/question/229354/excel-pdfbian-huan-command-nodpishe-ding/)
- [Change default resolution in batch PNG conversion [closed] - Ask LibreOffice](https://ask.libreoffice.org/en/question/68775/change-default-resolution-in-batch-png-conversion/)

--------------------------------------------------
Heikin Ashi candle code in pine script V5
In pinescript version 4, the Heikin Ashi candle open is calculated as:

```
ha_close = (open + high + low + close)/4
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```
However, it shows compalilation error in pinescript version 5:
```
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```

The compilation error &quot;Undeclared identifier &#39;ha_open&#39;&quot;.

I have no idea what to do to solve this. 
||||||||||||||In pinescript, you must declare variables before using them.  
You should use (for version 5) :

    var float ha_open = na
    ha_close = (open + high + low + close)/4
    ha_open := na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2

`var float ha_open = na` declare the variable as a float and initialize it to `na`

--------------------------------------------------
fatal error: opencv2/opencv_modules.hpp: No such file or directory #include &quot;opencv2/opencv_modules.hpp&quot;
Hello all I am trying to use opencv-c++ API (version 4.4.0) which I have built from source. It is installed in /usr/local/ and I was simply trying to load and display an image using the following code - 
```
#include &lt;iostream&gt;
#include &lt;opencv4/opencv2/opencv.hpp&gt;
#include &lt;opencv4/opencv2/core.hpp&gt;
#include &lt;opencv4/opencv2/imgcodecs.hpp&gt;
#include &lt;opencv4/opencv2/highgui.hpp&gt;
#include &lt;opencv4/opencv2/core/cuda.hpp&gt;

using namespace cv;

int main()
{
    std::string image_path = &quot;13.jpg&quot;;
    cv::Mat img = cv::imreadmulti(image_path, IMREAD_COLOR);
    if(img.empty())
    {
        std::cout&lt;&lt;&quot;COULD NOT READ IMAGE&quot;&lt;&lt;std::endl;
        return 1;
    }
    imshow(&quot;Display Window&quot;, img);
    return 0;
}
```
And when I compile it throws the following error during compilation - 
```
In file included from /CLionProjects/opencvTest/main.cpp:2:
/usr/local/include/opencv4/opencv2/opencv.hpp:48:10: fatal error: opencv2/opencv_modules.hpp: No such file or directory
 #include &quot;opencv2/opencv_modules.hpp&quot;
```
My Cmake is as follows - 

```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
include_directories(&quot;/usr/local/include/opencv4/opencv2/&quot;)
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest PUBLIC &quot;/usr/local/lib/&quot;)
```
I do not know what am I doing wrong here.. This might be a noob question, But I ahev just started using opencv in C++
||||||||||||||The solution is to just include_directories path till `/usr/local/opencv4` and it works perfectly.

However, the best way I believe is to use the `find_package` function. I updated my Cmake to the following and it takes care of linking during build. 
```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
find_package(OpenCV REQUIRED)
include_directories(${OpenCV_INCLUDE_DIRS})
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest ${OpenCV_LIBS})
``` 


--------------------------------------------------
how to get the url parameters from http
I am working in a very rudimentary &quot;routing&quot; system for small CMS in nodejs without express or any framework. My aim is to have very few dependencies. 
For templating I found jrender that works fine in the sample route &quot;hey&quot; below: 

    var http = require(&#39;http&#39;)
    var jsrender = require (&#39;jsrender&#39;);    
    
    var html = jsrender.renderFile(&#39;./templates/hey.html&#39;, {name: &quot;Jim&quot;, age: &quot;22&quot;});
        
    
    http.createServer(function (req, res) {
    	res.writeHead(200, {&#39;Content-Type&#39;: &#39;text/html&#39;}); // http header
    
    	var url = req.url;
    	if(url ===&#39;/about&#39;){
            console.log (req.url)
      		res.write(&quot;hey&quot;); //write a response
      		res.end(); //end the response
            
    	}else if(url ===&#39;/contact&#39;){
      		res.write(&#39;&lt;h1&gt;contact us page&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
            
        }else if(url ===&#39;/hey&#39;){
      		res.write(html); //write a response
      		res.end(); //end the response    
            
    	}else{
      		res.write(&#39;&lt;h1&gt;Hello World!&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
    	}
    
    }).listen(3000, function(){
    	console.log(&quot;Judge Dress live on port 3000&quot;); //the server object listens on port 3000
    }); 


My problem is to get a parameter for a page e.g. /?pages=pagename to have dynamic routes. Is there any way to extact this parameter from req.url ? 

||||||||||||||You can use the node.js built-in 'querystring' module. To get "me" from "http://localhost:3000/about/?pages=me"

    const querystring = require('querystring');     
    console.log(querystring.parse(req.url)["/about/?pages"])

--------------------------------------------------
Regex to match all strings of given format with given exceptions
I&#39;m really struggling with this one. I tried to search from left to right, but still can&#39;t figure this out.

I have a list of strings with random amount of tags, each placed in brackets, randomly positioned within each string. Few examples may look as follows.


```
[tag1][tag4] Desired string - with optional dash [tag10]
[tag1][tag2][tag3] Desired string [tag10]
[tag3][tag1][tag2][tag5] Desired - string (with suffix)
[tag2][tag5][tag4] [Animation] Target string [tag10]
[tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
```

What I&#39;m trying to achieve is to extract from each string the content without tags, which are enclosed in brackets. The only exception is tag **[Animation]** or **[Animations]**. In case, one of these tags appear, I want to extract them as well together with the desired string.

So in case of list above, the desired output would be following. (I don&#39;t care about the whitespace around extracted strings, it will be trimmed afterwards.)

```
Desired string - with optional dash
Desired string
Desired - string (with suffix)
[Animation] Target string
[Animations](prefix)Desired - string (and suffix)
```


Originally, I was using as simple regex as `\[.*?\]`. Which matched all tags in brackets, and I simply replaced everything with empty string.

```python
re_pattern = r&quot;\[.*?\]&quot;
re.sub(re_pattern, &#39;&#39;, dirty_string).strip()
```

However, now I found a need to have an exception for tags **[Animation]** and **[Animations]**, and really can&#39;t figure it out. Your help would be much appreciated.
Thanks.
||||||||||||||You could use the better `regex` module with the following expression:

    \[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*

In `Python`, this could be

    import regex as re
    
    data = """
    [tag1][tag4] Desired string - with optional dash [tag10]
    [tag1][tag2][tag3] Desired string [tag10]
    [tag3][tag1][tag2][tag5] Desired - string (with suffix)
    [tag2][tag5][tag4] [Animation] Target string [tag10]
    [tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
    """
    
    pattern = re.compile(r'\[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*')
    
    print(pattern.sub("", data))

And would yield

    Desired string - with optional dash 
    Desired string 
    Desired - string (with suffix)
    [Animation] Target string 
    [Animations](prefix)Desired - string (and suffix)



--------------------------------------------------
Node.js server that accepts POST requests
I&#39;m trying to allow javascript to communicate with a Node.js server. 

**POST request (web browser)**

    var	http = new XMLHttpRequest();
    var params = &quot;text=stuff&quot;;
    http.open(&quot;POST&quot;, &quot;http://someurl.net:8080&quot;, true);
    
    http.setRequestHeader(&quot;Content-type&quot;, &quot;application/x-www-form-urlencoded&quot;);
    http.setRequestHeader(&quot;Content-length&quot;, params.length);
    http.setRequestHeader(&quot;Connection&quot;, &quot;close&quot;);
    
    alert(http.onreadystatechange);
    http.onreadystatechange = function() {
      if (http.readyState == 4 &amp;&amp; http.status == 200) {
        alert(http.responseText);
      }
    }
    
    http.send(params);

Right now the Node.js server code looks like this. Before it was used for GET requests. I&#39;m not sure how to make it work with POST requests.

**Server (Node.js)**

    var server = http.createServer(function (request, response) {
      var queryData = url.parse(request.url, true).query;
    
      if (queryData.text) {
        convert(&#39;engfemale1&#39;, queryData.text, response);
    	response.writeHead(200, {
    	  &#39;Content-Type&#39;: &#39;audio/mp3&#39;, 
    	  &#39;Content-Disposition&#39;: &#39;attachment; filename=&quot;tts.mp3&quot;&#39;
    	});
      } 
      else {
        response.end(&#39;No text to convert.&#39;);
      }
    }).listen(8080);
||||||||||||||The following code shows how to read values from an HTML form. As @pimvdb said you need to use the request.on('data'...) to capture the contents of the body.
```
const http = require('http')

const server = http.createServer(function(request, response) {
  console.dir(request.param)

  if (request.method == 'POST') {
    console.log('POST')
    var body = ''
    request.on('data', function(data) {
      body += data
      console.log('Partial body: ' + body)
    })
    request.on('end', function() {
      console.log('Body: ' + body)
      response.writeHead(200, {'Content-Type': 'text/html'})
      response.end('post received')
    })
  } else {
    console.log('GET')
    var html = `
			<html>
				<body>
					<form method="post" action="http://localhost:3000">Name: 
						<input type="text" name="name" />
						<input type="submit" value="Submit" />
					</form>
				</body>
			</html>`
    response.writeHead(200, {'Content-Type': 'text/html'})
    response.end(html)
  }
})

const port = 3000
const host = '127.0.0.1'
server.listen(port, host)
console.log(`Listening at http://${host}:${port}`)


```

If you use something like [Express.js][1] and [Bodyparser](https://www.npmjs.com/package/body-parser) then it would look like this since Express will handle the request.body concatenation


```
var express = require('express')
var fs = require('fs')
var app = express()

app.use(express.bodyParser())

app.get('/', function(request, response) {
  console.log('GET /')
  var html = `
    <html>
        <body>
            <form method="post" action="http://localhost:3000">Name: 
                <input type="text" name="name" />
                <input type="submit" value="Submit" />
            </form>
        </body>
    </html>`
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end(html)
})

app.post('/', function(request, response) {
  console.log('POST /')
  console.dir(request.body)
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end('thanks')
})

const port = 3000
app.listen(port)
console.log(`Listening at http://localhost:${port}`)

```

  [1]: http://expressjs.com/


--------------------------------------------------
How can I validate an email address using a regular expression?
Over the years I have slowly developed a [regular expression][1] that validates *most* email addresses correctly, assuming they don&#39;t use an IP address as the server part.

I use it in several PHP programs, and it works most of the time.  However, from time to time I get contacted by someone that is having trouble with a site that uses it, and I end up having to make some adjustment (most recently I realized that I wasn&#39;t allowing four-character [TLDs][2]).

*What is the best regular expression you have or have seen for validating emails?*

I&#39;ve seen several solutions that use functions that use several shorter expressions, but I&#39;d rather have one long complex expression in a simple function instead of several short expression in a more complex function.

  [1]: http://en.wikipedia.org/wiki/Regular_expression
  [2]: https://en.wikipedia.org/wiki/Top-level_domain



||||||||||||||The [fully RFC 822 compliant regex][1] is inefficient and obscure because of its length.  Fortunately, RFC 822 was superseded twice and the current specification for email addresses is [RFC 5322][2].  RFC 5322 leads to a regex that can be understood if studied for a few minutes and is efficient enough for actual use.

One RFC 5322 compliant regex can be found at the top of the page at http://emailregex.com/ but uses the IP address pattern that is floating around the internet with a bug that allows `00` for any of the unsigned byte decimal values in a dot-delimited address, which is illegal.  The rest of it appears to be consistent with the RFC 5322 grammar and passes several tests using `grep -Po`, including cases domain names, IP addresses, bad ones, and account names with and without quotes.

Correcting the `00` bug in the IP pattern, we obtain a working and fairly fast regex.  (Scrape the rendered version, not the markdown, for actual code.)

 > (?:[a-z0-9!#$%&'\*+/=?^\_\`{|}~-]+(?:\\.[a-z0-9!#$%&'\*+/=?^_\`{|}~-]+)\*|"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])\*")@(?:(?:\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?\\.)+\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])

or:

```
(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])
```

Here is [diagram][3] of [finite state machine][4] for above regexp which is more clear than regexp itself
[![enter image description here][5]][5]


The more sophisticated patterns in Perl and PCRE (regex library used e.g. in PHP) can [correctly parse RFC 5322 without a hitch][6]. Python and C# can do that too, but they use a different syntax from those first two. However, if you are forced to use one of the many less powerful pattern-matching languages, then it’s best to use a real parser.

It's also important to understand that validating it per the RFC tells you absolutely nothing about whether that address actually exists at the supplied domain, or whether the person entering the address is its true owner. People sign others up to mailing lists this way all the time. Fixing that requires a fancier kind of validation that involves sending that address a message that includes a confirmation token meant to be entered on the same web page as was the address. 

Confirmation tokens are the only way to know you got the address of the person entering it. This is why most mailing lists now use that mechanism to confirm sign-ups. After all, anybody can put down `president@whitehouse.gov`, and that will even parse as legal, but it isn't likely to be the person at the other end.

For PHP, you should *not* use the pattern given in [Validate an E-Mail Address with PHP, the Right Way][7] from which I quote:

> There is some danger that common usage and widespread sloppy coding will establish a de facto standard for e-mail addresses that is more restrictive than the recorded formal standard.

That is no better than all the other non-RFC patterns. It isn’t even smart enough to handle even [RFC 822][8], let alone RFC 5322. [This one][6], however, is.

If you want to get fancy and pedantic, [implement a complete state engine][9]. A regular expression can only act as a rudimentary filter. The problem with regular expressions is that telling someone that their perfectly valid e-mail address is invalid (a false positive) because your regular expression can't handle it is just rude and impolite from the user's perspective. A state engine for the purpose can both validate and even correct e-mail addresses that would otherwise be considered invalid as it disassembles the e-mail address according to each RFC. This allows for a potentially more pleasing experience, like

>The specified e-mail address 'myemail@address,com' is invalid. Did you mean 'myemail@address.com'?

See also [Validating Email Addresses][10], including the comments. Or [Comparing E-mail Address Validating Regular Expressions][11].

[![Regular expression visualization](https://i.stack.imgur.com/SrUwP.png)](https://i.stack.imgur.com/SrUwP.png)

[Debuggex Demo][12]


  [1]: http://ex-parrot.com/~pdw/Mail-RFC822-Address.html
  [2]: https://datatracker.ietf.org/doc/html/rfc5322
  [3]: https://regexper.com/#(%3F%3A%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B(%3F%3A%5C.%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B)*%7C%22(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21%5Cx23-%5Cx5b%5Cx5d-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)*%22)%40(%3F%3A(%3F%3A%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%5C.)%2B%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%7C%5C%5B(%3F%3A(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D))%5C.)%7B3%7D(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D)%7C%5Ba-z0-9-%5D*%5Ba-z0-9%5D%3A(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21-%5Cx5a%5Cx53-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)%2B)%5C%5D)
  [4]: https://en.wikipedia.org/wiki/Finite-state_machine
  [5]: https://i.stack.imgur.com/YI6KR.png
  [6]: https://stackoverflow.com/questions/201323/what-is-the-best-regular-expression-for-validating-email-addresses/1917982#1917982
  [7]: http://www.linuxjournal.com/article/9585
  [8]: https://datatracker.ietf.org/doc/html/rfc822
  [9]: http://cubicspot.blogspot.com/2012/06/correct-way-to-validate-e-mail-address.html
  [10]: http://worsethanfailure.com/Articles/Validating_Email_Addresses.aspx
  [11]: http://fightingforalostcause.net/misc/2006/compare-email-regex.php
  [12]: https://www.debuggex.com/r/aH_x42NflV8G-GS7

--------------------------------------------------
JPA generating broken SQL when using native query and pageable
Using Spring-Boot 2.7.7, when I attempt to create a native PostgreSQL query that receives a Pageable and outputs a Page, it seems to generate a broken SQL for one of the page attributes

This is the Query I used for the function:

```
    @Query(value =&quot;SELECT * &quot; +
            &quot;FROM propriedade &quot; +
            &quot;INNER JOIN proprietario &quot; +
            &quot;  ON proprietario.id = propriedade.proprietario_id &quot; +
            &quot;WHERE proprietario.nome ILIKE %:proprietario% &quot;,
            nativeQuery = true
    )
    Page&lt;Propriedade&gt; findByProprietarioLikePage(Pageable pageable, @Param(&quot;proprietario&quot;) String proprietario);
```
When I try to call the function, it generates a few SQL commands, but breaks in this one:
```
Hibernate: select count(INNER) FROM propriedade INNER JOIN proprietario   ON proprietario.id = propriedade.proprietario_id WHERE proprietario.nome ILIKE ?   AND condominio_id = ? 
2023-08-07 10:34:57.361  WARN 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42601
2023-08-07 10:34:57.361 ERROR 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: syntax error at or near &quot;)&quot;

```
If I try to run this count on PSQL, I get the same error:
```
ERROR:  syntax error at or near &quot;)&quot;
LINE 1: select count(INNER) FROM propriedade INNER JOIN proprietario...
                          ^
```
The problem seems to be with the generated query to count the entries in the table
||||||||||||||Please, try to add an alias, like "p", after "propriedade". It'll be like this: 

    @Query(value ="SELECT * " +
            "FROM propriedade p " +
            "INNER JOIN proprietario " +
            "  ON proprietario.id = p.proprietario_id " +
            "WHERE proprietario.nome ILIKE %:proprietario% ",
            nativeQuery = true
    )

--------------------------------------------------
Get child node index
In straight up javascript (i.e., no extensions such as jQuery, etc.), is there a way to determine a child node&#39;s index inside of its parent node without iterating over and comparing all children nodes?

E.g.,

    var child = document.getElementById(&#39;my_element&#39;);
    var parent = child.parentNode;
    var childNodes = parent.childNodes;
    var count = childNodes.length;
    var child_index;
    for (var i = 0; i &lt; count; ++i) {
      if (child === childNodes[i]) {
        child_index = i;
        break;
      }
    }

Is there a better way to determine the child&#39;s index?
||||||||||||||you can use the `previousSibling` property to iterate back through the siblings until you get back `null` and count how many siblings you've encountered:

    var i = 0;
    while( (child = child.previousSibling) != null ) 
      i++;
    //at the end i will contain the index.

Please note that in languages like Java, there is a `getPreviousSibling()` function, however in JS this has become a property -- `previousSibling`.

Use [previousElementSibling][2] or [nextElementSibling][1] to ignore text and comment nodes.


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element/nextElementSibling
  [2]: https://developer.mozilla.org/en-US/docs/Web/API/Element/previousElementSibling

--------------------------------------------------
Is there a way to inherit the parent __init__ arguments?
Suppose I have a basic class inheritance:

```
class A:
    def __init__(self, filepath: str, debug=False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, **kwargs):
        super(B, self).__init__(**kwargs)
        self.portnumber = portnumber
```

For typing and completion purposes, I would like to somehow &quot;forward&quot; the list of arguments from `A.__init__()` to `B.__init__()`.


Is there a way to do this? To have a type checker correctly infer the signature for `B.__init__(...)` and have an IDE be able to provide meaningful completions or checks?

---

[edit] after searching a little bit more, here is something that is perhaps closer to what I look:

if I declared `A` and `B` as _dataclasses_ :

```
from dataclasses import dataclass

@dataclass
class A:
    filepath: str
    debug: bool = False

@dataclass
class B(A):
    portnumber: int = 42
```

I can get the following hints in vscode with the standard pylance extension:
[![screen capture of vscode autocompletion][1]][1]

Could there be something similar to target just the `__init__()` method?  
perhaps by explicitly naming the base method that gets &quot;extended&quot; (e.g: a special `@extends(A.__init__)` decorator)?

  [1]: https://i.stack.imgur.com/Xv9L0m.png
||||||||||||||Yes this is possible, but personally I wouldn't recommend it - see below:

```py

from typing import TypedDict, Unpack

class AInterface(TypedDict):
    filepath: str
    debug: bool

class A:
    def __init__(self, **kwargs: Unpack[AInterface]):
        self.filepath = kwargs["filepath"]
        self.debug = kwargs["debug"]

class B(A):
    def __init__(self, portnumber: int, **kwargs: Unpack[AInterface]):
        super().__init__(**kwargs)
        self.portnumber = portnumber
```

By using the `TypedDict` we can structure the kwargs argument giving it a type, and allowing us to pass it through. If you have multiple inheritance you could even combine the interfaces together to produce the current kwargs type. When you use the `__init__` for A and B you still get warned if you miss parts of the `TypedDict`.

I would instead just pass the arguments down to the next layer manually:

```py
class A:
    def __init__(self, filepath: str, debug: bool =False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, filepath: str, debug: bool =False):
        super().__init__(filepath=filepath, debug = debug)
        self.portnumber = portnumber
```


--------------------------------------------------
MySQL / Laravel structure: monolith tables, or thousands of small tables?
**Short Version:**

Our webapp works with sets of scoped `project` data - that is, model relationships that will always relate to models in the same `project`, and strictly never cross over into other `project`s. Security / opacity between projects is a key requirement. The whole scoped relational ecosystem spans ~20 database tables so far.

We are currently managing this ecosystem with ~20 monolith tables, and enforcing opacity / security through code - but we&#39;re losing our grasp on it. We&#39;re considering adopting a structure where each `project` deploys its own clone of these ~20 tables into the same database. Are there any known fundamental drawbacks to having thousands of tables in one database, like increased storage size, slower performance, higher indexing overhead? Our team just doesn&#39;t have the database expertise to speak to flaws that might be introduced by this ourselves. 

If it makes any difference, we&#39;re using Laravel 10 - all models and relationships take advantage of Laravel / Eloquent structures. We&#39;re anticipating at least 200 projects active at a time, with a few being added or nuked each month.

-----

**Long Version:** We have an internal-use project org / management webapp, with some complicated requirements.

In broad strokes, there are `projects`, and each `project` has several related entities - `permissions`, `reports`, `tickets`, `labels`, `ticket_label`, `media`, `notifications`, many more. All related entities are scoped to their project - reports, labels, tickets, etc created inside a project by definition will never be transitioned to another, and no entities are generalized to multiple projects. Our project shareholders are adamant that a project&#39;s information is totally secure and opaque from other projects - no project should know any other project exists, and when a project is deleted there shouldn&#39;t be a trace of it left.

It&#39;s a little messy, because all these entities are binned into single tables with each other, regardless of project. There&#39;s extra work and middleware going into, for example, making sure some bad actor can&#39;t reassign a ticket ID in from an external project to gain access to its data. It&#39;s also complicated deleting a project and ensuring all nested / related data has been wiped - we get pretty far with appropriate foreign_keys and `-&gt;onDelete( &#39;cascade&#39; )`, but as the app gets more developed it&#39;s more and more difficult to **guarantee** that all data has been scrubbed when a project is deleted. There have been a few incidents where orphans containing sensitive information have been discovered. We&#39;ve been improving our tests and code when each is discovered, but it&#39;s becoming clear that we can&#39;t guarantee fallthroughs won&#39;t happen again - shareholders are expressing doubts.

Someone brought up that we can reduce a lot of complexity if we&#39;re able to generate a group of tables each time a new project is created. So, when project `0f9ebA2` is created, it creates tables `0f9ebA2_reports`, `0f9ebA2_tickets`, and so on as well. These tables will be identical structurally, so can all be created from the same migrations. In terms of convenience and cleanliness, the advantages are clear - foreign keys will be pointed to tables with the same prefix, and guarantee IDs outside of the project can&#39;t be assigned. It&#39;s also trivial to ensure all data has been scrubbed - just delete the tables. Many of the cross-pollination protections become obsolete, reduced to just access permissions.

The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach - and it doesn&#39;t seem like a common practice in MySQL, so there aren&#39;t a lot of articles or forums on the subject. We&#39;d like to get another perspective on this, and see if we&#39;re overlooking any fundamental flaws before we pull the trigger - like increased storage size, slower performance, higher indexing overhead. Matters of MySQL architecture and performance, over best practice and opinion.
||||||||||||||A few years ago I managed the databases at a company that had about 10,000 schemas, each schema had the same set of ~120 tables. They did this for similar reasons that you have, to make sure data for different clients is kept separate, for privacy and security reasons.

This was on MySQL 5.1 at the time. We found that after a few tens of thousands of tables on a given server, performance became a problem. It turns out that MySQL has internal data structures corresponding to each table, and they didn't architect this to handle so many tables. So eventually scanning lists of open tables becomes a bottleneck.

We split the schemas over seven servers, so each server didn't have so many tables. About 160,000 tables per server was our maximum.

I gave feedback to the MySQL product manager about this bottleneck, as they were developing a revamped implementation of the data dictionary for MySQL 8.0. He passed this along to the engineers, and they made sure to test scalability up to 1 million tables per server.

So definitely make sure that you use MySQL 8.0 or later to get this improvement.

But even with MySQL 8.0, this doesn't give you unlimited scalability. Eventually if you intend to keep growing, you must develop the capability to store data on more than one database server, and your applications need to have code to switch between database servers.

For example, in our case, one of the db servers had a simple table that stored a list of all the clients and which db server their data was stored on. This list was read at the startup of the app, and held in cache. It was a simple mapping list. Then on any request, the app could quickly tell which of seven db servers it should send the query. All the functions to run queries had an argument which was the db connection to use.

So in a way, "are there any performance limitations" is the wrong question. The assumption should be that there _are_ performance limitations, it's just a matter of how large can you grow until you hit those limits. Assume that you will.

Then the question is how to keep growing beyond those limits, and that means scaling out to multiple servers. Build this into your application design.

---

> The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach

Well, now's your opportunity to exercise your general software engineering skills, and develop some tests. 

You — or _someone_ on your team — hopefully have a degree in Computer Science? Well, approach the problem like a scientist. What type of tests would measure scalability of this kind? Presumably you'd need a lab where you could build a database with lots of tables. You'd need some scripts that can populate those tables, probably with a parameter so you can re-run the test at different scale. You'd need some way to drive query traffic in a repeatable fashion, and measure performance.

Then you need to develop requirements for scalability. What performance is acceptable? What rate of degradation is acceptable? Who gets to decide this?

No one starts with these skills. _They learn as they work._ "I don't have those skills" is not an excuse. They try something, they make mistakes, learn from them, improve their processes. 

You also need to learn that optimization and scalability is not about choosing the right technology. No technology scales if you use it improperly. Scalability comes from architecture, which should be where you have software engineering skills.

--------------------------------------------------
Cannot read properties of undefined (reading [api.reducerPath]) at Object.extractRehydrationInfo after clearing browser data
I have used redux persist with RTK query and redux toolkit. After clearing browser data manually from browser settings,
it could not rehydrate RTK query reducer and showing 

    Uncaught TypeError: Cannot read properties of undefined (reading &#39;notesApi&#39;)
        at Object.extractRehydrationInfo (notesApi.js:18:1)
        at createApi.ts:234:1
        at memoized (defaultMemoize.js:123:1)
        at createApi.ts:260:1
        at memoized (defaultMemoize.js:123:1)
        at createReducer.ts:239:1
        at Array.filter (&lt;anonymous&gt;)
        at reducer (createReducer.ts:236:1)
        at reducer (createSlice.ts:325:1)
        at combination (redux.js:560:1).

Here is the [screenshot of my problem][1].

Official Documentation says 

 - RTK Query supports rehydration via the extractRehydrationInfo option
   on createApi. This function is passed every dispatched action, and
   where it returns a value other than ***undefined***, that value is used to
   rehydrate the API state for fulfilled &amp; errored queries.

But what about ***undefined*** value like in my case?

This is my store




    const reducers = combineReducers({
      userReducer,
      [notesApi.reducerPath]: notesApi.reducer,
    });
    
    const persistConfig = {
      key: &quot;root&quot;,
      storage,
    };
    
    const persistedReducer = persistReducer(
      persistConfig,
      reducers
    );
    
    const store = configureStore({
      reducer: persistedReducer,
      middleware: (getDefaultMiddleware) =&gt;
        getDefaultMiddleware({
          serializableCheck: {
            ignoredActions: [FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER],
          },
        }).concat(notesApi?.middleware),
    });    
    
    export default store;




This is the notesApi



    export const notesApi = createApi({
     reducerPath: &quot;notesApi&quot; ,
      baseQuery: fetchBaseQuery({
        baseUrl: &quot;http://localhost:5000/api/notes/&quot;,
        prepareHeaders: (headers, { getState }) =&gt; {
          const token = getState().userReducer.userInfo.token;
          console.log(token);
          if (token) {
            headers.set(&quot;authorization&quot;, `Bearer ${token}`);
          }
          return headers;
        },
      }),
      extractRehydrationInfo(action, { reducerPath }) {
        if (action.type === REHYDRATE) {
            return action.payload[reducerPath]
        }
      },
      tagTypes: [&quot;notes&quot;],
    
      endpoints: (builder) =&gt; ({
        createNote: builder.mutation({
          query: (data) =&gt; ({
            url: `/create`,
            method: &quot;POST&quot;,
            body: data,
          }),
          invalidatesTags: [&quot;notes&quot;],
        }),
        getSingleNote: builder.query({
          query: (id) =&gt; ({
            url: `/${id}`,
          }),
          providesTags: [&quot;notes&quot;],
        })
    });
    export const {  useGetSingleNoteQuery,
      useCreateNoteMutation,
    } = notesApi;



  [1]: https://i.stack.imgur.com/jqawj.png
||||||||||||||I've run into this issue a few times and it seems to manifest when attempting to rehydrate the store when there isn't anything in localStorage to hydrate from.

The error is saying it can't read `"notesApi"` of undefined when running `extractRehydrationInfo`. `"notesApi"` is the API slice's `reducerPath` value. The action's payload is undefined.

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload[reducerPath]; // <-- action.payload undefined
      }
    },

To resolve this issue I've simply used the Optional Chaining operator on the action payload.

Example:

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload?.[reducerPath];
      }
    },

--------------------------------------------------
Pyspark. spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, java.net.SocketException: Connection reset
I am new to pyspark, and i&#39;m trying to run multiple time series in prophet with pyspark (as distributed computing because i have 100s of times series to predict) but i have error as below. 


```
import time 
start_time = time.time()
sdf = spark.createDataFrame(data)
print(&#39;%0.2f min: Lags&#39; % ((time.time() - start_time) / 60))
sdf.createOrReplaceTempView(&#39;Quantity&#39;)
spark.sql(&quot;select Reseller_City, Business_Unit, count(*) from Quantity group by Reseller_City, Business_Unit order by Reseller_City, Business_Unit&quot;).show()
query = &#39;SELECT Reseller_City, Business_Unit, conditions, black_week, promos, Sales_Date as ds, sum(Rslr_Sales_Quantity) as y FROM Quantity GROUP BY Reseller_City, Business_Unit, conditions, black_week, promos, ds ORDER BY Reseller_City, Business_Unit, ds&#39;
spark.sql(query).show()
sdf.rdd.getNumPartitions()
store_part = (spark.sql(query).repartition(spark.sparkContext.defaultParallelism[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;])).cache()

store_part.explain()

from pyspark.sql.types import *

result_schema =StructType([
  StructField(&#39;ds&#39;,TimestampType()),
  StructField(&#39;Reseller_City&#39;,StringType()),
  StructField(&#39;Business_Unit&#39;,StringType()),
  StructField(&#39;y&#39;,DoubleType()),
  StructField(&#39;yhat&#39;,DoubleType()),
  StructField(&#39;yhat_upper&#39;,DoubleType()),
  StructField(&#39;yhat_lower&#39;,DoubleType())
  ])
from pyspark.sql.functions import pandas_udf, PandasUDFType

@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )
def forecast_sales( store_pd ):
    
    model = Prophet(interval_width=0.95, holidays = lock_down)
    model.add_country_holidays(country_name=&#39;DE&#39;)
    model.add_regressor(&#39;conditions&#39;)
    model.add_regressor(&#39;black_week&#39;)
    model.add_regressor(&#39;promos&#39;)
    
    train = store_pd[store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;]
    future_pd = store_pd[store_pd[&#39;ds&#39;]&gt;=&#39;2021-10-01 00:00:00&#39;]
    model.fit(train[[&#39;ds&#39;, &#39;y&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])


    forecast_pd = model.predict(future_pd[[&#39;ds&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])  

    f_pd = forecast_pd[ [&#39;ds&#39;,&#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ].set_index(&#39;ds&#39;)

    #store_pd = store_pd.filter(store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;)

    st_pd = future_pd[[&#39;ds&#39;,&#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;]].set_index(&#39;ds&#39;)

    results_pd = f_pd.join( st_pd, how=&#39;left&#39; )
    results_pd.reset_index(level=0, inplace=True)

    results_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]] = future_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]].iloc[0]

    return results_pd[ [&#39;ds&#39;, &#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;, &#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ]
results = (store_part.groupBy([&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]).apply(forecast_sales).withColumn(&#39;training date&#39;, current_date() ))
results.cache()
results.show()
``` 

All the lines are executed perfectly but error the come from **results.show()**  line  I dont understand where i have done wrong, Much appreciated if someone helps me 

```
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-46-8c647e8bf4d9&gt; in &lt;module&gt;
----&gt; 1 results.show()

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\dataframe.py in show(self, n, truncate, vertical)
    438         &quot;&quot;&quot;
    439         if isinstance(truncate, bool) and truncate:
--&gt; 440             print(self._jdf.showString(n, 20, vertical))
    441         else:
    442             print(self._jdf.showString(n, int(truncate), vertical))

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\utils.py in deco(*a, **kw)
    126     def deco(*a, **kw):
    127         try:
--&gt; 128             return f(*a, **kw)
    129         except py4j.protocol.Py4JJavaError as e:
    130             converted = convert_exception(e.java_exception)

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
--&gt; 328                     format(target_id, &quot;.&quot;, name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o128.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 1243, Grogu.profiflitzer.local, executor driver): java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	... 1 more

```  
||||||||||||||You can also set the os env variables by following the below steps,
run this before SparkSession/SparkContext

    import os
    import sys
    
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

It worked for me

--------------------------------------------------
Calculate difference between 2 date / times in Oracle SQL
I have a table as follows:

    Filename - varchar
    Creation Date - Date format dd/mm/yyyy hh24:mi:ss
    Oldest cdr date - Date format dd/mm/yyyy hh24:mi:ss

How can I calcuate the difference in hours minutes and seconds (and possibly days) between the two dates in Oracle SQL?

Thanks


||||||||||||||You can substract dates in Oracle. This will give you the difference in days. Multiply by 24 to get hours, and so on.

    SQL> select oldest - creation from my_table;


If your date is stored as character data, you have to convert it to a date type first.


    SQL> select 24 * (to_date('2009-07-07 22:00', 'YYYY-MM-DD hh24:mi') 
                 - to_date('2009-07-07 19:30', 'YYYY-MM-DD hh24:mi')) diff_hours 
           from dual;
    
    DIFF_HOURS
    ----------
           2.5

---
*Note*:

This answer applies to dates represented by the Oracle data type `DATE`.
Oracle also has a data type `TIMESTAMP`, which can also represent a date (with time). If you subtract `TIMESTAMP` values, you get an `INTERVAL`; to extract numeric values, use the `EXTRACT` function.

--------------------------------------------------
Excel VBA Macro Returning &quot;Subscript Out of Range&quot; for Incremental Function Converting Hyperlinks to Raw URLs
I am writing an Excel VBA macro that is working fine except for one specific function, and I cannot figure out why it is failing; I&#39;m not a programmer, although I do have some understanding of the basic logic, just little/no experience at writing it, so apologies in advance if there is any confusion imparted by my attempted explanations. The error returned when attempting to execute the code in question is &quot;Run-time error code &#39;9&#39;: subscript out of range&quot;, and I&#39;ve copied the relevant code snippets below:

    &#39; Define variable for worksheet in question
    Dim wsSales As Worksheet
    Set wsSales = ThisWorkbook.Sheets(&quot;Sales&quot;)

    &#39; Find last row with data in it
    Dim lastRowSales As Long
    lastRowSales = wsSales.Cells(Rows.Count, &quot;J&quot;).End(xlUp).Row

    &#39; Loop through column J and convert hyperlinks to raw URLs
    For i = 2 To lastRowSales
        If wsSales.Cells(i, &quot;J&quot;).Hyperlinks.Count &gt; 0 Then
            wsSales.Cells(i, &quot;J&quot;).Value = wsSales.Hyperlinks(i).Address
        End If
    Next i`

- For extra info/context, column J of the Sales sheet referenced contains hyperlinked text (e.g., &quot;Object Name&quot; that points to a URL in a sales-related webpage), and I&#39;m trying to get the actual URL for each row in the range so I can output it elsewhere. Row 1 is a header row, so I&#39;m starting with &#39;i = 2&#39; to ignore it accordingly.
- What the above code ends up doing is partially successful, but specifically fails on the last row for some reason. So if I have, for example, 100 rows in column J of the Sales sheet (99 rows with data and 1 header row), it will successfully convert any hyperlinked values to a URL for the first 99 rows, but row 100 does not convert and Excel spits out the &#39;subscript out of range&#39; error. When looking at the highlighted code that failed after clicking &#39;Debug&#39; on the error pop-up in the VBA Editor, it is specifically the &#39;wsSales.Hyperlinks(i).Address&#39; part that returns a value of &#39;&lt;subscript out of range&gt;&#39;.
- Additionally, it does not actually convert things quite properly; for example, say that row 50 has a hyperlinked text string in it. Rather than converting cell J50 to show the URL that was in J50, it actually shows the URL for J51, and it does this for the entire range (where it&#39;s showing the URL of the cell below it, not the cell itself).
- If I start with &#39;i = 1&#39; instead to include checking the header row (which will never have a hyperlink, but I figured was worth testing), the function works identically - same behavior, same error, no difference at all relative to starting with &#39;i = 2&#39;. That seems to imply to me the error is somewhere either in the logic before the function actually executes or my references in the function itself.
- I have also tested the above code with &quot;wsSales.Hyperlinks(1).address&quot; (1 instead of i) and it ends up completing successfully but using the same URL for the entire column J, so there seems to be a flaw with that logic as an alternative (presumably the static reference for the Hyperlinks object).  The same is true if I use &#39;2&#39; instead of &#39;1&#39;, so I suspect that using any digit will give me the same core problem.
- I feel like there must be something wrong with either my function or some variable I&#39;ve defined that is causing this, but after looking extensively through my code and attempting to &#39;rubber ducky&#39; troubleshoot it, I&#39;m still coming up blank.


I&#39;ve used essentially the exact same logic for multiple other formulas that compose the rest of the larger macro and they all work properly, but this function specifically fails to work as expected; every other &#39;for i = # To [value]&#39; iterates successfully and commenting out the above code snippet from the larger macro enables the full macro to work exactly as expected, just not this function. Does anyone have any thoughts or suggestions for why this may be failing to function as expected? Any ideas for what logic I should check, what may be failing, or a better way to do this? Any advice would be greatly appreciated, thanks!
||||||||||||||Refer to the hyperlink in *each specific cell*, not the [`Worksheet.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.worksheet.hyperlinks) collection:
```
For i = 2 To lastRowSales
    If wsSales.Cells(i, "J").Hyperlinks.Count > 0 Then
        wsSales.Cells(i, "J").Value = wsSales.Cells(i, "J").Hyperlinks(1).Address
    End If
Next i`
```
In other words, you want to use the [`Range.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.range.hyperlinks) property.

If you did want to use the `Worksheet.Hyperlinks` approach:

```
Dim h As Hyperlink
For Each h In wsSales.Hyperlinks
    h.Range.Value = h.Address
Next
```

--------------------------------------------------
How to send email attachments?
I am having problems understanding how to email an attachment using Python. I have successfully emailed simple messages with the `smtplib`. Could someone please explain how to send an attachment in an email. I know there are other posts online but as a Python beginner I find them hard to understand. 

    
||||||||||||||Here's another:

    import smtplib
    from os.path import basename
    from email.mime.application import MIMEApplication
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    from email.utils import COMMASPACE, formatdate
    
    
    def send_mail(send_from, send_to, subject, text, files=None,
                  server="127.0.0.1"):
        assert isinstance(send_to, list)
    
        msg = MIMEMultipart()
        msg['From'] = send_from
        msg['To'] = COMMASPACE.join(send_to)
        msg['Date'] = formatdate(localtime=True)
        msg['Subject'] = subject
 
        msg.attach(MIMEText(text))

        for f in files or []:
            with open(f, "rb") as fil:
                part = MIMEApplication(
                    fil.read(),
                    Name=basename(f)
                )
            # After the file is closed
            part['Content-Disposition'] = 'attachment; filename="%s"' % basename(f)
            msg.attach(part)

    
        smtp = smtplib.SMTP(server)
        smtp.sendmail(send_from, send_to, msg.as_string())
        smtp.close()


It's much the same as the first example... But it should be easier to drop in.

  [1]: http://snippets.dzone.com/posts/show/2038

--------------------------------------------------
Get handle to desktop / shell window
In one of my programs, I need to test if the user is currently focusing the desktop/shell window. Currently, I&#39;m using `GetShellWindow()` from *user32.dll* and compare the result to `GetForegroundWindow()`.

This approach is working until someone changes the desktop wallpaper, but as soon as the wallpaper is changed the handle from `GetShellWindow()` doesn&#39;t match the one from `GetForegroundWindow()` anymore and I don&#39;t quite get why that is. (**OS:** Windows 7 32bit)

Is there a better approach to check if the desktop is focused? Preferably one that won&#39;t be broken if the user changes the wallpaper?

**EDIT:** I designed a workaround: I&#39;m testing the handle to have a child of class `SHELLDLL_DefView`. If it has, the desktop is on focus. Whilst, it&#39;s working at my PC that doesn&#39;t mean it will work all the time.
||||||||||||||The thing changed a little bit since there are slideshows as wallpaper available in Windows 7.
You are right with WorkerW, but this works only with wallpaper is set to slideshow effect. 

When there is set the wallpaper mode to slideshow, you have to search for a window of class `WorkerW` and check the children, whether there is a `SHELLDLL_DefView`.
If there is no slideshow, you can use the good old `GetShellWindow()`.

I had the same problem some months ago and I wrote a function for getting the right window. Unfortunately I can't find it. But the following should work. Only the Win32 Imports are missing:

    public enum DesktopWindow
    {
        ProgMan,
        SHELLDLL_DefViewParent,
        SHELLDLL_DefView,
        SysListView32
    }
    
    public static IntPtr GetDesktopWindow(DesktopWindow desktopWindow)
    {
        IntPtr _ProgMan = GetShellWindow();
        IntPtr _SHELLDLL_DefViewParent = _ProgMan;
        IntPtr _SHELLDLL_DefView = FindWindowEx(_ProgMan, IntPtr.Zero, "SHELLDLL_DefView", null);
        IntPtr _SysListView32 = FindWindowEx(_SHELLDLL_DefView, IntPtr.Zero, "SysListView32", "FolderView");
    
        if (_SHELLDLL_DefView == IntPtr.Zero)
        {
            EnumWindows((hwnd, lParam) =>
            {
                if (GetClassName(hwnd) == "WorkerW")
                {
                    IntPtr child = FindWindowEx(hwnd, IntPtr.Zero, "SHELLDLL_DefView", null);
                    if (child != IntPtr.Zero)
                    {
                        _SHELLDLL_DefViewParent = hwnd;
                        _SHELLDLL_DefView = child;
                        _SysListView32 = FindWindowEx(child, IntPtr.Zero, "SysListView32", "FolderView"); ;
                        return false;
                    }
                }
                return true;
            }, IntPtr.Zero);
        }
    
        switch (desktopWindow)
        {
            case DesktopWindow.ProgMan:
                return _ProgMan;
            case DesktopWindow.SHELLDLL_DefViewParent:
                return _SHELLDLL_DefViewParent;
            case DesktopWindow.SHELLDLL_DefView:
                return _SHELLDLL_DefView;
            case DesktopWindow.SysListView32:
                return _SysListView32;
            default:
                return IntPtr.Zero;
        }
    }

In your case you would call `GetDesktopWindow(DesktopWindow.SHELLDLL_DefViewParent);` to get the top-level window for checking whether it is the foreground window.

--------------------------------------------------
Keil compiler v5 to v.6
I&#39;m forced to switch from ARMCC v5 to CLANG(v.6). Here is the problem. 
I have some struct that includes a pointer to the function which gets as a parameter pointer to the same structure. 
So I do 

```
struct _some_struct_s;
typedef void (*callback_f)(struct _some_struct *p);
 
typedef struct {
  callback_f fn;
  int        x; 
} some_type_s;

// init function
void init_some_struct (some_struct *p, callback_f f) {
  p-&gt;fn = f;
  p-&gt;x = 0;
}
```
In another file I&#39;m writing the callback() and calling init_some_struct()
```
some_type_s  my_struc;
void callback (some_type_s *p) {
  p-&gt;x++;
}
init_some_struct (&amp;my_struc, callback);
```
I had no issues with compiler 5 but a warning with version 6.
***
```
warning: incompatible function pointer types passing &#39;void (some_struct_s *)&#39; to parameter of type &#39;callback_f&#39; (aka &#39;void (*)(struct _some_struct_s *)&#39;) [-Wincompatible-function-pointer-types]
```
What can I do to avoid having this warning?


What can I do to avoid having this warning?

||||||||||||||1. `typedef (*callback_f)(struct _some_struct *p);` - you alias the type as function of pointer which returns `int`.

It should be `typedef void (*callback_f)(struct some_struct *p);`

2.     typedef struct {
            callback_f *fn;
   `fn` is a pointer to pointer to function. It should be `callback_f fn;`

```
typedef struct some_struct _some_struct_s;
typedef void callback_f(struct _some_struct *p);
 
typedef struct some_struct{
  callback_f *fn;
  int        x; 
} some_struct_s;

// init function
void init_some_struct (some_struct *p, callback_f *f) {
  p->fn = f;
  p->x = 0;
}
```



--------------------------------------------------
How to create mutually exclusive fields in Pydantic
I am using Pydantic to model an object. How can I make two fields mutually exclusive?

For instance, if I have the following model:

    class MyModel(pydantic.BaseModel):
        a: typing.Optional[str]
        b: typing.Optional[str]

I want field `a` and field `b` to be mutually exclusive. I want only one of them to be set. Is there a way to achieve that?
||||||||||||||You can use pydantic.validator decorator to add custom validations.

```lang-python
from typing import Optional
from pydantic import BaseModel, validator

class MyModel(BaseModel):
    a: Optional[str]
    b: Optional[str]

    @validator("b", always=True)
    def mutually_exclusive(cls, v, values):
        if values["a"] is not None and v:
            raise ValueError("'a' and 'b' are mutually exclusive.")

        return v
```

--------------------------------------------------
Web automation with Selenium + python and google chrome 115.x &gt;
How to use selenium and chrome CFT for web automation from chrome version 115.x using python?

I have an automation script that worked fine until chrome version 114.x. From version 115.x it stopped working due to the version update, but also due to the new method with chrome cft.
||||||||||||||After upgrading to Chrome version 115.x, my automation stopped working, and chrome driver versions were no longer released, because from chrome version 115.x, automations are performed by CFT (chrome for test ), which as I understand this browser remains static until user action, preventing automations from stopping due to automatic chrome updates and need for crhome driver replacement.
The problem was solved with the solution below:


```
# using selenium 4.8 and python 3.9

from selenium import webdriver
from selenium.webdriver.chrome.options import Options


options = Options()
options.binary_location = 'path to chrome.exe'
## this is the chromium for testing which can be downloaded from the link given below

driver = webdriver.Chrome(chrome_options = options, executable_path = 'path to chromedriver.exe')
## must be the same as the downloaded version of chrome cft.
```
As of today, the files can be downloaded from: https://googlechromelabs.github.io/chrome-for-testing/

Prefer the stable version and download the compatible browser and chromedriver.

The rest of the code continues to work.

source: https://stackoverflow.com/questions/45500606/set-chrome-browser-binary-through-chromedriver-in-python

--------------------------------------------------
How can I list the taints on Kubernetes nodes?
The [docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint) are great about explaining how to set a taint on a node, or remove one. And I can use `kubectl describe node` to get a verbose description of one node, including its taints. But what if I&#39;ve forgotten the name of the taint I created, or which nodes I set it on? Can I list all of my nodes, with any taints that exist on them?
||||||||||||||<!-- language-all: lang-bash -->

    kubectl get nodes -o json | jq '.items[].spec'

which will give the complete spec with node name, or:

    kubectl get nodes -o json | jq '.items[].spec.taints'

will produce the list of the taints per each node

--------------------------------------------------
How to write unitTest for methods using a stream as a parameter
I have class `ImportProvider` , and I want write unit test for Import method.

But this should be unit test, so I don&#39;t want to read from file to stream.
Any idea?

  

    public class ImportProvider : IImportProvider
    { 
         public bool Import(Stream stream)
         {
             //Do import
        
             return isImported;
         }
    }
        
    public interface IImportProvider
    {
          bool Import(Stream input);
    }

This is unit test:

    [TestMethod]
    public void ImportProvider_Test()
    {
        // Arrange           
        var importRepository = new Mock&lt;IImportRepository&gt;(); 
        var imp = new ImportProvider(importRepository.Object);
        //Do setup...

        // Act
        var test_Stream = ?????????????
        // This working but not option:
        //test_Stream = File.Open(&quot;C:/ExcelFile.xls&quot;, FileMode.Open, FileAccess.Read);
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }
||||||||||||||Use a MemoryStream. Not sure what your function expects, but to stuff a UTF-8 string into it for example:

    //Act
    using (var test_Stream = new MemoryStream(Encoding.UTF8.GetBytes("whatever")))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }

EDIT: If you need an Excel file, and you are unable to read files from disk, could you add an Excel file as an embedded resource in your test project? See [How to embed and access resources by using Visual C#][1]

You can then read as a stream like this:

    //Act
    using (var test_Stream = this.GetType().Assembly.GetManifestResourceStream("excelFileResource"))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }


  [1]: https://support.microsoft.com/en-us/kb/319292

--------------------------------------------------
Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
What is causing this build error:

```

- Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
- Plugin Repositories (could not resolve plugin artifact &#39;com.android.application:com.android.application.gradle.plugin:7.0.3&#39;)
  Searched in the following repositories:
    Gradle Central Plugin Repository
    Google
```

in `build.gradle` file

Expecting a successful android build
||||||||||||||In my case `settings.gradle` file was missing. You can create a file and place into project root folder.

**settings.gradle**:

    pluginManagement {
        repositories {
            gradlePluginPortal()
            google()
            mavenCentral()
        }
    }
    dependencyResolutionManagement {
        repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)
        repositories {
            google()
            mavenCentral()
        }
    }
    rootProject.name = "android-geocode"
    include ':app'

--------------------------------------------------
How do I use an API key/secret on Binance&#39;s TestNet?
Following the instructions here, https://docs.binance.org/smart-chain/wallet/arkane.html, I created a Binance SmartChain account with its &quot;0x&quot; prefixed wallet address.  I then added funds.  What I can&#39;t figure out is how I get a TestNet API key and secret so that I can test my Python API calls.  I create the client like so

	from binance.client import Client
	...
	auth_client = Client(key, b64secret)
     if account.testing:
     	auth_client.API_URL = &#39;https://testnet.binance.vision/api&#39;
 
How do I get an API key tied to my Binance SmartChain address?
||||||||||||||You have to create your API credentials from [here][1] and pass the testnet variable into the Client constructor. See the
[documentation][2].

```python
auth_client = Client(key, b64secret, testnet=True)
```

does the job.


  [1]: https://testnet.binance.vision/
  [2]: https://python-binance.readthedocs.io/en/latest/binance.html?highlight=client#binance.client.Client.__init__

--------------------------------------------------
org.gradle.kotlin.kotlin-dsl was not found
I am getting the following error while running the build

    FAILURE: Build failed with an exception.
    
    * Where:
    Build file &#39;/home/charming/mainframer/bigovlog_android/buildSrc/build.gradle.kts&#39; line: 4
    
    * What went wrong:
    Plugin [id: &#39;org.gradle.kotlin.kotlin-dsl&#39;, version: &#39;1.2.6&#39;] was not found in any of the following sources:
    
    - Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
    - Plugin Repositories (could not resolve plugin artifact &#39;org.gradle.kotlin.kotlin-dsl:org.gradle.kotlin.kotlin-dsl.gradle.plugin:1.2.6&#39;)
      Searched in the following repositories:
        Gradle Central Plugin Repository

my buildSrc/build.gradle.kts

    repositories {
        jcenter()
    }
    plugins {
        `kotlin-dsl`
        id(&quot;groovy&quot;)
    }
    dependencies{
        gradleApi()
        localGroovy()
    }

I tried everything but still not working
||||||||||||||Did you check that Android Studio wasn't running in Offline Mode? Take a look at `Preferences/Build, Execution, Deployment/Gradle/Global Gradle settings` and see if Offline Work is checked.

--------------------------------------------------
Typesetting New Functions in LaTeX
So, I just have a little question:

What is the &quot;best way&quot; to typeset new functions in LaTeX which aren&#39;t already included in the various packages?  Right now I&#39;m just using `\mbox` as my go-to method,  but I just was wondering if there was a more &quot;acceptable way of doing it (as with mbox, I have to make sure to include spaces around the text of the functions in order for it to not look too strange)

Here is an example:

    $y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$

which comes out looking like:

![$y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$][1]

Don&#39;t get me wrong... I think it looks fine, but I was just looking for some opinions (as far as best practices go).

  [1]: http://adamnbowen.com/images/error_function.jpg
||||||||||||||Use `\DeclareMathOperator` from package `amsmath`. For example,

```tex
\documentclass{article}
\usepackage{amsmath}
\DeclareMathOperator\erfi{Erfi}

\begin{document}
Consider $x + y + \erfi(t) = z$ for example.
\end{document}
```

produces

[![result][1]][1]

If you only need it once, you can also use `\operatorname`: you get the same output as above with

```tex
\documentclass{article}
\usepackage{amsmath}
\begin{document}
Consider $x + y + \operatorname{Erfi}(t) = z$ for example.
\end{document}
```

If you cannot use the `amsmath` package for some reason, you can manually do `\mathop{\mathrm{Erfi}}` like:

```
\documentclass{article}
\begin{document}
Consider $x + y + \mathop{\mathrm{Erfi}}(t) = z$ for example.
\end{document}
```

See the always-useful TeX FAQ, specifically [Defining a new log-like function in LaTeX](https://texfaq.org/FAQ-newfunction).

  [1]: https://i.stack.imgur.com/9GCor.png

--------------------------------------------------
C# change a string variable with List or Array
I have some static strings 

    static string   Robocopy_Mirror = &quot;[Robocopy_Mirror]&quot;; 
    static string   Robocopy_Copy = &quot;[Robocopy_Copy]&quot;;
    static string   Network_Path_1 = &quot;[Network_Path_1]&quot;;    // \\NAS\Sync\
    static string   Lokal_Path_1 = &quot;[Lokal_Path_1]&quot;;      // X:\Sync\

And I thought I could save some lines of code if I put them in a List and change the values in the List with a loop.

    List&lt;string&gt; variableListe = new List&lt;string&gt;()  
    {   
        Robocopy_Mirror , Robocopy_Copy , Network_Path_1, Lokal_Path_1, 
        File_Network_Sync_1, File_Lokal_Sync_1, File_Network_Sync_2, File_Lokal_Sync_2
    };


But I can&#39;t change the static variables. I guess the List object does not change the static variable? Is there a quick way to change it? 

    for (int i = 0; i &lt; variableListe.Count-1; i++)
    {
        variableListe[i] = AppConfig.ElementAt(configPathPosition);
    }
    Console.WriteLine(Robocopy_Mirror); 

    // prints  &quot;[Robocopy_Mirror]&quot; instead of like C:\robocopy


||||||||||||||The static variables store references to string objects in memory. The elements in the list also store references to the same string objects in memory, but _each element is it's own variable_ and those elements _do not store references to the static variables;_ they refer to the string objects directly. 

When you change an element in the list, you're changing the variable in the list to point to a new object in a new memory location. The static variables do not change and continue to refer to the same unchanged strings as they did before.


--------------------------------------------------
How do I run curl command from within a Kubernetes pod
I have the following questions:

1. I am logged into a Kubernetes pod using the following command:

        ./cluster/kubectl.sh exec my-nginx-0onux -c my-nginx -it bash

    The &#39;ip addr show&#39; command shows its assigned the ip of the pod. Since pod is a logical concept, I am assuming I am logged into a docker container and not a pod, In which case, the pod IP is same as docker container IP. Is that understanding correct?

2. from a Kubernetes node, I do `sudo docker ps` and then do the following:-

        sudo docker exec  71721cb14283 -it &#39;/bin/bash&#39;

    This doesn&#39;t work. Does someone know what I am doing wrong?

3. I want to access the nginx service I created, from within the pod using curl. How can I install curl within this pod or container to access the service from inside. I want to do this to understand the network connectivity.
||||||||||||||Here is how you get a curl command line within a kubernetes network to test and explore your internal REST endpoints.

To get a prompt of a busybox running inside the network, execute the following command. (A tip is to use one unique container per developer.)

```sh
kubectl run curl-<YOUR NAME> --image=radial/busyboxplus:curl -i --tty --rm
```

You may omit the --rm and keep the instance running for later re-usage. To reuse it later, type:

```sh
kubectl attach <POD ID> -c curl-<YOUR NAME> -i -t
```

Using the command `kubectl get pods` you can see all running POD's. The `<POD ID>` is something similar to `curl-yourname-944940652-fvj28`.

**EDIT:** Note that you need to login to google cloud from your terminal (once) before you can do this! Here is an example, make sure to put in your zone, cluster and project: 
```sh
gcloud container clusters get-credentials example-cluster --zone europe-west1-c --project example-148812
```

--------------------------------------------------
Execute CURL with kubectl
I am trying to execute `curl` command with `kubectl` like 

    kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;

Gives belob error

	OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused &quot;exec: \&quot;kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39;\&quot;: 
	stat kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;: no such file or directory&quot; 
    :unknown command terminated with exit code 126
I have tried to escape the quotes but no luck. Then I tried simple curl 

    kubectl exec -it POD_NAME curl http://localhost:8080/xyz

This gives proper output as excepted. Any help with this 

Update: 

But when I run interactive (`kubectl exec -it POD_NAME /bin/bash`) mode of container and then run the curl inside the container works like champ
||||||||||||||i think you need to do something like this:

```
kubectl exec POD_NAME curl "-X PUT http://localhost:8080/abc -H \"Content-Type: application/json\" -d '{\"name\":\"aaa\",\"no\":\"10\"}' "
```

what the error suggests is that its trying to interpret everything inside `""` as a single command, not as a command with parameters. so its essentially looking for an executable called that

--------------------------------------------------
Open in Safari with UIActivityViewController?
I&#39;m sharing a URL via UIActivityViewController. I&#39;d like to see &quot;Open in Safari&quot; or &quot;Open in browser&quot; appear on the share sheet, but it doesn&#39;t. Is there a way to make this happen?

Note: I am not interested in solutions that involve adding somebody else&#39;s library to my app. I want to understand how to do this, not just get it to happen.

||||||||||||||Yes, you could add your custom action to Share sheet in iOS


You would have to copy this class.

    class MyActivity: UIActivity {
        var _activityTitle: String
        var _activityImage: UIImage?
        var activityItems = [Any]()
        var action: ([Any]) -> Void
        
        init(title: String, image: UIImage?, performAction: @escaping ([Any]) -> Void) {
            _activityTitle = title
            _activityImage = image
            action = performAction
            super.init()
        }
        override var activityTitle: String? {
            return _activityTitle
        }
    
        override var activityImage: UIImage? {
            return _activityImage
        }
        override var activityType: UIActivity.ActivityType {
            return UIActivity.ActivityType(rawValue: "com.someUnique.identifier")
        }
    
        override class var activityCategory: UIActivity.Category {
            return .action
        }
        override func canPerform(withActivityItems activityItems: [Any]) -> Bool {
            return true
        }
        override func prepare(withActivityItems activityItems: [Any]) {
            self.activityItems = activityItems
        }
        override func perform() {
            action(activityItems)
            activityDidFinish(true)
        }
    }

Please go through the class you might need to change a few things.

This is how you use it.

        let customItem = MyActivity(title: "Open in Safari", image: UIImage(systemName: "safari")  ) { sharedItems in
            guard let url = sharedItems[0] as? URL else { return }
            UIApplication.shared.open(url)
        }

        let items = [URL(string: "https://www.apple.com")!]
        let ac = UIActivityViewController(activityItems: items, applicationActivities: [customItem])
        ac.excludedActivityTypes = [.postToFacebook]
        present(ac, animated: true)

I have done this for one action, and tested it, it works.

Similarly you could do it for other custom actions.

For more on it refer this link.
[Link To Detailed Post][1]


  [1]: https://www.hackingwithswift.com/articles/118/uiactivityviewcontroller-by-example


--------------------------------------------------
How to cache playwright-python contexts for testing?
I am doing some web scraping using [`playwright-python&gt;=1.41`][1], and have to launch the browser in a headed mode (e.g. `launch(headless=False)`.

For CI testing, I would like to somehow cache the headed interactions with Chromium, to enable offline testing:

- First invocation: uses Chromium to make real-world HTTP transactions
- Later invocations: uses Chromium, but all HTTP transactions read from a cache

How can this be done? I can&#39;t find any clear answers on how to do this.

  [1]: https://github.com/microsoft/playwright-python
||||||||||||||It might solve your problem using HAR-file recording:
1. Run the first test while [recording a HAR-file][1]
2. Storing the HAR-file as an artifact, in your repo or similar in your CI environment
3. Running test again [with recorded HAR-file][2]

Here is how to do that with `playwright==1.41.1` and `pytest-playwright==0.3.3`:

```python
import pathlib

import pytest
from playwright.sync_api import Browser, Playwright

CACHE_DIR = pathlib.Path(__file__).parent / "cache"


@pytest.fixture(name="example_har", scope="session")
def fixture_example_har(playwright: Playwright) -> pathlib.Path:
    har_file = CACHE_DIR / "example.har"
    with (
        playwright.chromium.launch(headless=False) as browser,
        browser.new_page() as page,
    ):
        page.route_from_har(har_file, url="*/**", update=True)
        page.goto("https://example.com/")
    return har_file


def test_caching(browser: Browser, example_har: pathlib.Path) -> None:
    with browser.new_context(offline=True) as context:
        page = context.new_page()
        page.route_from_har(example_har, url="*/**")
        page.goto("https://example.com/")
```

  [1]: https://playwright.dev/python/docs/mock#recording-a-har-file
  [2]: https://playwright.dev/python/docs/mock#replaying-from-har

--------------------------------------------------
Python: Using .format() on a Unicode-escaped string
I am using Python 2.6.5. My code requires the use of the &quot;more than or equal to&quot; sign. Here it goes:  

    &gt;&gt;&gt; s = u&#39;\u2265&#39;
    &gt;&gt;&gt; print s
    &gt;&gt;&gt; ≥
    &gt;&gt;&gt; print &quot;{0}&quot;.format(s)
    Traceback (most recent call last):
         File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; 
    UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\u2265&#39;
      in position 0: ordinal not in range(128)`  

Why do I get this error? Is there a right way to do this? I need to use the `.format()` function.

||||||||||||||Just make the second string also a unicode string

    >>> s = u'\u2265'
    >>> print s
    ≥
    >>> print "{0}".format(s)
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    UnicodeEncodeError: 'ascii' codec can't encode character u'\u2265' in position 0: ordinal not in range(128)
    >>> print u"{0}".format(s)
    ≥
    >>> 



--------------------------------------------------
Only Content controls are allowed directly in a content page that contains Content controls in ASP.NET
I have an application which has a master page and child pages. My application is working fine on local host (on my intranet). But as soon as I put it on a server that is on the internet, I get the error shown below after clicking on any menus.

&gt; Only Content controls are allowed directly in a content page that contains Content controls.

![screenshot][1]




  [1]: http://i.stack.imgur.com/b21sZ.png
||||||||||||||
Double and triple check your opening and closing Content tags throughout your child pages.

**Confirm that they** 

 - are in existence
 - are spelled correctly
 - have an ID
 - have runat="server"
 - have the correct ContentPlaceHolderID

--------------------------------------------------
Apollo Client is not reading variables passed in using useQuery hook
Having a weird issue passing variables into the useQuery hook.

The query:
```
const GET_USER_BY_ID= gql`
  query($id: ID!) {
    getUser(id: $id) {
      id
      fullName
      role
    }
  }
`;
```
Calling the query:
```
const DisplayUser: React.FC&lt;{ id: string }&gt; = ({ id }) =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID, {
    variables: { id },
  });

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Rendering the component:
```
&lt;DisplayUser id=&quot;5e404fa72b819d1410a3164c&quot; /&gt;
```

This yields the error: 
```
&quot;Argument \&quot;id\&quot; of required type \&quot;ID!\&quot; was provided the variable \&quot;$id\&quot; which was not provided a runtime value.&quot;
```

Calling the query from GraphQL Playground returns the expected result:
```
{
  &quot;data&quot;: {
    &quot;getUser&quot;: {
      &quot;id&quot;: &quot;5e404fa72b819d1410a3164c&quot;,
      &quot;fullName&quot;: &quot;Test 1&quot;,
      &quot;role&quot;: &quot;USER&quot;
    }
  }
}
```
And calling the query without a variable but instead hard-coding the id:
```
const GET_USER_BY_ID = gql`
  query {
    getUser(id: &quot;5e404fa72b819d1410a3164c&quot;) {
      id
      fullName
      role
    }
  }
`;

const DisplayUser: React.FC = () =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID);

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Also returns the expected result.

I have also attempted to test a similar query that takes `firstName: String!` as a parameter which also yields an error saying that the variable was not provided a runtime value. This query also works as expected when hard-coding a value in the query string.

This project was started today and uses `&quot;apollo-boost&quot;: &quot;^0.4.7&quot;`, `&quot;graphql&quot;: &quot;^14.6.0&quot;`, and `&quot;react-apollo&quot;: &quot;^3.1.3&quot;`.
||||||||||||||[Solved]

In reading through the stack trace I noticed the issue was referencing `graphql-query-complexity` which I was using for validationRules. I removed the validation rules and now everything works! Granted I don't have validation at the moment but at least I can work from here. Thanks to everyone who took the time to respond!

--------------------------------------------------
Why it is a StackOverFlow Exception?
Why following code throws `StackoverflowException`? 

    class Foo
    {
        Foo foo = new Foo();
    }
    class Program
    {
        static void Main(string[] args)
        {
            new Foo();
        }
    }
||||||||||||||In `Main` you create a new `Foo` object, invoking its constructor.
Inside the `Foo` constructor, you create a different `Foo` instance, again invoking the `Foo` constructor.

This leads to infinite recursion and a `StackOverflowException` being thrown.

--------------------------------------------------
Function to aggregate json
Assume I have a gcs bucket with json files with the following structure:

```
[
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeid&quot;: &quot;Y1&quot;,
    &quot;storeName&quot;: &quot;alibaba1&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.8/3.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y2&quot;,
     &quot;storeName&quot;: &quot;alibaba2&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.7/2.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y3&quot;,
     &quot;storeName&quot;: &quot;alibaba3&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;2.7/4.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y4&quot;,
     &quot;storeName&quot;: &quot;alibaba4&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;3.7/5.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  }
]
```

What I want to do is to aggregate the different values by summing ```a, b,c, d, f,g``` and taking the average of ```e``` to return one single ```json``` like

```
[
{
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;a&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;b&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;c&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;d&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;e&quot;: &quot;average over all first instance/average over all second instance&quot;,
    &quot;f&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;g&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
  }
]
``` 

Not that any of the values in ```*/*/*``` could be NaN and that the data in ```e``` could be a string ```data unvavailable```.

In have created this function 

```
def format_large_numbers_optimized(value):
    abs_values = np.abs(value)
    mask = abs_values &gt;= 1e6
    formatted_values = np.where(mask, 
                                np.char.add(np.round(value / 1e6, 2).astype(str), &quot;M&quot;), 
                                np.round(value, 2).astype(str))
    return formatted_values

def process_json_data_optimized(json_list):
    result = {}
    keys = set(json_list[0].keys()) - {&#39;Id&#39;, &#39;Name&#39;, &#39;storeid&#39;, &#39;storeName&#39;}
    for key in keys:
        result[key] = {&#39;values&#39;: []}
    for json_data in json_list:
        for key in keys:
            value = json_data.get(key, &#39;0&#39;)  
            result[key][&#39;values&#39;].append(value)
    for key in keys:
        all_values_processed = []
        for value in result[key][&#39;values&#39;]:
            if isinstance(value, str) and &#39;/&#39; in value:
                processed_values = [float(v) if v != &#39;data unavailable&#39; else 0 for v in value.split(&#39;/&#39;)]
            elif isinstance(value, float) or isinstance(value, int):
                processed_values = [value]
            else:
                processed_values = [0.0]  
            all_values_processed.append(processed_values)
        numeric_values = np.array(all_values_processed)
        if numeric_values.ndim == 1:
            numeric_values = numeric_values[:, np.newaxis]
        summed_values = np.sum(numeric_values, axis=0)
        formatted_summed_values = &#39;/&#39;.join(format_large_numbers_optimized(summed_values))
        result[key][&#39;summed&#39;] = formatted_summed_values
    processed_result = {key: data[&#39;summed&#39;] for key, data in result.items()}
    processed_result[&#39;Id&#39;] = json_list[0][&#39;Id&#39;]
    processed_result[&#39;Name&#39;] = json_list[0][&#39;Name&#39;]
    return processed_result
```

But it does not create what I expect. I am a at a total loss. Would really appreciate any help.
||||||||||||||Note that you are placing the values as lists `all_values_processed`.
Assuming that the `/` character is just a separator, and that what you want by replacing `all_values_processed.append(processed_values)` by `all_values_processed += processed_values`. Or even better you could just aggregate the values.

For instance you could have a function to aggregate like this

```lang-py
import math
def agg_func(value, initial):
  v_count, v_sum = initial
  if isinstance(value, str) and '/' in value:
    for v in value.split('/'):
      if v != 'data unavailable' :
         v = float(v)
         if not math.isnan(v):
           v_sum += v
           v_count += 1
  elif isinstance(value, float) or isinstance(value, int):
    if not math.isnan(value):
      v_sum += value
      v_count += 1
  return v_count, v_sum
```

A function that aggregate the given keys in the json

```lang-py
def agg_json(v_list, fields):
  state = {k: (0, 0) for k in fields}
  for item in v_list:
    for k in fields:
      if k in item:
        state[k] = agg(item[k], state[k])
  return state
```


Now
```lang-py
state = agg_json(json_list, ['a', 'b', 'c', 'd', 'e', 'f', 'g'])
```

will give you a dictionary with tuples containing the count and the sum for each field. To get your final answer you could do

```lang-py
result = {k: v[1] / v[0] if k == 'e' else v[1] for k, v in state.items()}
```

--------------------------------------------------
color text in divs with two colors using css only - tricky
OK, let me rewrite my question in another words so it looks clear and interesting: [jsFiddle][1]


  [1]: http://jsfiddle.net/xY6T3/1/

I need a pure css solution that colorizes the lines of text in the color depending whether the line is odd or even.

The example of code could be :

    &lt;div class=&quot;main&quot;&gt;
        &lt;div class=&quot;zipcode12345&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode23456&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode90033&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode11321&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;

Is it possible to make it with css? As you see [@ jsFiddle][1], it is not colorized as expected.

So, the main div is &quot;main&quot;.
The inner `div`s always have class names in format &quot;zipcodeXXXXX&quot;, as you see.
The number of zipcodeXXXXX is variable, the number of `myclass` is variable.
However, the odd lines should be always red and the even lines should be always blue.
Does pure css solution exist?

That would be kind of 

    .myclass:nth-child(2n+1){
     color:red;
    }
    .myclass:nth-child(2n){
     color:blue;
    }

if we could igonre `&quot;zipcodeXXXXX&quot;` divs, right?

Thank you.
||||||||||||||Simply apply different odd/even rules to the parent elements as well as the child elements:

<!-- language: lang-css -->

    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(odd),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(even) {
        color: red;
    }
    
    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(even),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(odd) {
        color: blue;
    }

[**JSFiddle demo**][1].


  [1]: http://jsfiddle.net/xY6T3/9/

--------------------------------------------------
How to populate columns in a table using JavaScript
I need to create a simple table using JavaScript based on an array with nested objects, which should have only two columns. In the first cell of the first column of the table, the Processor header is specified, after which the corresponding processor models are written to the lower cells. In the first cell of the second column, the Processor frequency header is indicated, after which the frequencies are written to the lower cells. I was able to generate code for this task, but it doesn&#39;t work correctly. Instead of writing keys to column cells after the first iteration, for some reason, it takes into account unnecessary objects. That&#39;s why you get an undefined value in the table headers and a re-duplication. Please tell me how to solve this problem. 

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    let processorFrequency = [
        {
            titleOne : &#39;Processor&#39;, values : [
                {name : &#39;80386LC (1988г.)&#39;},
                {name : &#39;80486DX4 (1994г.)&#39;},
                {name : &#39;Pentium MMX (1997г.)&#39;},
                {name : &#39;Pentium II (1998г.)&#39;},
                {name : &#39;Pentium III (1999г.)&#39;},
                {name : &#39;Pentium IV&#39;},
                {name : &#39;Athlon-Athlon XP&#39;},
                {name : &#39;Athlon 64&#39;},

            ]
        },
        {
            titleTwo : &#39;Processor frequency&#39;, values : [
                {name : &#39;33-60&#39;},
                {name : &#39;80-133&#39;},
                {name : &#39;160-233&#39;},
                {name : &#39;260-550&#39;},
                {name : &#39;300-1400&#39;},
                {name : &#39;1600-3800&#39;},
                {name : &#39;1400-3200&#39;},
                {name : &#39;2600-3800&#39;},
            ]
    },
    ]



    function Test(){
        let table = document.getElementsByTagName(&#39;table&#39;)[0];
        for (let i = 0; i &lt; processorFrequency.length; i++) {
            var pf = processorFrequency[i];
            let tableRow = document.createElement(&#39;tr&#39;);
            let tdOne = document.createElement(&#39;td&#39;);
            let tdTwo = document.createElement(&#39;td&#39;);
            let txtOne = document.createTextNode(pf.titleOne);
            let txtTwo = document.createTextNode(pf.titleTwo);
            
            tdOne.className = &#39;head&#39;;
            tdTwo.className = &#39;head&#39;;

            tdOne.appendChild(txtOne);
            tdTwo.appendChild(txtTwo);

            tableRow.appendChild(tdOne);
            tableRow.appendChild(tdTwo);
            table.appendChild(tableRow);

            var values = pf.values;
            for (let j = 0; j &lt; values.length; j++) {
                let value = values[j];
                let tableRow = document.createElement(&#39;tr&#39;);
                let td = document.createElement(&#39;td&#39;);
                let txt = document.createTextNode(value.name);
                td.appendChild(txt);
                tableRow.appendChild(td);
                table.appendChild(tableRow);
            }
        }
    } 

    Test();

&lt;!-- language: lang-css --&gt;

    table td, table th {
      border: 1px solid black;
      padding: 5px;
    }

&lt;!-- language: lang-html --&gt;

    &lt;table&gt;&lt;!-- Contents will be created via JavaScript --&gt;
    &lt;/table&gt;

&lt;!-- end snippet --&gt;


||||||||||||||Seems like the issue is that you are creating a new row for each processor AND frequency value, which results in extra rows being added to the table. The correct way should be create a single row for each processor, with two cells (one for the processor model and one for the processor frequency):

    let processorFrequency = [
        {
            titleOne: 'Processor', values: [
                { name: '80386LC (1988г.)' },
                { name: '80486DX4 (1994г.)' },
                { name: 'Pentium MMX (1997г.)' },
                { name: 'Pentium II (1998г.)' },
                { name: 'Pentium III (1999г.)' },
                { name: 'Pentium IV' },
                { name: 'Athlon-Athlon XP' },
                { name: 'Athlon 64' },
            ]
        },
        {
            titleTwo: 'Processor frequency', values: [
                { name: '33-60' },
                { name: '80-133' },
                { name: '160-233' },
                { name: '260-550' },
                { name: '300-1400' },
                { name: '1600-3800' },
                { name: '1400-3200' },
                { name: '2600-3800' },
            ]
        },
    ];
    
    function Test() {
        let table = document.getElementsByTagName('table')[0];
    
        for (let i = 0; i < processorFrequency[0].values.length; i++) {
            let tableRow = document.createElement('tr');
            
            // Processor Name Cell
            let tdOne = document.createElement('td');
            let txtOne = document.createTextNode(processorFrequency[0].values[i].name);
            tdOne.appendChild(txtOne);
            tableRow.appendChild(tdOne);
    
            // Processor Frequency Cell
            let tdTwo = document.createElement('td');
            let txtTwo = document.createTextNode(processorFrequency[1].values[i].name);
            tdTwo.appendChild(txtTwo);
            tableRow.appendChild(tdTwo);
    
            table.appendChild(tableRow);
        }
    }
    
    Test();

--------------------------------------------------
System.UnauthorizedAccessException: Access to the path &quot;...&quot; is denied
  I have C# wpf installation done with .net using click once installation. All works fine. Then I have the following code which is part of the installed program:

    String destinationPath = System.Windows.Forms.Application.StartupPath + &quot;\\&quot; + fileName;
    File.Copy(path, destinationPath, true);
    this.DialogResult = true;
    this.Close();

But I get this error:

&gt;System.UnauthorizedAccessException: Access to the path C:\user\pc\appdata\local\apps\2.0.......  is denied.
&gt;
&gt;at System.IO.File.InternalCopy(String sourceFileName, String destFileName, Boolean overwrite, Boolean checkHost)
&gt;       at System.IO.File.Copy(String sourceFileName, String destFileName, Boolean overwrite)

Is it a permission error or do I need to tweak something in my code?

What puzzles me is why the user is able to install the program using click once into that directory without any issues, but uploading a file to it doesn&#39;t work?
||||||||||||||When installing an application the installer usually asks for administrative privileges. If the user chooses "Yes" the program will run and have read and write access to a larger variety of paths than what a normal user has. If the case is such that the installer did not ask for administrative privileges, it might just be that ClickOnce automatically runs under some sort of elevated privileges.

I'd suggest you write to the local appdata folder instead, but if you feel you really want to write to the very same directory as your application you must first run your app with administrator privileges.

To make your application always ask for administrator privileges you can modify your app's manifest file and set the `requestedExecutionLevel` tag's `level` attribute to `requireAdministrator`:

    <requestedExecutionLevel level="requireAdministrator" uiAccess="false" />

You can read a bit more in [**How do I force my .NET application to run as administrator?**](https://stackoverflow.com/questions/2818179/how-do-i-force-my-net-application-to-run-as-administrator)

--------------------------------------------------
Create a NuGet package for .NET8 MAUI with Azure DevOps
I have created a `.NET8 MAUI Class Library` to use in MAUI projects. The repo is in `Azure DevOps` and I was trying to build and publish the package via NuGet.

For that, I wrote a YAML file

    trigger:
    - main
    
    pool:
      vmImage: ubuntu-latest
    
    steps:
    - task: UseDotNet@2
      displayName: &#39;Use dotnet 8&#39;
      inputs:
        version: &#39;8.0.x&#39;
    - task: CmdLine@2
      inputs:
        script: &#39;dotnet workload install maui&#39;
    - task: DotNetCoreCLI@2
      displayName: Restore packages
      inputs:
        command: &#39;restore&#39;
        feedsToUse: &#39;select&#39;
        vstsFeed: &#39;c800d0d7-e2af-4567-997f-de7cf7888e6c&#39;
    - task: DotNetCoreCLI@2
      displayName: Build project
      inputs:
        command: &#39;build&#39;
        projects: &#39;**/PSC.Maui.Components.BottomSheet.csproj&#39;
        arguments: &#39;--configuration $(buildConfiguration)&#39;

When the pipeline runs, I get this error

    Generating script.
    Script contents:
    dotnet workload install maui
    ========================== Starting Command Output ===========================
    /usr/bin/bash --noprofile --norc /home/vsts/work/_temp/42901c0d-f407-4f75-912b-f93132efa865.sh
    Workload ID maui isn&#39;t supported on this platform.
    
    ##[error]Bash exited with code &#39;1&#39;.
    Finishing: CmdLine


[![enter image description here][1]][1]

Then, I tried to create the NuGet package locally, but it was not recognized by the NuGet website when I uploaded it.

How can I change the pipeline?

  [1]: https://i.stack.imgur.com/loBv9.png
||||||||||||||.Net MAUI does not support Linux, therefore you can neither build to it or from it.  

See [here](https://learn.microsoft.com/en-us/dotnet/maui/supported-platforms?view=net-maui-8.0)  

--------------------------------------------------
How to specify multiple locators for Selenium web element using the FindBy and PageFactory mechanisms
I like to use `PageFactory` with `@FindBy` annotations in my automation framework to auto-locate elements in my page object classes. 

I have one WebElement for which I need to be able to specify a couple of different locators. I thought FindBys was my solution, but apparently, that is not how it works. It&#39;s the equivalent of `driver.findElement(option1).findelement.(option2)`. That&#39;s not what I need. I need something that will find an element by one or the other locators. If one doesn&#39;t work, then use the other locator. Is there a way to do this in Selenium using FindBy annotations?
||||||||||||||There is apparently a new feature in Selenium as of May this year -- the @FindAll annotation that does exactly what I need;

http://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/support/FindAll.html
http://selenium.10932.n7.nabble.com/Pull-Request-62-Add-a-FindAll-annotation-to-the-Java-Page-Factory-td24814.html

--------------------------------------------------
Store cout from function as string
I have a function that takes in a vector of integers and outputs them via `std::cout`. 

    #include &lt;iostream&gt;
    #include &lt;vector&gt;
    
    void final_sol(std::vector&lt;int&gt; list){
        for (int i ; i &lt; list.size() ; i++){
            std::cout &lt;&lt; list[i] &lt;&lt; &quot; &quot;;
        }
    }
    
    int main(){
        std::vector&lt;int&gt; list = {1, 2, 3, 4, 5};
        final_sol(list);
        return 0;
    }
However, from this point I would like to have a way to quickly obtain the outputs of `final_sol(vector)` as a string. One way to do this would be to modify the original function to also create the string. However, I am not interested in modifying `final_sol(vector)`. Is there another way I could store the outputs as a string?
||||||||||||||Provide overload:
```
void final_sol(std::ostream& out, const std::vector<int>& list){
    for (int i = 0; i < list.size() ; i++){
        out << list[i] << " ";
    }
}

void final_sol(const std::vector<int>& list){
    final_sol(std::cout, list);
}
```
This way you existing calling code will not be impacted - most probably this is what you want: not modifying function signature. Not what you described: not do not modifying implementation of final_sol.

Then you can do:
```cpp
std::ostringstream str;
final_sol(str, list);
auto s = str.str()
```


--------------------------------------------------
FastAPI runs api-calls in serial instead of parallel fashion
I have the following code:

```python
import time
from fastapi import FastAPI, Request
    
app = FastAPI()
    
@app.get(&quot;/ping&quot;)
async def ping(request: Request):
        print(&quot;Hello&quot;)
        time.sleep(5)
        print(&quot;bye&quot;)
        return {&quot;ping&quot;: &quot;pong!&quot;}
```
If I run my code on localhost - e.g., `http://localhost:8501/ping` - in different tabs of the same browser window, I get:
```
Hello
bye
Hello
bye
```
instead of:
```
Hello
Hello
bye
bye
```
I have read about using `httpx`, but still, I cannot have a true parallelization. What&#39;s the problem?
||||||||||||||As per [FastAPI's documentation][1]:

> When you declare a path operation function with normal `def` instead
> of `async def`, it is run in an external threadpool **that is then
> `await`ed**, instead of being called directly (as it would block the
> server).

also, as described [here][2]:

> If you are using a third party library that communicates with
> something (a database, an API, the file system, etc.) and doesn't have
> support for using `await`, (this is currently the case for most
> database libraries), then declare your path operation functions as
> normally, with just `def`.
> 
> If your application (somehow) doesn't have to communicate with
> anything else and wait for it to respond, use `async def`.
> 
> If you just don't know, use normal `def`.
> 
> **Note**: You can mix `def` and `async def` in your path operation functions as much as you need and define each one using the best
> option for you. FastAPI will do the right thing with them.
> 
> Anyway, in any of the cases above, FastAPI **will still work
> asynchronously** and be extremely fast.
> 
> But by following the steps above, it will be able to do some
> performance optimizations.



Thus, `def` endpoints (in the context of asynchronous programming, a function defined with just `def` is called *synchronous* function), in FastAPI, run in a separate thread from an external threadpool that is then `await`ed, and hence, FastAPI will still work *asynchronously*. In other words, the server will process requests to such endpoints *concurrently*. Whereas, `async def` endpoints run in the [`event loop`][3]&mdash;on the main (single) thread&mdash;that is, the server will also process requests to such endpoints *concurrently*/*asynchronously*, **as long as there is** an [`await`][4] call to non-blocking I/O-bound operations inside such `async def` endpoints/routes, such as *waiting* for (1) data from the client to be sent through the network, (2) contents of a file in the disk to be read, (3) a database operation to finish, etc., (have a look [here][5]). If, however, an endpoint defined with `async def` does not `await` for something inside, in order to give up time for other tasks in the `event loop` to run (e.g., requests to the same or other endpoints, background tasks, etc.), each request to such an endpoint will have to be completely finished (i.e., exit the endpoint), before returning control back to the `event loop` and allow other tasks to run. In other words, in such cases, the server will process requests *sequentially*. **Note** that the same concept not only applies to FastAPI endpoints, but also to [`StreamingResponse`'s generator function][6] (see [`StreamingResponse`][7] class implementation), as well as [`Background Tasks`][8] (see [`BackgroundTask`][9] class implementation); hence, after reading this answer to the end, you should be able to decide whether you should define a FastAPI endpoint, `StreamingResponse`'s generator, or background task function with `def` or `async def`. 

The keyword `await` (which works only within an `async def` function) passes function control back to the `event loop`. In other words, it suspends the execution of the surrounding [coroutine][10] (i.e., a coroutine object is the result of calling an `async def` function), and tells the `event loop` to let some other task run, until that `await`ed task is completed. **Note** that just because you may define a custom function with `async def` and then `await` it inside your `async def` endpoint, it doesn't mean that your code will work asynchronously, if that custom function contains, for example, calls to `time.sleep()`, CPU-bound tasks, non-async I/O libraries, or any other blocking call that is incompatible with asynchronous Python code. In FastAPI, for example, when using the `async` methods of [`UploadFile`][11], such as `await file.read()` and `await file.write()`, FastAPI/Starlette, behind the scenes, actually runs such *synchronous* [File objects' methods][12] in a separate thread from the external threadpool (using the `async` [`run_in_threadpool()`][13] function) and `await`s it; otherwise, such methods/operations would block the `event loop`&mdash;you can find out more by looking at the [implementation of the `UploadFile` class][14]. The number of worker threads of that external threadpool can be adjusted as required&mdash;please have a look at [this answer][15] for more details.

**Note**  that `async` does not mean *parallel*, but *concurrently*. Asynchronous code with [`async` and `await` is many times summarised as using coroutines][16]. **Coroutines** are collaborative (or [cooperatively multitasked][17]), meaning that "at any given time, a program with coroutines is running **only** one of its coroutines, and this running coroutine suspends its execution only when it explicitly requests to be suspended" (see [here][18] and [here][19] for more info on coroutines). As described in [this article][20]:

> Specifically, whenever execution of a currently-running coroutine
> reaches an `await` expression, the coroutine may be suspended, and
> another previously-suspended coroutine may resume execution if what it
> was suspended on has since returned a value. Suspension can also
> happen when an `async for` block requests the next value from an
> asynchronous iterator or when an `async with` block is entered or
> exited, as these operations use `await` under the hood.

If, however, a blocking I/O-bound or CPU-bound operation was directly executed/called inside an `async def` function/endpoint, it would **block the main thread**, and hence, the `event loop` (as the `event loop` runs in the main thread). Hence, a blocking operation such as `time.sleep()` in an `async def` endpoint would block the entire server (as in the code example provided in your question). Thus, if your endpoint is not going to make any `async` calls, you could declare it with normal `def` instead, in which case, FastAPI would run it in a separate thread from the external threadpool and `await` it, as explained earlier (more solutions are given in the following sections). Example:
```python
@app.get("/ping")
def ping(request: Request):
	#print(request.client)
	print("Hello")
	time.sleep(5)
	print("bye")
	return "pong"
```

Otherwise, if the functions that you had to execute inside the endpoint are `async` functions that you had to `await`, you should define your endpoint with `async def`. To demonstrate this, the example below uses the [`asyncio.sleep()`][21] function (from the [`asyncio`][22] library), which provides a non-blocking sleep operation. The `await asyncio.sleep()` method will suspend the execution of the surrounding coroutine (until the sleep operation is completed), thus allowing other tasks in the `event loop` to run. Similar examples are given [here][23] and [here][24] as well.
```python
import asyncio
 
@app.get("/ping")
async def ping(request: Request):
	#print(request.client)
	print("Hello")
	await asyncio.sleep(5)
	print("bye")
	return "pong"
```

**Both** the endpoints above will print out the specified messages to the screen in the same order as mentioned in your question&mdash;if two requests arrived at around the same time&mdash;that is:
```text
Hello
Hello
bye
bye
```

### Important Note
When you call your endpoint for the second (third, and so on) time, please remember to do that from **a tab that is isolated from the browser's main session**; otherwise, succeeding requests (i.e., coming after the first one) will be blocked by the browser (on **client side**), as the browser will be waiting for response from the server for the previous request before sending the next one. You can confirm that by using `print(request.client)` inside the endpoint, where you would see the `hostname` and `port` number being the same for all incoming requests&mdash;if requests were initiated from tabs opened in the same browser window/session)&mdash;and hence, those requests would be processed sequentially, because of the browser sending them sequentially in the first place. To **solve** this, you could either:
1. Reload the same tab (as is running), or
2. Open a new tab in an Incognito Window, or
3. Use a different browser/client to send the request, or
4. Use the `httpx` library to [make asynchronous HTTP requests][25], along with the [*awaitable*][26] [`asyncio.gather()`][27], which allows executing multiple asynchronous operations concurrently and then returns a list of results in the **same** order the awaitables (tasks) were passed to that function (have a look at [this answer][28] for more details).

   **Example**:
   ```python
   import httpx
   import asyncio

   URLS = ['http://127.0.0.1:8000/ping'] * 2

   async def send(url, client):
       return await client.get(url, timeout=10)

   async def main():
       async with httpx.AsyncClient() as client:
           tasks = [send(url, client) for url in URLS]
           responses = await asyncio.gather(*tasks)
           print(*[r.json() for r in responses], sep='\n')

   asyncio.run(main())
   ```
   In case you had to call different endpoints that may take different time to process a request, and you would like to print the response out on client side as soon as it is returned from the server&mdash;instead of waiting for `asyncio.gather()` to gather the results of all tasks and print them out in the same order the tasks were passed to the `send()` function&mdash;you could replace the `send()` function of the example above with the one shown below:
   ```
   async def send(url, client):
       res = await client.get(url, timeout=10)
       print(res.json())
       return res
   ```

`Async`/`await` and Blocking I/O-bound or CPU-bound Operations
--------------------------------------

If you are required to use `async def` (as you might need to `await` for coroutines inside your endpoint), but also have some _synchronous_ blocking I/O-bound or CPU-bound operation (long-running computation task) that will block the `event loop` (essentially, the entire server) and won't let other requests to go through, for example:
```python 
@app.post("/ping")
async def ping(file: UploadFile = File(...)):
    print("Hello")
	try:
		contents = await file.read()
		res = cpu_bound_task(contents)  # this will block the event loop
	finally:
		await file.close()
	print("bye")
    return "pong"
```

then:

1. You should check whether you could change your endpoint's definition to normal `def` instead of `async def`. For example, if the only method in your endpoint that has to be awaited is the one reading the file contents (as you mentioned in the comments section below), you could instead declare the type of the endpoint's parameter as `bytes` (i.e., `file: bytes = File()`) and thus, FastAPI would read the file for you and you would receive the contents as `bytes`. Hence, there would be no need to use `await file.read()`. Please note that the above approach should work for small files, as the enitre file contents would be stored into memory (see the [documentation on `File` Parameters][29]); and hence, if your system does not have enough RAM available to accommodate the accumulated data (if, for example, you have 8GB of RAM, you can’t load a 50GB file), your application may end up crashing. Alternatively, you could call the `.read()` method of the [`SpooledTemporaryFile`][30] directly (which can be accessed through the `.file` attribute of the `UploadFile` object), so that again you don't have to `await` the `.read()` method&mdash;and as you can now declare your endpoint with normal `def`, each request will run in a **separate thread** (example is given below). For more details on how to upload a `File`, as well how Starlette/FastAPI uses `SpooledTemporaryFile` behind the scenes, please have a look at [this answer][31] and [this answer][32].

   ```python 
   @app.post("/ping")
   def ping(file: UploadFile = File(...)):
       print("Hello")
	   try:
		   contents = file.file.read()
		   res = cpu_bound_task(contents)
	   finally:
		   file.file.close()
       print("bye")
       return "pong"
   ```

2. Use FastAPI's (Starlette's) [`run_in_threadpool()`][13] function from the `concurrency` module&mdash;as @tiangolo suggested [here][33]&mdash;which "will run the function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked" (see [here][34]). As described by @tiangolo [here][35], "`run_in_threadpool` is an `await`able function; the first parameter is a normal function, the following parameters are passed to that function directly. It supports both *sequence* arguments and *keyword* arguments".

   ```python
   from fastapi.concurrency import run_in_threadpool

   res = await run_in_threadpool(cpu_bound_task, contents)
   ```

3. Alternatively, use `asyncio`'s [`loop.run_in_executor()`][36]&mdash;after obtaining the running `event loop` using [`asyncio.get_running_loop()`][37]&mdash;to run the task, which, in this case, you can `await` for it to complete and return the result(s), before moving on to the next line of code. Passing `None` to the *executor* argument, the *default* executor will be used; which is a [`ThreadPoolExecutor`][38]:

   ```python
   import asyncio

   loop = asyncio.get_running_loop()
   res = await loop.run_in_executor(None, cpu_bound_task, contents)
   ```
   or, if you would like to [pass keyword arguments][39] instead, you could use a `lambda` expression (e.g., `lambda: cpu_bound_task(some_arg=contents)`), or, preferably, [`functools.partial()`][40], which is specifically recommended in the documentation for [`loop.run_in_executor()`][36]:
   ```python
   import asyncio
   from functools import partial

   loop = asyncio.get_running_loop()
   res = await loop.run_in_executor(None, partial(cpu_bound_task, some_arg=contents))
   ```

   In Python 3.9+, you could also use [`asyncio.to_thread()`][41] to asynchronously run a synchronous function in a separate thread&mdash;which, essentially, uses `await loop.run_in_executor(None, func_call)` under the hood, as can been seen in the [implementation of `asyncio.to_thread()`][42]. The `to_thread()` function takes the name of a blocking function to execute, as well as any arguments (`*args` and/or `**kwargs`) to the function, and then returns a coroutine that can be `await`ed. Example:
   ```
   import asyncio

   res = await asyncio.to_thread(cpu_bound_task, contents)
   ```
    
   **Note** that as explained in [**this answer**][15], passing `None` to the `executor` argument **does not** create a new `ThreadPoolExecutor` every time you call `await loop.run_in_executor(None, ...)`, but instead re-uses the *default* executor with the *default* number of worker threads (i.e., `min(32, os.cpu_count() + 4)`). Thus, depending on the requirements of your application, that number might be quite low. In that case, you should rather use a custom [`ThreadPoolExecutor`][38]. For instance:
   ```python
   import asyncio
   import concurrent.futures

   loop = asyncio.get_running_loop()
   with concurrent.futures.ThreadPoolExecutor() as pool:
	   res = await loop.run_in_executor(pool, cpu_bound_task, contents)
   ```
   I would strongly recommend having a look at the linked answer above to learn about the difference between using `run_in_threadpool()` and `run_in_executor()`, as well as how to create a re-usable custom `ThreadPoolExecutor` at the application startup, and adjust the number of maximum worker threads as needed.

4. `ThreadPoolExecutor` will successfully prevent the `event loop` from being blocked, but won't give you the **performance improvement** you would expect from running **code in parallel**; especially, when one needs to perform `CPU-bound` tasks, such as the ones described [here][43] (e.g., audio or image processing, machine learning, and so on). It is thus preferable to **run CPU-bound tasks in a separate process**&mdash;using [`ProcessPoolExecutor`][44], as shown below&mdash;which, again, you can integrate with `asyncio`, in order to `await` it to finish its work and return the result(s). As described [here][45], it is important to protect the entry point of the program to avoid recursive spawning of subprocesses, etc. Basically, your code must be under [`if __name__ == '__main__'`][46]. 

   ```python
   import concurrent.futures
   
   loop = asyncio.get_running_loop()
   with concurrent.futures.ProcessPoolExecutor() as pool:
       res = await loop.run_in_executor(pool, cpu_bound_task, contents) 
   ```
   Again, I'd suggest having a look at the linked answer earlier on how to create a re-usable `ProcessPoolExecutor` at the application startup. You might find [this answer][47] helpful as well.

5. Use **more [workers][48]** to take advantage of multi-core CPUs, in order to run multiple processes in parallel and be able to serve more requests. For example, `uvicorn main:app --workers 4` (if you are using [Gunicorn as a process manager with Uvicorn workers][49], please have a look at [**this answer**][50]). When using 1 worker, only one process is run. When using multiple workers, this will spawn multiple processes (all single threaded). Each process has a separate Global Interpreter Lock (GIL), as well as its own `event loop`, which runs in the main thread of each process and executes all tasks in its thread. That means, there is only one thread that can take a lock on the interpreter of each process; unless, of course, you employ additional threads, either outside or inside the `event loop`, e.g., when using a `ThreadPoolExecutor` with `loop.run_in_executor`, or defining endpoints/background tasks/`StreamingResponse`'s generator with normal `def` instead of `async def`, as well as when calling `UploadFile`'s methods (see the first two paragraphs of this answer for more details).

   **Note:** Each worker ["has its own things, variables and memory"][51]. This means that `global` variables/objects, etc., won't be shared across the processes/workers. In this case, you should consider using a database storage, or  Key-Value stores (Caches), as described [here][52] and [here][53]. Additionally, note that "if you are consuming a large amount of memory in your code, **each process** will consume an equivalent amount of memory".


6. If you need to perform **heavy background computation** and you don't necessarily need it to be run by the same process (for example, you don't need to share memory, variables, etc), you might benefit from using other bigger tools like [Celery][54], as described in [FastAPI's documentation][55].


  [1]: https://fastapi.tiangolo.com/async/#path-operation-functions
  [2]: https://fastapi.tiangolo.com/async/#concurrency-and-async-await
  [3]: https://docs.python.org/3/library/asyncio-eventloop.html
  [4]: https://stackoverflow.com/questions/38865050/is-await-in-python3-cooperative-multitasking
  [5]: https://fastapi.tiangolo.com/async/#asynchronous-code
  [6]: https://stackoverflow.com/a/75760884/17865804
  [7]: https://github.com/encode/starlette/blob/31164e346b9bd1ce17d968e1301c3bb2c23bb418/starlette/responses.py#L235
  [8]: https://fastapi.tiangolo.com/tutorial/background-tasks/
  [9]: https://github.com/encode/starlette/blob/33f46a13625bcca4b7520e33be299a23b2e2b26c/starlette/background.py#L15
  [10]: https://docs.python.org/3/library/asyncio-task.html#coroutines
  [11]: https://fastapi.tiangolo.com/tutorial/request-files/#uploadfile
  [12]: https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects
  [13]: https://github.com/encode/starlette/blob/b8ea367b4304a98653ec8ce9c794ad0ba6dcaf4b/starlette/concurrency.py#L35
  [14]: https://github.com/encode/starlette/blob/048643adc21e75b668567fc6bcdd3650b89044ea/starlette/datastructures.py#L426
  [15]: https://stackoverflow.com/a/77941425/17865804
  [16]: https://fastapi.tiangolo.com/async/#coroutines
  [17]: https://en.wikipedia.org/wiki/Cooperative_multitasking
  [18]: https://stackoverflow.com/questions/553704/what-is-a-coroutine
  [19]: https://stackoverflow.com/questions/1934715/difference-between-a-coroutine-and-a-thread
  [20]: https://jwodder.github.io/kbits/posts/pyasync-fundam/
  [21]: https://docs.python.org/3/library/asyncio-task.html#asyncio.sleep
  [22]: https://docs.python.org/3/library/asyncio.html
  [23]: https://docs.python.org/3/library/asyncio-task.html#coroutine
  [24]: https://stackoverflow.com/a/56730924
  [25]: https://www.python-httpx.org/async/#making-async-requests
  [26]: https://docs.python.org/3/library/asyncio-task.html#awaitables
  [27]: https://docs.python.org/3/library/asyncio-task.html#asyncio.gather
  [28]: https://stackoverflow.com/a/74239367/17865804
  [29]: https://fastapi.tiangolo.com/tutorial/request-files/#define-file-parameters
  [30]: https://docs.python.org/3/library/tempfile.html#tempfile.SpooledTemporaryFile
  [31]: https://stackoverflow.com/a/70657621/17865804
  [32]: https://stackoverflow.com/a/70667530/17865804
  [33]: https://github.com/tiangolo/fastapi/issues/1066#issuecomment-612940187
  [34]: https://bocadilloproject.github.io/guide/async.html#common-patterns
  [35]: https://gitter.im/tiangolo/fastapi?at=5ce550f675d9a575a625feb7
  [36]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor
  [37]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop
  [38]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
  [39]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio-pass-keywords
  [40]: https://docs.python.org/3/library/functools.html#functools.partial
  [41]: https://docs.python.org/3/library/asyncio-task.html#asyncio.to_thread
  [42]: https://github.com/python/cpython/blob/c5660ae96f2ab5732c68c301ce9a63009f432d93/Lib/asyncio/threads.py#L12
  [43]: https://fastapi.tiangolo.com/async/#is-concurrency-better-than-parallelism
  [44]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
  [45]: https://stackoverflow.com/q/15900366
  [46]: https://stackoverflow.com/questions/419163/what-does-if-name-main-do
  [47]: https://stackoverflow.com/a/77862153/17865804
  [48]: https://fastapi.tiangolo.com/deployment/server-workers/
  [49]: https://fastapi.tiangolo.com/deployment/server-workers/#gunicorn-with-uvicorn-workers
  [50]: https://stackoverflow.com/a/71613757/17865804
  [51]: https://fastapi.tiangolo.com/deployment/concepts/#memory-per-process
  [52]: https://stackoverflow.com/a/71537393/17865804
  [53]: https://stackoverflow.com/a/65699375/17865804
  [54]: https://docs.celeryq.dev/
  [55]: https://fastapi.tiangolo.com/tutorial/background-tasks/#caveat

--------------------------------------------------
Is it possible to access Svelte store from external js files?
I am wondering if i would be able to access my *Svelte* store values from a plain .js file.

I am trying to write functions returning a dynamic value based on a store value, to import them in any component.
But in a plain .js file I can&#39;t just access the store value with the $ sign..

Quick exemple of a basic function that uses a store value and could be used on multiple components: 

```js
//in .svelte

function add() {
    $counter = $counter + 1;
}
```

*EDIT: rephrasing a bit*

*EDIT:*
Found a solution but i don&#39;t really know if it&#39;s really optimized..

```js
//in .js file

import { get } from &quot;svelte/store&quot;;
import { counter } from &quot;./stores&quot;;

export function add() {
    var counterRef = get(counter);
    counter.set(counterRef + 1);
}
```
||||||||||||||In addition to rixo's answer, a better way to implement `add` is to use the store's `update` method:

```js
import { counter } from "./stores";

export function add() {
    counter.update(n => n + 1);
}
```

You could also create a [custom store](https://svelte.dev/tutorial/custom-stores) that implemented that logic.

--------------------------------------------------
Is CP437 decoding broken for control characters?
According to the [Wikipedia page for Code Page 437](https://en.wikipedia.org/wiki/Code_page_437) the byte values `\x01` through `\x1f` should decode to graphic characters, e.g. `b&#39;\x01&#39;` equates to ☺ `&#39;\u263A&#39;`. But that&#39;s not what `decode` produces:

    &gt;&gt;&gt; b&#39;\x01&#39;.decode(&#39;cp437&#39;)
    &#39;\x01&#39;

That was Python 3.6 but 2.7 does the same, for all 31 byte values.
||||||||||||||While there were graphics associated with the byte range `\x01` through `\x1f`, those graphics were only used in some contexts. In other contexts, those code points would be interpreted as control characters, as in ASCII. Quoting an [IBM page on CP437][1]:

> Code points X'01' through X'1F' and X'7F' may be controls or graphics depending on context. For displays the hexadecimal code in a memory-mapped 
video display buffer is a graphic. For printers the graphics context is established by a preceding control sequence in the data stream. There are two 
such control sequences: ESC X'5C' and ESC X'5E' named Print All Characters and Print Single Character respectively. In other situations the code 
points in question are used as controls.



Python's CP437 decoding is based on the [Unicode mappings on Unicode.org][2], which use the control character interpretation.

The [Unicode FAQ implies][3] that "The correct Unicode mappings for the special graphic characters (01-1F, 7F) of CP437 and other DOS-type code pages" should be available at https://www.unicode.org/Public/MAPPINGS, but digging down there only turns up the mappings with the control characters, and a [page][4] linking to several IBM websites. Digging through IBM's sites turns up ftp://ftp.software.ibm.com/software/globalization/gcoc/attachments/CP00437.txt, which gives graphical mappings for `\x01`-`\x1f` in terms of IBM's [GCGID system][5], but not in terms of Unicode.

I don't know if there actually *is* an official mapping, from either IBM or Unicode, that gives canonical Unicode mappings for `\x01`-`\x1f` in terms of the graphical interpretation of CP437.


  [1]: http://www-01.ibm.com/software/globalization/cp/cp00437.html
  [2]: ftp://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/PC/CP437.TXT
  [3]: http://unicode.org/faq/char_combmark.html#5
  [4]: http://www.unicode.org/Public/MAPPINGS/VENDORS/IBM/IBM_conversions.html
  [5]: https://www-01.ibm.com/software/globalization/gcgid/gcgid.html

--------------------------------------------------
Converting from Python-Polars to Rust-Polars
I have the following working Python polars code. I am learning Rust and am interested in converting Python to Rust.

```
df = df.with_columns(pl.concat([pl.col(base).slice(0, period).rolling_mean(period), pl.col(base).slice(period,None)]).alias(&#39;con&#39;))
```
How to convert the same in Rust? It might be very trivial, still not sure where I am going wrong.

```
let rolling_options = RollingOptions {
        window_size : Duration::parse(duration_str.as_str()),
        ..Default::default()
    };
let a = col(base).slice(0,period).rolling_mean(rolling_options);
let b = col(base).slice(period,lit(Null {}));

let temp_df = df.with_column(concat([a, b], UnionArgs::default()));
```
I keep getting the following error

&gt;mismatched types
expected enum `Expr`
   found enum `Result&lt;LazyFrame, PolarsError&gt;`

When i checked the data types of **a** and **b**, to my surprise they are **LazyFrame** and not **Expr**

[rolling_mean][1] as per the document returns **Expr** and so does [slice][2]. Not sure what I am missing.


  [1]: https://docs.rs/polars/latest/polars/prelude/enum.Expr.html#method.rolling_mean
  [2]: https://docs.rs/polars/latest/polars/prelude/enum.Expr.html#method.slice
||||||||||||||The result datatype you are getting is an enum meant to represent whether the operation was successful (returns a lazyFrame) or it failed (returns polarserror).

you should be able to 'uwnrap' the result

This link should cover the different ways to handle errors/results in rust:
https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html

--------------------------------------------------
Is Element.tagName always uppercase?
Reading at [MDN about Element.tagName][1] it states:

&gt;On HTML elements in DOM trees flagged as HTML documents, tagName returns the element name in the uppercase form.

My question is: is this trustable? Does IE (old and modern) behave as expected? Is this likely to change? or is it better to always work with `el.tagName.toLowerCase()`?


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element.tagName
||||||||||||||You don't have to `toLowerCase` or whatever, browsers do behave the same on this point (surprisingly huh?).

About the rationale, once I had discussion with a colleague who's very professional on W3C standards. One of his opinions is that using uppercase TAGNAME would be much easier to recognize them out of user content. That's quite persuasive for me.

-------------
**Edit:** As @adjenks says, XHTML doctype returns mixed-case tagName *if the document is served as `Content-Type: application/xhtml+xml`*. Test page: http://programming.enthuses.me/tag-node-case.php?doc=x

Technically, please read this spec for more info: http://www.w3.org/TR/REC-DOM-Level-1/level-one-core.html#ID-745549614

> Note that this (tagName) is **case-preserving in XML**, as are all of the operations of the DOM. The HTML DOM returns the tagName of an HTML element in the canonical uppercase form, regardless of the case in the source HTML document.

As of asker's question: this is trustable. Breaking change is not likely to happen in HTML spec.

--------------------------------------------------
How to validate more than one field of a Pydantic model?
I want to validate three model Fields of a Pydantic model. To do this, I am importing [`root_validator`][1] from pydantic, however I am getting the error below:
```py3
from pydantic import BaseModel, ValidationError, root_validator
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ImportError: cannot import name &#39;root_validator&#39; from &#39;pydantic&#39; (C:\Users\Lenovo\AppData\Local\Programs\Python\Python38-32\lib\site-packages\pydantic\__init__.py)
```

I tried this:
```python
@validator
def validate_all(cls, v, values, **kwargs):
    ...
```

I am inheriting my pydantic model from some common fields parent model. Values showing only parent class fields, but not my child class fields. For example:

```py3
class Parent(BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str
    
    @validator
    def validate_all(cls, v, values, **kwargs):
        #here values showing only (name and comment) but not address and phone.
        ...
```


  [1]: https://pydantic-docs.helpmanual.io/usage/validators/#root-validators
||||||||||||||To extend on the answer of `Rahul R`, this example shows in more detail how to use the `pydantic` validators.

This example contains all the necessary information to answer your question.

Note, that there is also the option to use a `@root_validator`, as mentioned by `Kentgrav`, see the example at the bottom of the post for more details.

```python
import pydantic

class Parent(pydantic.BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str

    # If you want to apply the Validator to the fields "name", "comments", "address", "phone"
    @pydantic.validator("name", "comments", "address", "phone")
    @classmethod
    def validate_all_fields_one_by_one(cls, field_value):
        # Do the validation instead of printing
        print(f"{cls}: Field value {field_value}")

        return field_value  # this is the value written to the class field

    # if you want to validate to content of "phone" using the other fields of the Parent and Child class
    @pydantic.validator("phone")
    @classmethod
    def validate_one_field_using_the_others(cls, field_value, values, field, config):
        parent_class_name = values["name"]
        parent_class_address = values["address"] # works because "address" is already validated once we validate "phone"
        # Do the validation instead of printing
        print(f"{field_value} is the {field.name} of {parent_class_name}")

        return field_value 

Customer(name="Peter", comments="Pydantic User", address="Home", phone="117")
```
**Output**
```cmd
<class '__main__.Customer'>: Field value Peter
<class '__main__.Customer'>: Field value Pydantic User
<class '__main__.Customer'>: Field value Home
<class '__main__.Customer'>: Field value 117
117 is the phone number of Peter
Customer(name='Peter', comments='Pydantic User', address='Home', phone='117')
```

To answer your question in more detail:

Add the fields to validate to the `@validator` decorator directly above the validation function.
- `@validator("name")` uses the field value of `"name"` (e.g. `"Peter"`) as input to the validation function. All fields of the class and its parent classes can be added to the `@validator` decorator.
- the validation function (`validate_all_fields_one_by_one`) then uses the field value as the second argument (`field_value`) for which to validate the input. The return value of the validation function is written to the class field. The signature of the validation function is `def validate_something(cls, field_value)` where the function and variable names can be chosen arbitrarily (but the first argument should be `cls`). According to Arjan (https://youtu.be/Vj-iU-8_xLs?t=329), also the `@classmethod` decorator should be added.


If the goal is to validate one field by using other (already validated) fields of the parent and child class, the full signature of the validation function is `def validate_something(cls, field_value, values, field, config)` (the argument names `values`,`field` and `config` **must** match) where the value of the fields can be accessed with the field name as key (e.g. `values["comments"]`).

**Edit1**: If you want to check only input values of a certain type, you could use the following structure:
```python
@validator("*") # validates all fields
def validate_if_float(cls, value):
    if isinstance(value, float):
        # do validation here
    return value
```

**Edit2**: Easier way to validate all fields together using `@root_validator`:
```python
import pydantic

class Parent(pydantic.BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str

    @pydantic.root_validator()
    @classmethod
    def validate_all_fields_at_the_same_time(cls, field_values):
        # Do the validation instead of printing
        print(f"{cls}: Field values are: {field_values}")
        assert field_values["name"] != "invalid_name", f"Name `{field_values['name']}` not allowed."
        return field_values
```

**Output**:

```python
Customer(name="valid_name", comments="", address="Street 7", phone="079")
<class '__main__.Customer'>: Field values are: {'name': 'valid_name', 'comments': '', 'address': 'Street 7', 'phone': '079'}
Customer(name='valid_name', comments='', address='Street 7', phone='079')
```

```python
Customer(name="invalid_name", comments="", address="Street 7", phone="079")
ValidationError: 1 validation error for Customer
__root__
  Name `invalid_name` not allowed. (type=assertion_error)
```

--------------------------------------------------
What is Python&#39;s bytes type actually used for?
Could somebody explain the general purpose of [the bytes type in Python 3](https://docs.python.org/3/library/stdtypes.html#bytes-objects), or give some examples where it is preferred over other data types? 

I see that the advantage of [bytearrays](https://docs.python.org/3/library/stdtypes.html#bytearray-objects) over strings is their mutability, but what about bytes? So far, the only situation where I actually needed it was sending and receiving data through sockets; is there something else? 
||||||||||||||Possible duplicate of [what is the difference between a string and a byte string][1]

In short, the bytes type is a sequence of bytes that have been encoded and are ready to be stored in memory/disk. There are many types of encodings (utf-8, utf-16, windows-1255), which all handle the bytes differently. The bytes object can be decoded into a str type.

The str type is a sequence of unicode characters. The str needs to be encoded to be stored, but is mutable and an abstraction of the bytes logic. 

There is a strong relationship between `str` and `bytes`. `bytes` can be decoded into a `str`, and `str`s can be encoded into bytes. 

You typically only have to use `bytes` when you encounter a string in the wild with a unique encoding, or when a library requires it. `str` , especially in python3, will handle the rest. 

More reading [here][2] and [here][3]



  [1]: https://stackoverflow.com/questions/6224052/what-is-the-difference-between-a-string-and-a-byte-string
  [2]: https://eli.thegreenplace.net/2012/01/30/the-bytesstr-dichotomy-in-python-3
  [3]: https://betterprogramming.pub/strings-unicode-and-bytes-in-python-3-everything-you-always-wanted-to-know-27dc02ff2686

--------------------------------------------------
Process terminated. Couldn&#39;t find a valid ICU package installed on the system in Asp.Net Core 3 - ubuntu
I am trying to run a Asp.Net Core 3 application in Ubuntu 19.10 thru terminal using `dotnet run` command but it does not seem to work. I get this error.

&gt; ```none
&gt; Process terminated. Couldn&#39;t find a valid ICU package installed on the system.
&gt; Set the configuration flag System.Globalization.Invariant to true if you want
&gt; to run with no globalization support.   
&gt;  at System.Environment.FailFast(System.String)   
&gt;  at System.Globalization.GlobalizationMode.GetGlobalizationInvariantMode()
&gt;  at System.Globalization.GlobalizationMode..cctor()   
&gt;  at System.Globalization.CultureData.CreateCultureWithInvariantData()   
&gt;  at System.Globalization.CultureData.get_Invariant()   
&gt;  at System.Globalization.CultureInfo..cctor()   
&gt;  at System.StringComparer..cctor()   
&gt;  at System.StringComparer.get_OrdinalIgnoreCase()   
&gt;  at Microsoft.Extensions.Configuration.ConfigurationProvider..ctor()   
&gt;  at Microsoft.Extensions.Configuration.EnvironmentVariables.EnvironmentVariablesConfigurationSource.Build(Microsoft.Extensions.Configuration.IConfigurationBuilder)
&gt;  at Microsoft.Extensions.Configuration.ConfigurationBuilder.Build()   
&gt;  at Microsoft.AspNetCore.Hosting.GenericWebHostBuilder..ctor(Microsoft.Extensions.Hosting.IHostBuilder)
&gt;  at Microsoft.Extensions.Hosting.GenericHostWebHostBuilderExtensions.ConfigureWebHost(Microsoft.Extensions.Hosting.IHostBuilder, System.Action&#39;1&lt;Microsoft.AspNetCore.Hosting.IWebHostBuilder&gt;)   
&gt;  at Microsoft.Extensions.Hosting.GenericHostBuilderExtensions.ConfigureWebHostDefaults(Microsoft.Extensions.Hosting.IHostBuilder, System.Action&#39;1&lt;Microsoft.AspNetCore.Hosting.IWebHostBuilder&gt;)   
&gt;  at WebApplication.Program.CreateHostBuilder(System.String[])   
&gt;  at WebApplication.Program.Main(System.String[])
&gt; ```

I installed the dotnet core sdk using the ubuntu store and after that I also installed Rider IDE.

The weird thing here is that when I run the app using Rider it runs fine, the only issue is using terminal dotnet core commands.

Does anybody know what might be the issue ?

The application is created using Rider. I don&#39;t think that this plays a role but just as a side fact.

I know there are also other ways to install dotnet core in ubuntu but since the sdk is available in the ubuntu story I thought it should work out of the box and of course its an easier choice.

Also tried this [one](https://stackoverflow.com/questions/58132275/ci-cannot-build-net-project-fails-with-couldnt-find-a-valid-icu-package-ins) but does not seem to work for me. Still the same issue happens after running the commands.

||||||||||||||The alternative solution as described in [Microsoft documentation][1] is to set environment variable before running your app 

    export DOTNET_SYSTEM_GLOBALIZATION_INVARIANT=1


  [1]: https://learn.microsoft.com/en-us/dotnet/core/run-time-config/globalization

--------------------------------------------------
Python: Should I save PyPi packages offline as a backup?
**My Python projects heavily depends on PyPi packages**.&lt;br&gt;
I want to make sure that: in any time in the future: the packages required by my apps will always be available online on PyPi.&lt;br&gt;
For example:-&lt;br&gt;
I found a project on Github that requires PyQt4.&lt;br&gt;
when I tried to run it on my Linux machine,&lt;br&gt;
it crashed on startup because it can&#39;t find PyQt4 package on PyPi.&lt;br&gt;
&gt; NB: I know that PyQt4 is deprecated

I searched a lot to find an archive for PyPi that still holds PyQt4 package, but I couldn&#39;t find them anywhere.&lt;br&gt;

so I had to rewrite that app to make it work on PyQt5.&lt;br&gt;
I only changed the code related to the UI (ie: PyQt4).&lt;br&gt;
other functions were still working.&lt;br&gt;

so the only problem with that app was that PyQt4 package was removed from PyPi.&lt;br&gt;
&lt;hr&gt;&lt;br&gt;
so, my question is: should I save a backup of the PyPi packages I use ?
||||||||||||||**TL;DR**

YES if you want availability... The next big question is **how** best to keep a backup version of the dependencies? There are some suggestions at the end of the answer.

**Long Verion:**

Your questions touches on the concept of "Availability" which one of the three pillars of Information Assurance (or Information Security). The other two pillars are Confidentiality and Integrity... The CIA triad.

PyPi packages are maintained by the owners of those packages, a project that depends on a package and list it as a dependency must take into account the possibility that the owner of the package will pull the package or a version of the package out of PyPi at any moment.

Important python packages with many dependencies usually are maintained by foundations or organisations that are more responsible with dealing with downstream dependants packages and projects. However keeping support for old packages is very costly and requires extra effort and usually maintainers sets a date for end of support, or publish a the package lifecycle where they state when a specific version will be removed from the public PyPi server.

Once that happens, the dependant have to update the code (as you did), or provide the original dependency via alternative means.

This topic is very important for procurement in Libraries, Universities, Labs, Companies, and Government Agencies where a software tool might have dependencies on other software packages (or ecosystem), and where "availability" should be addressed adequately. Addressing might mean ensuring high availability, but it could also mean living with the risk of losing availability of the packages... The choices you make for "security" of your project should be informed by a risk analysis.

Now to make sure that dependencies are always available... I quickly compiled the following list. Note that each option has pros and cons. You should evaluate these and other options based on your needs:

 1. Store the virtual environment along with the code. Once you create a virtual environment and install the packages you require for the project in that virtual environment, you can keep the virtual environment for posterity.
 2. Host your own PyPi instance (or mirror) and keep a copy of packages you depend upon hosted on it https://packaging.python.org/en/latest/guides/hosting-your-own-index/ 
 3. Use an "artifact management tool" such as Artifactory from https://jfrog.com/artifact-management/, where you can not only host python packages but also docker images, nmap packages, and other kinds of artifacts.
 4. Get the source code of all dependencies, and always build from source.

Let me know if you think of other ideas so that I add them to the list...

--------------------------------------------------
How to create a windowless application in C#?
I am new to C# and want to make a program that runs without a console/GUI, but can&#39;t figure out how. Is that even possible?

The only options I found were minimizing the window or hiding it AFTER start, but I want it to start without a window/visible in the task bar. I am aware of the fact that I (of course) won&#39;t be able to write to the console...
||||||||||||||What you're looking for is a windows service. You can create one, or at least I see the option to (in VS2022), when creating a new project.

Just create new project and search `Windows Service` and check the one that says `C#`. Pay attention though, one says VB.

Creating a window form and then hiding it.. unless you actually want to bring it up at some point, would be something I would advise against.

    using System;
    using System.Collections.Generic;
    using System.Linq;
    using System.ServiceProcess;
    using System.Text;
    using System.Threading.Tasks;
    
    namespace TestService
    {
        internal static class Program
        {
            /// <summary>
            /// The main entry point for the application.
            /// </summary>
            static void Main()
            {
                ServiceBase[] ServicesToRun;
                ServicesToRun = new ServiceBase[]
                {
                    new Service1()
                };
                ServiceBase.Run(ServicesToRun);
            }
        }
    }



--------------------------------------------------
How to draw tiled image with QT
I&#39;m writing interface with C++/Qt in QtCreator&#39;s designer. What element to chose to make as a rect with some background image?

And the second question: how to draw tiled image? I have and image with size (1&#215;50) and I want to render it for the parent width. Any ideas?

-----------------

    mTopMenuBg = QPixmap(&quot;images/top_menu_bg.png&quot;);
    mTopMenuBrush = QBrush(mTopMenuBg);
    mTopMenuBrush.setStyle(Qt::TexturePattern);
    mTopMenuBrush.setTexture(mTopMenuBg);
    
    ui-&gt;graphicsView-&gt;setBackgroundBrush(mTopMenuBrush);

&gt; QBrush: Incorrect use of
&gt; TexturePattern
||||||||||||||If you just want to show an image you can use [QImage][1].  To make a background with the image tiled construct a [QBrush][2] with the QImage.  Then, if you were using [QGraphicsScene][3] for example, you could set the bursh as the background brush.

Here is an example which fills the entire main window with the tiled image "document.png":

    int main(int argc, char *argv[]) {
    	QApplication app(argc, argv);
    	QMainWindow *mainWindow = new QMainWindow();
    
    	QGraphicsScene *scene = new QGraphicsScene(100, 100, 100, 100);
    	QGraphicsView *view = new QGraphicsView(scene);
    	mainWindow->setCentralWidget(view);
    
    	QImage *image = new QImage("document.png");
    	if(image->isNull()) {
    		std::cout << "Failed to load the image." <<std::endl;
    	} else {
    		QBrush *brush = new QBrush(*image);
    		view->setBackgroundBrush(*brush);
    	}
    
    	mainWindow->show();
    	return app.exec();
    }

The resulting app:  
![screen shot][4]


Alternatively, it seems that you could use [style sheets][5] with any widget and change the [background-image][6] property on the widget.  This has more integration with QtDesigner as you can set the style sheet and image in [QtDesigner][7].


  [1]: https://doc.qt.io/qt-6/qimage.html
  [2]: https://doc.qt.io/qt-6/qbrush.html
  [3]: https://doc.qt.io/qt-6/qgraphicsscene.html
  [4]: http://i.stack.imgur.com/1LK7W.jpg
  [5]: https://doc.qt.io/qt-6/stylesheet-reference.html
  [6]: https://doc.qt.io/qt-6/stylesheet-reference.html#background-image-prop
  [7]: https://doc.qt.io/qt-6/designer-stylesheet.html

--------------------------------------------------
Autofilter and set field using variable
I&#39;m using a serch criteria to get the number/index location of a column to use in the field section of the autofilter. Getting an error &quot;runtime err 1004: Autofilter method of range class failed&quot; not sure if it&#39;s possible. I can see in the degug the variable is holding the correct number

```

Private Sub cmdExtract1_Click()
Dim ws As Worksheet
    Dim lngLastRow As Long
    Dim rngData As Range
    Dim iColNumber As Integer
    
    
     Dim strSearch As String
    Dim aCell As Range
  
    Set ws = Worksheets(&quot;Detail Excel&quot;)
    ws.Activate


    &#39;Identify the last row and use that info to set up the Range
    With ws
    ws.Range(&quot;1:1&quot;).Select
        lngLastRow = ActiveSheet.Cells.Find(&quot;*&quot;, SearchOrder:=xlByRows, SearchDirection:=xlPrevious).Row

strSearch = &quot;Deleted App&quot;

    Set aCell = Sheet1.Rows(1).Find(What:=strSearch, LookIn:=xlValues, _
    LookAt:=xlWhole, SearchOrder:=xlByRows, SearchDirection:=xlNext, _
    MatchCase:=False, SearchFormat:=False)
    
     iColNumber = aCell.Column
         
    End With
    

&#39;Offer Date: include dates, remove blanks
Application.DisplayAlerts = False &#39;switching off the alert button
ws.Range(&quot;A1&quot; &amp; &quot;:y&quot; &amp; lngLastRow).AutoFilter Field:=iColNumber, Criteria1:=&quot;&quot;
ws.Range(&quot;A2&quot; &amp; &quot;:y&quot; &amp; lngLastRow).SpecialCells(xlCellTypeVisible).Delete
Application.DisplayAlerts = True &#39;switching on the alert button

On Error Resume Next
ws.ShowAllData
```




search variable then set the cell column number to a variable
||||||||||||||If you only want to delete rows with blanks you could do that without AutoFilter:

```
Private Sub cmdExtract1_Click()
    Dim ws As Worksheet, strSearch As String, m As Variant
  
    Set ws = Worksheets("Detail Excel")
    strSearch = "Deleted App"
    
    m = Application.match(strSearch, ws.rows(1), 0)
    
    If IsError(m) Then 'no header match?
        MsgBox "Header '" & strSearch & "' not found!", vbExclamation
        Exit Sub
    Else
        With ws.Range(ws.Cells(2, m), ws.Cells(rows.count, m).End(xlUp))
            On Error Resume Next 'ignore error in case no blanks
            .SpecialCells(xlCellTypeBlanks).EntireRow.Delete
            On Error GoTo 0      'stop ignoring errors
        End With
    End If

End Sub
```

EDIT: using autofilter

```
Private Sub cmdExtract1_Click()
    Dim ws As Worksheet, strSearch As String, m As Variant
    Dim lr As Long, lc As Long
  
    Set ws = Worksheets("Detail Excel")
    strSearch = "Deleted App"
    
    m = Application.match(strSearch, ws.rows(1), 0)
    
    If IsError(m) Then 'no header match?
        MsgBox "Header '" & strSearch & "' not found!", vbExclamation
        Exit Sub
    Else
        lr = LastOccupiedRow(ws)
        lc = ws.Cells(1, ws.Columns.count).End(xlToLeft).Column 'last header
        With ws.Range("A1", ws.Cells(lr, lc))
            .AutoFilter Field:=m, Criteria1:=""
            .Offset(1).SpecialCells(xlCellTypeVisible).EntireRow.Delete
        End With
        ws.ShowAllData
    End If
End Sub

Function LastOccupiedRow(ws As Worksheet) As Long
    Dim f As Range
    Set f = ws.Cells.Find("*", SearchOrder:=xlByRows, SearchDirection:=xlPrevious)
    If Not f Is Nothing Then LastOccupiedRow = f.row
End Function
```

--------------------------------------------------
How do I decrypt email from the href value
I am trying to decrypt email from the href value.
I encountered this problem while doing a web scraping task using python.

```html
&lt;a href=&quot;javascript:linkTo_UnCryptMailto(%27ocknvq%2CkphqBngjocpp0ej%27);&quot;&gt;
```

https://i.stack.imgur.com/yB8vo.png

 Whenever I click on *Email*, it directs me to Outlook mail application.From there I can get the email.However, it want the email without actually going to the mail application and in the console.

I have tried various decryption methods discussed on this website but it didn&#39;t work.Can someone give me a hint about what type of method for encryption is used?

||||||||||||||If you are looking to reverse engineer the encryption, you will need to go through the site's source and look for the JS function `linkTo_UnCryptMailto`.

More simply though, if you open dev tools (Ctrl + Shift + I) on chrome and click on the console, you should just be able to type `linkTo_UnCryptMailto(%27ocknvq%2CkphqBngjocpp0ej%27)` and view the result since the function appears to be global.

--------------------------------------------------
Show Bootstrap modal using (#myModal).modal(&#39;show&#39;) from Angular component
I have an Angular 8 project that is using bootstrap. I am trying to show the modal dialog inside my component using the `$(&#39;myModal&#39;).modal(&#39;show&#39;)` inside my component&#39;s *Typescript* file.

Here&#39;s my component&#39;s file:

    import {Component, OnInit} from &#39;@angular/core&#39;;
    import {Router} from &#39;@angular/router&#39;;
    // import * as $ from &#39;jquery&#39;;
    import * as bootstrap from &#39;bootstrap&#39;;
    
    @Component({
      selector: &#39;app-xyz&#39;,
      templateUrl: &#39;./xyz.component.html&#39;,
      styleUrls: [&#39;./xyz.css&#39;]
    })
    export class XyzComponent implements OnInit {
    
      constructor(private router: Router) {
      }
    
      ngOnInit() {
      }
    
      submit() {
        $(&#39;#confirm&#39;).modal(&#39;show&#39;);
      }
    
    }

Upon invoking the submit() funciton on click I get the following error: `ERROR TypeError: $(...).modal is not a function`

I installed bootstrap and jquery using `npm install bootstrap` --save and `npm install jquery --save`.

I even installed *ngx-bootstrap*.

However, when I uncomment the line importing *jQuery* I get a different error: `ERROR TypeError: jquery__WEBPACK_IMPORTED_MODULE_3__(...).modal is not a function`
||||||||||||||Check your angular.json file to make sure that the Jquery js file is included in your scripts array.

`"scripts": [
    "node_modules/jquery/dist/jquery.min.js",
]`

Then in your component TS file declare var $ instead of trying to import it:

`declare var $: any;`

That should allow you to trigger the modal via

`$('#confirm').modal();`

Also make sure that the HTML is correct. Example Modal:


    <div class="modal fade" id="confirm" tabindex="-1" role="dialog" data-backdrop="static" aria-labelledby="noDataModalCenterTitle" aria-hidden="true">
        <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
                <div class="modal-header" style="background-color:lightgrey">
                    <h5 class="modal-title" id="noDataModalLongTitle">No Data</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                    </button>
                </div>
                <div class="modal-body">
                    <i class="fas fa-check fa-4x mb-3 animated rotateIn"></i>
                    No Data Found. Please expand your search criteria and try again.
                </div>
                <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Ok</button>
                </div>
            </div>
        </div>
    </div>

Hope this helps!

--------------------------------------------------
cypher: Count distinct paths between two nodes disregarding link type
I&#39;m use cypher to count the number of paths between two nodes.  I have this query:

    MATCH p=(a)-[*1..2]-(b)
    where ID(a) = &quot;node1&quot;
    and ID(b) = &quot;node2&quot;
    return COUNT(p)

However, many of my nodes have multiple links to the same node with different relationship types.  I&#39;d like to ONLY count the distinct paths regardless of the relationship type.  

For example, the paths returned may be as follows:

    (node1)-[rel_type_a]-(node3)-[rel_type_b]-(node2) 
    (node1)-[rel_type_c]-(node3)-[rel_type_d]-(node2) 
    (node1)-[rel_type_e]-(node3)-[rel_type_f]-(node2) 
The query above counts this as 3 paths, but I only want to count this as a single path since all of the nodes are are the same, I&#39;m not interested in the relationship types.

Thanks in advance!

||||||||||||||I have found the following query that works:

    MATCH p=(a)-[*1..2]-(b)
    where ID(a) = "node1"
    and ID(b) = "node2"
    return count(distinct(nodes(p)))

If there is a better way to do this I'm all ears!

--------------------------------------------------
OffsetDateTime date object not getting stored in db the way date is set in object
I have a model which has startDateTime field which is storing DateTime in OffsetDateTime format.

```
startDateTime: 2023-07-25T04:40:46.143-08:00
```

However when we store the above object in our PostgreSQL db on GCP, it is getting stored in below format:  
`2023-07-25 12:40:46.143+00`

It looks like it is adjusting the above object to UTC time.  
But my requirement is, it sohuld store in the same format which is there in object and **should not adjust** to UTC time.

I explored the methods given [here](https://docs.oracle.com/javase/8/docs/api/java/time/OffsetDateTime.html), but none of the methods is satisfying my requirement.

Can someone please suggest if there is a way to acheive this. Any help would be appreciated.

Code I used and input is ```startDateTime: 2023-07-25T12:40:46.143Z```
and ``` &quot;timeZoneOffset&quot;: &quot;UTC-08:00&quot; ```. I am converting input to expected OffsetDateTime based on timezoneOffset value.
```
 val actualDateTime: OffsetDateTime = OffsetDateTime.parse(startDateTime)
            val zoneOffset: ZoneOffset = ZoneOffset.of(timeZoneOffset.replace(&quot;UTC&quot;, &quot;&quot;))
            val expectedDateTime: OffsetDateTime = actualDateTime.withOffsetSameInstant(zoneOffset)
            return expectedDateTime.toString()
```

DDL:
```
CREATE TABLE IF NOT EXISTS TRANSACTION
(
    id                uuid                     DEFAULT,
    start_date_time   TIMESTAMP WITH TIME ZONE NOT NULL,
    end_date_time     TIMESTAMP WITH TIME ZONE NOT NULL,
    currency          VARCHAR(5)               NOT NULL,
    country           VARCHAR(50)              NOT NULL,
    created_at        TIMESTAMP WITH TIME ZONE NOT NULL,
    created_by        VARCHAR (255)            NOT NULL,
    startDateTime     TIMESTAMP WITH TIME ZONE NOT NULL
)
```
||||||||||||||Your best practice is to think of time zone as a presentation attribute and the timestamp itself as an instant in time.  If your business need requires that you preserve the timezone of the input field, then you must store that in another field outside of the timestamp.

The type "timestamp with timezone" is used by postgresql only to interpret the timezone offset from an input string.  That timezone data is not preserved by the database.  The timestamp is always stored in UTC, and the time zone is immediately lost.

--------------------------------------------------
Chrome Devtools formatter for javascript proxy
I&#39;ve recently started using proxies in one of my projects.  The one downside of this has been that when inspecting the object in a debugger, it&#39;s now wrapped by the proxy [javascript proxy](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy).

[![enter image description here][1]][1]

Intead of seeing `[[Handler]],[[Target]],[[isRevoked]]` I would prefer to just see the object referenced by `[[Target]]`.

It&#39;s a minor inconvenience but I think that it could be solved with a [Chrome Devtools custom formatter](https://www.mattzeunert.com/2016/02/19/custom-chrome-devtools-object-formatters.html).

Seems like this would be fairly common, but I can&#39;t find any existing formatters.  Just wanted to double check that there wasn&#39;t already one out there before I go down the road of writing my own.


  [1]: https://i.stack.imgur.com/alW5J.png
||||||||||||||So it turns out this is quite difficult to achieve.  The first problem is that it's [impossible to identify a Proxy][1] without:

[A:][2] Adding a custom symbol to your proxy implementation (if you control the Proxy init code)

[B:][3] Overriding the `window.Proxy` prototype and using a Weakset to basically track every proxy init 

On top of that, there is no way to access to original `[[Target]]` object.  However, running `JSON.parse(JSON.stringify(obj))` does seems to work well for just `console.log` purposes.

Assuming you don't have control to modify the Proxy handler, this is what your solution would look like:

```
// track all proxies in weakset (allows GC)
const proxy_set = new WeakSet();
window.Proxy = new Proxy(Proxy, {
      construct(target, args) {
        const proxy = new target(args[0], args[1]);
        proxy_set.add(proxy);
        return proxy;
      },
});

window.devtoolsFormatters = [{
  header(obj: any) {
    try {
      if (!proxy_set.has(obj)) {
        return null;
      }
      return ['object', {object: JSON.parse(JSON.stringify(obj))}]; //hack... but seems to work
    } catch (e) {
      return null;
    }
},
  hasBody() {
      return false;
  },
}];
```

  [1]: https://stackoverflow.com/questions/36372611/how-to-test-if-an-object-is-a-proxy
  [2]: https://stackoverflow.com/a/37198132/800619
  [3]: https://stackoverflow.com/a/53463589/800619

--------------------------------------------------
How to select n columns from a matrix minimizing a given function
I must buy **one of each product**, but I can visit **no more then n shops**. Which n shops should I choose to spend the least amount of money? Products are not divisible, every shop have full inventory.

|           | Shop A | Shop B | Shop C |
| --------- | ------ | ------ | ------ |
| Product 1 | $10.00 | $12.00 | $9.99  |
| Product 2 | $8.50  | $9.99  | $7.99  |
| Product 3 | $15.00 | $14.50 | $16.99 |


So I need to  minimize
``df.min(axis=1).sum() ``, where df represents any combination of n columns.

Can I do better than check all the combinations by brute force? Greedy approach, or dynamic programming don&#39;t work here. Sorting columns also doesn&#39;t help, because a shop with half of its products prohibitively expensive and the other half almost free, can have a biggest total sum of its products, but still be the best candidate.
||||||||||||||This code solves the problem. Sadly I am not interested in the output itself, but in complexity. How many steps should I do when trying to simulate underlying algorithm on paper?


    import pulp
    
    # You could formulate this as in integer linear program with binary variables:
    problem = pulp.LpProblem("Product Purchase Problem", pulp.LpMinimize)
    
    # Define the decision variables
    stores = ["Store 1", "Store 2", "Store 3"]
    products = ["Product 1", "Product 2", "Product 3"]
    
    # Xij = 1 if product j is purchased at store i, else = 0;
    X = pulp.LpVariable.dicts("X", [(i, j) for i in stores for j in products], cat="Binary")
    # Si = 1 if store i is available, else = 0.
    S = pulp.LpVariable.dicts("S", stores, cat="Binary")
    
    # Define the objective function
    C = {
        ("Store 1", "Product 1"): 1,
        ("Store 1", "Product 2"): 3,
        ("Store 1", "Product 3"): 4,
        ("Store 2", "Product 1"): 3,
        ("Store 2", "Product 2"): 1,
        ("Store 2", "Product 3"): 3,
        ("Store 3", "Product 1"): 2,
        ("Store 3", "Product 2"): 3,
        ("Store 3", "Product 3"): 1,
    }
    
    # Minimize Σij CijXij, where Cij is the cost of purchasing product j from store i
    problem += pulp.lpSum([C[(i, j)] * X[(i, j)] for i in stores for j in products])
    
    # Subject to constraints
    for j in products:
    # Σi Xij = 1 for all products j (each product is purchased at some store)
        problem += pulp.lpSum([X[(i, j)] for i in stores]) == 1
        for i in stores:
    # Xij <= Si for all i,j (can only purchase at store i if store i is available)
            problem += pulp.lpSum(X[(i, j)]) <= S[i]
    
    # Subject to constraint Σi Si <= n (at most n stores available)
    problem += pulp.lpSum([S[i] for i in stores]) <= 2
    
    # Solve the problem
    problem.solve()
    
    # Print the results
    for v in problem.variables():
        print(v.name, "=", v.varValue)
    print("Total Cost =", pulp.value(problem.objective))

--------------------------------------------------
Django - Search Query Results Loading Incorrectly
On my search results page for some reason when a logged in user makes a search query in my gaming application all the results display instead of the query itself.

What needs to get fixed with what I currently have?

I’m building a gaming application where logged in users have access to play different games based on their rank in our application… so when a user makes a search for lets say the letter ``d`` all the unlocked games and locked games that have the letter ``d`` should display. I&#39;m currently getting that but the search query shows all results that don&#39;t inlcude letter ``d``. 


This is only happening when a user is logged in. When a user is logged out the search works correctly, a search query for ``d`` will show results of every game with the character ``d`` in it. 

Any help is gladly appreciated!

Thanks!

Below is my code. 

**models.py**

    class Game_Info(models.Model):
        id = models.IntegerField(primary_key=True, unique=True, blank=True, editable=False)
        game_title = models.CharField(max_length=100, null=True)
        game_rank = models.IntegerField(default=1)
        game_image = models.ImageField(default=&#39;default.png&#39;, upload_to=&#39;game_covers&#39;, null=True, blank=True)
    
    class User_Info(models.Model):
        id = models.IntegerField(primary_key=True, blank=True)
        image = models.ImageField(default=&#39;/profile_pics/default.png&#39;, upload_to=&#39;profile_pics&#39;, null=True, blank=True)
        user = models.OneToOneField(settings.AUTH_USER_MODEL,blank=True, null=True, on_delete=models.CASCADE)
        rank = models.IntegerField(default=1)    


**views.py**

    def is_valid_queryparam(param):
        return param != &#39;&#39; and param is not None
    
    def search_filter_view(request):
        user_profile_games_filter = Game_Info.objects.all()
        user_profile = User_Info.objects.all()
        title_query = request.POST.get(&#39;q&#39;)  
    
        if is_valid_queryparam(title_query):
            user_profile_games_filter = user_profile_games_filter.filter(game_title__icontains=title_query)
    
        if request.user.is_authenticated:
            user_profile = User_Info.objects.filter(user=request.user)
            user_profile_game_obj = User_Info.objects.get(user=request.user)
            user_profile_rank = int(user_profile_game_obj.rank)
    
    
            user_profile_games_filter = Game_Info.objects.annotate(
                user_unlocked_game=Case(
                    When(game_rank__lte=user_profile_rank, then=Value(True)),
                    default=Value(False),
                    output_field=BooleanField()
                )
            )
    
            context = {
                &#39;user_profile&#39;: user_profile,  
                &#39;user_profile_games_filter&#39;: user_profile_games_filter,
                &#39;title_query&#39; : title_query
            }
    
        else:
            context = {
                &#39;user_profile&#39;: user_profile,
                &#39;user_profile_games_filter&#39;: user_profile_games_filter,
                &#39;title_query&#39; : title_query
           }
    
        return render(request, &quot;search_results.html&quot;, context)


**search.html**

    &lt;h1&gt;Results for &amp;#34;{{ title_query }}&amp;#34;&lt;/h1&gt;
    
    
                {% for content in user_profile_games_filter %}
                    {% if content.user_unlocked_game %}
                            &lt;!-- unlocked games logic --&gt;
                            &lt;a class=&quot;game-tile-container&quot; href=&quot;{% url &#39;detail&#39; content.pk %}&quot;&gt;
                                &lt;li class=&quot;results_info&quot;&gt;
                                    &lt;img src= &quot;{{ content.game_image.url }}&quot;&gt; 
                                    &lt;span class=&quot;results_title&quot;&gt;{{ content.game_title }}&lt;/span&gt;
                                &lt;/li&gt;
                            &lt;/a&gt;
            
                            {% else %}
                            &lt;!-- locked games logic --&gt;
                            &lt;a class=&quot;game-tile-container&quot; href=&quot;{% url &#39;detail&#39; content.pk %}&quot;&gt;
                                &lt;li class=&quot;results_info&quot;&gt;
                                    &lt;div class=&quot;locked_game&quot;&gt;
                                        &lt;img class=&quot;lock-img&quot; src={% static &#39;images/treasure-chest-closed-alt.png&#39; %} /&gt;
                                        &lt;img src= &quot;{{ content.game_image.url }}&quot;&gt; 
                                        &lt;button class=&quot;level-up&quot;&gt;Reach level {{ content.game_rank }} to unlock&lt;/button&gt;
                                    &lt;/div&gt;
                                    &lt;span class=&quot;results_title&quot;&gt;{{ content.game_title }}&lt;/span&gt;
                
                                &lt;/li&gt;
                            &lt;/a&gt;
                    {% endif %}
                {% endfor %}


  

||||||||||||||Issue:

    user_profile_games_filter = Game_Info.objects.annotate(
                user_unlocked_game=Case(
                    When(game_rank__lte=user_profile_rank, then=Value(True)),
                    default=Value(False),
                    output_field=BooleanField()
                )
            )

**SOLUTION** for anyone new; found after a long search and talk in the comments!:

Create a new variable where you store a list of the ID's of the filtered objects, in this case is the variable `user_profile_games_filter`:

    NEW_VARIABLE = user_profile_games_filter.values_list('id', flat=True)

Next, with the filtered objects, assign it to the desired object, making sure that you filter the id using `id__in` and it should be equal to the list of IDs. No loop needed!

    user_profile_games_filter = Game_Info.objects.filter(id__in=NEW_VARIABLE).annotate(...)

--------------------------------------------------
How to extract the Vector from an OpenAI Embeddings Call?
I use nearly the same code as here in this Git Repo to get Embeddings from OpenAI:
https://gist.github.com/limcheekin/997de2ae0757cd46db796f162c3dd58c

    oai = OpenAI(
    # This is the default and can be omitted
    api_key=&quot;sk-.....&quot;,
    )

    def get_embedding(text_to_embed, openai):
       
        response = openai.embeddings.create(
            model= &quot;text-embedding-ada-002&quot;,
            input=[text_to_embed]
        )
        
        return response
    
    embedding_raw = get_embedding(text,oai)

According to the Git Repo the Vector should be in `response[&#39;data&#39;][0][&#39;embedding&#39;]`. But it isn&#39;t in my case.

When I print the response Variable, I got this:

    print(embedding_raw)

Output: 

    CreateEmbeddingResponse(data=[Embedding(embedding=[0.009792150929570198, -0.01779201813042164, 0.011846082285046577, -0.0036859565880149603, -0.0013213189085945487, 0.00037509595858864486,..... -0.0121011883020401, -0.015751168131828308], index=0, object=&#39;embedding&#39;)], model=&#39;text-embedding-ada-002&#39;, object=&#39;list&#39;, usage=Usage(prompt_tokens=360, total_tokens=360))


Sorry, I&#39;m new to python, but how can I access the vector data?
||||||||||||||Simply return just the embedding vector as follows:

    def get_embedding(text_to_embed, openai):
       
        response = openai.embeddings.create(
            model= "text-embedding-ada-002",
            input=[text_to_embed]
        )
        
        return response.data[0].embedding # Change this

    embedding_raw = get_embedding(text,oai)

--------------------------------------------------
React Redux Reducer: &#39;this.props.tasks.map is not a function&#39; error
I am making a React Redux example; however, I ran into an issue and get the error below:

&gt; TypeError: this.props.tasks.map is not a function
[Learn More]

I have tried many things and I cannot seem to understand why this is not working. I believe it is when the allReducers maps the tasks from the Tasks function. I have fixed this error back and forth but then it would complain it was undefined. I would fix that and loop back to this issue. Any help would be appreciated. Im sure I am making a simple mistake. Below are my following files

**App.js**
&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    import React from &#39;react&#39;;
    import TaskBoard from &quot;../containers/task-board&quot;;
    require(&#39;../../scss/style.scss&#39;);

    const App = () =&gt; (
        &lt;div&gt;
            &lt;h2&gt;Task List&lt;/h2&gt;
            &lt;hr /&gt;
            &lt;TaskBoard/&gt;
        &lt;/div&gt;
    );

    export default App;


&lt;!-- end snippet --&gt;

**index.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

        import {combineReducers} from &#39;redux&#39;;
        import {Tasks} from &#39;./reducer-tasks&#39;;
        const allReducers = combineReducers({
            tasks: Tasks
        });

        export default allReducers

&lt;!-- end snippet --&gt;

**task-board.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

        import React, {Component} from &#39;react&#39;;
        import {bindActionCreators} from &#39;redux&#39;;
        import {connect} from &#39;react-redux&#39;;
        import {deleteTaskAction} from &#39;../actions/ActionIndex&#39;;
        import {editTaskAction} from &#39;../actions/ActionIndex&#39;;
        class TaskBoard extends Component {
            renderList() {
                return this.props.tasks.map((task) =&gt; {
                    if(task.status == &quot;pending&quot;){
                        return (&lt;li key={task.id}&gt;
                            {task.id} {task.description}
                            &lt;button type=&quot;button&quot;&gt;Finish&lt;/button&gt;
                            &lt;button type=&quot;button&quot;&gt;Edit&lt;/button&gt;
                            &lt;button onClick={() =&gt; this.props.deleteTask(task)} type=&quot;button&quot;&gt;Delete&lt;/button&gt;
                        &lt;/li&gt;
                    );
                }
            });
        }
        render() {
            if (!this.props.tasks) {
                console.log(this.props.tasks);
                return (&lt;div&gt;You currently have no tasks, please first create one...&lt;/div&gt;);
            }
            return (
                &lt;div&gt;
                    {this.renderList()}
                &lt;/div&gt;
            );
        }
    }
        function mapStateToProps(state) {
            return {
                tasks: state.tasks
            };
        }
        function matchDispatchToProps(dispatch){
            return bindActionCreators(
            {
                deleteTask: deleteTaskAction,
                editTask: editTaskAction
            }, dispatch)
        }
        export default connect(mapStateToProps,matchDispatchToProps)(TaskBoard);

&lt;!-- end snippet --&gt;

**reducer-tasks.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    const initialState = {
    	tasks: [
            {
                id: 1,
                description: &quot;This is a task&quot;,
                status: &quot;pending&quot;
            },
            {
                id: 2,
                description: &quot;This is another task&quot;,
                status: &quot;pending&quot;
            },
            {
                id: 3,
                description: &quot;This is an easy task&quot;,
                status: &quot;pending&quot; 

            }
    	]
    }

    export function Tasks (state = initialState, action) {
        switch (action.type) {
            case &#39;ADD_TASK&#39;:
                return Object.assign({}, state, {
                	tasks: [
                		...state.tasks,
                		{
                			description: action.text,
                			status: action.status
                		}
                	]
                })
                break;

            case &#39;EDIT_TASK&#39;:
                return action.payload;
                break;

            case &#39;DELETE_TASK&#39;:
                return Object.assign({}, state, {
                	status: action.status
                })
                break;
        }

        return state;
    }

&lt;!-- end snippet --&gt;

**actionindex.js**
&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;


        export const addTaskAction = (task) =&gt; {
            return {
                type: &#39;ADD_TASK&#39;,
                text: &quot;Here is a sample description&quot;,
                status: &quot;pending&quot;
            }
        };
        export const deleteTaskAction = (task) =&gt; {
            return {
                type: &#39;DELETE_TASK&#39;,
                status: &quot;deleted&quot;
            }
        };
        export const editTaskAction = (task) =&gt; {
            return {
                type: &#39;EDIT_TASK&#39;,
                payload: task
            }
        };

&lt;!-- end snippet --&gt;


||||||||||||||It's because the function 'map' can only be used for arrays, not for objects.

If you print out this.props.tasks in the render function of task-board.js you'll see that it's an OBJECT which contains the tasks array, not the actual tasks array itself.

So to fix this it's quite easy, instead of:

        return this.props.tasks.map((task) => {

it's 

        return this.props.tasks.tasks.map((task) => {

Then it works

--------------------------------------------------
Google Chrome Extension - waiting until page loads
In my Google Chrome Extension, I have a [Content Script][1] (content.js) and a [Background Page][2] (background.html). I have context.js checking for a keyword that appears on the page. However, I want to wait until the page is fully loaded until I search the page, because the keyword may occur at the bottom of the page. 

See [Page action by content][3] sandwich example ([files][4]), this is basically what I am doing. If you load the extension you&#39;ll see the extension only works when the word &quot;sandwich&quot; appears at the top of the page.


  [1]: http://code.google.com/chrome/extensions/content_scripts.html
  [2]: http://code.google.com/chrome/extensions/background_pages.html
  [3]: http://code.google.com/chrome/extensions/samples.html
  [4]: http://src.chromium.org/viewvc/chrome/trunk/src/chrome/common/extensions/docs/examples/api/pageAction/pageaction_by_content/
||||||||||||||Try to add this to the "content_scripts" part of your manifest.json file.
```json
"run_at": "document_end"
```
https://developer.chrome.com/docs/extensions/mv3/content_scripts/

--------------------------------------------------
No matching constructor for initialization of &#39;v8::ScriptOrigin&#39; || candidate constructor (the implicit move constructor) not viable
I am using invoking a  class inside my project src c++ file as
```
ScriptOrigin script_origin(
        isolate_,
        script_name,
        Integer::New(isolate_, 0),                            // line offset
        Integer::New(isolate_, 0),                            // column offset
        False(isolate_),                                      // isCrossOrigin
        Local&lt;Integer&gt;(),                                     // scriptId
        Local&lt;Value&gt;(),                                       // sourceMapURL
        False(isolate_),                                      // isOpaque
        False(isolate_),                                      // isWASM
        su.IsNoModule() ? False(isolate_) : True(isolate_));
  ```
Tried hardcoding the Local&lt;Integer&gt;() as a random integer value also . Still getting the below error

    error: no matching constructor for initialization of &#39;v8::ScriptOrigin&#39;
          ScriptOrigin script_origin(
    note: candidate constructor (the implicit copy constructor) not viable: requires 1 argument, but 10 were provided 
    note: candidate constructor (the implicit move constructor) not viable: requires 1 argument, but 10 were provided


Am i invoking the class wrongly? i do not understand the mismatch arguments note


This is the source code of the class inside a v8 include file

```
 class V8_EXPORT ScriptOrigin {
 public:
  V8_INLINE ScriptOrigin(Isolate* isolate, Local&lt;Value&gt; resource_name,
                         int resource_line_offset = 0,
                         int resource_column_offset = 0,
                         bool resource_is_shared_cross_origin = false,
                         int script_id = -1,
                         Local&lt;Value&gt; source_map_url = Local&lt;Value&gt;(),
                         bool resource_is_opaque = false, bool is_wasm = false,
                         bool is_module = false,
                         Local&lt;Data&gt; host_defined_options = Local&lt;Data&gt;())
      : v8_isolate_(isolate),
        resource_name_(resource_name),
        resource_line_offset_(resource_line_offset),
        resource_column_offset_(resource_column_offset),
        options_(resource_is_shared_cross_origin, resource_is_opaque, is_wasm,
                 is_module),
        script_id_(script_id),
        source_map_url_(source_map_url),
        host_defined_options_(host_defined_options) {
    VerifyHostDefinedOptions();
  }
};
```


||||||||||||||Many of the parameters you're passing have the wrong type. For example, `Integer::New(isolate_, 0)` produces a `v8::Local<v8::Integer>`, not a C++ `int`. Similarly, `False(isolate_)` produces a `v8::Local<v8::Boolean>`, not a C++ `bool`.

This isn't really related to V8: whenever working with C++, you need to care about the types of your values.

The part where the compiler says ...
```none
note: candidate constructor (the implicit copy constructor) not 
viable: requires 1 argument, but 10 were provided 

note: candidate constructor (the implicit move constructor) not 
viable: requires 1 argument, but 10 were provided
```
... is just about that it _tried_ to match the 10 arguments you supplied with the copy constructor and move constructor (both of which expect 1 argument only), but that failed.

--------------------------------------------------
SSIS Excel Destination Editor closes unexpectedly
I&#39;m fairly new to SSIS, but what I&#39;m trying to do should be simple:

I have a Data Flow task that has an OLE DB Source feeding into an Excel Destination. The issue though, is I can&#39;t configure the Excel Destination correctly. I&#39;m able to connect my Excel connection manager, but when I hit the &quot;New...&quot; button next to the &quot;Name of the Excel sheet&quot; dropdown, the Excel Destination Editor window just closes instead of opening a different dialog. In the image below, I highlighted the button that&#39;s closing the window.

![SSIS Button](https://i.stack.imgur.com/BR2TV.png)

In general, I&#39;m following this guide [How to use SSIS to Export to Excel][1] (current step is just above the second to last image in the article).


  [1]: http://knowlton-group.com/using-ssis-to-export-data-to-excel/
||||||||||||||I think the issue lies with the Excel connection instead. Once I changed the output file to .xls instead of .xlsx and changed the connection to Excel 97-2003, I was able to create a new Excel Sheet for the file.

--------------------------------------------------
Is it possible to display toolbar options below textarea in Quilljs editor?
How to display toolbar below `textarea`.

My code: 
    

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    var quill = new Quill(&#39;#txtMessage&#39;, {
      theme: &#39;snow&#39;,
      modules: {
        toolbar: {
          container: [
            [&#39;bold&#39;, &#39;italic&#39;, &#39;underline&#39;],
            [{
              &#39;list&#39;: &#39;ordered&#39;
            }, {
              &#39;list&#39;: &#39;bullet&#39;
            }],
            [&#39;clean&#39;],
            [&#39;code-block&#39;],
            [{
              &#39;variables&#39;: [&#39;{Name}&#39;, &#39;{Email}&#39;]
            }],
          ],
          handlers: {
            &quot;variables&quot;: function(value) {
              if (value) {
                const cursorPosition = this.quill.getSelection().index;
                this.quill.insertText(cursorPosition, value);
                this.quill.setSelection(cursorPosition + value.length);
              }
            }
          }
        }
      }
    });

    // Variables
    const placeholderPickerItems = Array.prototype.slice.call(document.querySelectorAll(&#39;.ql-variables .ql-picker-item&#39;));
    placeholderPickerItems.forEach(item =&gt; item.textContent = item.dataset.value);
    document.querySelector(&#39;.ql-variables .ql-picker-label&#39;).innerHTML = &#39;Variables&#39; + document.querySelector(&#39;.ql-variables .ql-picker-label&#39;).innerHTML;

&lt;!-- language: lang-html --&gt;

    &lt;script src=&quot;//cdn.quilljs.com/1.3.6/quill.js&quot;&gt;&lt;/script&gt;
    &lt;link href=&quot;//cdn.quilljs.com/1.3.6/quill.snow.css&quot; rel=&quot;stylesheet&quot;/&gt;
    &lt;link href=&quot;//cdn.quilljs.com/1.3.6/quill.bubble.css&quot; rel=&quot;stylesheet&quot;/&gt;

    &lt;div id=&quot;txtMessage&quot;&gt;&lt;/div&gt;

&lt;!-- end snippet --&gt;

Output for the above code:
[![enter image description here][1]][1]

I want output as follows:
[![enter image description here][2]][2]
  [1]: https://i.stack.imgur.com/PQtJv.png
  [2]: https://i.stack.imgur.com/bQlmC.png

How to accomplish above result.
||||||||||||||I can't see why not use only css.

Something like this:

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    var quill = new Quill('#editor-container', {
      modules: {
        toolbar: [
          [{
            header: [1, 2, false]
          }],
          ['bold', 'italic', 'underline'],
          ['image', 'code-block']
        ]
      },
      placeholder: 'Compose an epic...',
      theme: 'snow' // or 'bubble'
    });

<!-- language: lang-css -->

    #editor-container {
      height: 375px;
    }

    .editor-wrapper {
      position: relative;
    }

    .ql-toolbar {
      position: absolute;
      bottom: 0;
      width: 100%;
      transform: translateY(100%);
    }

<!-- language: lang-html -->

    <script src="//cdn.quilljs.com/1.3.6/quill.js"></script>
    <link href="//cdn.quilljs.com/1.3.6/quill.snow.css" rel="stylesheet"/>
    <link href="//cdn.quilljs.com/1.3.6/quill.bubble.css" rel="stylesheet"/>
    <div class="editor-wrapper">
      <div id="editor-container">
      </div>
    </div>

<!-- end snippet -->

https://codepen.io/moshfeu/pen/wXwqmg

--------------------------------------------------
Can I install Visual Studio without Admin rights?
I use a machine where I don&#39;t have administrator rights. I&#39;ve been able to run programs without admin rights by extracting the program&#39;s .zip file to a directory I have created on my desktop. However, I can&#39;t find such a .zip file for Visual Studio.

Is there a way to install Visual Studio Community Edition without administrator rights?


||||||||||||||Practically no. Visual Studio (Express and above, excluding VS Code) consists of multiple components that must be installed as admin, and will be required for the app you're debugging to be available as system-wide component. It *might* be possible to use [ThinApp](https://www.vmware.com/products/thinapp.html) or its equivalent, but ThinApp can't even work with [VS 2010](https://www.vmware.com/support/thinapp4/doc/releasenotes_thinapp52.html) and it was by far the best of its class.

A (resource intensive) alternative to get VS on any PC will be packaging a VM with VS installed, either creating one yourself or get a [ready-made](https://developer.microsoft.com/en-us/windows/downloads/virtual-machines) ones. VirtuaBox is available as [portable fork](http://www.vbox.me/) if you can't even get Hyper-V tools installed. But this still require kernel drivers installation, which means at least one-time admin access. Depending on your internet connection & budget, it might be more practical to setup a VPS with VS installed, then remote there.

--------------------------------------------------
ggplot2: how to produce smaller points
I have a large dataset that I am plotting using a scatter plot. These points have a unique combination of x,y and therefore they don&#39;t overlap, but some of them are very close to each other therefore I&#39;m plotitng them with small size.  

1- How to produce smaller point symbols (smaller `size`) so that the areas are proportional. In this example, the last point does not have an area proportional to the `size`. I was expecting it 10 smaller than the middle one e.g.:

    df &lt;- data.frame(c1 = 1:3, c2 = c(1,1,1))
    ggplot(df) + geom_point(aes(x= c1, y = c2), size = c(1, 0.1, 0.01)) 

2- How does the `size` in ggplot2 matches the R graphics `cex` argument e.g.:  `plot(df$c2 ~ df$c1, cex = c(1, 0.1, 0.01))`. 
Thanks
||||||||||||||There is a `size = ` argument to `geom_point`, but you either specify a size for all points:

    + geom_point(size = 0.5)

Or you map the size to one of the columns in your data using `aes`:

    + geom_point(aes(size = c2))

In the latter case, you can control the range of sizes using `scale_size_continuous`. The default is min = 1, max = 6. To get _e.g._ min = 2, max = 8:

    + geom_point(aes(size = c2)) + scale_size_continuous(range = c(2, 8))

- Note that the "ggplot2 way" is to map data to geoms, not to assign values to each observation
- and no, size here is different to `cex`

--------------------------------------------------
Can&#39;t close Excel completely using win32com on Python
This is my code, and I found many answers for [VBA][1], .NET framework and is pretty strange. When I execute this, Excel closes.

    from win32com.client import DispatchEx
    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wbs.Close()
    excel.Quit()
    wbs = None
    excel = None # &lt;-- Excel Closes here

But when I do the following, it does not close.

    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wb = wbs.Open(&#39;D:\\Xaguar\\A1.xlsm&#39;)
    wb.Close(False)
    wbs.Close()
    excel.Quit()
    wb = None
    wbs = None
    excel = None  # &lt;-- NOT Closing !!!

I found some possible answer in Stack Overflow question *[Excel process remains open after interop; traditional method not working][2]*. The problem is that is not Python, and I don&#39;t find `Marshal.ReleaseComObject` and `GC`. I looked over all the demos on `...site-packages/win32com` and others.

Even it does not bother me if I can just get the PID and kill it.

I found a workaround in *[Kill process based on window name (win32)][3]*.

May be not the proper way, but a workround is:

    def close_excel_by_force(excel):
        import win32process
        import win32gui
        import win32api
        import win32con

        # Get the window&#39;s process id&#39;s
        hwnd = excel.Hwnd
        t, p = win32process.GetWindowThreadProcessId(hwnd)
        # Ask window nicely to close
        win32gui.PostMessage(hwnd, win32con.WM_CLOSE, 0, 0)
        # Allow some time for app to close
        time.sleep(10)
        # If the application didn&#39;t close, force close
        try:
            handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, p)
            if handle:
                win32api.TerminateProcess(handle, 0)
                win32api.CloseHandle(handle)
        except:
            pass

    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wb = wbs.Open(&#39;D:\\Xaguar\\A1.xlsm&#39;)
    wb.Close(False)
    wbs.Close()
    excel.Quit()
    wb = None
    wbs = None
    close_excel_by_force(excel) # &lt;--- YOU #@#$# DIEEEEE!! DIEEEE!!!

  [1]: http://en.wikipedia.org/wiki/Visual_Basic_for_Applications
  [2]: https://stackoverflow.com/questions/8977571/excel-process-remains-open-after-interop-traditional-method-not-working
  [3]: http://python.6.n6.nabble.com/Kill-process-based-on-window-name-win32-td1042063.html

||||||||||||||Try this:

    wbs.Close()
    excel.Quit()
    del excel # this line removed it from task manager in my case


--------------------------------------------------
Log4j2 createOnDemand=&quot;true&quot; does not allow creation of new file on a daily basis
`Log4j2 createOnDemand=&quot;true&quot;` does not allow creation of new file on a daily basis in-spite of using `RollingFile Appenders` with `TimeBasedTriggeringPolicy`.

Below is my `log4j2.xml` file.

I have two `appenders`, one is for all logs, another is for a custom purpose, which needs to be generated only on demand, but the `createOnDemand` is overriding the Rolling nature of the log and it is not allowing to create new log file for the custom log.

    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;Configuration status=&quot;WARN&quot;&gt;
    	&lt;Appenders&gt;
    		&lt;RollingFile name=&quot;App&quot; 
    				fileName=&quot;app.log&quot; 
    				filePattern=&quot;app.%d{yyyy-MM-dd}.log&quot;&gt;
    			&lt;PatternLayout pattern=&quot;[%t] %d{yyyy-MM-dd HH:mm:ss,SSS zzz} %-5p %l - %m%n&quot; /&gt;
    			&lt;Policies&gt;
    				&lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;
    			&lt;/Policies&gt;
    		&lt;/RollingFile&gt;
    		&lt;RollingFile name=&quot;custom&quot;
    				 fileName=&quot;appCustom.log&quot;
    				 filePattern=&quot;appCustom.%d{yyyy-MM-dd-HH-mm}.log&quot;
    				 createOnDemand=&quot;true&quot;&gt;
    		&lt;PatternLayout pattern=&quot;[%t] %d{yyyy-MM-dd HH:mm:ss,SSS zzz} %-5p %l - %m%n&quot; /&gt;
    		&lt;Policies&gt;
    			&lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;
    		&lt;/Policies&gt;
    		&lt;/RollingFile&gt;
    	&lt;/Appenders&gt;
    	&lt;Loggers&gt;
    		&lt;Logger name=&quot;AppLogger&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
    				&lt;AppenderRef ref=&quot;App&quot;/&gt;
    			&lt;/Logger&gt;
    		&lt;Logger name=&quot;customLogger&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
    			&lt;AppenderRef ref=&quot;custom&quot;/&gt;
    		&lt;/Logger&gt;
    		&lt;Root level=&quot;info&quot;&gt;
    				&lt;AppenderRef ref=&quot;file&quot; /&gt;
    		&lt;/Root&gt;
    	&lt;/Loggers&gt;
    &lt;/Configuration&gt;
||||||||||||||I have found the fix for the above issue.
This was an existing bug in lo4j2 which is fixed in the version - [2.13.1][1]

Below are the links :

https://issues.apache.org/jira/browse/LOG4J2-2759

https://logging.apache.org/log4j/2.x/changes-report.html#a2.13.3

I was using 2.11.0

Upgrading resolved my issue.


  [1]: https://blogs.apache.org/logging/entry/log4j-2-13-1-released

--------------------------------------------------
How to get the Slack DM channel ID of the Slack App
I have created a simple Slack App app where the only purpose is to send a message to a channel. I understand that there is the `conversations.list` API to list all public channels to get the correct ID. 

However, as a first step, I just want to send the message to the app channel itself. If I use the D... ID it works as expected. No invite by the channel is needed. But how do I get this ID? `conversations.list` only returns publich channels, but no the app channel itself.
||||||||||||||In Slack, there is no such thing as an app's channel. There is a DM channel between every user and your app/bot. In these terms, to send a DM message from your app/bot to the user, you need to know `ID` of this user and specify it as a `channel` argument of the `postMessage` API request.

--------------------------------------------------
How do I make a custom class that&#39;s serializable with dataclasses.asdict()?
I&#39;m trying to use a dataclass as a (more strongly typed) dictionary in my application, and found this strange behavior when using a custom type subclassing `list` within the dataclass. I&#39;m using Python 3.11.3 on Windows.

```python
from dataclasses import dataclass, asdict

class CustomFloatList(list):
    def __init__(self, args):
        for i, arg in enumerate(args):
            assert isinstance(arg, float), f&quot;Expected index {i} to be a float, but it&#39;s a {type(arg).__name__}&quot;

        super().__init__(args)

    @classmethod
    def from_list(cls, l: list[float]):
        return cls(l)

@dataclass
class Poc:
    x: CustomFloatList

p = Poc(x=CustomFloatList.from_list([3.0]))
print(p)  # Prints Poc(x=[3.0])
print(p.x)  # Prints [3.0]
print(asdict(p))  # Prints {&#39;x&#39;: []}
```

This does not occur if I use a regular list[float], but I&#39;m using a custom class here to enforce some runtime constraints.

How do I do this correctly?

I&#39;m open to just using `.__dict__` directly, but I thought `asdict()` was the more &quot;official&quot; way to handle this

A simple modification makes the code behave as expected, but is slightly less efficient:

```python
from dataclasses import dataclass, asdict

class CustomFloatList(list):
    def __init__(self, args):
        dup_args = list(args)
        for i, arg in enumerate(dup_args):
            assert isinstance(arg, float), f&quot;Expected index {i} to be a float, but it&#39;s a {type(arg).__name__}&quot;

        super().__init__(dup_args)

    @classmethod
    def from_list(cls, l: list[float]):
        return cls(l)

@dataclass
class Poc:
    x: CustomFloatList

p = Poc(x=CustomFloatList.from_list([3.0]))
print(p)
print(p.x)
print(asdict(p))
```
||||||||||||||If you look at the [source code of `asdict`](https://github.com/python/cpython/blob/d334122d2295a4863384676a3ce313a831b12335/Lib/dataclasses.py#L1364), you'll see that passes a generator expression that recursively calls itself on the elements of a list when it encounters a list:
```
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)
```



 But *your implementation depletes any iterator it gets in `__init__` before the `super` call*. 

Don't do that. You'll have to "cache" the values if you want to use the superclass constructor. Something like:

```
class CustomFloatList(list):
    def __init__(self, args):
        args = list(args)
        for i, arg in enumerate(args):
            assert isinstance(arg, float), f"Expected index {i} to be a float, but it's a {type(arg).__name__}"

        super().__init__(args)
```

Or perhaps:

```
class CustomFloatList(list):
    def __init__(self, args):
        super().__init__(args)
        for i, arg in enumerate(self):
            if not isinstance(arg, float):
                raise TypeError(f"Expected index {i} to be a float, but it's a {type(arg).__name__}")
```

--------------------------------------------------
How would I run PHP code when input box changes?
I&#39;m trying to do something with PHP when a text box changes. I can do this with JavaScript with onchange but that doesn&#39;t work with PHP.


I already have a PHP function in my already existing PHP code. When a text box value changes, I want it to run the function.

||||||||||||||The reason you can do it in JavaScript is because you're in the same scope...the client side. PHP is a server-side language, so it has no notion of what is happening on the client-side, unless you explicitly tell it.

To tell PHP to evaluate, and possible return the response of a function call, you have to pass it the value of the input using a network call, such as a fetch or ajax request.

Your question shows that you really don't understand PHP, and you need to learn the fundamentals of it. PHP does not run client-side, and JavaScript does not run server-side (again, unless you're using Node, in which case you wouldn't be using PHP).

--------------------------------------------------
Listener method using Spring and ActiveMQ throws &quot;Property name cannot be null&quot; warnings repeatedly
I&#39;ve attempted to implement ActiveMQ using Spring in two places. Both implementations have had this issue. Sending either an HTTP Request using postman or directly entering a message in the ActiveMQ console causes the following Error to be repeated infinitely:

```
2024-02-02T17:08:56.317-06:00 ERROR 2264 --- [ntContainer#0-1] c.j.a.config.JmsConfig$JMSErrorHandler   : Error in listener
```

```
org.springframework.jms.listener.adapter.ListenerExecutionFailedException: Listener method &#39;public void com.jackhodge.activemqlearning.consumer.component.MessageConsumer.messageListener(com.jackhodge.activemqlearning.model.SystemMessage)&#39; threw exception
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:118) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.onMessage(MessagingMessageListenerAdapter.java:84) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:783) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:741) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:719) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:333) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:270) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1258) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1248) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:1141) ~[spring-jms-6.1.3.jar:6.1.3]
  at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]
Caused by: java.lang.NullPointerException: Property name cannot be null
  at org.apache.activemq.command.ActiveMQMessage.getObjectProperty(ActiveMQMessage.java:575) ~[activemq-client-jakarta-5.18.3.jar:5.18.3]
  at org.apache.activemq.command.ActiveMQMessage.getStringProperty(ActiveMQMessage.java:683) ~[activemq-client-jakarta-5.18.3.jar:5.18.3]
  at org.springframework.jms.support.converter.MappingJackson2MessageConverter.getJavaTypeForMessage(MappingJackson2MessageConverter.java:456) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.support.converter.MappingJackson2MessageConverter.fromMessage(MappingJackson2MessageConverter.java:241) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener.extractMessage(AbstractAdaptableMessageListener.java:250) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter.extractPayload(AbstractAdaptableMessageListener.java:472) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter$LazyResolutionMessage.unwrapPayload(AbstractAdaptableMessageListener.java:539) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter$LazyResolutionMessage.getPayload(AbstractAdaptableMessageListener.java:521) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.annotation.support.PayloadMethodArgumentResolver.resolveArgument(PayloadMethodArgumentResolver.java:122) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:118) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:147) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:115) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:110) ~[spring-jms-6.1.3.jar:6.1.3]
  ... 10 common frames omitted
```

What stands out is `Error in listener [...] Property name cannot be null`.

The error still occurs when I don&#39;t do anything in my listener, and Logs/Breakpoints in the `messageListener` aren&#39;t sent nor activated.

Here&#39;s the simple app in which the error is occurring:

**JmsConfig**

```java
@Configuration
@EnableJms
public class JmsConfig {

    Logger logger = LoggerFactory.getLogger(JMSErrorHandler.class);
    
    @Bean
    public DefaultJmsListenerContainerFactory jmsListenerContainerFactory(
            ConnectionFactory connectionFactory,
            DefaultJmsListenerContainerFactoryConfigurer configurer,
            JMSErrorHandler defaultErrorHandler){
        DefaultJmsListenerContainerFactory jmsListenerContainerFactory = new DefaultJmsListenerContainerFactory();
    
        jmsListenerContainerFactory.setConnectionFactory(connectionFactory);
        jmsListenerContainerFactory.setConcurrency(&quot;1&quot;); // start w/ 5 consumers; auto-scale to 10 consumers as necessary
    
        jmsListenerContainerFactory.setErrorHandler(defaultErrorHandler);
        jmsListenerContainerFactory.setMessageConverter(this.jacksonJmsMessageConverter());
    
        configurer.configure(jmsListenerContainerFactory, connectionFactory);
        return jmsListenerContainerFactory;
    
    }
    
    
    @Service
    public class JMSErrorHandler implements ErrorHandler {
        @Override
        public void handleError(Throwable t) {
            logger.error(&quot;Error in listener &quot;, t);
        }
    }
    
    @Bean
    public MessageConverter jacksonJmsMessageConverter() {
        MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();
        converter.setTargetType(MessageType.TEXT);
        converter.setObjectMapper(new ObjectMapper());
        return converter;
    }

**PublishController**

```java
package com.jackhodge.activemqlearning.controller;

import com.jackhodge.activemqlearning.model.SystemMessage;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.jms.core.JmsTemplate;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RestController;

@RestControllerpublic class PublishController {// helperclass for sending/receiving messages
    // Spring JMS abstraction API: Distills and simplifies process; abstracts away boilerplate code
    private JmsTemplate jmsTemplate;

    @Autowired
    public PublishController(JmsTemplate jmsTemplate) {
        this.jmsTemplate = jmsTemplate;
    }

    // post method to trigger publishing of messages
    // Requests to here at sent to the Messaging Broker
    @PostMapping(&quot;/publishMessage&quot;)
    public ResponseEntity&lt;String&gt; publishMessage(@RequestBody SystemMessage systemMessage){
        try{
            jmsTemplate.convertAndSend(&quot;jackhodge-queue&quot;, systemMessage.toString());
            return new ResponseEntity&lt;&gt;(&quot;I, Jack Hodge, sent your message.&quot;, HttpStatus.OK);
        } catch (Exception e){
            return new ResponseEntity&lt;&gt;(e.getMessage(), HttpStatus.INTERNAL_SERVER_ERROR);
        }

    }
}
```

**SystemMessage**

```java
package com.jackhodge.activemqlearning.model;

import lombok.Setter;
import org.springframework.beans.factory.annotation.Autowired;
//import java.io.Serializable;

@Setter
public class SystemMessage {
    private String source;
    private String message;
    
    public SystemMessage(String source, String message) {
    
        this.source = source;
        this.message = message;
    }
    
    
    @Override
    public String toString() {
        return &quot;SystemMessage{&quot; +
                &quot;source=&#39;&quot; + source + &#39;\&#39;&#39; +
                &quot;, message=&#39;&quot; + message + &#39;\&#39;&#39; +
                &#39;}&#39;;
    }

}
```

**MessageConsumer**

```java
package com.jackhodge.activemqlearning.consumer.component;

import com.jackhodge.activemqlearning.model.SystemMessage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.jms.annotation.JmsListener;
import org.springframework.stereotype.Component;

@Componentpublic class MessageConsumer {

    public static final Logger LOGGER = LoggerFactory.getLogger(MessageConsumer.class);
    
    // Consumes from the Messaging broker
    @JmsListener(destination = &quot;jackhodge-queue&quot;)
    public void messageListener(SystemMessage systemMessage){
        LOGGER.info(&quot;Message Received {}&quot;, systemMessage);
    }

}
```

I&#39;m somewhat new to Swing and ActiveMQ and this problem has stumped me -- every avenue of breakpoints/logging/message sources as far as I&#39;m capable of has been tried. Thank you!

Sending this request via both Postman and the ActiveMQ console both resulted in the same Error regardless:

```
{     
    &quot;source&quot;:&quot;jeff bezo&quot;,     
    &quot;message&quot;:&quot;hello&quot; 
}
```
||||||||||||||I fixed this by setting setTypeIdPropertyName in my MessageConverter Bean:


    @Bean
    public MessageConverter jacksonJmsMessageConverter(){
            MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();
            converter.setTargetType(MessageType.TEXT);
            converter.setTypeIdPropertyName("_type");
            converter.setObjectMapper(new ObjectMapper());
            return converter;
        }




--------------------------------------------------
Start and kill background process within one Makefile recipe
Within one `make` recipe, I am trying to:
1. Run a server process in the background
2. Run a command that uses the server, in the foreground
3. Kill the background server process after foreground task completes

The below is where I am at (inspired by https://stackoverflow.com/a/30171236), but it doesn&#39;t quite work because `SERVER_PID` is currently empty (possibly related to this empty PID: https://stackoverflow.com/q/5768034).

```make
run-server:	## Run the server in the foreground.
	run server

run-kill-server:	## Run the server while using it.
	@$(MAKE) run-server&amp;
	export SERVER_PID=$! &amp;&amp; cmd-that-uses-server &amp;&amp; kill $(SERVER_PID)
```

https://stackoverflow.com/questions/7668311/makefile-run-processes-in-background is also related, except I believe it &quot;leaks&quot; the background processes, the recipes there don&#39;t try to `kill` spawned background processes
||||||||||||||> Within one make recipe, I am trying to:
> 
>  1.  Run a server process in the background
>  2.  Run a command that uses the server, in the foreground
>  3.  Kill the background server process after foreground task completes

But you're *not* doing all that in one recipe.  You're using two.

Bringing it all into one recipe would be a step in the right direction, though It will not in itself provide a complete solution.

> `SERVER_PID` is currently empty

Yes, because your recipe is setting *shell* variable `SERVER_PID`, but trying to reference a *`make`* variable of that name.  And also trying to reference a `make` variable named `!`, where you appear to want the shell variable of that name.  You need to escape your `$` by doubling it when you want to pass it through to the shell.

Additionally, each logical line of your recipe runs in a separate shell.  In the one where you define and later user `SERVER_PID`, no background process is ever run.

You probably want something more like this:
```
run-job:
        run server & export SERVER_PID=$$!; cmd-that-uses-server; kill $${SERVER_PID}
```

--------------------------------------------------
python while loop if all conditions are equal then do another random choice from list
This is my python code:
```python
import secrets
from time import sleep

ids = [{&#39;id&#39;: number} for number in range(1, 5+1)]

rand1 = secrets.choice(ids)
rand2 = secrets.choice(ids)
rand3 = secrets.choice(ids)

n = 0
while rand1[&#39;id&#39;] == rand2[&#39;id&#39;] == rand3[&#39;id&#39;]:
        n += 1
        print(&#39;Before&#39;)
        print(rand1[&#39;id&#39;], rand2[&#39;id&#39;], rand3[&#39;id&#39;])
        sleep(1)
        rand1 = secrets.choice(ids)
        rand2 = secrets.choice(ids)
        rand3 = secrets.choice(ids)
        print(&#39;After&#39;)
        print(rand1[&#39;id&#39;], rand2[&#39;id&#39;], rand3[&#39;id&#39;])
```
I&#39;m going to reach this:

&gt; do the while loop and choose a random id until none of the
&gt; rand1[&#39;id&#39;], rand2[&#39;id&#39;] and rand3[&#39;id&#39;] are equal.
&gt; 
&gt; Even two of them are equal, then do another for loop.
||||||||||||||Looping is not the right way to do this.  Just shuffle and deal:
```
import random

nums = list(range(1,5+1))
random.shuffle(nums)
ids = [{'id': n} for n in nums[:3]]
```

--------------------------------------------------
Avoiding duplicate tasks in celery broker
I want to create the following flow using celery configuration\api: 

 - Send TaskA(argB) Only if celery queue has no TaskA(argB) already pending

Is it possible? how?
||||||||||||||I cannot think of a way but to 

 1. Retrieve all executing and scheduled tasks via [`celery inspect`](http://docs.celeryproject.org/en/latest/userguide/workers.html#inspecting-workers)
    
 2. Iterate through them to see if your task is there.

check [this](https://stackoverflow.com/questions/5544629/retrieve-list-of-tasks-in-a-queue-in-celery) SO question to see how the first point is done.

good luck

--------------------------------------------------
How do I late-resolve * in .csproj files?
I have a `.csproj` file that looks like this:

````
&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;net8.0&lt;/TargetFramework&gt;
    &lt;LangVersion&gt;12.0&lt;/LangVersion&gt;
    &lt;DebugType&gt;portable&lt;/DebugType&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
	&lt;None Remove=&quot;*.dat&quot; /&gt;
    &lt;None Include=&quot;*.dat&quot; CopyToOutputDirectory=&quot;Always&quot; /&gt;
  &lt;/ItemGroup&gt;

  &lt;Target Name=&quot;PrecompileScript&quot; BeforeTargets=&quot;BeforeCompile&quot;&gt;
    &lt;Exec Command=&quot;dotnet run -c $(Configuration) --no-build --project ../DatGenerator/DatGenerator.csproj -directory $(ProjectDir)&quot; /&gt;
  &lt;/Target&gt;
&lt;/Project&gt;
````

The problem is `*.dat` is expanded too soon and doesn&#39;t actually pick up any files. How do I expand `*.dat` after the `&lt;Exec` directive that emits the `*.dat` runs? No, `-directory $(ProjectDir)/bin/$(Configuration)/$(TargetFramework)` isn&#39;t right. That doesn&#39;t work at all; see how there isn&#39;t a `&lt;OutputType&gt;exe&lt;/OutputType&gt;`. The copy build outputs built-in functionality needs to work.

Listing each `.dat` file manually is pretty bad. I need to pick up changes automatically here.
||||||||||||||To late-resolve a wildcard, move the item `Include` inside a target.

e.g. Change your code to

```C#
<Project Sdk="Microsoft.NET.Sdk">
  <PropertyGroup>
    <TargetFramework>net8.0</TargetFramework>
    <LangVersion>12.0</LangVersion>
    <DebugType>portable</DebugType>
  </PropertyGroup>

  <Target Name="PrecompileScript" BeforeTargets="BeforeBuild">
    <Exec Command="dotnet run -c $(Configuration) --no-build --project ../DatGenerator/DatGenerator.csproj -directory $(ProjectDir)" />
    <ItemGroup>
      <None Remove="*.dat" />
      <None Include="*.dat" CopyToOutputDirectory="PreserveNewest" />
    </ItemGroup>
  </Target>
</Project>
```

"[How MSBuild builds projects][1]" explains the evaluation and execution phases but, briefly:

 - When building a project, MSBuild has an evaluation phase followed by an executuion phase.
 - 'Top level' `PropertyGroup` and `ItemGroup` elements are evaluated in the evalution phase.
 - Target order is determined in the evaluation phase.
 - Targets are run in the execution phase.
 - `PropertyGroup` and `ItemGroup` elements within a target are evaluated when the target is run.

**Note** I updated the answer to change from using `BeforeCompile` to using `BeforeBuild`.

  [1]: https://learn.microsoft.com/en-us/visualstudio/msbuild/build-process-overview?view=vs-2022

--------------------------------------------------
react-google-maps/api DirectionsService keeps rerendering itself
I have written this code in react JS to using &quot;react-google-maps/api&quot; to calculate route between two points. Now my google map keeps rerendering itself until it gives &quot;DIRECTIONS_ROUTE: OVER_QUERY_LIMIT&quot; error. I don&#39;t know what&#39;s the issue. Help would be appreciated because I am a beginner in react and google-API and also I haven&#39;t found a lot of guides of google API in react.

Here is my code:

    import React from &quot;react&quot;;
    import {
      GoogleMap,
      useLoadScript,
      DirectionsService,
      DirectionsRenderer,
    } from &quot;@react-google-maps/api&quot;;
    
    const libraries = [&quot;places&quot;, &quot;directions&quot;];
    const mapContainerStyle = {
      width: &quot;100%&quot;,
      height: &quot;50vh&quot;,
    };
    const center = {
      lat: 31.582045,
      lng: 74.329376,
    };
    const options = {};
    
    const MainMaps = () =&gt; {
      const { isLoaded, loadError } = useLoadScript({
        googleMapsApiKey: &quot;********&quot;,
        libraries,
      });
    
      const [origin2, setOrigin2] = React.useState(&quot;lahore&quot;);
      const [destination2, setDestination2] = React.useState(&quot;gujranwala&quot;);
      const [response, setResponse] = React.useState(null);
    
      const directionsCallback = (response) =&gt; {
        console.log(response);
    
        if (response !== null) {
          if (response.status === &quot;OK&quot;) {
            setResponse(response);
          } else {
            console.log(&quot;response: &quot;, response);
          }
        }
      };
    
      const mapRef = React.useRef();
      const onMapLoad = React.useCallback((map) =&gt; {
        mapRef.current = map;
      }, []);
      if (loadError) return &quot;Error loading maps&quot;;
      if (!isLoaded) return &quot;loading maps&quot;;
    
      const DirectionsServiceOption = {
        destination: destination2,
        origin: origin2,
        travelMode: &quot;DRIVING&quot;,
      };
    
      return (
        &lt;div&gt;
          &lt;GoogleMap
            mapContainerStyle={mapContainerStyle}
            zoom={8}
            center={center}
            onLoad={onMapLoad}
          &gt;
            {response !== null &amp;&amp; (
              &lt;DirectionsRenderer
                options={{
                  directions: response,
                }}
              /&gt;
            )}
    
            &lt;DirectionsService
              options={DirectionsServiceOption}
              callback={directionsCallback}
            /&gt;
          &lt;/GoogleMap&gt;
        &lt;/div&gt;
      );
    };
    
    export default MainMaps;




||||||||||||||The rendering issue appears to be with the library itself. One alternative I can suggest is to instead use/load Google Maps API script instead of relying on 3rd party libraries. This way, you can just follow the [official documentation](https://developers.google.com/maps/documentation) provided by Google.

By loading the script, we can now follow their [Directions API documentation](https://developers.google.com/maps/documentation/javascript/directions):

Here is a sample app for your reference: https://stackblitz.com/edit/react-directions-64165413

`App.js`
```

    import React, { Component } from 'react';
    import { render } from 'react-dom';
    import Map from './components/map';
    import "./style.css";
    
    class App extends Component {
     
      render() {
        return (
           <Map 
            id="myMap"
            options={{
              center: { lat: 31.582045, lng: 74.329376 },
              zoom: 8
            }}
          />
        );
      }
    }
    
    export default App;

```

`map.js`
```

    import React, { Component } from "react";
    import { render } from "react-dom";
    
    class Map extends Component {
      constructor(props) {
        super(props);
        this.state = {
          map: "",
          origin: "",
          destination: ""
        };
        this.handleInputChange = this.handleInputChange.bind(this); 
        this.onSubmit = this.onSubmit.bind(this);
      }
    
      onScriptLoad() {
        this.state.map = new window.google.maps.Map(
          document.getElementById(this.props.id),
          this.props.options
        );
      }
    
      componentDidMount() {
        if (!window.google) {
          var s = document.createElement("script");
          s.type = "text/javascript";
          s.src = `https://maps.google.com/maps/api/js?key=YOUR_API_KEY`;
          var x = document.getElementsByTagName("script")[0];
          x.parentNode.insertBefore(s, x);
          // Below is important.
          //We cannot access google.maps until it's finished loading
          s.addEventListener("load", e => {
            this.onScriptLoad();
          });
        } else {
          this.onScriptLoad();
        }
      }
    
      onSubmit(event) {    
        this.calculateAndDisplayRoute();
        event.preventDefault();
      }
    
      calculateAndDisplayRoute() {
        var directionsService = new google.maps.DirectionsService();
        var directionsRenderer = new google.maps.DirectionsRenderer();
        directionsRenderer.setMap(this.state.map);
        directionsService.route(
          {
            origin: { query: this.state.origin },
            destination: { query: this.state.destination },
            travelMode: "DRIVING"
          },
          function(response, status) {
            if (status === "OK") {
              directionsRenderer.setDirections(response);
            } else {
              window.alert("Directions request failed due to " + status);
            }
          }
        );
        
      }
    
      handleInputChange(event) {
        const target = event.target;
        const value = target.type === "checkbox" ? target.checked : target.value;
        const name = target.name;
    
        this.setState({
          [name]: value
        });
      }
      addMarker(latLng) {
        var marker = new window.google.maps.Marker({
          position: { lat: -33.8569, lng: 151.2152 },
          map: this.state.map,
          title: "Hello Sydney!"
        });
        var marker = new google.maps.Marker({
          position: latLng,
          map: this.state.map
        });
      }
    
      render() {
        return (
          <div>
            <input
              id="origin"
              name="origin"
              value={this.state.origin}
              placeholder="Origin"
              onChange={this.handleInputChange}
            />
            <input
              id="destination"
              name="destination"
              value={this.state.destination}
              placeholder="Destination"
              onChange={this.handleInputChange}
            />
            <button id="submit" onClick={this.onSubmit}>
              Search
            </button>
            <div className="map" id={this.props.id} />
          </div>
        );
      }
    }
    
    export default Map;

```

--------------------------------------------------
At what point does binary search become more efficient than sequential search?
I&#39;ve been learning a lot about algorithms lately, and the binary searched is hailed for its efficiency in finding an item in large amounts of **sorted** data. But what if the data is not sorted to begin with? at what points does a binary search provide an efficiency boost against sequential search, with binary search having to sort the given array first off THEN search. I&#39;m interested in seeing at what points binary search passes over sequential search, if anyone has tested this before i would love to see some results.

Given an array foo[BUFF] with 14 elements

    1 3 6 3 1 87 56 -2 4 61 4 9 81 7

I would assume a sequential sort would be more efficient to find a given number, let&#39;s say... 3, because binary search would need to first sort the array **THEN** search for the number 3. BUT:

Given an array bar[BUFF] with one thousand elements held

    1 2 4 9 -2 3 8 9 4 12 4 56 //continued

A call to sort then binary search should in theory be more efficiently if i am not mistaken.


||||||||||||||In an unsorted array where no information is known, you are going to have to do linear time search.

Linear time search checks each element once, so it's complexity is `O(n)`. Comparing that to sorting. Sorting algorithms which must check each element more than once and have a complexity of `O(n * log n)`. So to even get it sorted is slower than a sequential search. Even though binary search is `O(log n)` it's pretty useless when you just have arbitrarily ordered data.

If your going to search for stuff multiple times though, consider sorting first as it'll increase your efficiency in the long run.

--------------------------------------------------
GPG error: http://archive.debian.org lenny/updates Release: The following signatures were invalid: KEYEXPIRED 1356982504
I am getting following error while update my source lists

    $ sudo apt-get update
    
    Reading package lists... Done
    
    W: GPG error: http://archive.debian.org lenny/updates Release: The following signatures were invalid: KEYEXPIRED 1356982504
    
    W: You may want to run apt-get update to correct these problems



**How to resolve this issue?**

||||||||||||||To find any expired repository keys and their IDs, use apt-key as follows:

      apt-key list | grep expired

You will get a result similar to the following:

      pub   4096R/BE1DB1F1 2011-03-29 [expired: 2014-03-28]

The key ID is the bit after the / i.e. BE1DB1F1 in this case.

To update the key, run

      sudo apt-key adv --recv-keys --keyserver keys.gnupg.net BE1DB1F1



--------------------------------------------------
Select a Dictionary&lt;T1, T2&gt; with LINQ
I have used the &quot;select&quot; keyword and extension method to return an `IEnumerable&lt;T&gt;` with LINQ, but I have a need to return a generic `Dictionary&lt;T1, T2&gt;` and can&#39;t figure it out.  The example I learned this from used something in a form similar to the following: 

    IEnumerable&lt;T&gt; coll = from x in y 
        select new SomeClass{ prop1 = value1, prop2 = value2 };

I&#39;ve also done the same thing with extension methods.  I assumed that since the items in a  `Dictionary&lt;T1, T2&gt;` can be iterated as `KeyValuePair&lt;T1, T2&gt;` that I could just replace &quot;SomeClass&quot; in the above example with &quot;`new KeyValuePair&lt;T1, T2&gt; { ...`&quot;, but that didn&#39;t work (Key and Value were marked as readonly, so I could not compile this code).

Is this possible, or do I need to do this in multiple steps?

Thanks.


||||||||||||||The extensions methods also provide a [ToDictionary][1] extension.  It is fairly simple to use, the general usage is passing a lambda selector for the key and getting the object as the value, but you can pass a lambda selector for both key and value.

    class SomeObject
    {
        public int ID { get; set; }
        public string Name { get; set; }
    }

    SomeObject[] objects = new SomeObject[]
    {
        new SomeObject { ID = 1, Name = "Hello" },
        new SomeObject { ID = 2, Name = "World" }
    };

    Dictionary<int, string> objectDictionary = 
        objects.ToDictionary(
            o => o.ID, 
            o => o.Name);

Then `objectDictionary[1]` Would contain the value "Hello"

  [1]: http://msdn.microsoft.com/en-us/library/system.linq.enumerable.todictionary.aspx

--------------------------------------------------
How do I get this code to accommodate any given number by only using boolean logic (no conditionals/ functions)
Here is the problem:

```
number = 1101
#You may modify the lines of code above, but don&#39;t move them!
#When you Submit your code, we&#39;ll change these lines to
#assign different values to the variables.
#
#The number above represents a binary number. It will always
#be up to eight digits, and all eight digits will always be
#either 1 or 0.
#
#The string gives the binary representation of a number. In
#binary, each digit of that string corresponds to a power of
#2. The far left digit represents 128, then 64, then 32, then
#16, then 8, then 4, then 2, and then finally 1 at the far right.
#
#So, to convert the number to a decimal number, you want to (for
#example) add 128 to the total if the first digit is 1, 64 if the
#second digit is 1, 32 if the third digit is 1, etc.
#
#For example, 00001101 is the number 13: there is a 0 in the 128s
#place, 64s place, 32s place, 16s place, and 2s place. There are
#1s in the 8s, 4s, and 1s place. 8 + 4 + 1 = 13.
#
#Note that although we use &#39;if&#39; a lot to describe this problem,
#this can be done entirely boolean logic and numerical comparisons.
#
#Print the number that results from this conversion.
```


---



Here is my code

```
##Add your code here!

number_str = str(number) # &quot;1101&quot;
first_num = int(number_str[-1]) * 1
#print(&quot;first num:&quot;, first_num)
second_num = int(number_str[-2]) * 2
#print(&quot;second num:&quot;, second_num) 
third_num = int(number_str[-3]) * 4
#print(&quot;Third num:&quot;, third_num)
fourth_num = int(number_str[-4]) * 8
#print(&quot;fourth num:&quot;, fourth_num)
fifth_num = int(number_str[-5]) * 16
sixt_num = int(number_str[-6]) * 32
seventh_num = int(number_str[-7]) * 64
decimal = first_num + second_num + third_num + fourth_num + fifth_num + sixt_num + seventh_num
print(decimal)
```
The error I got was:
We found a few things wrong with your code. The first one is shown below, and the rest can be found in full_results.txt in the dropdown in the top left:
We tested your code with number = 1010111. We expected your code to print this:
87
However, it printed this:
7

------------------------
I understand that I hard-coded this problem to accommodate 4 digits. I would like this to work for any given numbers without throwing an IndexError: string index out of range.

I appreciate your help.
||||||||||||||I don't like doing people's homework for them, but in this case I think the example says more than an explanation.

You do this conversion one character at a time, from left to right.  At each step, you shift the result left by one, and if the digit is '1', you add it in.
```
number = 1011
decimal = 0
for c in str(number):
    decimal = decimal * 2 + (c=='1')
print(decimal)
```
If that's too clever, replace `(c=='1')` with `int(c)`.

--------------------------------------------------
How can I calculate a hash for a filesystem-directory using Python?
I&#39;m using this code to calculate hash value for a file: 

    m = hashlib.md5()
	with open(&quot;calculator.pdf&quot;, &#39;rb&#39;) as fh:
		while True:
			data = fh.read(8192)
			if not data:
				break
			m.update(data)
		hash_value = m.hexdigest()
		
		print  hash_value

when I tried it on a folder &quot;folder&quot;I got 

    IOError: [Errno 13] Permission denied: folder


How could I calculate the hash value for a folder ?
||||||||||||||This [Recipe][1] provides a nice function to do what you are asking. I've modified it to use the MD5 hash, instead of the SHA1, as your original question asks

	def GetHashofDirs(directory, verbose=0):
	  import hashlib, os
	  SHAhash = hashlib.md5()
	  if not os.path.exists (directory):
		return -1
		
	  try:
		for root, dirs, files in os.walk(directory):
		  for names in files:
			if verbose == 1:
			  print 'Hashing', names
			filepath = os.path.join(root,names)
			try:
			  f1 = open(filepath, 'rb')
			except:
			  # You can't open the file for some reason
			  f1.close()
			  continue

		    while 1:
    		  # Read file in as little chunks
	    	  buf = f1.read(4096)
		      if not buf : break
    		  SHAhash.update(hashlib.md5(buf).hexdigest())
			f1.close()

	  except:
		import traceback
		# Print the stack traceback
		traceback.print_exc()
		return -2

	  return SHAhash.hexdigest()


You can use it like this:

    print GetHashofDirs('folder_to_hash', 1)

The output looks like this, as it hashes each file:

<!-- language: lang-none -->

    ...
    Hashing file1.cache
    Hashing text.txt
    Hashing library.dll
    Hashing vsfile.pdb
    Hashing prog.cs
    5be45c5a67810b53146eaddcae08a809

The returned value from this function call comes back as the hash. In this case, `5be45c5a67810b53146eaddcae08a809`

  [1]: http://code.activestate.com/recipes/576973-getting-the-sha-1-or-md5-hash-of-a-directory/

--------------------------------------------------
Browser-side JS: File System API vs File System Access API?
There was a File System API but shown as deprecated now:&lt;br/&gt; https://developer.mozilla.org/en-US/docs/Web/API/Window/requestFileSystem

There is now another, File System Access API:&lt;br/&gt; https://developer.mozilla.org/en-US/docs/Web/API/File_System_Access_API

What happened to the old API and why was it discontinued? Should the new File System Access API be stable in all common browsers?
||||||||||||||It turned out that File System Access API is not deprecated, it's just not standardised (May 2021); the deprecated one is the function `window.requestFileSystem`; the same function on Chromium-based browsers is `window.webkitRequestFileSystem`.

`File System API` is for creating a virtual drive (temporary or persistent) for each website when using browser-based db (IndexedDB) is not necessary especially for the purpose of storing files.<br/>
https://developer.mozilla.org/en-US/docs/Web/API/FileSystem

`File System Access API` is different, it is for accessing the real file system of the OS. This API is now standardised and available on Chromium-based browsers (May 2021). Firefox has not yet adapted this API.<br/>
https://developer.mozilla.org/en-US/docs/Web/API/File_System_Access_API

Status of these APIs: https://developer.mozilla.org/en-US/docs/Web/API


--------------------------------------------------
How to simulate a loop&#39;s &#39;break&#39; statement inside an array-iterating, custom implemented, &#39;forEach&#39; function/method?
What is the best way to implement a simulation of a loop&#39;s `break` statement, when one does iterate through a user/engine defined function?

    forEach([0, 1, 2, 3, 4], function (n) {
      console.log(n);
      
      if (n === 2) {
        break;
      }
    });

I&#39;ve thought of implementing `forEach` in a way that would break when the function returns `false`. But I would like to hear thoughts on how that is normally done.

||||||||||||||`return`ing `false` is the most common way to do it. That's what jQuery's iterator function [`.each()`][1] does:

> We can break the $.each() loop at a particular iteration by making the
> callback function return **false**. Returning non-false is the same as a
> continue statement in a for loop; it will skip immediately to the next
> iteration.

And its *very* simplified implementation:

    each: function( object, callback ) {
      var i = 0, length = object.length,
      for ( var value = object[0]; 
            i < length && callback.call( value, i, value ) !== false; // break if false is returned by the callback 
            value = object[++i] ) {}
      return object;
    }

  [1]: http://api.jquery.com/jQuery.each/

--------------------------------------------------
Understanding OpenMP shortcomings regarding fork
***I wish to understand what do they mean here. Why would this program &quot;hang&quot;?***

From https://bisqwit.iki.fi/story/howto/openmp/  

&gt; OpenMP and `fork()` It is worth mentioning that using OpenMP in a
&gt; program that calls `fork()` requires special consideration. This
&gt; problem only affects GCC; ICC is not affected.   If your program
&gt; intends to become a background process using `daemonize()` or other
&gt; similar means, you must not use the OpenMP features before the fork.
&gt; After OpenMP features are utilized, a fork is only allowed if the
&gt; child process does not use OpenMP features, or it does so as a
&gt; completely new process (such as after `exec()`).
&gt; 
&gt; This is an example of an erroneous program:
&gt; 
&gt;     #include &lt;stdio.h&gt;   
&gt;     #include &lt;sys/wait.h&gt;   
&gt;     #include &lt;unistd.h&gt;
&gt;     
&gt;     void a(){
&gt;         #pragma omp parallel num_threads(2)
&gt;         {
&gt;             puts(&quot;para_a&quot;); // output twice
&gt;         }
&gt;         puts(&quot;a ended&quot;); // output once   
&gt;     }
&gt;        
&gt;     void b(){
&gt;         #pragma omp parallel num_threads(2)
&gt;         {
&gt;             puts(&quot;para_b&quot;);
&gt;         }
&gt;         puts(&quot;b ended&quot;);   
&gt;     }
&gt;     
&gt;     int main(){    
&gt;         a();   // Invokes OpenMP features (parent process)   
&gt;         int p = fork();    
&gt;         if(!p){
&gt;             b(); // ERROR: Uses OpenMP again, but in child process
&gt;             _exit(0);    
&gt;         }    
&gt;         wait(NULL);    
&gt;         return 0;   
&gt;     }
&gt; 
&gt; When run, this program hangs, never reaching the line that outputs &quot;b
&gt; ended&quot;. There is currently no workaround as the libgomp API does not
&gt; specify functions that can be used to prepare for a call to `fork()`.


||||||||||||||The code as posted violates the POSIX standard.

The [POSIX `fork()` standard states][1]:

> A process shall be created with a single thread. If a multi-threaded
> process calls fork(), the new process shall contain a replica of the
> calling thread and its entire address space, possibly including the
> states of mutexes and other resources. **Consequently, to avoid
> errors, the child process may only execute async-signal-safe
> operations until such time as one of the `exec` functions is called.**

Running OMP-parallelized code is clearly violating the above restriction.
  [1]: http://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html

--------------------------------------------------
How to replace reference to a file with its contents?
Supose that I have a directory containing among others two markdown files 

```
a note.md
another one.md
```

with the `a note.md` containing

```
Here is a description of an idea.

![[another one.md]]
```

and the `another one.md` containing

```
Here is another idea related to it.
```

I am looking for a command in bash that would 

1. take `a note.md`, 
2. replace that the reference `![[another one.md]]` in the `a note.md` with the actual contents of the `another one.md`, and 
3. return the result (so that I could pipe it to Pandoc).

The output in this example would contain

```
Here is a description of an idea.

Here is another idea related to it.
```

---

Why? Obsidian markdown note-taking app allows [embedding file contents](https://help.obsidian.md/Linking+notes+and+files/Embed+files#Embed+a+note+in+another+note) into markdown files  using `![[]]` as described above. However, when converting such files using Pandoc, the references are treated as text. So I am looking for a way to add the embedded content prior to Pandoc conversion.

||||||||||||||If you can use perl :

```
perl -i -pe 's/!\[\[(.*?)]]/`cat "$1"`/eg' "a note.md"
```

`-i` option changes "a note.md", so you may want to do a backup before running the command.

@jhnc provided a much safer version :

```
perl -0777pe 's/!\[\[([^]]+)]]/ -f $1 && `cat \Q$1` =~ s#^\s*(.*?)\s*$#$1#sr || $& /eg' 'a note.md' > 'result.md'
```

--------------------------------------------------
Regex to match key with optional quotes and optional separator
I&#39;m trying to build a regex where it should be able to get all the values of the key mentioned in the log satisfying the following conditions

 1. It can be either in single quote, double quote or no quote.
 2. Key will be either followed by `: - `
 3. Value can either be in single quote, double quote or no quote.

Here is the example I have tried out.

    [&#39;&quot;](?:accountNumber|subNo)[&#39;&quot;][:-]\s*[&#39;&quot;](\d+)[&#39;&quot;]

This regex is finding out the `accountNumber` in the second part of the log

    23:22:12.127 DEBUG Getting Service Details for: accountNumber-&quot;525012078&quot;, subNo 5488870689 
    23:22:12.403 INFO  /subscriptions, [{&quot;accountNumber&quot;:&quot;1233&quot;,&quot;subNo&quot;:&quot;123&quot;,&quot;type&quot;:...}}] 

Qn: Need to find out the `accountNumber` or `subNo` from the log, in both JSON format and statement log.
||||||||||||||You can use
```none
(['"]|)\b(?:accountNumber|subNo)\1[:-]\s*(['"])(\d+)\2
```
See the [regex demo][1].

See the [Java code][2]:
```java
String s = "23:22:12.127 DEBUG Getting Service Details for: accountNumber-\"525012078\", subNo 5488870689 \n23:22:12.403 INFO  /subscriptions, [{\"accountNumber\":\"1233\",\"subscriptionNo\":\"123\",\"type\":...}}] ";
Pattern pattern = Pattern.compile("(['\"]|)\\b(?:accountNumber|subNo)\\1[:-]\\s*(['\"])(\\d+)\\2");
Matcher matcher = pattern.matcher(s);
while (matcher.find()){
	System.out.println(matcher.group(3)); 
} 
```
*Details*:

 - `(['"]|)\b(?:accountNumber|subNo)` - either `"` or `'` captured into Group 1  or an empty space + word boundary + `accountNumber` or `subNo`
 - `\1` - Same value as in Group 1
 - `[:-]` - a `:` or `-`
 - `\s*` - zero or more whitespaces
 - `(['"])` - Group 2: a `'` or `"` char
 - `(\d+)` - Group 3 (the result): one or more digits
 - `\2` - same value as in Group 2.

  [1]: https://regex101.com/r/QNQ3NX/2
  [2]: https://ideone.com/oGhQxI

--------------------------------------------------
Check the total number of parameters in a PyTorch model
How do I count the total number of parameters in a PyTorch model? Something similar to `model.count_params()` in Keras.
||||||||||||||PyTorch doesn't have a function to calculate the total number of parameters as Keras does, but it's possible to sum the number of elements for every parameter group:

    pytorch_total_params = sum(p.numel() for p in model.parameters())

If you want to calculate only the _trainable_ parameters:

    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

---
_Answer inspired by [this answer](https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9) on PyTorch Forums_.

--------------------------------------------------
Pylint warn the usage of print statement
I am using `pylint_django` for my django project. And I want to disable print statement usage or warn about it at least. Because I am using custom logger class. But there is no any warn about usage of print.


```[MASTER]

extension-pkg-whitelist=

ignore=CVS

ignore-patterns=

jobs=1


limit-inference-results=100

load-plugins=

persistent=yes

suggestion-mode=yes

unsafe-load-any-extension=no


[MESSAGES CONTROL]
confidence=

disable=missing-docstring,
        invalid-name,
        astroid-error,
        protected-access,
        broad-except

enable=c-extension-no-member, print-statement


[REPORTS]
evaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)

output-format=text

reports=no

score=yes


[REFACTORING]

max-nested-blocks=5

never-returning-functions=sys.exit


[LOGGING]

logging-format-style=old

logging-modules=logging

....
```

How can i solve this issue?

VsCode settings.json
```
{
  &quot;python.linting.pylintEnabled&quot;: true,
  &quot;python.linting.enabled&quot;: true,
  &quot;python.linting.flake8Enabled&quot;: false,
  &quot;python.linting.prospectorEnabled&quot;: false,
  &quot;python.linting.pylintArgs&quot;: [
    &quot;--load-plugins=pylint_django&quot;,
    &quot;--rcfile=.pylintrc&quot;,
    &quot;--enable=print-statement&quot;
  ]
}
```
||||||||||||||You can do that using [the deprecated checkers ``bad-functions`` options](https://pylint.pycqa.org/en/latest/user_guide/configuration/all-options.html#deprecated-builtins-options):

    [tool.pylint]
    bad-functions = ["map", "filter", "print"]

--------------------------------------------------
How to convert Top-and-Bottom 3d video to side-by-side 3d video with FFmpeg
I have a top-and-bottom 3d video, and i want to look at it with Gear VR, but Gear VR only support side-by-side video, so i need to convert it to side-by-side, while i don&#39;t know how to use ffmpeg to achieve it,does anyone knows ? thanks very much.

||||||||||||||See [stereo3d](http://ffmpeg.org/ffmpeg-filters.html#stereo3d) filter documentation:

`ffmpeg -i top-and-bottom.mov -vf stereo3d=abl:sbsl -c:a copy side-by-side.mov`

--------------------------------------------------
Docker: How to solve the public key error in ubuntu while installing docker
I am getting the below error message when running the below command for installing docker and kubernetes in Ubuntu server. 


    root@master:/home/ubuntu# add-apt-repository \
    &gt;   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
    &gt;   $(lsb_release -cs) \
    &gt;   stable&quot;
    Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease
    Get:2 https://download.docker.com/linux/ubuntu bionic InRelease [64.4 kB]
    Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease
    Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease
    Hit:5 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease
    **Err:2 https://download.docker.com/linux/ubuntu bionic InRelease
      The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY 7EA0A9C3F273FCD8**
    Reading package lists... Done
    W: GPG error: https://download.docker.com/linux/ubuntu bionic InRelease: The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY 7EA0A9C3F273FCD8
    **E: The repository &#39;https://download.docker.com/linux/ubuntu bionic InRelease&#39; is not signed.**
    N: Updating from such a repository can&#39;t be done securely, and is therefore disabled by default.
    N: See apt-secure(8) manpage for repository creation and user configuration details.
    root@master:/home/ubuntu#




I have also ran the below command but no luck

    root@master:/# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 7EA0A9C3F273FCD8
    Executing: /tmp/apt-key-gpghome.rDOuMCVLF2/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 7EA0A9C3F273FCD8
    gpg: keyserver receive failed: No keyserver available


||||||||||||||# EDIT: This answer apparently does not work any more

Run this to add the correct key:

    # Does not work any more
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

Source: https://docs.docker.com/install/linux/docker-ce/ubuntu/

--------------------------------------------------
Create an arc between two points in matplotlib
I am trying to recreate the chart below using matplotlib:
[![enter image description here][1]][1]

I have most of it done but, I just cant figure out how to create the arcs between the years:

    import matplotlib.pyplot as plt
    from scipy.interpolate import interp1d
    import numpy as np
    import pandas as pd
    
    colors = [&quot;#CC5A43&quot;,&quot;#2C324F&quot;,&quot;#5375D4&quot;,]
    
    data = {
        &quot;year&quot;: [2004, 2022, 2004, 2022, 2004, 2022],
        &quot;countries&quot; : [ &quot;Denmark&quot;, &quot;Denmark&quot;, &quot;Norway&quot;, &quot;Norway&quot;,&quot;Sweden&quot;, &quot;Sweden&quot;,],
        &quot;sites&quot;: [4,10,5,8,13,15]
    }
    df= pd.DataFrame(data)
    df = df.sort_values([ &#39;year&#39;], ascending=True ).reset_index(drop=True)
    df[&#39;ctry_code&#39;] = df.countries.astype(str).str[:2].astype(str).str.upper()
    df[&#39;year_lbl&#39;] =&quot;&#39;&quot;+df[&#39;year&#39;].astype(str).str[-2:].astype(str)
    sites = df.sites
    lbl1 = df.year_lbl
    
    
    fig, ax = plt.subplots( figsize=(6,6),sharex=True, sharey=True, facecolor = &quot;#FFFFFF&quot;, zorder= 1)
    
    
    ax.scatter(sites, sites, s= 340, c= colors*2 , zorder = 1)
    ax.set_xlim(0, sites.max()+3)
    ax.set_ylim(0, sites.max()+3)
    ax.axline([ax.get_xlim()[0], ax.get_ylim()[0]], [ax.get_xlim()[1], ax.get_ylim()[1]], zorder = 0, color =&quot;#DBDEE0&quot; )
    
    for i, l1 in zip(range(0,6), lbl1) :
        ax.annotate(l1, (sites[i], sites[i]), color = &quot;w&quot;,va= &quot;center&quot;, ha = &quot;center&quot;)
    
 
    ax.set_axis_off()

Which gives me this:
[![enter image description here][2]][2]

I have tried both [mpatches.arc][3] and [patches and path][4] but cant make it work.


  [1]: https://i.stack.imgur.com/P9NGe.png
  [2]: https://i.stack.imgur.com/UaONB.png
  [3]: https://stackoverflow.com/questions/30642391/how-to-draw-a-filled-arc-in-matplotlib
  [4]: https://stackoverflow.com/questions/50346166/draw-an-arc-as-polygon-using-start-end-center-and-radius-using-python-matplotl
||||||||||||||# Semicircle arc between two points
To draw a semicircle between two points:
- the center of the two points will be the center of the circle
- for a circular arc, `width` and `height` both need to be set to the diameter; that diameter is the distance between the two points (square root of sum of squares of the x and y differences)
- the starting angle can be calculated by the arc tangent of the vector from one point to the other
- the final angle will be 180º further

Encapsulated in a function, together with a little test:
```python
import matplotlib.pyplot as plt
from matplotlib. Patches import Arc
import numpy as np

def draw_semicircle(x1, y1, x2, y2, color='black', lw=1, ax=None):
    '''
    draw a semicircle between the points x1,y1 and x2,y2
    the semicircle is drawn to the left of the segment
    '''
    ax = ax or plt.gca()
    # ax. Scatter([x1, x2], [y1, y2], s=100, c=color)
    startangle = np.degrees(np.arctan2(y2 - y1, x2 - x1))
    diameter = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)  # Euclidian distance
    ax.add_patch(Arc(((x1 + x2) / 2, (y1 + y2) / 2), diameter, diameter, theta1=startangle, theta2=startangle + 180,
                     edgecolor=color, facecolor='none', lw=lw, zorder=0))

angle = np.linspace(0, 38, 80)
x = angle * np.cos(angle)
y = - angle * np.sin(angle)
fig, ax = plt.subplots()
for x1, y1, x2, y2 in zip(x[:-1], y[:-1], x[1:], y[1:]):
    draw_semicircle(x1, y1, x2, y2, color='fuchsia', lw=2)

ax.set_aspect('equal')  # show circles without deformation
ax.autoscale_view()  # fit the arc into the data limits
ax. Axis('off')
plt.show()
```
[![matplotlib semicircles between two points][1]][1]


# Specific code for the data in the question
Here is an adaption of the code for your case (180º arc on a 45º line).  The text can be positioned using the x coordinate of the first and the y coordinate of the second point.
```python
import matplotlib.pyplot as plt
from matplotlib. Patches import Arc
import pandas as pd
import math

colors = ["#CC5A43", "#2C324F", "#5375D4"]
data = {
    "year": [2004, 2022, 2004, 2022, 2004, 2022],
    "countries": ["Denmark", "Denmark", "Norway", "Norway", "Sweden", "Sweden"],
    "sites": [4, 10, 5, 8, 13, 15]
}
df = pd.DataFrame(data)
df = df.sort_values(['year'], ascending=True).reset_index(drop=True)
df['ctry_code'] = df.countries.astype(str).str[:2].astype(str).str.upper()
df['year_lbl'] = "'" + df['year'].astype(str).str[-2:].astype(str)
sites = df.sites
lbl1 = df.year_lbl
countries = df.ctry_code

fig, ax = plt.subplots(figsize=(6, 6), sharex=True, sharey=True, facecolor="#FFFFFF", zorder=1)

ax. Scatter(sites, sites, s=340, c=colors * 2, zorder=1)
ax.set_xlim(0, sites.max() + 3)
ax.set_ylim(0, sites.max() + 3)
ax.set_aspect('equal')
ax.axline([ax.get_xlim()[0], ax.get_ylim()[0]], [ax.get_xlim()[1], ax.get_ylim()[1]], zorder=0, color="#DBDEE0")

for site, l1 in zip(sites, lbl1):
    ax.annotate(l1, (site, site), color="w", va="center", ha="center")

for x1, x2, color, country in zip(sites[:len(sites) // 2], sites[len(sites) // 2:], colors, countries):
    center = (x1 + x2) / 2
    diameter = math.sqrt((x2 - x1) ** 2 + (x2 - x1) ** 2)  # Euclidian distance
    ax.add_patch(Arc((center, center), diameter, diameter, theta1=45, theta2=225,
                     edgecolor=color, facecolor='none', lw=2))
    ax.annotate(country, (x1, x2), color=color, va="center", ha="center",
                bbox=dict(boxstyle="round, pad=0.5", facecolor="aliceblue", edgecolor=color, lw=2))
ax.set_axis_off()
plt.show()
``` 
[![matplotlib arcs between points][2]][2]


  [1]: https://i.stack.imgur.com/GY9Hl.png
  [2]: https://i.stack.imgur.com/A5Spr.png

--------------------------------------------------
TypeScript error on context value type mismatch
I am implementing React context using TypeScript and I seem to be getting and error on the `value` prop of my context.

Here is what the error says:
```
Type &#39;{ createReferral: (referralData: { actionStepId: number; appTypeId: string; comment: string; createdDate: Date; createdBy: string; createdByDisplayName: string; currentAssigneeDisplayName: string; ... 4 more ...; submissionId: string; }) =&gt; void; getCurrentUser: () =&gt; AccountInfo; }&#39; is not assignable to type &#39;SelectedContextType&#39;.
```

So it looks like there is a mismatch between my expected context value types and the actual value types.

I declare my context as follows:
```
interface SelectedContextType {
  createReferral: (referralData: {
    actionStepId: number
    appTypeId: string
    comment: string
    createdDate: Date
    createdBy: string
    createdByDisplayName: string
    currentAssigneeDisplayName: string
    currentAssigneeGRNID: string
    reasonTypeId: number | string
    referralTypeId: number | string
    statusTypeId: number
    submissionId: string
  }) =&gt; void
  getCurrentUser: () =&gt; { name: string; username: string }
}

export const AppContext = createContext&lt;SelectedContextType | undefined&gt;(
  undefined
)
```

and I declare these functions in my context:
```
const AppProvider = ({ children, msalContext }: AppProps) =&gt; {
  function getCurrentUser() {
    return msalContext.accounts[0]
  }

  function createReferral(referralData: {
    actionStepId: number
    appTypeId: string
    comment: string
    createdDate: Date
    createdBy: string
    createdByDisplayName: string
    currentAssigneeDisplayName: string
    currentAssigneeGRNID: string
    reasonTypeId: number | string
    referralTypeId: number | string
    statusTypeId: number
    submissionId: string
  }) {
    referralAxios
      .post(&#39;/api/ReferralMasters&#39;, {
        actionStepID: referralData.actionStepId,
        appTypeID: referralData.appTypeId,
        comment: referralData.comment,
        createdDate: referralData.createdDate,
        createdBy: referralData.createdBy,
        createdByDisplayName: referralData.createdByDisplayName,
        currentAssigneeDisplayName: referralData.currentAssigneeDisplayName,
        currentAssigneeGRNID: referralData.currentAssigneeGRNID,
        reasonTypeID: referralData.reasonTypeId,
        referralTypeID: referralData.referralTypeId,
        statusTypeID: referralData.statusTypeId,
        submissionID: Number(referralData.submissionId),
      })
      .then(function (response) {
        console.log(response)
        if (response.data.actionStepID === 2) {
          sendReferralEmail(response.data)
        }
      })
      .catch(function (error) {
        console.log(error, &#39;testError&#39;)
      })
  }

  const appContext = { createReferral, getCurrentUser }
  return (
    &lt;AppContext.Provider value={appContext}&gt;{children}&lt;/AppContext.Provider&gt;
  )
}

export default withMsal(AppProvider)
```

I am not sure what exactly the error is saying. I see it referencing `AccountInfo` which does not exist as a type anywhere in my app.


The `msalContext.accounts[0]` is an object that looks like:
```
authorityType: &quot;MSSTS&quot;
environment: &quot;login.windows.net&quot;
homeAccountId: &quot;89887r89e&quot;
idTokenClaims: {aud: &#39;1799}
localAccountId: &quot;60000&quot;
name: &quot;User&quot;
username: &quot;User@STAR.COM&quot;
```

example: [example][1]


  [1]: https://www.typescriptlang.org/play?#code/JYWwDg9gTgLgBAJQKYEMDGMA0cDec1SoxIDCEAdsQB7wC+cAZlBCHAESHoxsBQoksXHACSAWQDOKADZlKSGtgDuwGAAsJ0uPSYs4AcgACKAF4BXQgHoQkqQFpOGPTz5yoDdEjgBBMGAAKzGDiuDxw+KrAUgAmhOQAXIioGAB0yFwAchBRSKFw1tKy1DAJYjaF8jA8tM7Aru5ongDKSFJIGEhR5TQAKgCeYJ44uWEEREjIDEhQUNIJABSEk9PSACIoMCgJQ2E7O1zAFI3EYMJRCeSmIABGU8O7cCi+fQOnCeIwULUA5ne7aCwgJCUN4fb6-HajdYdNbEBIwnL3CGcYhRABCvRBn3IP0RI2RHXRK2A4jAUhQvXSKEBmLBuPw5liMC84nEwC+5CQSCJJLJFKpSBp2PBIwZQKZLLZHKQAHEEOlhCtBTjcZxxBRnkhXnALtcpnAAD5wd5Y5WIxZTGZSDVanU3KAGo2goV097rUzia1nbWXO3Co2mK4gYmsiha420na0ACUcAAvAA+OAANwgwCidy+SBgJFFlAAquIpvMYwmhOR+UqANxwd1TcvUx0mrRVZzyATwf7kd7eXxdeCx-D4vsAHmarXanQoRQ1DtM5GyDFqHXjczuc4XS-TUecne7Pn8zCTab1A7meDQEWisWw+RkU4qWgS+4CECCJcT20Yc4wB3IcEz2a5jABZTHMMafiMFBqq0yRSBAXxzLefbJOg-xzjA4gANoAAwALrbrshAwOYf5IfeNAoWgaGUFheG5NUdwMN+MC-oOYwTBa0gLEgSyWjCmwhHS+yHMcNo+rcQlPP0mpeuGzq4v8ICAsCjYRoikIovCcJQn6GkEhiqnyep+Jor03KkuSlINnJpr3GgQHMqy7KcuZvJWQKhm2X8DkSs5MpygqSp+qq6rSWJur2oaNnBTxnFWmFXq2nqUVOl5OyusRHoJec4lQH64gBkGEqhrJqV3NGgm4hYFhwMkdXlc4fxQfAjxgH2cZCHpHHLFI2AATm0xiiB9rVGEREkXAq67MO+7IS+R7ZPaSbSKYSCxjgrV9rQ8Y4BekQxECtDDhYs3kTAyTzceUDxrk260EAA
||||||||||||||[the AccountInfo.name value is nullable](https://github.com/AzureAD/microsoft-authentication-library-for-js/blob/e7a55511680aec602310f63db0bb5b7d2b07fab3/lib/msal-common/src/account/AccountInfo.ts#L20)

just make it nullable either in your code:
```ts
  getCurrentUser: () => { name?: string; username: string }
```

--------------------------------------------------
Allow multiple CORS domain in express js
How do I allow multiple domains for CORS in express in a simplified way.

I have

     cors: {
            origin: &quot;www.one.com&quot;;
        }
    
        app.all(&#39;*&#39;, function(req, res, next) {
                res.header(&quot;Access-Control-Allow-Origin&quot;, cors.origin);
                res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept&quot;);
                next();
            });

This works when there is only one domain mentioned in `origin`

But if I want to have `origin` as an array of domains and I want to allow CORS for all the domains in the origin array, I would have something like this - 

    cors: {
                origin: [&quot;www.one.com&quot;,&quot;www.two.com&quot;,&quot;www.three.com&quot;];
            }
    
But then the problem is this below code would not work - 

    app.all(&#39;*&#39;, function(req, res, next) {
                    res.header(&quot;Access-Control-Allow-Origin&quot;, cors.origin);
                    res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept&quot;);
                    next();
                });

How do I make `res.header` take an array of domains via `cors.origin` ?
||||||||||||||The value of `Access-Control-Allow-Origin` must be a string, not a list. So to make it dynamic you need to get the requesting origin from the `Origin` HTTP request header, check it against your array of authorized origins. If it's present, then add that origin as the value of the `Access-Control-Allow-Origin` header; otherwise, use a default value, which would prohibit unauthorized domains from accessing the API.

There is no native implementation for this. You can do it yourself using the code below. 

    cors: {
      origin: ["www.one.com","www.two.com","www.three.com"],
      default: "www.one.com"
    }

    app.all('*', function(req, res, next) {
      const origin = cors.origin.includes(req.header('origin').toLowerCase()) ? req.headers.origin : cors.default;
      res.header("Access-Control-Allow-Origin", origin);
      res.header("Access-Control-Allow-Headers", "Origin, X-Requested-With, Content-Type, Accept");
      next();
    });


--------------------------------------------------
Python 3.Kivy. Is there any way to limit entered text in TextInput widget?
I&#39;m writing kivy app and resently I faced with a  problem of unlimited inputing text in TextInput widget. Is there any solution to this problem?
||||||||||||||A possible solution is to create a new property and overwrite the insert_text method:

    
    from kivy.app import App
    from kivy.uix.textinput import TextInput
    from kivy.properties import NumericProperty
    
    
    class MyTextInput(TextInput):
        max_characters = NumericProperty(0)
        def insert_text(self, substring, from_undo=False):
            if len(self.text) > self.max_characters and self.max_characters > 0:
                substring = ""
            TextInput.insert_text(self, substring, from_undo)
    
    class MyApp(App):
        def build(self):
            return MyTextInput(max_characters=4)
    
    
    if __name__ == '__main__':
        MyApp().run()

--------------------------------------------------
Generating random numbers over a range in Go
All the integer functions in [`math/rand`](http://golang.org/pkg/math/rand/#Int) generate non-negative numbers.

    rand.Int() int              // [0, MaxInt]
    rand.Int31() int32          // [0, MaxInt32]
    rand.Int31n(n int32) int32  // [0, n)
    rand.Int63() int64          // [0, MaxInt64]
    rand.Int63n(n int64) int64  // [0, n)
    rand.Intn(n int) int        // [0, n)

I would like to generate random numbers in the range **[-m, n)**. In other words, I would like to generate a mix of positive and negative numbers.
||||||||||||||I found this example at [Go Cookbook](http://golangcookbook.blogspot.com/2012/11/generate-random-number-in-given-range.html), which is equivalent to `rand.Range(min, max int)` (if that function existed):

```go
rand.Intn(max - min) + min
```

--------------------------------------------------
C#: Create a virtual drive in Computer
Is there any way to create a virtual drive in &quot;(My) Computer&quot; and manipulate it, somewhat like JungleDisk does it?

It probably does something like:

    override OnRead(object sender, Event e) {
        ShowFilesFromAmazon();
    }

Are there any API:s for this? Maybe to write to an XML-file or a database, instead of a real drive.

---

The [Dokan Library][1] seems to be the answer that mostly corresponds with my question, even though [System.IO.IsolatedStorage][2] seems to be the most standardized and most Microsoft-environment adapted.


  [1]: http://dokan-dev.net/en/
  [2]: http://msdn.microsoft.com/en-us/library/system.io.isolatedstorage.aspx
||||||||||||||You can use the <a href="https://dokan-dev.github.io/">Dokan library</a> to create a virtual drive. There is a .Net wrapper for interfacing with C#.

--------------------------------------------------
In Excel how could I change font colour according to content of a cell
I have a list of playing cards with each card in its own cell e.g.
3h
2s
Kd
Ah
Jc....
To help with visualisation I wanted to change the font colour of the hearts and diamonds to red.
Of course manually would be an option, but very tedious

I have tried (unsuccessfully) to write a vba script to do a search and replace using
https://stackoverflow.com/questions/17684155/excel-vba-find-and-replace as an example.
However as I am NOT changing the text it just looped continually. I also could not make the search format specific.

```
Sub Main()
Dim c As Range
Dim redCell As String

With Worksheets(1).Range(&quot;A1:A52&quot;)
Application.FindFormat.Clear
Application.FindFormat.Font.Color = rgbBlack &#39;Automatic was a problem
    Set c = .Find(&quot;h&quot;, LookIn:=xlValues, Searchformat:=True)
    If Not c Is Nothing Then
        redCell = c.Address
        Do
            Range(redCell).Font.Color = -16776961
            Set c = .FindNext(c)
        Loop While Not c Is Nothing
    End If
End With
End Sub
```

I also tried to use conditional formatting but could not find a criteria that worked.

    =OR(FIND(&quot;h&quot;,A1)=2,FIND(&quot;d&quot;,A1)=2) 

generates a #VALUE! error
Any pointers would be gratefully received.

||||||||||||||You can use this formula for your format condition:

    =OR(ISNUMBER(FIND("h",A1)),ISNUMBER(FIND("d",A1)))

Your attempt returns an error as `FIND("d",A1)` returns an error (no d found in A1). By using `ISNUMBER` `FIND("d",A1)` will return `false` instead of an error.


--------------------------------------------------
vs code not opening up in windows
whenever i try to open my vs code editor, nothing happens it doesn&#39;t launch and even there are no errors..!! And i am confused what&#39;s wrong here in my vs code. Please anyone help me fix it..!!

Below are the verbose command i typed in the terminal..

```
C:\Users\Avinash&gt;code . --verbose

[main 2020-05-10T05:17:56.317Z] Error: UNKNOWN: unknown error, mkdir
[main 2020-05-10T05:17:56.318Z] Lifecycle#kill()
[main 2020-05-10T05:17:56.320Z] [File Watcher (node.js)] Error: UNKNOWN: unknown error, stat &#39;c:\Users\Avinash Maurya\AppData\Roaming\Code\User&#39;
```
||||||||||||||no need of Unistalling, just go to your vscode-setup and reinstall it. (by this procedure all of your's settings, files , extensions etc.. will be restored as it is.)

--------------------------------------------------
How to copy a file from one folder to another using VBScript
How can I copy a file from one folder to another using VBScript?

I had tried this below one from the information provide on the internet:

    dim filesys
    
    set filesys=CreateObject(&quot;Scripting.FileSystemObject&quot;)
    
    If filesys.FileExists(&quot;c:\sourcefolder\anyfile.txt&quot;) Then
    
    filesys.CopyFile &quot;c:\sourcefolder\anyfile.txt&quot;, &quot;c:\destfolder\&quot;

When I execute this, I get a &#39;permission denied&#39; error. 
||||||||||||||Try this.  It will check to see if the file already exists in the destination folder, and if it does will check if the file is read-only.  If the file is read-only it will change it to read-write, replace the file, and make it read-only again.

    Const DestinationFile = "c:\destfolder\anyfile.txt"
    Const SourceFile = "c:\sourcefolder\anyfile.txt"
    
    Set fso = CreateObject("Scripting.FileSystemObject")
    	'Check to see if the file already exists in the destination folder
    	If fso.FileExists(DestinationFile) Then
    		'Check to see if the file is read-only
    		If Not fso.GetFile(DestinationFile).Attributes And 1 Then 
    			'The file exists and is not read-only.  Safe to replace the file.
    			fso.CopyFile SourceFile, "C:\destfolder\", True
    		Else 
    			'The file exists and is read-only.
    			'Remove the read-only attribute
    			fso.GetFile(DestinationFile).Attributes = fso.GetFile(DestinationFile).Attributes - 1
    			'Replace the file
    			fso.CopyFile SourceFile, "C:\destfolder\", True
    			'Reapply the read-only attribute
    			fso.GetFile(DestinationFile).Attributes = fso.GetFile(DestinationFile).Attributes + 1
    		End If
    	Else
    		'The file does not exist in the destination folder.  Safe to copy file to this folder.
    		fso.CopyFile SourceFile, "C:\destfolder\", True
    	End If
    Set fso = Nothing

--------------------------------------------------
How does Spring auto convert objects to json for @RestController
I&#39;m looking at code in which I&#39;m assuming spring decides to use Jackson behind the scenes to auto convert an object to json for a @RestController
```
@RestController 
@RequestMapping(&quot;/api&quot;)
public class ApiController {

    private RoomServices roomServices;

    @Autowired
    public ApiController(RoomServices roomServices) {
        this.roomServices = roomServices;
    }

    @GetMapping(&quot;/rooms&quot;)
    public List&lt;Room&gt; getAllRooms() {
        return this.roomServices.getAllRooms();
    }
}
```
The Room class is just a plain java class with some fields, getters/setters. There is no Jackson or any other explicit serialization going on in the code. Although this does return json when checking the url. I tried looking through the spring documentation but I&#39;m not quite sure what I&#39;m looking for. What is the name for this process in spring / how does it work? I tried with just @Controller and it broke. Is this functionality coming from @RestController? 
||||||||||||||If you are using [Spring Boot Starter Web][1], you can see that it's using [Spring Boot Starter JSON][2] through the compile dependencies, and Jackson is the dependency of the Start JSON library. So, you're assumption is right (Spring is using Jackson for JSON conversion by default)

Spring use it's AOP mechanism to intercept the mapping methods in `@Controller` (you can see that [`@RestController`][3] is actually a `@Controller` with `@ResponseBody`), spring create a proxy object (using JDK proxy or through cglib) for the class that annotated with `@Controller`.

When the request flow is processing, the program who really call the mapping method will be lead to the proxy first, the proxy will invoke the real `@Controller` object's method and convert it's returning value to JSON String using Jackson Library (if the method is annotated with `@ResponseBody`) and then return the JSON String back to the calling program.


  [1]: https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-web/2.4.1
  [2]: https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-json/2.4.1
  [3]: https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/bind/annotation/RestController.html

--------------------------------------------------
mount: unknown filesystem type &#39;vmhgsf&#39;
I&#39;m trying to mount my Windows shared folder in CentOS using command: 

    ~mount -t vmhgfs .host:/shared-folder /var/www/html/

Unfortunately I get :

    ~mount: unknown filesystem type &#39;vmhgfs&#39;

error. I tried to use:

    ~/usr/bin/vmhgfs-fuse /mnt

but mountpoint is not empty...

Is there any way to mount this folder on VMware player?
||||||||||||||Cyb

Try this:

    vmhgfs-fuse .host:/shared-folder /var/www/html/
you might need to use **sudo** on this

--------------------------------------------------
Why do we require “requires requires”?
One of the corners of C++20 constraints is that there are certain situations in which you have to write `requires requires`. For instance, this example from [\[expr.prim.req\]/3](http://eel.is/c++draft/expr#prim.req-3):

&gt; A _requires-expression_ can also be used in a _requires-clause_ ([temp]) as a way of writing ad hoc constraints on template arguments such as the one below:
&gt;
&gt; 
    template&lt;typename T&gt;
      requires requires (T x) { x + x; }
        T add(T a, T b) { return a + b; }

&gt; The first requires introduces the _requires-clause_, and the second introduces the _requires-expression_. 

What is the technical reason behind needing that second `requires` keyword? Why can&#39;t we just allow writing:

    template&lt;typename T&gt;
      requires (T x) { x + x; }
        T add(T a, T b) { return a + b; }

&lt;sub&gt;(Note: please don&#39;t answer that the grammar `requires` it)&lt;/sub&gt;
||||||||||||||It is because the grammar requires it. It does.

A `requires` constraint does not *have to* use a `requires` expression. It can use any more-or-less arbitrary boolean constant expression. Therefore, `requires (foo)` must be a legitimate `requires` constraint.

A `requires` *expression* (that thing that tests whether certain things follow certain constraints) is a distinct construct; it's just introduced by the same keyword. `requires (foo f)` would be the beginning of a valid `requires` expression.

What you want is that if you use `requires` in a place that accepts constraints, you should be able to make a "constraint+expression" out of the `requires` clause.

So here's the question: if you put `requires (foo)` into a place that is appropriate for a requires constraint... how far does the parser have to go before it can realize that this is a requires *constraint* rather than a constraint+expression the way you want it to be?

Consider this:

````
void bar() requires (foo)
{
  //stuff
}
````

If `foo` is a type, then `(foo)` is a parameter list of a requires expression, and everything in the `{}` is not the body of the function but the body of that `requires` expression. Otherwise, `foo` is an expression in a `requires` clause.

Well, you could say that the compiler should just figure out what `foo` is first. But C++ *really* doesn't like it when the basic act of parsing a sequence of tokens requires that the compiler figure out what those identifiers mean before it can make sense of the tokens. Yes, C++ is context-sensitive, so this does happen. But the committee prefers to avoid it where possible.

So yes, it's grammar.

--------------------------------------------------
Jquery clone row and its all elements with different id
HTML Table whose 2nd row which I want to clone is&lt;br/&gt;

    &lt;table id=&quot;tblDoc&quot; class=&quot;doc-Table&quot;&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;label&gt;Document Description&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Custom&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;File Type&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Ref&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Document&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr id=&quot;uploadrow_0&quot;&gt;
        &lt;td&gt;
            &lt;asp:DropDownList ID=&quot;ddlDocumentDescription_0&quot; runat=&quot;server&quot;&gt;&lt;/asp:DropDownList&gt;
        &lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;txtCustomFileName_0&quot; type=&quot;text&quot; class=&quot;upload-TextBoxes&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;select id=&quot;ddlFileType_0&quot; class=&quot;upload-Dropdowns&quot;&gt;
                &lt;option value=&quot;0&quot;&gt;--Select--&lt;/option&gt;
                &lt;option value=&quot;1&quot;&gt;A&lt;/option&gt;
                &lt;option value=&quot;2&quot;&gt;B&lt;/option&gt;
            &lt;/select&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;txtReferenceNo_0&quot; type=&quot;text&quot; class=&quot;upload-TextBoxes&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;fileDocument_0&quot; class=&quot;file-upload&quot; type=&quot;file&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
    
&lt;div&gt;
    &lt;span id=&quot;addAnother&quot; class=&quot;add-another&quot;&gt;+ Add Another&lt;/span&gt;
&lt;/div&gt;


I want to make a copy of second row each time on add another button.So I  have used  &lt;br/&gt;

    $(document).ready(function () {
        $(&quot;#addAnother&quot;).click(function () {
            addAnotherRow();
        });
    });

    function addAnotherRow() {
        var row = $(&quot;#tblDoc tr:nth-child(2)&quot;).clone();
        $(&#39;#tblDoc&#39;).append(row);
    }

When I clone it give same id for second row.&lt;br/&gt;&lt;br/&gt;
I want second row with id:&lt;br/&gt;
1 - uploadrow_1&lt;br/&gt;
2 - ddlDocumentDescription_1 (Its a asp.net control so id will not look like this)&lt;br/&gt;
3 - txtCustomFileName_1&lt;br/&gt;
4 - ddlFileType_1&lt;br/&gt;
5 - txtReferenceNo_1&lt;br/&gt;
6 - fileDocument_1&lt;br/&gt;
and so on.&lt;br/&gt;&lt;br/&gt;
Thanks in advance for any help.


||||||||||||||http://jsfiddle.net/y7q6x4so/3/

Select the last row and add id incrementing by one all the time.    

        function addAnotherRow() {
            var row = $("#tblDoc tr").last().clone();
            var oldId = Number(row.attr('id').slice(-1));
            var id = 1 + oldId;
            
            
            row.attr('id', 'uploadrow_' + id );
            row.find('#txtCustomFileName_' + oldId).attr('id', 'txtCustomFileName_' + id);
            row.find('#ddlDocumentDescription_' + oldId).attr('id', 'ddlDocumentDescription_' + id);
            row.find('#ddlFileType_' + oldId).attr('id', 'ddlFileType_' + id);
            row.find('#txtReferenceNo_' + oldId).attr('id', 'txtReferenceNo_' + id);
            row.find('#fileDocument_' + oldId).attr('id', 'fileDocument_' + id);
            
            $('#tblDoc').append(row);
        }

![enter image description here][1]


  [1]: http://i.stack.imgur.com/zx5a2.png

--------------------------------------------------
Trying to create edge list (weighted) to create adjacency list
I am storing the open coords as two attributes in one list:
```
self.x, self.y = []
```
My attempt at an edge list (lifted from stack overflow lol):
```
edge = []
        for i in self.x, self.y:
            for j in i[1]:
                edge.append([i[0], j])
                for i in edge:
                    print(i)
```

Whenever I try this the error:
```
for j in i[1]:
TypeError: &#39;int&#39; object is not subscriptable
```
comes up.

I&#39;m guessing this is because it&#39;s a tuple? I am trying to create an adjacency list with weighted edges of the distance between the coords, but I haven&#39;t thought about adding the weights yet. 

The coords, when added to a big list, look like this:
```
[[[0, 0], [1, 0], [2, 0].....]]
```
But when I insert them into the class, I do it separately.

On another note, can I store the all coords into one attribute like this effectively? Or would the attribute overwrite each time and only do one coord??



I was expecting, or hoped, that it would create edges between nodes to then create an adjacency list (of which I have no clue how to code either). With the completed graph, I aim to create an a* algorithm..

Sorry, if this may be obvious but I haven&#39;t coded properly in a very long time. I am aware it is kinda messy.

Thank you.
||||||||||||||Something like this?

~~~python
import itertools
import math

class Graph:
    def __init__(self):
        self.coords = []  # ← array of coords tuples (x, y)
        self.adjacency_list = (
            {}
        )  # ^ dictionary (hash lookup structure of 'key': 'value' pairs):
        #      where each key is a tuple (x, y)
        #      and each value is a list of (neighbor, weight) tuples.

    def add_coord(self, x, y):
        self.coords.append((x, y))

    def calculate_distance(self, coord1, coord2):
        x1, y1 = coord1
        x2, y2 = coord2
        return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)

    def build_adjacency_list(self):
        for coord1 in self.coords:
            self.adjacency_list[coord1] = []
            for coord2 in self.coords:
                if coord1 != coord2:
                    distance = self.calculate_distance(coord1, coord2)
                    self.adjacency_list[coord1].append((coord2, distance))


# Demo adjacency list
g = Graph()
for (x, y) in list(itertools.product(range(3), range(3))):
    print(x, y)
    g.add_coord(x, y)
g.build_adjacency_list()

# Print out the adjacency list
for coord, neighbors in g.adjacency_list.items():
    print(f"\n{coord}: \n{neighbors}")
~~~

which prints:

~~~none
(0, 0): 
[((0, 1), 1.0), ((0, 2), 2.0), ((1, 0), 1.0), ((1, 1), 1.4142135623730951), ((1, 2), 2.23606797749979), ((2, 0), 2.0), ((2, 1), 2.23606797749979), ((2, 2), 2.8284271247461903)]

(0, 1): 
[((0, 0), 1.0), ((0, 2), 1.0), ((1, 0), 1.4142135623730951), ((1, 1), 1.0), ((1, 2), 1.4142135623730951), ((2, 0), 2.23606797749979), ((2, 1), 2.0), ((2, 2), 2.23606797749979)]

(0, 2): 
[((0, 0), 2.0), ((0, 1), 1.0), ((1, 0), 2.23606797749979), ((1, 1), 1.4142135623730951), ((1, 2), 1.0), ((2, 0), 2.8284271247461903), ((2, 1), 2.23606797749979), ((2, 2), 2.0)]

(1, 0): 
[((0, 0), 1.0), ((0, 1), 1.4142135623730951), ((0, 2), 2.23606797749979), ((1, 1), 1.0), ((1, 2), 2.0), ((2, 0), 1.0), ((2, 1), 1.4142135623730951), ((2, 2), 2.23606797749979)]

(1, 1): 
[((0, 0), 1.4142135623730951), ((0, 1), 1.0), ((0, 2), 1.4142135623730951), ((1, 0), 1.0), ((1, 2), 1.0), ((2, 0), 1.4142135623730951), ((2, 1), 1.0), ((2, 2), 1.4142135623730951)]

(1, 2): 
[((0, 0), 2.23606797749979), ((0, 1), 1.4142135623730951), ((0, 2), 1.0), ((1, 0), 2.0), ((1, 1), 1.0), ((2, 0), 2.23606797749979), ((2, 1), 1.4142135623730951), ((2, 2), 1.0)]

(2, 0): 
[((0, 0), 2.0), ((0, 1), 2.23606797749979), ((0, 2), 2.8284271247461903), ((1, 0), 1.0), ((1, 1), 1.4142135623730951), ((1, 2), 2.23606797749979), ((2, 1), 1.0), ((2, 2), 2.0)]

(2, 1): 
[((0, 0), 2.23606797749979), ((0, 1), 2.0), ((0, 2), 2.23606797749979), ((1, 0), 1.4142135623730951), ((1, 1), 1.0), ((1, 2), 1.4142135623730951), ((2, 0), 1.0), ((2, 2), 1.0)]

(2, 2): 
[((0, 0), 2.8284271247461903), ((0, 1), 2.23606797749979), ((0, 2), 2.0), ((1, 0), 2.23606797749979), ((1, 1), 1.4142135623730951), ((1, 2), 1.0), ((2, 0), 2.0), ((2, 1), 1.0)]
~~~

`itertools.product(range(3), range(3))` produces: 
~~~none
0 0
0 1
0 2
1 0
1 1
1 2
2 0
2 1
2 2
~~~

In calling the method to build the adjacency list, all coordinates-tuple combinatorial (cartesian product, specifically) pairwise (excluding self-pairs...) connections were added to the dictionary for each individual coordinates tuple. This is what is printed above as the output. Below this is visualized.

<hr>

## Visualize the adjacency list as a network graph

~~~python
import networkx as nx
from pyvis.network import Network

# Create a NetworkX graph
G_nx = nx.Graph()

# Add edges to the NetworkX graph
for coord, neighbors in g.adjacency_list.items():
    for neighbor, weight in neighbors:
        G_nx.add_edge(coord, neighbor, weight=weight)

# Create a Pyvis network
net = Network(
    notebook=True,
    cdn_resources="remote",
    width="100%",
    bgcolor="white",
    font_color="red",
)
net.repulsion()

# Add nodes to the Pyvis network
for coord in g.adjacency_list.keys():
    net.add_node(str(coord), size=5) # *

# Add edges to the Pyvis network
for coord, neighbors in g.adjacency_list.items():
    for neighbor, weight in neighbors:
        net.add_edge(str(coord), str(neighbor), weight=weight)


for edge in net.edges:
    source, target = edge["from"], edge["to"]
    weight = G_nx[eval(source)][eval(target)]["weight"]
    edge["label"] = str(round(weight, 2))


net.show("example.html")
~~~

><sup>* _**Note**_: The size of nodes is ok to explicitly set here because all nodes have the same number of edges anyways. By default, they come out in the visualized graph a bit too big IMO.</sup>

[![Networkx+pyvis graph of adjacency list][1]][1]

And also, visualizing a slightly different pyvis+networkx graph, where the calculated distances (used to represent 'weights' in the example here) have an influence (e.g., by instead setting `net.barnes_hut()`):

[![Same network graph different 'physics' setting][2]][2]


  [1]: https://i.stack.imgur.com/Bt9Sn.png
  [2]: https://i.stack.imgur.com/Fq17m.gif

--------------------------------------------------
ckeditor&#39;s popup input fields dont work when used with bootstrap 5 modal (ckeditor 4)
I have come across an error while using ckeditor in bootstrap 5 modal and it looks like it&#39;s a very known error and many have given solution for it for different bootstrap versions but i am not able to figure out one for bootstrap 5, please have a look.

Here is the problem with solution:- https://stackoverflow.com/a/31679096

Other similar problems:-

https://stackoverflow.com/questions/19570661/ckeditor-plugin-text-fields-not-editable

https://stackoverflow.com/questions/14420300/bootstrap-with-ckeditor-equals-problems/18554395#18554395

Mainly what would be the alternative of below line for bootstrap 5. $.fn.modal.Constructor.prototype.enforceFocus

If I search for it in bootstrap 4 js file I&#39;m able to find fn.modal.Constructor in there but not in bootstrap 5. Please if someone can recreate the verified solution in the above link according to bootstrap 5 it would be very appreciated. Thank you for your time.

[image describing problem][1]

Also few notes:- 
1. All the other input types like checkboxes and dropdown works but not just text field.

2. I have also tried removing tabindex=&quot;-1&quot; from bootstrap modal code but the problem remains.


  [1]: https://i.stack.imgur.com/X04EL.png


||||||||||||||Thanks for this. Saved me a lot of head scratching. As of Bootstrap 5.3, this requires a small tweak:

```
bootstrap.Modal.prototype._initializeFocusTrap = function () { return { activate: function () { }, deactivate: function () { } } };
```

--------------------------------------------------
Getting NameResolutionError: Failed to resolve &#39;oauth2.googleapis.com&#39; trying to upload file to Google Cloud Storage via app on local k8s cluster
I have Kubernetes deployment with the following config

```
resource &quot;kubernetes_deployment&quot; &quot;batch-producer&quot; {
  metadata {
    name = var.app-name
    namespace = var.k8s-namespace.metadata[0].name
    labels = {
      app = var.app-name
    }
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        app = var.app-name
      }
    }

    template {
      metadata {
        labels = {
          app = var.app-name
        }
      }

      spec {
        container {
          name  = var.app-name
          image = var.docker-image

          image_pull_policy = &quot;Never&quot;

          port { container_port = 80 }
          port { container_port = 443 }

          command = [
            &quot;sh&quot;,
            &quot;-exc&quot;,
            &lt;&lt;-EOT
            mkdir /secrets
            echo ${var.storage-sa-key} | base64 --decode &gt; ./secrets/gcp_creds.json
            python ./run.py
            
            EOT
            ,
            &quot;&quot;
          ]

          env_from {
            config_map_ref {
              name = &quot;batch-producer-config&quot;
            }
          }

          env {
            name = &quot;GOOGLE_APPLICATION_CREDENTIALS&quot;
            value = &quot;/secrets/gcp_creds.json&quot;
          }
        }
      }
    }
  }
}
```

And app should just write file to GCS
```
class GCSWriter(AbstractWriter):
    def __init__(self, properties: dict):
        self.storage_client = storage.Client() \
            .from_service_account_json(json_credentials_path=os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;])
        self.bucket_name = properties.get(&quot;bucket_name&quot;)
        self.logger = get_logger()

    def write(self, source_path):
        # time.sleep(100)
        bucket = self.storage_client.bucket(self.bucket_name)
        blob = bucket.blob(os.path.join(
            &quot;incomes_data_source&quot;,
            dt.today().strftime(&#39;%Y/%m/%d&#39;),
            os.path.split(source_path)[1]))
        blob.upload_from_filename(source_path)
        self.logger.info(&quot;File &#39;%s&#39; was uploaded to GCS successfully&quot;, source_path)
```
App deploys, but I&#39;m getting the following error after some time:
```
HTTPSConnectionPool(host=&#39;oauth2.googleapis.com&#39;, port=443): Max retries exceeded with url: /token (Caused by NameResolutionError(&quot;&lt;urllib3.connection.HTTPSConnection object at 0x7f993b606bd0&gt;: Failed to resolve &#39;oauth2.googleapis.com&#39; ([Errno -3] Temporary failure in name resolution)&quot;))
```


I tried to ping google.com or download random file via curl from pod - success.
I also tried to use existing access-key.json running same container just via docker - it also works and I can see uploaded file in GCS.
Looking for a clue how I can resolve this.
||||||||||||||Thanks to Vasilii Angapov comment I found this issue - https://github.com/docker/for-mac/issues/7110.

I didn't dive deep into root cause, I just followed recommendation for downgrading to CoreDNS-1.10.0.

```
kubectl edit deployment/coredns -n kube-system
```
Changed version from 1.11.1 to 1.10.0 and wait for deployment to restart.

Everything works after that in my setup (Docker Desktop v4.27.1, Kubernetes v1.29.1)

I also tried to launch same configuration via Rancher Desktop with Kubernetes v1.28.n, it also works well.

--------------------------------------------------
How to generate time based UUIDs?
I want to generate time-based universally unique identifier (UUID) in Java. 

The method [`java.util.UUID.randomUUID()`][1] generates a [UUID Version 4][2] where 122 of the 128 bits are from a [cryptographically-strong][3] random number generator. 

How to generate a [Version 1][4] (time based) UUID ? Is there a separate library for that or is it some how provided in the Java 7 API and I am missing it.


  [1]: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/UUID.html#randomUUID()
  [2]: https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_(random)
  [3]: https://en.wikipedia.org/wiki/Strong_cryptography#Cryptographically_strong_algorithms
  [4]: https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_1_(date-time_and_MAC_address)
||||||||||||||FasterXML Java Uuid Generator (JUG)

https://github.com/cowtowncoder/java-uuid-generator

    UUID uuid = Generators.timeBasedGenerator().generate();

--------------------------------------------------
Oracle with PHP on Docker
I&#39;m trying to install an Oracle database drive for my Laravel application. I&#39;m using Laravel Sail to provide Docker.

The problem is that the Oracle driver can&#39;t build. This message occurs:

```
/usr/bin/ld: skipping incompatible /opt/oracle/instantclient_19_14/libclntsh.so when searching for -lclntsh
/usr/bin/ld: cannot find -lclntsh
/usr/bin/ld: skipping incompatible /opt/oracle/instantclient_19_14/libclntsh.so when searching for -lclntsh
collect2: error: ld returned 1 exit status
make: *** [Makefile:227: oci8.la] Error 1
ERROR: `make&#39; failed
```

My dockerfile: https://pastebin.com/RTPWt1XK

I&#39;m using MacBook Pro (v. 12 with M1)
||||||||||||||Using the **Instant Client for Linux ARM** ([instantclient-basic-linux.arm64-19.10.0.0.0][1]) and **PHP 8.2**

This `dockerfile` works for me:

```
FROM ubuntu:22.04

LABEL maintainer="Taylor Otwell"

ARG WWWGROUP
ARG NODE_VERSION=18
ARG POSTGRES_VERSION=14

WORKDIR /var/www/html

ENV DEBIAN_FRONTEND noninteractive
ENV TZ=UTC

RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

RUN apt-get update \
    && apt-get install -y gnupg gosu curl wget ca-certificates zip unzip git supervisor sqlite3 libcap2-bin libpng-dev python2 dnsutils \
    && curl -sS 'https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x14aa40ec0831756756d7f66c4f4ea0aae5267a6c' | gpg --dearmor | tee /etc/apt/keyrings/ppa_ondrej_php.gpg > /dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/ppa_ondrej_php.gpg] https://ppa.launchpadcontent.net/ondrej/php/ubuntu jammy main" > /etc/apt/sources.list.d/ppa_ondrej_php.list \
    && apt-get update \
    && apt-get install -y php8.2-cli php8.2-dev \
       php8.2-pgsql php8.2-sqlite3 php8.2-gd \
       php8.2-curl \
       php8.2-imap php8.2-mysql php8.2-mbstring \
       php8.2-xml php8.2-zip php8.2-bcmath php8.2-soap \
       php8.2-intl php8.2-readline \
       php8.2-ldap \
       php8.2-msgpack php8.2-igbinary php8.2-redis php8.2-swoole \
       php8.2-memcached php8.2-pcov php8.2-xdebug \
    && php -r "readfile('https://getcomposer.org/installer');" | php -- --install-dir=/usr/bin/ --filename=composer \
    && curl -sLS https://deb.nodesource.com/setup_$NODE_VERSION.x | bash - \
    && apt-get install -y nodejs \
    && npm install -g npm \
    && curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | tee /etc/apt/keyrings/yarn.gpg >/dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/yarn.gpg] https://dl.yarnpkg.com/debian/ stable main" > /etc/apt/sources.list.d/yarn.list \
    && curl -sS https://www.postgresql.org/media/keys/ACCC4CF8.asc | gpg --dearmor | tee /etc/apt/keyrings/pgdg.gpg >/dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/pgdg.gpg] http://apt.postgresql.org/pub/repos/apt jammy-pgdg main" > /etc/apt/sources.list.d/pgdg.list \
    && apt-get update \
    && apt-get install -y yarn \
    && apt-get install -y mysql-client \
    && apt-get install -y postgresql-client-$POSTGRES_VERSION \
    && apt-get -y autoremove \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

ENV LD_LIBRARY_PATH="/opt/oracle/instantclient_19_10/"
ENV ORACLE_HOME="/opt/oracle/instantclient_19_10/"
ENV OCI_HOME="/opt/oracle/instantclient_19_10/"
ENV OCI_LIB_DIR="/opt/oracle/instantclient_19_10/"
ENV OCI_INCLUDE_DIR="/opt/oracle/instantclient_19_10/sdk/include"
ENV OCI_VERSION=19

# Download Oracle
RUN mkdir /opt/oracle \
    && cd /opt/oracle \
    && wget https://download.oracle.com/otn_software/linux/instantclient/191000/instantclient-basic-linux.arm64-19.10.0.0.0dbru.zip \
    && wget https://download.oracle.com/otn_software/linux/instantclient/191000/instantclient-sdk-linux.arm64-19.10.0.0.0dbru.zip \
    && unzip /opt/oracle/instantclient-basic-linux.arm64-19.10.0.0.0dbru.zip -d /opt/oracle \
    && unzip /opt/oracle/instantclient-sdk-linux.arm64-19.10.0.0.0dbru.zip -d /opt/oracle \
    && rm -rf /opt/oracle/*.zip \
    && echo /opt/oracle/instantclient_19_10 > /etc/ld.so.conf.d/oracle-instantclient.conf \
    && ldconfig

# Configure Oracle
RUN apt-get update \
    && apt-get install -y \
      php-dev \
      php-pear \
      build-essential \
      libaio1 \
      libaio-dev \
      freetds-dev
RUN pecl channel-update pecl.php.net \
    && echo 'instantclient,/opt/oracle/instantclient_19_10' | pecl install oci8 \
    && echo extension=oci8.so >> /etc/php/8.2/cli/php.ini \
    && echo "extension=oci8.so" >> /etc/php/8.2/mods-available/oci8.ini

RUN setcap "cap_net_bind_service=+ep" /usr/bin/php8.2

RUN groupadd --force -g $WWWGROUP sail
RUN useradd -ms /bin/bash --no-user-group -g $WWWGROUP -u 1337 sail

COPY start-container /usr/local/bin/start-container
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf
COPY php.ini /etc/php/8.2/cli/conf.d/99-sail.ini
RUN chmod +x /usr/local/bin/start-container

EXPOSE 8000

ENTRYPOINT ["start-container"]

```


  [1]: https://www.oracle.com/database/technologies/instant-client/linux-arm-aarch64-downloads.html

--------------------------------------------------
Combining filters between multiple columns
How to sum filter using conditions in multiple columns

        A      B          C        D
      Ben      1         Tom       1
      Joe      3         Ben       4
      Tom      2         Ben       1

I want to get the sum of B,D where A,C does not equal Joe...basically get everyone&#39;s hours except Joes.

update: f you on the negative score...the system does not let me preview the question before posting...so forced to edit on the fly.



This seems so simple but I have been racking my brain trying to get it to work...maybe an array?


UPDATE:  it was as simple as a SUMIF! I&#39;ve used sumif many times lol.  Thanks to googlesheetsguy
||||||||||||||Here's a possbile solution

```cpp
=INDEX(QUERY(WRAPROWS(TOCOL(A2:D4),2),"select sum(Col2) where Col1 <> 'Joe'"),2)
```

[![enter image description here][1]][1]

If you only have two ranges, you can also use:

```cpp
=SUM(FILTER({B2:B4;D2:D4},"Joe"<>{A2:A4;C2:C4}))
```

[![enter image description here][2]][2]

But if you have a lot it becomes unpractical.

  [1]: https://i.stack.imgur.com/iy9Ex.png
  [2]: https://i.stack.imgur.com/U7BRf.png

--------------------------------------------------
Laravel and ngrok: url domain is not correct for routes and assets
My setup:

- Homestead on Mac OSX with multiple sites configured
- I have one site setup using domfit.test as the local domain (auto mapped using hostsupdater)

My problem:

If I `vagrant ssh`, and then `share domfit.test` I get a random generated ngrok url as you&#39;d expect (http://whatever.ngrok.io), however when I access this URL all my resources / routes are being prefixed with `http://domfit.test/` (http://domfit.test/login for instance)

I&#39;ve tried the following:

- Setting APP_URL as the ngrok URL
- `php artisan config:clear`
- `php artisan cache:clear`
- `{{ url(&#39;login&#39;) }}`
- `{{ route(&#39;login&#39;) }}`

My understanding is that `url()` should return the actual URL that the browser requested (rather than using `APP_URL`) but it always returns `domfit.test`.

If I rename my site in `Homestead.yaml` (for example to `newdomfit.test`) and re-provision then this is the domain that `url()` and `route()` uses, regardless of my `APP_URL`. So the `Homestead.yaml` seems to be forcing that domain. Which begs the question - how are you meant to actually use the share functionality?

I&#39;m new to Laravel so I am not sure if all of this is expected behavior and I am misunderstanding something? 

I just want my links and resources in templates to work for local (`domfit.test`), shared (`ngrok`) and eventually production with the same piece of code. My worry is I will have to change all of my `route()` or `url()` references when I attempt to put this website live.

**EDIT BELOW**

OK I&#39;ve just tried again. Changed `APP_URL` for `ngrok`:

Searched my entire codebase for `domfit.test`, and only some random session files seem to have references:

code/domfit/storage/framework/sessions/

    APP_NAME=DomFit
    APP_VERSION=0.01
    APP_ENV=local
    APP_KEY=XXXX
    APP_DEBUG=true
    APP_URL=http://04b7beec.ngrok.io

Then in my Controller I have it doing this for some simple debugging:

    echo(url(&#39;/login&#39;));
    echo(route(&#39;login&#39;));
    echo($_SERVER[&#39;HTTP_HOST&#39;]);
    echo($_SERVER[&#39;HTTP_X_ORIGINAL_HOST&#39;]);

If I use the `ngrok` URL the output I get is:

    http://domfit.test/login
    http://domfit.test/login
    domfit.test
    04b7beec.ngrok.io

I don&#39;t understand how `$_SERVER[&#39;HTTP_HOST&#39;]` is returning the wrong url?

It looks like it could be related to this: https://github.com/laravel/valet/issues/342

**ANOTHER EDIT**

It looks like it has to do with Homestead&#39;s `share` command:

    function share() {
    if [[ &quot;$1&quot; ]]
    then
        ngrok http ${@:2} -host-header=&quot;$1&quot; 80
    else
        echo &quot;Error: missing required parameters.&quot;
        echo &quot;Usage: &quot;
        echo &quot;  share domain&quot;
        echo &quot;Invocation with extra params passed directly to ngrok&quot;
        echo &quot;  share domain -region=eu -subdomain=test1234&quot;
    fi
}

Which passes the option `-host-header` to `ngrok` which according to their documentation:

&gt; Some application servers like WAMP, MAMP and pow use the Host header for determining which development site to display. For this reason, ngrok can rewrite your requests with a modified Host header. Use the -host-header switch to rewrite incoming HTTP requests.

If I use `ngrok` without it, then the website that gets displayed is a different one (because I have multiple sites configured in Homestead) - so I&#39;m still not sure how to get around this. For the time being I could disable the other sites as I&#39;m not actively developing those.
||||||||||||||### Update for ngrok 3.0+

ngrok 3.0 stopped using the `X-Original-Host` header and started using the `X-Forwarded-Host` header.

Therefore, if using ngrok 3.0+ with TrustedProxies set to trust all proxies (`protected $proxies = '*';`), then there should be nothing else that needs to change.

However, if not using TrustedProxies, all the below information is still relevant, just replace any references of `HTTP_X_ORIGINAL_HOST` with `HTTP_X_FORWARDED_HOST`.

### For ngrok < 3.0

Even though you're going to the ngrok url, the host header in the request is still set as the name of your site. Laravel uses the host header to build the absolute url for links, assets, etc. ngrok includes the ngrok url in the `X-Original-Host` header, but Laravel doesn't know anything about that.

There are two basic solutions to the issue:

1. update the request with the proper server and header values, or
2. use the `forceRootUrl()` method to ignore the server and header values.

---

**TrustedProxies and Forwarded Host**

If you're using TrustedProxies (default in Laravel >= 5.5), and you have it configured to trust all proxies (`protected $proxies = '*';`), you can set the `X-Forwarded-Host` header to the `X-Original-Host` header. Laravel will then use the value in the `X-Forwarded-Host` header to build all absolute urls.

You can do this at the web server level. For example, if you're using apache, you can add this to your `public/.htaccess` file:

    # Handle ngrok X-Original-Host Header
    RewriteCond %{HTTP:X-Original-Host} \.ngrok\.io$ [NC]
    RewriteRule .* - [E=HTTP_X_FORWARDED_HOST:%{HTTP:X-Original-Host}]

If you prefer to handle this in your application instead of the web server, you will need to update the Laravel request. There are plenty of places you could choose to do this, but one example would be in your `AppServiceProvider::boot()` method:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $request->server->set('HTTP_X_FORWARDED_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
            $request->headers->set('X_FORWARDED_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

---

**Not Using TrustedProxies**

If you're not using TrustedProxies, you can't use the `.htaccess` method. However, you can still update the server and headers values in your application. In this case, you'd need to overwrite the Host header:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $request->server->set('HTTP_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
            $request->headers->set('HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

---

**Using `forceRootUrl()`**

If you don't want to modify any headers or the Laravel request, you can simply tell the URL generator what root url to use. The URL generator has a `forceRootUrl()` method that you can use to tell it to use a specific value instead of looking at the request. Again, in your `AppServiceProvider::boot()` method:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $this->app['url']->forceRootUrl($request->server->get('HTTP_X_FORWARDED_PROTO').'://'.$request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

--------------------------------------------------
Switching between GCC and Clang/LLVM using CMake
I have a number of projects built using CMake and I&#39;d like to be able to easily switch between using GCC or Clang/LLVM to compile them. I believe (please correct me if I&#39;m mistaken!) that to use Clang I need to set the following:

        SET (CMAKE_C_COMPILER             &quot;/usr/bin/clang&quot;)
        SET (CMAKE_C_FLAGS                &quot;-Wall -std=c99&quot;)
        SET (CMAKE_C_FLAGS_DEBUG          &quot;-g&quot;)
        SET (CMAKE_C_FLAGS_MINSIZEREL     &quot;-Os -DNDEBUG&quot;)
        SET (CMAKE_C_FLAGS_RELEASE        &quot;-O4 -DNDEBUG&quot;)
        SET (CMAKE_C_FLAGS_RELWITHDEBINFO &quot;-O2 -g&quot;)
        
        SET (CMAKE_CXX_COMPILER             &quot;/usr/bin/clang++&quot;)
        SET (CMAKE_CXX_FLAGS                &quot;-Wall&quot;)
        SET (CMAKE_CXX_FLAGS_DEBUG          &quot;-g&quot;)
        SET (CMAKE_CXX_FLAGS_MINSIZEREL     &quot;-Os -DNDEBUG&quot;)
        SET (CMAKE_CXX_FLAGS_RELEASE        &quot;-O4 -DNDEBUG&quot;)
        SET (CMAKE_CXX_FLAGS_RELWITHDEBINFO &quot;-O2 -g&quot;)
        
        SET (CMAKE_AR      &quot;/usr/bin/llvm-ar&quot;)
        SET (CMAKE_LINKER  &quot;/usr/bin/llvm-ld&quot;)
        SET (CMAKE_NM      &quot;/usr/bin/llvm-nm&quot;)
        SET (CMAKE_OBJDUMP &quot;/usr/bin/llvm-objdump&quot;)
        SET (CMAKE_RANLIB  &quot;/usr/bin/llvm-ranlib&quot;)

Is there an easy way of switching between these and the default GCC variables, preferably as a system-wide change rather than project specific (i.e. not just adding them into a project&#39;s CMakeLists.txt)?

Also, is it necessary to use the `llvm-*` programs rather than the system defaults when compiling using clang instead of gcc? What&#39;s the difference?
||||||||||||||CMake honors the environment variables `CC` and `CXX` upon detecting the C and C++ compiler to use:

    $ export CC=/usr/bin/clang
    $ export CXX=/usr/bin/clang++
    $ cmake ..
    -- The C compiler identification is Clang
    -- The CXX compiler identification is Clang

The compiler specific flags can be overridden by putting them into a make override file and pointing the [`CMAKE_USER_MAKE_RULES_OVERRIDE`][1] variable to it. Create a file `~/ClangOverrides.txt` with the following contents:

    SET (CMAKE_C_FLAGS_INIT                "-Wall -std=c11")
    SET (CMAKE_C_FLAGS_DEBUG_INIT          "-g")
    SET (CMAKE_C_FLAGS_MINSIZEREL_INIT     "-Os -DNDEBUG")
    SET (CMAKE_C_FLAGS_RELEASE_INIT        "-O3 -DNDEBUG")
    SET (CMAKE_C_FLAGS_RELWITHDEBINFO_INIT "-O2 -g")
    
    SET (CMAKE_CXX_FLAGS_INIT                "-Wall -std=c++17")
    SET (CMAKE_CXX_FLAGS_DEBUG_INIT          "-g")
    SET (CMAKE_CXX_FLAGS_MINSIZEREL_INIT     "-Os -DNDEBUG")
    SET (CMAKE_CXX_FLAGS_RELEASE_INIT        "-O3 -DNDEBUG")
    SET (CMAKE_CXX_FLAGS_RELWITHDEBINFO_INIT "-O2 -g")

The suffix `_INIT` will make CMake initialize the corresponding `*_FLAGS` variable with the given value. Then invoke `cmake` in the following way:

    $ cmake -DCMAKE_USER_MAKE_RULES_OVERRIDE=~/ClangOverrides.txt ..

Finally to force the use of the LLVM binutils, set the internal variable `_CMAKE_TOOLCHAIN_PREFIX`. This variable is honored by the `CMakeFindBinUtils` module:

    $ cmake -D_CMAKE_TOOLCHAIN_PREFIX=llvm- ..

Setting `_CMAKE_TOOLCHAIN_LOCATION` is no longer necessary for CMake version 3.9 or newer.

Putting this all together you can write a shell wrapper which sets up the environment variables `CC` and `CXX` and then invokes `cmake` with the mentioned variable overrides. 

Also see this [CMake FAQ][2] on make override files.


  [1]: https://cmake.org/cmake/help/latest/variable/CMAKE_USER_MAKE_RULES_OVERRIDE.html
  [2]: https://gitlab.kitware.com/cmake/community/-/wikis/FAQ#make-override-files

--------------------------------------------------
How to put each element of array into another array in same order
I have first array:

    Array
    (
        [0] =&gt; generala value 1
        [1] =&gt; specificatii value 1
    )

and second array is:

 

       Array
    (
        [0] =&gt; Array
            (
                [title] =&gt; generala title 1
                [atribute_cat_id] =&gt; 1
                [product_id] =&gt; 98
            )
    
        [1] =&gt; Array
            (
                [title] =&gt; specificatii title 1
                [atribute_cat_id] =&gt; 2
                [product_id] =&gt; 98
            )    
    )

I want to get this array form:

    Array
    (
        [0] =&gt; Array
            (
                [title] =&gt; generala title 1
                [atribute_cat_id] =&gt; 1
                [product_id] =&gt; 98
                [value] =&gt; generala value 1
            )
    
        [1] =&gt; Array
            (
                [title] =&gt; specificatii title 1
                [atribute_cat_id] =&gt; 2
                [product_id] =&gt; 98
                [value] =&gt; specificatii value 1
            )
    
    )

I tried with array_merge but this method put all elements from first array to each element from second array! 
Every time both arrays have same number of elements! 
Any ideea?
Thank you!
||||||||||||||Loop over one of the arrays, using the indexes to access the corresponding element in the other array.

```
foreach ($first_array AS $i => $value) {
    $second_array[$i]['value'] = $value;
}
```


--------------------------------------------------
A* algorithm only exploring a few nodes before stopping - without reaching goal node
I am attempting to implement the A* algorithm but it only goes to three nodes before just stopping completely.

This is the algorithm code:
```
def AStar(start_node, end_node):
    openSet = PriorityQueue()
    openSet.enequeue(0, start_node)

    infinity = float(&quot;inf&quot;)

    gCost = {}
    fCost = {}
    cameFrom = {}

    for node in graph:
        gCost[node] = infinity
        fCost[node] = infinity
    gCost[start_node] = 0
    fCost[start_node] = heuristic(start_node, end_node)

    while not openSet.isEmpty():
        current = openSet.dequeue()  # Doesn&#39;t work yet

        if current == end_node:
            RetracePath(cameFrom, end_node)

        for neighbour in find_neighbors(start_node, graph):
            tempGCost = gCost[current] + 1

            if tempGCost &lt; gCost[neighbour]:
                cameFrom[neighbour] = current
                gCost[neighbour] = tempGCost
                fCost[neighbour] = tempGCost + heuristic(neighbour, end_node)

                if not openSet.contains(neighbour):
                    openSet.enequeue(fCost[neighbour], neighbour)

        print(f&quot;Came from: {cameFrom}\nCurrent: {current}&quot;)
    return False
```

And this is the code that finds the adjacent nodes:
```

def find_neighbors(node, graph):
    x, y = node
    neighbors = []

    right_neighbor = (x + 1, y)
    left_neighbor = (x - 1, y)
    lower_neighbor = (x, y + 1)
    upper_neighbor = (x, y - 1)

    if right_neighbor in graph:
        neighbors.append(right_neighbor)
    if left_neighbor in graph:
        neighbors.append(left_neighbor)
    if lower_neighbor in graph:
        neighbors.append(lower_neighbor)
    if upper_neighbor in graph:
        neighbors.append(upper_neighbor)
```

And this is an example of what is being outputted:
```
Enemy coords: (6, 2)
Player coords: 10, 2
Enemy neighbours: [(7, 2), (6, 3)]
Priority Queue: [[0, (6, 2)]]
Priority Queue: [[4, (7, 2)]]
Priority Queue: [[4, (7, 2)], [6, (6, 3)]]
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (6, 2)
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (7, 2)
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (6, 3)
```
Before it just stops the code without reaching the goal coordinates (player coords)

lmk if you have any questions about the code,
thank you.


||||||||||||||Your code check only the neighbors of the start node, when you call `find_neighbors` you always use `start_node` you should use `current` instead : 

```python
for neighbour in find_neighbors(current, graph):
    # your code
```

--------------------------------------------------
Why isn&#39;t my script printing all my results to the file?
I have the simple code below that loops through a dataframe and prints the results to the screen and also to a file.

My nag issue is however, it prints all the data to the screen just perfectly, but the file is only getting the last end of the data.

Here is my code:

    for star in Constellation_data(starDf.values.tolist()):
        print(star)
        sourceFile = open(&#39;stars.txt&#39;, &#39;w&#39;)
        print(star, file = sourceFile)
        sourceFile.close()

I open the file, then print to it, then close.  So I not sure why it doesn&#39;t contain all the data like the screen has.

Thanks!
||||||||||||||"w" deletes the existing file so for each iteration of the loop, you delete any previous content written. The normal way to handle this issue is to open the file once before the loop

    with open('stars.txt', 'w') as sourceFile:
        for star in Constellation_data(starDf.values.tolist()):
            print(star)
            print(star, file = sourceFile)

Note the `with` clause - it will automatically close the file when done.

If there is a reason why you want to close the file on each write (perhaps another file is reading it or you want to save state more often), then you can use append mode. I've added code to delete the old file and then append on each loop. The first append will create the file.

    if os.path.exists('stars.txt'):
        os.remove('stars.txt')
    for star in Constellation_data(starDf.values.tolist()):
        with open('stars.txt', 'a') as sourceFile:
            print(star)
            print(star, file = sourceFile)


--------------------------------------------------
DI resolves service by Interface of object and not by actual type
Let assume that we have following code

We have commands that all implement same interface
``` 
public interface ICommand {}
```
and that we have Command handlers that implement following interface 

``` 
public interface ICommandHandler&lt;T&gt; where T: ICommand {}
```

Actual implementations of command handlers are registered to DI. 

Now, if we have method that builds Commands based on some condition

```
public ICommand BuildCommand()
{
   if(someCondition) return new CommandA();
   else return new CommandB();
}
```

and we use it in code like this 
``` c#
public class SomeClass
{
  IServiceProvider _serviceProvder; 
  public void method_1()
  {
    ICommand command = BuildCommand();
    HandleCommand(command);
  }

  public void HandleCommand&lt;T&gt;(T command) 
  {
    var handler = _serviceProvider.GetRequiredService&lt;ICommandHandler&lt;T&gt;&gt;();
    handler.Handle();
  }
```
if will throw an error stating that it cannot resolve service 
``` ICommandHandler&lt;ICommand&gt; ```

I would expect DI to resolve by actual type (either CommandA or CommandB). How to get past this? 
||||||||||||||> I would expect DI to resolve by actual type (either CommandA or CommandB). How to get past this?

It can't resolve this, because of the following reasons:

* At compile time, you are supplying `ICommandHandler<ICommand>` to the `GetRequiredService<T>` method; it is given no runtime information that would allow it to spot anything different.
* When asked to resolve `ICommandHandler<ICommand>`, the container can't return anything else, because `ICommandHandler<ICommand>` is a different type to `ICommandHandler<CommandA>`. And even if it could, the request would even be ambiguous, because it could result in either an `ICommandHandler<CommandA>` *or* an `ICommandHandler<CommandB>`. Which one should it return?
* It's impossible to cast an `ICommandHandler<ICommand>` to `ICommandHandler<CommandA>` or vise versa in .NET, unless you make `ICommandHandler<T>` [variant][1] (i.e. you need to mark `T` with either `in` or `out`).

The solution here is to resort to using Reflection. For instance:

``` c#
public void HandleCommand(ICommand command) 
{
    Type handlerType = typeof(ICommandHandler<>).MakeGenericType(command.GetType());

    dynamic handler = _serviceProvider.GetRequiredService(handlerType);

    handler.Handle((dynamic)command);
}
```

In this example I'm using the `dynamic` keyword for simplicity. There are pros and cons to using this keyword. Obvious downside is of course loss of compile-time support. This is, IMO, not a big issue, because you would typically only have a single place in the application that calls command handlers using reflection, and that code can easily be tested. A more important downside, however, is that it will fail if a resolved handler implementation is internal - even when the `ICommandHandler<T>` is defined as `public`.

So instead of using dynamic, you can also invoke the method using the Reflection API:

```
public void HandleCommand(ICommand command) 
{
    Type handlerType = typeof(ICommandHandler<>).MakeGenericType(command.GetType());

    object handler = _serviceProvider.GetRequiredService(handlerType);

    MethodInfo method = handlerType.GetMethod("Handle");

    try
    {
        method.Invoke(handler, new object[] { command });
    }
    catch (TargetInvocationException ex)
    {
        ExceptionDispatchInfo.Capture(ex.InnerException).Throw();
    }
}
```


  [1]: https://learn.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/covariance-contravariance/variance-in-generic-interfaces

--------------------------------------------------
Generate P random N-dimensional points from list of ALL possible pairwise distances
I would like to generate random N-dimensional points with the constraint of having precise Euclidean distances (known) between each other.

Number of points `P = 100`
Number of dimensions of the hyperspace `N = 512`

Consequently, the possible number of pairwise distances is given by the formula `L = P*(P-1)/2`.
If `P = 100`, then `L = 4950`.

Let&#39;s say I have a list of 4950 distance values, where each value refers to a precise point-point combination.

Is it possible to implement this using numpy?

It is trivial to do it when considering pairs of points (`P = 2`) as `L = 1`, but I&#39;m trying to figure out if it can it be generalized to higher values of `P`?

This is my implementation for `P = 2`, considering `set_dist` as the desired distance value.

```
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances

N = 512

set_dist = 5.

point_0 = np.random.rand(N).reshape(1, -1)
point_1 = np.random.rand(N).reshape(1, -1)
rand_dist = euclidean_distances(point_0, point_1)
point_0 = point_0 * set_dist / rand_dist
point_1 = point_1 * set_dist / rand_dist
```
||||||||||||||With more spatial dimensions than points (i.e. *N* > *P*) this should be possible, if the distances are valid, which in particular means they have to satisfy the triangle inequality.

Let's take *N* = 3 for intuition. The first point you can pick anywhere. The distance between first and second defined a sphere around the first. The second has to lie somewhere on that sphere. The third has two distances to points you already placed. It has to lie on the intersection of the two corresponding spheres, which is a circle. A potential fourth point would lie on one of two points where three spheres intersect. For a fifth point you'd run out of dimensions, and rounding errors might make it difficult to satisfy all requirements simultaneously even if the distances originate from a real 3d configuration. That's why *N* > *P* is useful as you likely avoid this headache.

In terms of implementation, the above suggests that you would need to uniformly sample from hyperspheres of decreasing dimensions. You'd also have to inspect hyperspheres, and translate from the sample space to the actual positions. I don't know what rolls numpy has to offer to help with any of this.

Personally I'd also explore a different approach: generate the whole configuration in a simple well-defined position and orientation, then apply a random isometry (rotation, translation, perhaps reflection) to it. You would place the first point in the origin. The second point goes on the positive <i>x</i><sub>1</sub>-axis. The third on the <i>x</i><sub>1</sub>-<i>x</i><sub>2</sub>-plane with positive <i>x</i><sub>2</sub>-coordinate, and so on. So the number of zeros in the coordinate vector decreases by one for each point, and the newest coordinate is always positive. This should in general give you uniquely determined coordinates, shifting the whole randomisation to an operation on the complete configuration.

I haven't yet read the literature but I guess randomised sampling of isometries should have been discussed somewhere. But perhaps just applying a sequence of random operations, like some rotations around specific axes, will already make the result random enough? Depends on your requirements.

--------------------------------------------------
When I append items to a 2d list, it doesn&#39;t add to the same list
```
class PriorityQueue:
    def __init__(self):
        self.q = []

    def enqueue(self, priority, item):
        self.q.append([priority, item])
        self.q = sorted(self.q)
        return self.q


x = PriorityQueue()
print(x.enqueue(3, &quot;Potato&quot;))

y = PriorityQueue()
print(y.enqueue(1, &quot;Egg&quot;))
```

I&#39;m trying to do a priority list but it won&#39;t sort.

output;
```
[[3, &#39;Potato&#39;]]
[[1, &#39;Egg&#39;]]
```
How do I fix this?
||||||||||||||You are always creating a new `PriorityQueue`, I guess you want to have one queue:

    class PriorityQueue:
    
        def __init__(self):
            self.q = []
    
        def enqueue(self, priority, item):
            self.q.append([priority, item])
            self.q = sorted(self.q)
            return self.q
    
    
    y = PriorityQueue()
    print(y.enqueue(3, "Egg"))
    print(y.enqueue(4, "Potato"))
    print(y.enqueue(2, "Chesse"))
    print(y.enqueue(1, "Cake"))

Out:

    [[3, 'Egg']]
    [[3, 'Egg'], [4, 'Potato']]
    [[2, 'Cheese'], [3, 'Egg'], [4, 'Potato']]
    [[1, 'Cake'], [2, 'Cheese'], [3, 'Egg'], [4, 'Potato']]

--------------------------------------------------
Github workflow does not read variables from environments
Following is my simple github workflow. It is intended to print an environment variable. 
```
name: verify

on:
  workflow_dispatch:

jobs:
  read_env_variables:
    environment: build 
    runs-on: [ self-hosted, onprem_dae, docker ]
    steps:
      - name: cat on branch file
        run: |
          echo ${{ env.SOME_VARIABLE }}
```

I have created an environment named &quot;build&quot;. In this environment, I have an environment variable named `SOME_VARIABLE` set to *xyz*. 

When the workflow is triggered, I expected to echo value *xyz* but actual value is &quot;&quot;. Is there something missing? 

||||||||||||||Your issue here is related to the syntax.

To use the `${{ env.SOME_VARIABLE }}` syntax, you need to set an env variable at the workflow, job or step level.

**Here is an example:**

```yaml
name: Environment Workflow

on:
  workflow_dispatch:

env:
  WORKFLOW_VARIABLE: WORKFLOW

jobs:

  job1:
    runs-on: ubuntu-latest
    env:
      JOB_VARIABLE: JOB
    steps:
      - name: Run Commands with various variables
        if: ${{ env.WORKFLOW_VARIABLE == 'WORKFLOW' }}
        env:
          STEP_VARIABLE: STEP
        run: |
          echo "Hello World"
          echo "This is the $WORKFLOW_VARIABLE environment variable"
          echo "This is the $JOB_VARIABLE environment variable"
          echo "This is the $STEP_VARIABLE environment variable"
```

* * *

Now, if you want to use the **environment secrets for deployment**, [as explained here on the Github Documentation][1], the syntax would be different using the `job_id.environment` [as you are already using following this doc][2].

Here is an example:

```yaml
  job4:
    runs-on: ubuntu-latest
    environment: build
    steps:
      - name: Show repo env secret
        run: |
          echo ${{ secrets.REPO_ENV_SECRET }}
```

Note that **this variable is a secret**, therefore you won't be able to see it through an echo command on the step (it will show `***`)

* * *

Here is the workflow I used to validate all this implementation if you want to take a look:
- [workflow yaml file][3]
- [workflow run][4]


  [1]: https://docs.github.com/en/actions/deployment/targeting-different-environments/using-environments-for-deployment
  [2]: https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions#jobsjob_idenvironment
  [3]: https://github.com/GuillaumeFalourd/poc-github-actions/blob/main/.github/workflows/10-environment-workflow.yml
  [4]: https://github.com/GuillaumeFalourd/poc-github-actions/actions/runs/7297762218

--------------------------------------------------
How do I check the type of widget in GTK+3.0?
I saw [this][1] post but it was for Python so that doesn&#39;t help me too much. I&#39;m programming in C++, working on a code-base that I didn&#39;t write. I see some checks like `GTK_IS_ENTRY` and `GTK_IS_COMBO_BOX`, but I&#39;m not sure where this person found these or what other `GTK_IS_...` there are. Is there a reference to these somewhere? I searched online and also on the Gtk/GLib websites but I couldn&#39;t find anything. Thanks!


  [1]: https://stackoverflow.com/questions/60112777/find-type-of-gtk-widgets
||||||||||||||The type checks macros are typically part of the API contract for a GObject, and they are [conventionally provided by the library][1], so they don't end up in the documentation. All they do is call [`G_TYPE_CHECK_INSTANCE_TYPE`][2] with the given GType macro, like `GTK_TYPE_ENTRY` or `GTK_TYPE_COMBO_BOX`.


[1]: https://developer-old.gnome.org/gobject/stable/gtype-conventions.html
[2]: https://developer-old.gnome.org/gobject/stable/gobject-Type-Information.html#G-TYPE-CHECK-INSTANCE-TYPE:CAPS

--------------------------------------------------
Match all characters between two commas or between ,&quot; and &quot;, with regex powershell
Using powershell regex I would like it to find the first match between two commas or between ,&quot; and &quot;,

Example:

```
&quot;0x00000000&quot;,&quot;What do you want to eat? fish, meat or\n eggs?&quot;,&quot;&quot;
&quot;0x00030002&quot;,&quot;What do you want to eat?&quot;,&quot;&quot;
0x00030002,What do you want to eat?,
```

I want it to become:

```
What do you want to eat? fish, meat or eggs?
What do you want to eat?
What do you want to eat?
```




I tried this code but it doesn&#39;t behave correctly:

`(?&lt;=,&quot;|\?&lt;=,).*(?=&quot;,.*?|\?=,.*?)`

||||||||||||||Rather than using a regular expression for this (which has some pitfalls), I would use the native [`ConvertFrom-Csv` cmdlet](https://learn.microsoft.com/powershell/module/microsoft.powershell.utility/convertfrom-csv) for this:

    $List = @'
    "0x00000000","What do you want to eat? fish, meat or eggs?",""
    '"0x00030002","What do you want to eat?",""
    0x00000000,What do you want to eat? fish, meat or eggs?,
    0x00030002,What do you want to eat?,
    "0x00000000","What do you want to eat? ""fish"", ""meat"" or ""eggs?"""
    '@ -Split '\r?\n'

    $List | ConvertFrom-Csv -Header Hex, String, Rest | Select-Object -Expand String

    What do you want to eat? fish, meat or eggs?
    What do you want to eat?
    What do you want to eat? fish
    What do you want to eat?
    What do you want to eat? "fish", "meat" or "eggs?"

--------------------------------------------------
Why is Rails validator not using normalized value?
My model has a decimal amount attribute.

```
create_table :foos do |t|
  t.decimal :amount
end

class Foo &lt; ApplicationRecord
end
```

I always want the amount to be negative, so I add a normalisation:

```
class Foo &lt; ApplicationRecord
  normalizes :amount, with: -&gt; amount { - amount.abs }
end
```

This seems to work perfectly.

Now, to be safe, I add a validation:

```
class Foo &lt; ApplicationRecord
  normalizes :amount, with: -&gt; amount { - amount.abs }
  validates :amount, numericality: {less_than: 0}
end
```

Now when I set the amount to a positive value, although the normalisation converts it to a negative value, the validator seems to think the value is still positive and adds a validation error.

```
foo = Foo.new amount: 4
foo.amount  # =&gt; -4
foo.valid?  # =&gt; false
foo.errors  # =&gt; #&lt;ActiveModel::Error attribute=amount, type=less_than, options={:value=&gt;4, :count=&gt;0}&gt;
```

According to the tests for `normalizes`, [normalisation happens before validation](https://github.com/rails/rails/blob/0add5dba834f2f1b84fcf1bd1b758545b325fb73/activerecord/test/cases/normalized_attribute_test.rb#L35).

How can I get this to work?
||||||||||||||Numericality validator seems to be specifically using raw value for validation:  
*https://github.com/rails/rails/blob/v7.1.3/activemodel/lib/active_model/validations/numericality.rb#L129*

```rb
if record.respond_to?(came_from_user)
  if record.public_send(came_from_user)
    raw_value = record.public_send(:"#{attr_name}_before_type_cast")
```

Don't know if that is another bug or intentional. You could write your own validation to bypass this problem:

```rb
validate do
  errors.add(:amount, :less_than, value: amount, count: 0) unless amount.negative?
end
```

--------------------------------------------------
add recyclerview and cardview dependencies to gradle module
i want to add recyclerview and cardview dependencies to gradle module but it keeps giving error : the library should not use different version(25) then compile sdk version(26) ... i have latest updated android studio sdk version 26... here is code:apply plugin: &#39;com.android.application&#39;

    android {
        compileSdkVersion 26
        buildToolsVersion &quot;26.0.0&quot;
        defaultConfig {
            applicationId &quot;com.dpl_it.m.hamzam.widgets&quot;
            minSdkVersion 15
            targetSdkVersion 26
            versionCode 1
            versionName &quot;1.0&quot;
            testInstrumentationRunner &quot;android.support.test.runner.AndroidJUnitRunner&quot;
        }
        buildTypes {
            release {
                minifyEnabled false
                proguardFiles getDefaultProguardFile(&#39;proguard-android.txt&#39;), &#39;proguard-rules.pro&#39;
            }
        }
    }
    
    dependencies {
        compile fileTree(include: [&#39;*.jar&#39;], dir: &#39;libs&#39;)
        androidTestCompile(&#39;com.android.support.test.espresso:espresso-core:2.2.2&#39;, {
            exclude group: &#39;com.android.support&#39;, module: &#39;support-annotations&#39;
        })
        compile &#39;com.android.support:appcompat-v7:26.0.0&#39;
        compile &#39;com.android.support.constraint:constraint-layout:1.0.2&#39;
        testCompile &#39;junit:junit:4.12&#39;
        compile &#39;com.android.support:cardview-v7:25.4.0&#39;
        compile &#39;com.android.support:recyclerview-v7:25.4.0&#39;
    
    }


||||||||||||||Your dependency should be

    compile 'com.android.support:cardview-v7:26.0.0-beta2'
    compile 'com.android.support:recyclerview-v7:26.0.0-beta2'

but the support library for SDK 26 is in beta. See here for the recent notes

https://developer.android.com/topic/libraries/support-library/revisions.html

--------------------------------------------------
IntelliJ System.out.println() - Cannot resolve method println(java.lang.String)
I am using IntelliJ IDEA, learning Java. All went well until yesterday, when the mentioned error occurred.

I didn&#39;t make any changes. I was looking for the solution the following ways:

1. reboot the pc
2. restart IntelliJ.
3. delete the project directory and use another one (both on desktop) 

nothing helps. buy running simple hello world method. It keeps showing this error:

[![screenshot][1]][1]

Is there someone able to help me?


  [1]: http://i.stack.imgur.com/yyvkp.jpg
||||||||||||||ok, is solved.

file -> invalidated caches / Restart

--------------------------------------------------
CSS: Control space between bullet and &lt;li&gt;
I&#39;d like to control how much horizontal space a bullet pushes its `&lt;li&gt;` to the right in an `&lt;ol&gt;` or `&lt;ul&gt;`.

That is, instead of always having

    *  Some list text goes
       here.

I&#39;d like to be able to change that to be

    *         Some list text goes
              here.

or
 
    *Some list text goes
     here.

I looked around but could only find instructions for shifting the entire block left or right, for example, http://www.alistapart.com/articles/taminglists/
||||||||||||||Put its content in a `span` which is relatively positioned, then you can control the space by the `left` property of the `span`.

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-css -->

    li span {
      position: relative;
      left: -10px;
    }

<!-- language: lang-html -->

    <ul>
      <li><span>item 1</span></li>
      <li><span>item 2</span></li>
      <li><span>item 3</span></li>
    </ul>

<!-- end snippet -->



--------------------------------------------------
Is it possible for a client to receive an http response but the server not be certain that it did?
Is there every a case, with HTTP or HTTPS, where a server sends an HTTP response to a client, the client gets the response in full, but the server cannot be certain that the client got the response in full, for example if the final ACK or FIN message from the client wasn&#39;t received by the server?

And, if so, what are the conditions under which this might occur?

I looked through several RFCs and googled around, but couldn&#39;t find any relevant answers.
||||||||||||||Let's ask slightly different questions:

* Q1: Is it possible for the server to determine its response was successfully sent?
  
  A: Yes.

* Q2: Is it possible for the server to detect an error occurred sending its response?

  A: Yes.

* Q3: What happens if the TCP/IP connection abnormally terminates before a message is completely received by the client?

  A: Both the client and the server get a RST.

The answer to Q1 and Q2 is "Yes" on either/both of two different levels:

  * [TCP/IP connection level](https://accedian.com/blog/close-tcp-sessions-diagnose-disconnections): the connection is closed gracefully ... or not.

  * [HTTP "Persistent Connection" (RFC 2616)](https://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html): provides additional Application Layer error reporting capabilities.


--------------------------------------------------
Shadcn UI installation breaks Tailwind CSS
Shadcn UI (https://ui.shadcn.com/) was working fine until I just for a couple weeks until yesterday, when I ran my NextJS app in my local host and none of the tailwind was working. To debug the issue, I created a blank NextJS 13 app in a completely new file location, and everything worked fine; tailwind was working on the default nextJS 13 page. I then ran

```
npx shadcn-ui init
```

without installing any of the components. Which did not spit out any errors, but then none of the tailwind styling worked anymore.

my tailwind.config.js after instillation:

```
/** @type {import(&#39;tailwindcss&#39;).Config} */
module.exports = {
  darkMode: [&quot;class&quot;],
  content: [
    &#39;./pages/**/*.{ts,tsx}&#39;,
    &#39;./components/**/*.{ts,tsx}&#39;,
    &#39;./app/**/*.{ts,tsx}&#39;,
	],
  theme: {
    container: {
      center: true,
      padding: &quot;2rem&quot;,
      screens: {
        &quot;2xl&quot;: &quot;1400px&quot;,
      },
    },
    extend: {
      colors: {
        border: &quot;hsl(var(--border))&quot;,
        input: &quot;hsl(var(--input))&quot;,
        ring: &quot;hsl(var(--ring))&quot;,
        background: &quot;hsl(var(--background))&quot;,
        foreground: &quot;hsl(var(--foreground))&quot;,
        primary: {
          DEFAULT: &quot;hsl(var(--primary))&quot;,
          foreground: &quot;hsl(var(--primary-foreground))&quot;,
        },
        secondary: {
          DEFAULT: &quot;hsl(var(--secondary))&quot;,
          foreground: &quot;hsl(var(--secondary-foreground))&quot;,
        },
        destructive: {
          DEFAULT: &quot;hsl(var(--destructive))&quot;,
          foreground: &quot;hsl(var(--destructive-foreground))&quot;,
        },
        muted: {
          DEFAULT: &quot;hsl(var(--muted))&quot;,
          foreground: &quot;hsl(var(--muted-foreground))&quot;,
        },
        accent: {
          DEFAULT: &quot;hsl(var(--accent))&quot;,
          foreground: &quot;hsl(var(--accent-foreground))&quot;,
        },
        popover: {
          DEFAULT: &quot;hsl(var(--popover))&quot;,
          foreground: &quot;hsl(var(--popover-foreground))&quot;,
        },
        card: {
          DEFAULT: &quot;hsl(var(--card))&quot;,
          foreground: &quot;hsl(var(--card-foreground))&quot;,
        },
      },
      borderRadius: {
        lg: &quot;var(--radius)&quot;,
        md: &quot;calc(var(--radius) - 2px)&quot;,
        sm: &quot;calc(var(--radius) - 4px)&quot;,
      },
      keyframes: {
        &quot;accordion-down&quot;: {
          from: { height: 0 },
          to: { height: &quot;var(--radix-accordion-content-height)&quot; },
        },
        &quot;accordion-up&quot;: {
          from: { height: &quot;var(--radix-accordion-content-height)&quot; },
          to: { height: 0 },
        },
      },
      animation: {
        &quot;accordion-down&quot;: &quot;accordion-down 0.2s ease-out&quot;,
        &quot;accordion-up&quot;: &quot;accordion-up 0.2s ease-out&quot;,
      },
    },
  },
  plugins: [require(&quot;tailwindcss-animate&quot;)],
}
```

my utils.ts after instillation

```
import { ClassValue, clsx } from &quot;clsx&quot;
import { twMerge } from &quot;tailwind-merge&quot;
 
export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
```

[the default page after instillation](https://i.stack.imgur.com/Ta8jG.png)

EDIT: after some testing, the issue seems to be coming from the globals.css and tailwind.config.js, still not sure what about them though.
||||||||||||||In my case the Shadcn components did not find the styles exported by tailwind.
As answered by @moyindavid the problem is in tailwind.config.

Solution:
Add the '@' folder to the exports of the tailwind attributes.

```
module.exports = {
   darkMode: ["class"],
   content: [
     './pages/**/*.{ts,tsx}',
     './components/**/*.{ts,tsx}',
     './app/**/*.{ts,tsx}',
     './@/**/*.{ts,tsx}', // <- HERE
     ],
```

--------------------------------------------------
How to disable/enable the row in table with the same button?
In my table I have one buttons . In the table I want to disable/enable the entire row with the help of the same button. the button default is enable.
I want to each row all can enable/disable when click the button of the row .
When I click on &#39;enable&#39; button the entire row color will change to red and the button value change to &#39;disable&#39;. 
 click again to the &#39;disable&#39; button , thenthe entire row color recovery and the button value change to &#39;enable&#39;. 
How to do it . Help needed.

  part of  my code : 
[jsfiddle][1]


  [1]: https://jsfiddle.net/rZqLX/1/
    
    &lt;table &gt;
    &lt;tr&gt;
        &lt;th&gt;Value1&lt;/td&gt;
        &lt;th&gt;Value2&lt;/td&gt;
        &lt;th&gt;
            &lt;input type=&quot;button&quot; value=&quot;enable&quot; /&gt;
        &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Value3&lt;/td&gt;
        &lt;th&gt;Value4&lt;/td&gt;
        &lt;th&gt;
            &lt;input type=&quot;button&quot; value=&quot;enable&quot; /&gt;
        &lt;/th&gt;
    &lt;/tr&gt;
    &lt;/table&gt;


||||||||||||||You could change the value of the button and then check if it's 'enable' or 'disable'

Here's the code:

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    $('td input[type="button"]').on('click', function() {
        $(this).val((_, val) => val == "enable" ? "disable" : "enable");
        $(this).closest('tr').toggleClass('selected');
    });

<!-- language: lang-css -->

    .selected {
        background-color:red;
    }
    table {
        padding:0px;
        border-collapse: collapse;
    }

<!-- language: lang-html -->

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
    <table>
        <tr>
            <td>Value1</td>
            <td>Value2</td>
            <td>
                <input type="button" value="enable" />
            </td>
        </tr>
        <tr>
            <td>Value3</td>
            <td>Value4</td>
            <td>
                <input type="button" value="enable" />
            </td>
        </tr>
    </table>

<!-- end snippet -->



--------------------------------------------------
MySql error: incompatible with sql_mode=only_full_group_by
I&#39;ve inherited a CodeIgniter query that generates this SQL

EXAMPLE:

    SELECT `users`.`id`, `users`.`username`, `users`.`email`, `users`.`photo`, `users`.`rating`
    FROM `pool_details`
    JOIN `users` ON `users`.`id` = `pool_details`.`captain_id`
    WHERE `pool_details`.`pool_type` =0
    AND `pool_details`.`pool_close` &gt; &#39;2020-01-02 18:39:42&#39;
    GROUP BY `pool_details`.`captain_id`
    ORDER BY `pool_details`.`members_count` DESC
    LIMIT 20

&gt; ERROR - 2020-01-02 18:39:42 --&gt; Query error: Expression #1 of ORDER BY
&gt; clause is not in GROUP BY clause and contains nonaggregated column
&gt; &#39;pool_details.members_count&#39; which is not functionally
&gt; dependent on columns in GROUP BY clause; this is incompatible with
&gt; sql_mode=only_full_group_by - Invalid query:

Here is the same data with some &quot;extra columns&quot; and the offending clauses removed:

    SELECT users.id, users.username, users.email,users.rating,pool_details.members_count,pool_details.pool_type,pool_details.pool_close
    FROM  pool_details
    JOIN  users ON users.id = pool_details.captain_id
    WHERE pool_details.pool_type = 0 AND pool_details.pool_close &gt; &#39;2020-01-02 18:39:42&#39;
    ORDER BY pool_details.members_count DESC;
    //GROUP BY `pool_details`.`captain_id`
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    | id | username | email                    | rating | members_count | pool_type | pool_close          |
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         | 2020-01-04 03:00:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         | 2020-01-03 23:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         
    ...
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 00:00:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 00:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-04 21:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 03:00:00 |
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    28 rows in set (0.00 sec)

What I want is to:

  1. Show User&#39;s username, email and rating
  2. Order by &quot;members_count&quot;
  3. Show the user only *once*

For example:

    SELECT DISTINCT users.id, users.username, users.email,users.rating
    FROM  pool_details
    JOIN  users ON users.id = pool_details.captain_id
    WHERE pool_details.pool_type = 0 AND pool_details.pool_close &gt; &#39;2020-01-02 18:39:42&#39;;
    +----+----------+--------------------------+--------+
    | id | username | email                    | rating |
    +----+----------+--------------------------+--------+
    |  5 | wheel    | wheel@boxpik.com         | NULL   |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |
    +----+----------+--------------------------+--------+
    2 rows in set (0.00 sec)
    &lt;= This shows the users individually ... but it&#39;s *NOT* ordered by &quot;members_count&quot;.

Q: Is there any combination of &quot;GROUP BY&quot; and/or &quot;DISTINCT&quot; that I can use with mySql 5.7 that will give me the result set I need?


||||||||||||||Other than the `ORDER BY`, you seem to just want `EXISTS`.  That said, you can use another subquery in the `ORDER BY`:

    SELECT u.*
    FROM users u
    WHERE EXISTS (SELECT 1
                  FROM pool_details pd
                  WHERE pd.captain_id = u.id AND
                        pd.pool_type = 0 AND
                        pd.pool_close > '2020-01-02 18:39:42'
                 )
    ORDER BY (SELECT MAX(pd2.members_count)
              FROM pool_details pd2
              WHERE pd2.captain_id = u.id AND
                    pd2.pool_type = 0 AND
                    pd2.pool_close > '2020-01-02 18:39:42'
             ) DESC
    LIMIT 20

EDIT:

You can also write this as:

    SELECT u.*,
           (SELECT MAX(pd2.members_count)
            FROM pool_details pd2
            WHERE pd2.captain_id = u.id AND
                  pd2.pool_type = 0 AND
                  pd2.pool_close > '2020-01-02 18:39:42'
          ) as max_members_count
    FROM users u
    HAVING max_member_count IS NOT NULL
    ORDER BY max_member_count DESC
    LIMIT 20

Or:

    SELECT u.*
    FROM users u JOIN
         (SELECT pd2.captain_id, MAX(pd2.members_count) as max_member_count
          FROM pool_details pd2
          WHERE pd2.pool_type = 0 AND
                pd2.pool_close > '2020-01-02 18:39:42'
         ) pd
         ON pd.captain_id = u.id
    ORDER BY max_member_count DESC
    LIMIT 20l


--------------------------------------------------
Using a java class to create a database
I&#39;m working in an application that uses servlets and mysql.

I&#39;d like to create a .jar file able to create the database that the application will be using. This will only be done once, in order to create the db.

I&#39;ve no problem in getting to access to a database, doing something like this:

    Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance();
    Connection conexion = (Connection)DriverManager.getConnection(&quot;jdbc:mysql://localhost/test&quot;,&quot;admin&quot;,&quot;admin&quot;);
    if (!conexion.isClosed())
    {	
       Statement st = (Statement) conexion.createStatement();
       ResultSet rs = st.executeQuery(&quot;select * from table_name&quot; );
    }
    conexion.close();

This is ok, but what I need to do is to create a new database (and its tables) from a java class, is that possible?

I&#39;m trying this:		

    Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance();
    Connection conexion = (Connection)DriverManager.getConnection(&quot;jdbc:mysql://localhost/mysql&quot;,&quot;admin&quot;,&quot;admin&quot;);
		
    Statement st = (Statement) conexion.createStatement();       
    st.executeUpdate(&quot;CREATE DATABASE hrapp&quot;);

but I&#39;m getting the following error:

    Exception in thread &quot;main&quot; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Access denied for user &#39;admin&#39;@&#39;localhost&#39; to database &#39;hrapp&#39;
    	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    	at java.lang.reflect.Constructor.newInstance(Unknown Source)
    	at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    	at com.mysql.jdbc.Util.getInstance(Util.java:381)
    	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1030)
    	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
    	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3491)
    	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3423)
    	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1936)
    	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2060)
    	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2536)
    	at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1564)
    	at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1485)
    	at BaseDatosSetup.BaseDatosSetup.main(BaseDatosSetup.java:18)

I solved it by granting the create action to the user. I don&#39;t know why, I was doing it as an administrator.
||||||||||||||W3CSchools.com -- [SQL CREATE DATABASE Statement][1]. You wouldn't use `executeQuery` though. Instead use `executeUpdate`.

[Here][2] is a simple example.

As mentioned by other users, you probably don't want to be creating databases from your code. It just isn't good practice.


  [1]: http://www.w3schools.com/SQl/sql_create_db.asp
  [2]: http://www.java2s.com/Code/Java/Database-SQL-JDBC/CreateDatabaseforMySQL.htm

--------------------------------------------------
Server Error in &#39;/&#39; Application. Object reference not set to an instance of an object
I&#39;ve been trying to figure it out but not getting it.

**Description:** An unhandled exception occurred during the execution of the current web request. Please review the stack trace for more information about the error and where it originated in the code.

**Exception Details:**

    System.NullReferenceException: Object reference not set to an instance of an object.

**Source Error:**

An unhandled exception was generated during the execution of the current web request. Information regarding the origin and location of the exception can be identified using the exception stack trace below.

**Stack Trace:**


    [NullReferenceException: Object reference not set to an instance of an object.]
       Microsoft.WebTools.BrowserLink.Runtime.Tracing.PageInspectorHttpModule.OnPreRequestHandlerExecute(Object sender, EventArgs e) +662
       System.Web.SyncEventExecutionStep.System.Web.HttpApplication.IExecutionStep.Execute() +141
       System.Web.HttpApplication.ExecuteStepImpl(IExecutionStep step) +74
       System.Web.HttpApplication.ExecuteStep(IExecutionStep step, Boolean&amp; completedSynchronously) +92
||||||||||||||If you are using VS2019, make sure to uncheck Enable Browser Link
[VSCode EnableBroswerLink][1]


  [1]: https://i.stack.imgur.com/xlU6n.png

--------------------------------------------------
How to create a PDF/A from command line with Libre Office Draw in headless mode?
LibreOffice Draw allows you to open a non PDF/A file and export this a PDF/A-1b or PDF/A-2b file.

[![export as PDF][1]][1]

The same is possible from the command line by calling on macOS

```bash
/Applications/LibreOffice.app/Contents/MacOS/soffice --headless \
        --convert-to pdf:draw_pdf_Export \
        --outdir ./pdfout \
        ./input-non-pdfa.pdf
```

or an a Linux simply

```bash
libreoffice --headless \
        --convert-to pdf:draw_pdf_Export \
        --outdir ./pdfout \
        ./input-non-pdfa.pdf
```

On the command line it is possible to tell the `convert-to` to create a pdf and use LibreOffice Draw to do this by telling `--convert-to pdf:draw_pdf_Export`. 

Is there also a way to tell LibreOffice to produce a PDF/A document in **headless** mode?

  [1]: https://i.stack.imgur.com/mpSA6.png
||||||||||||||For PDF/A-1(means `PDF/A-1b`?):
```
soffice --headless --convert-to pdf:"writer_pdf_Export:SelectPdfVersion=1" --outdir outdir input.pdf
```
Change the value from `1` to `2` for PDF/A-2, here is the Libreoffice source code [Common.xcs](https://github.com/LibreOffice/core/blob/d4f5299fd2806d8f5dcd467742effeaa0dee8863/officecfg/registry/schema/org/openoffice/Office/Common.xcs#L5417-L5449), [pdfexport.cxx](https://github.com/LibreOffice/core/blob/e83b5f6a015269ed7e5407a8440c0fc99fcfa397/filter/source/pdf/pdfexport.cxx#L590-L623) and [pdffilter.cxx](https://github.com/LibreOffice/core/blob/bdbb5d0389642c0d445b5779fe2a18fda3e4a4d4/filter/source/pdf/pdffilter.cxx#L85).

- (Maybe outdated) [API/Tutorials/PDF export - Apache OpenOffice Wiki](https://wiki.openoffice.org/wiki/API/Tutorials/PDF_export)
- [Python Guide - PDF export filter data - The Document Foundation Wiki](https://wiki.documentfoundation.org/Macros/Python_Guide/PDF_export_filter_data)
- [excel->pdf変換 command のdpi設定 - Ask LibreOffice](https://ask.libreoffice.org/ja/question/229354/excel-pdfbian-huan-command-nodpishe-ding/)
- [Change default resolution in batch PNG conversion [closed] - Ask LibreOffice](https://ask.libreoffice.org/en/question/68775/change-default-resolution-in-batch-png-conversion/)

--------------------------------------------------
how extract &#39;cancellationDate&#39; this:
	
    [&quot;{\&quot;RequestedByUser\&quot;:false,\&quot;RequestedBySystem\&quot;:null,\&quot;RequestedBySellerNotification\&quot;:null,\&quot;RequestedByPaymentNotification\&quot;:true,\&quot;Reason\&quot;:null,\&quot;CancellationDate\&quot;:\&quot;2024-01-16T00:40:59.0928615+00:00\&quot;}&quot;]

tryingjson_extract, jsonpath and nothing
||||||||||||||We have a JSON string inside a list. If you want to extract values from this JSON string, you can use the json module in Python. 

Try like this 

```python
import json

# Your input data
data = ["{\"RequestedByUser\":false,\"RequestedBySystem\":null,\"RequestedBySellerNotification\":null,\"RequestedByPaymentNotification\":true,\"Reason\":null,\"CancellationDate\":\"2024-01-16T00:40:59.0928615+00:00\"}"]

# Assuming there is only one element in the list, you can access it using data[0]
json_data = json.loads(data[0])
cancellation_date = json_data["CancellationDate"]
print("CancellationDate:", cancellation_date)
```

Output:

```
CancellationDate: 2024-01-16T00:40:59.0928615+00:00
```


--------------------------------------------------
Is it in line with the DCO that a github sign-off needs and publishes full name + an email &quot;that matches the commit author&quot;?
Coming from Stack Overflow where a pseudo name is normal and enough, a github beginner like me does not expect to have to sign-off a git pull request with the full name and kind of full-name-email being published. Going over to github, I simply do not expect more than what Stack Overflow is asking for. I thought the other contributors on github just chose willingly to sign with their full names and respective e-mails, and I was astonished to see my personal mail being published.

The tasks you follow to do the pull request on github (not from the DCO, this was just a helping comment):

&gt; You need sign-off your PR with your email address. Below are steps to
&gt; sign-off a commit. At first, you need configure your git with user
&gt; name and email: `git config --global user.name &quot;FIRST_NAME LAST_NAME&quot;`
&gt; `git config --global user.email &quot;MY_NAME@example.com&quot;`
&gt; Next run `git push --force-with-lease origin YOURBRANCHNAME`



I have read the DCO Developer Certificate of Origin now in the github version https://github.com/apps/dco and in the original version https://developercertificate.org/.

The github version asks for more than the original DCO, in my opinion.
&gt; It requires all commit messages to contain the Signed-off-by line with an email address that matches the commit author.


further below...


&gt; Contributors sign-off that they adhere to these requirements by adding a Signed-off-by line to commit messages.
&gt; This is my commit message
&gt; 
&gt; Signed-off-by: Random J Developer &lt;random@developer.example.org&gt;

Here you could already discuss if &quot;Random J Developer&quot; has to be the full name or just a pseudo name, and also whether the name (or pseudo name respectively) should be part of the mail. The original DCO speaks just generally of the personal information in the sign-off:


&gt; I understand and agree that this project and the contribution are
&gt; public and that a record of the contribution (including all personal
&gt; information I submit with it, including my sign-off) is maintained
&gt; indefinitely and may be redistributed consistent with this project or
&gt; the open source license(s) involved.

In its intro, the github DCO mentions the email that &quot;matches the commit author&quot; as the core of the personal information, and later adds the name in the example. This &quot;matches the commit author&quot; is already a stricter requirement than the original DCO is asking for, thus this requirement could already be questioned. From the original DCO I read the option to put your full name and full name email, but not the need to do so, as the github user name and a mail that includes the github author name would be personal information enough to identify you as well, which is the main requirement. From the github DCO I read the wish that you put your full name, but it is only in the example, not in the text, and I could also go around that now by putting my github username and an email that does not show my full name but includes my github name, and still following the DCO, as I read it.

My final question after this long explanation:
Is the github DCO requirement of full name and an &quot;email address that matches the commit author&quot; in line with the official DCO? Or does it ask too much, and a pseudo name + email using that pseudo name would be already enough? Or as a third option, would a pseudo name + email not using any pseudo or full name already be enough?


p.s.:

To anyone of github reading this. If publishing the full name and respective email is really needed, I simply would like to be informed about this when doing my first pull request, because few people will read the DCO before starting.
||||||||||||||There are two items here which are separate and different.  One is the commit metadata which is stored in your commits, which is set with `user.name` and `user.email`.  This information is embedded by Git (not GitHub) in all commits you make so that people know who the author and committer are.

It is not required that `user.name` reflect your personal name, but it is customary.  Some projects, such as Git, strongly prefer that people use their personal name unless they are primarily known by a pseudonym, such as chromatic, the Perl contributor.  Other projects do not care.

GitHub itself does not impose any restrictions on either one (other, possibly, than that it be in UTF-8) but it does use the email address embedded in your commits (the `user.email` value) to attribute the commit to your account.  If the commit email address doesn't match any account, then it displays it as associated with the name (`user.name`) value you've specified.

In addition, some projects, such as Git, require that authors use the sign-off functionality (`git commit -s`) to intentionally state that they grant the rights to the project stated in the Developer's Certificate of Origin (or another project-specific document).  This is essentially a legal statement that the contributor has the right to contribute that code to the project.  The fact that this is a legal statement is why many projects prefer a personal name rather than a pseudonym.

When you use the `-s` (`--signoff`) option to `git commit`, Git embeds both the `user.name` and `user.email` value into the commit message as a sign-off in the format normally used inside the commit object.  A sign-off usually cannot be parsed without both.

A contributor who submits someone else's code to a project using the DCO needs to sign-off that commit.  So, for example, the Git for Windows maintainer provided me a patch which he signed off, and when I contributed it to Git, I added my sign-off, certifying that I had received it under those terms and not modified it. The Git project maintainer will sign-off the commit as well, asserting the same thing.

Thus, if you choose to add the DCO add-on to your project, it is reasonable that one of the sign-offs on the commit match an email address on your account.  Note that, as far as I'm aware, the DCO add-on doesn't require your name to match, which is reasonable: some people use a nickname or shortened form of their name, and a bit-for-bit match would be overly burdensome.

As I mentioned, GitHub doesn't require much of anything about your name and email, and technically the DCO add-on (which is a matter of project policy) does not require anything beyond a matching email.  Git will, however, embed whatever name you give it inside your commits; the only change with the DCO is that you agree that the project can maintain that indefinitely.

--------------------------------------------------
using Django crispy forms, how do you shrink the text box for a TextField?
The text boxes that crispy forms lay out for TextField columns are too tall for my app, they are taking up too much screen space.  The user can make them taller if desired by dragging the bottom frame line, but the minimum size is too tall.  I would like the default height to be 4 rows or so.

I tried some ideas I got off related posts, but nothing  worked -- the text boxes were still the same height and too tall.

Here is the model I am working on at the moment:

    class Brand(models.Model):
        cTitle          = models.CharField(
                            &#39;brand name&#39;, max_length = 48, db_index = True)
        bWanted         = models.BooleanField(
                            &#39;want anything from this brand?&#39;, default = True )
        bAllOfInterest  = models.BooleanField(
                            &#39;want everything from this brand?&#39;, default = True )
        cLookFor        = models.TextField(
                            &#39;Considered a hit if this text is found &#39;
                            &#39;(each line evaluated separately, &#39;
                            &#39;put different look for tests on different lines)&#39;,
                            null=True, blank = True )
        iStars          = IntegerRangeField(
                            &#39;desireability, 10 star brand is most desireable&#39;,
                            min_value = 0, max_value = 10, default = 5 )
        cComment        = models.TextField( &#39;comments&#39;, null = True, blank = True )
        cNationality    = CountryField( &quot;nationality&quot;, null = True )
        cExcludeIf      = models.TextField(
                            &#39;Not a hit if this text is found &#39;
                            &#39;(each line evaluated separately, &#39;
                            &#39;put different exclude tests on different lines)&#39;,
                            null=True, blank = True )
        iLegacyKey      = models.PositiveIntegerField(&#39;legacy key&#39;, null = True )
        tLegacyCreate   = models.DateTimeField( &#39;legacy row created on&#39;,
                            null=True, blank = True )
        tLegacyModify   = models.DateTimeField( &#39;legacy row updated on&#39;,
                            null=True, blank = True )
        iUser           = models.ForeignKey( User, verbose_name = &#39;Owner&#39;,
                            on_delete=models.CASCADE )
        tCreate         = models.DateTimeField( &#39;created on&#39;, auto_now_add= True )
        tModify         = models.DateTimeField( &#39;updated on&#39;, auto_now    = True )
        #
    
        def __str__(self):
            return self.cTitle
        
        class Meta():
            verbose_name_plural = &#39;brands&#39;
            ordering            = (&#39;cTitle&#39;,)
            db_table            = verbose_name_plural

But only these fields are on the form:

    tModelFields = (
        &#39;cTitle&#39;,
        &#39;bWanted&#39;,
        &#39;bAllOfInterest&#39;,
        &#39;cLookFor&#39;,
        &#39;iStars&#39;,
        &#39;cComment&#39;,
        &#39;cNationality&#39;,
        &#39;cExcludeIf&#39; )

Guidance would be appreciated.

||||||||||||||Since you are using `crispy_forms` according to their documentation you could use `Layouts` to define some attributes of the field's "element" that will be added to the template, [as it is explained here in the documentation][1].

In this case it would be something like this: `Field('cExcludeIf', rows='4')`


  [1]: https://django-crispy-forms.readthedocs.io/en/latest/layouts.html#layout-objects-attributes

--------------------------------------------------
PotentialStubbingProblem solved by moving a class. How does it work?
Here&#39;s an MRE:
```java
package com.example.mockitomre;

public interface DocumentedEndpoint {
    EndpointDetails getDetails();
}
```
```java
package com.example.mockitomre;

public interface EndpointDetails {
    String getPath();
}
```
```java
package com.example.mockitomre;

public interface EndpointSieve {
    boolean isAllowed(DocumentedEndpoint endpoint);
}
```
```java
package com.example.mockitomre;

import org.springframework.util.AntPathMatcher;

public class EndpointSieveConfig {
    public EndpointSieve errorPathEndpointSieve(GatewayMeta gatewayMeta, AntPathMatcher antPathMatcher) {
        return endpoint -&gt; gatewayMeta.getIgnoredPatterns().stream()
                .noneMatch(ignoredPattern -&gt; antPathMatcher.match(ignoredPattern, endpoint.getDetails().getPath()));
    }
}
```
```java
package com.example.mockitomre;

import lombok.Getter;

import java.util.List;

@Getter
public final class GatewayMeta {
    private List&lt;String&gt; ignoredPatterns;
}
```
```java
package com.example.mockitomre;

import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.util.AntPathMatcher;

import java.util.List;

import static org.assertj.core.api.AssertionsForClassTypes.assertThat;
import static org.mockito.Mockito.RETURNS_DEEP_STUBS;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@ExtendWith(MockitoExtension.class)
class ErrorPathEndpointSieveTest {
    private final EndpointSieveConfig endpointSieveConfig = new EndpointSieveConfig();
    @Mock
    private GatewayMeta gatewayMetaMock;
    @Mock
    private AntPathMatcher antPathMatcherMock;
    private EndpointSieve errorPathEndpointSieve;

    @Test
    void doesntAllowIgnoredPatterns() {
        String ignoredPattern = &quot;/ignored-path/**&quot;;
        String anotherIgnoredPattern = &quot;/*/another-ignored-path&quot;;
        when(gatewayMetaMock.getIgnoredPatterns()).thenReturn(List.of(
                ignoredPattern, anotherIgnoredPattern
        ));

        String pathToExclude = &quot;/ignored-path&quot;;
        String anotherPathToExclude = &quot;/it-is/another-ignored-path&quot;;

        when(antPathMatcherMock.match(ignoredPattern, pathToExclude)).thenReturn(true);
        when(antPathMatcherMock.match(anotherIgnoredPattern, anotherPathToExclude)).thenReturn(true);

        DocumentedEndpoint endpointToExclude = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(endpointToExclude.getDetails().getPath()).thenReturn(pathToExclude);

        DocumentedEndpoint anotherEndpointToExclude = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(anotherEndpointToExclude.getDetails().getPath()).thenReturn(anotherPathToExclude);

        String okPath = &quot;/another-ignored-path/on-second-thought-it-is-not&quot;;
        DocumentedEndpoint endpointToKeep = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(endpointToKeep.getDetails().getPath()).thenReturn(okPath);

        errorPathEndpointSieve = endpointSieveConfig.errorPathEndpointSieve(gatewayMetaMock, antPathMatcherMock);

        assertThat(errorPathEndpointSieve.isAllowed(endpointToExclude)).isFalse();
        assertThat(errorPathEndpointSieve.isAllowed(anotherEndpointToExclude)).isFalse();

        assertThat(errorPathEndpointSieve.isAllowed(endpointToKeep)).isTrue();
    }
}
```
```xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;3.2.2&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;mockito-mre&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;mockito-mre&lt;/name&gt;
    &lt;description&gt;mockito-mre&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;17&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
```
So here&#39;s the problem: once I run the test, I get
```
org.mockito.exceptions.misusing.PotentialStubbingProblem: 
Strict stubbing argument mismatch. Please check:
 - this invocation of &#39;match&#39; method:
    antPathMatcherMock.match(
    &quot;/ignored-path/**&quot;,
    &quot;/it-is/another-ignored-path&quot;
);
    -&gt; at com.example.mockitomre.EndpointSieveConfig.lambda$errorPathEndpointSieve$0(EndpointSieveConfig.java:8)
 - has following stubbing(s) with different arguments:
    1. antPathMatcherMock.match(
    &quot;/*/another-ignored-path&quot;,
    &quot;/it-is/another-ignored-path&quot;
);
      -&gt; at com.example.mockitomre.ErrorPathEndpointSieveTest.doesntAllowIgnoredPatterns(ErrorPathEndpointSieveTest.java:37)
Typically, stubbing argument mismatch indicates user mistake when writing tests.
Mockito fails early so that you can debug potential problem easily.
However, there are legit scenarios when this exception generates false negative signal:
  - stubbing the same method multiple times using &#39;given().will()&#39; or &#39;when().then()&#39; API
    Please use &#39;will().given()&#39; or &#39;doReturn().when()&#39; API for stubbing.
  - stubbed method is intentionally invoked with different arguments by code under test
    Please use default or &#39;silent&#39; JUnit Rule (equivalent of Strictness.LENIENT).
For more information see javadoc for PotentialStubbingProblem class.

	at org.springframework.util.AntPathMatcher.match(AntPathMatcher.java:195)
	at com.example.mockitomre.EndpointSieveConfig.lambda$errorPathEndpointSieve$0(EndpointSieveConfig.java:8)
```
I don&#39;t know what exactly Mockito wants from me, but I made some experiments and it&#39;s not about

1) mocking the same method with different arguments;
2) the fact that the method is also called with set of arguments not involved in any stubbing.
```java
// here, Mockito is fine with calling match(&quot;/ignored-path/**&quot;, &quot;/it-is/another-ignored-path&quot;)

    @Test
    void doesntAllowIgnoredPatterns() {
        when(antPathMatcherMock.match(&quot;/ignored-path/**&quot;, &quot;/ignored-path&quot;)).thenReturn(true);
        when(antPathMatcherMock.match(&quot;/*/another-ignored-path&quot;, &quot;/it-is/another-ignored-path&quot;)).thenReturn(true);

        antPathMatcherMock.match(&quot;/ignored-path/**&quot;, &quot;/ignored-path&quot;);
        antPathMatcherMock.match(&quot;/*/another-ignored-path&quot;, &quot;/it-is/another-ignored-path&quot;);

        antPathMatcherMock.match(&quot;/ignored-path/**&quot;, &quot;/it-is/another-ignored-path&quot;);
    }
```

You know what helps (beside ditching Mockito&#39;s extension, I hate it, it brings more problems that it solves)? Making `EndpointSieveConfig` a nested class of `ErrorPathEndpointSieveTest` like so:
```java
package com.example.mockitomre;

import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.util.AntPathMatcher;

import java.util.List;

import static org.assertj.core.api.AssertionsForClassTypes.assertThat;
import static org.mockito.Mockito.RETURNS_DEEP_STUBS;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@ExtendWith(MockitoExtension.class)
class ErrorPathEndpointSieveTest {
    private final EndpointSieveConfig endpointSieveConfig = new EndpointSieveConfig();
    @Mock
    private GatewayMeta gatewayMetaMock;
    @Mock
    private AntPathMatcher antPathMatcherMock;
    private EndpointSieve errorPathEndpointSieve;

    @Test
    void doesntAllowIgnoredPatterns() {
        String ignoredPattern = &quot;/ignored-path/**&quot;;
        String anotherIgnoredPattern = &quot;/*/another-ignored-path&quot;;
        when(gatewayMetaMock.getIgnoredPatterns()).thenReturn(List.of(
                ignoredPattern, anotherIgnoredPattern
        ));

        String pathToExclude = &quot;/ignored-path&quot;;
        String anotherPathToExclude = &quot;/it-is/another-ignored-path&quot;;

        when(antPathMatcherMock.match(ignoredPattern, pathToExclude)).thenReturn(true);
        when(antPathMatcherMock.match(anotherIgnoredPattern, anotherPathToExclude)).thenReturn(true);

        DocumentedEndpoint endpointToExclude = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(endpointToExclude.getDetails().getPath()).thenReturn(pathToExclude);

        DocumentedEndpoint anotherEndpointToExclude = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(anotherEndpointToExclude.getDetails().getPath()).thenReturn(anotherPathToExclude);

        String okPath = &quot;/another-ignored-path/on-second-thought-it-is-not&quot;;
        DocumentedEndpoint endpointToKeep = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(endpointToKeep.getDetails().getPath()).thenReturn(okPath);

        errorPathEndpointSieve = endpointSieveConfig.errorPathEndpointSieve(gatewayMetaMock, antPathMatcherMock);

        assertThat(errorPathEndpointSieve.isAllowed(endpointToExclude)).isFalse();
        assertThat(errorPathEndpointSieve.isAllowed(anotherEndpointToExclude)).isFalse();

        assertThat(errorPathEndpointSieve.isAllowed(endpointToKeep)).isTrue();
    }

    public class EndpointSieveConfig {
        public EndpointSieve errorPathEndpointSieve(GatewayMeta gatewayMeta, AntPathMatcher antPathMatcher) {
            return endpoint -&gt; gatewayMeta.getIgnoredPatterns().stream()
                    .noneMatch(ignoredPattern -&gt; antPathMatcher.match(ignoredPattern, endpoint.getDetails().getPath()));
        }
    }
}
```
Now it passes! The questions:

1. What does Mockito want from me?
2. Why does making that weird change make Mockito happy?

No, [this question][1] doesn&#39;t help

## Anything that doesn&#39;t address moving the class is **not** an answer to this question. Stop the &quot;duplicate&quot; nonsense please and actually read the question

To those suggesting [Javadoc][2]: it&#39;s not a reliable source of information. I literally pasted the code from there, and it *doesn&#39;t* trigger `PotentialStubbingProblem`, it triggers `UnnecessaryStubbingException`
```java
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;

import static org.mockito.BDDMockito.given;

@ExtendWith(MockitoExtension.class)
public class MockitoTest {
    @Mock
    SomeClass mock;
    @Test
    void test() {
        //test method:
        Something something = new Something();
        given(mock.getSomething(100)).willReturn(something);

        //code under test:
        Something something2 = mock.getSomething(50); // &lt;-- stubbing argument mismatch
    }

    abstract class SomeClass {
        abstract Something getSomething(int arg);
    }

    class Something {}
}
```


  [1]: https://stackoverflow.com/questions/52139619/simulation-of-service-using-mockito-2-leads-to-stubbing-error
  [2]: https://www.javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/exceptions/misusing/PotentialStubbingProblem.html
||||||||||||||Firstly, let's agree on terminology [Strictness in Mockito
#769](https://github.com/mockito/mockito/issues/769)

> Strictness in Mockito can have 2 forms:
>
> - Strict stubbing that requires that all declared stubs are actually used
> - Strict mocks, that require all non-void methods to be stubbed before they are called
>
>Future direction:
>
> - strict stubbing on by default, opt-out available
> - strict mocking off by default, opt-in available

In your case, we are asking talking strict stubbing behaviour.

strict stubbing is turned on implicitly in DefaultMockitoSessionBuilder:
```lang-java
Strictness effectiveStrictness = this.strictness == null ? Strictness.STRICT_STUBS : this.strictness;
```

Let's turn on STRICT_STUBS explicitely for 2 mocks:
- one created via `@Mock` and MockitoExtension
- one created via Mockito.mock
```lang-java
@Mock(strictness = Mock.Strictness.STRICT_STUBS)
private PathMatcher pathMatcherMock;

private PathMatcher pathMatcherMock = Mockito.mock(PathMatcher.class, withSettings().strictness(org.mockito.quality.Strictness.STRICT_STUBS));
```

The goal is to have the mocks as similar as possible - without implicit defaults.

**Problem 1: Mockito.mock version does not throw PotentialStubbingProblem**

Inspection of the mock's `CreationSettings.stubbingLookupListeners` shows that they are empty. [DefaultStubbingLookupListener](https://github.com/mockito/mockito/blob/219350728d0f1bad9739bfa054074d6c43f1bdb1/src/main/java/org/mockito/internal/junit/DefaultStubbingLookupListener.java) is a listener responsible for throwing `PotentialStubbingProblem`, and its code is never executed.

IMHO this is confusing and poorly documented (maybe a bug?).

**Problem 2: @Mock version throws PotentialStubbingProblem, but moving of code to test file resolves PotentialStubbingProblem**

Inspection of the mock's `CreationSettings.stubbingLookupListeners` shows that they contain a `DefaultStubbingLookupListener`

The code responsible for this behaviour is again in [DefaultStubbingLookupListener](https://github.com/mockito/mockito/blob/219350728d0f1bad9739bfa054074d6c43f1bdb1/src/main/java/org/mockito/internal/junit/DefaultStubbingLookupListener.java#L75)

```lang-java
private static List<Invocation> potentialArgMismatches(
        Invocation invocation, Collection<Stubbing> stubbings) {
    List<Invocation> matchingStubbings = new LinkedList<>();
    for (Stubbing s : stubbings) {
        if (UnusedStubbingReporting.shouldBeReported(s)
                && Objects.equals(
                        s.getInvocation().getMethod().getName(),
                        invocation.getMethod().getName())
                // If stubbing and invocation are in the same source file we assume they are in
                // the test code,
                // and we don't flag it as mismatch:
                && !Objects.equals(
                        s.getInvocation().getLocation().getSourceFile(),
                        invocation.getLocation().getSourceFile())) {
            matchingStubbings.add(s.getInvocation());
        }
    }
    return matchingStubbings;
}
```

IMHO this is confusing at best.


--------------------------------------------------
How to enter full screen mode in Flutter web app
Is there a way of making a flutter web app enter fullscreen mode (hide addressbar, tabsbar and taskbar)? Or is there a way of programmatically pressing F11?

I&#39;ve tried...

``` dart
@override
void dispose() {
  SystemChrome.setEnabledSystemUIOverlays(SystemUiOverlay.values);
  super.dispose();
}

@override
initState() {
  SystemChrome.setEnabledSystemUIOverlays([]);
  super.initState();
}
```
but it didn&#39;t work in the web app (I wasn&#39;t expecting it to)
||||||||||||||You might try this:

```dart
import 'dart:html';

void goFullScreen() {
  document.documentElement.requestFullscreen();
}

--------------------------------------------------
spring boot starter graphql not working
I recently started working with `graphql` and found it very intriguing. Since most of my `rest` apps were in `java`, I decided to do a quick setup using the provided [spring boot starter](https://github.com/graphql-java/graphql-spring-boot) project by the `graphql-java` team. It comes with `graph-iql` autoconf spring setup, which makes it easier to query `/graphql` endpoint. 

After spending a few good hours on the project setup in IDEA, I was able to run the [graphql-sample-app](https://github.com/graphql-java/graphql-spring-boot/tree/master/graphql-sample-app). But I think my servlet is still not enabled, and only the `graphiql` endpoint is running, as the default query is returning `404`. 

This is `application.yml`:

    spring:
          application:
                   name: graphql-todo-app
    server:
          port: 9000
    
    graphql:
          spring-graphql-common:
                   clientMutationIdName: clientMutationId
                   injectClientMutationId: true
                   allowEmptyClientMutationId: false
                   mutationInputArgumentName: input
                   outputObjectNamePrefix: Payload
                   inputObjectNamePrefix: Input
                   schemaMutationObjectName: Mutation
          servlet:
                 mapping: /graphql
                 enabled: true
                 corsEnabled: true
    
    graphiql:
        mapping: /graphiql
        enabled: true

This is what my `build.gradle` file looks like:

    buildscript {
        repositories {
            maven { url &quot;https://plugins.gradle.org/m2/&quot; }
            maven { url &#39;http://repo.spring.io/plugins-release&#39; }
        }
        dependencies {
            classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:1.5.2.RELEASE&quot;)
            classpath &quot;com.jfrog.bintray.gradle:gradle-bintray-plugin:1.6&quot;
        }
    }
    
    apply plugin: &#39;java&#39;
    apply plugin: &#39;org.springframework.boot&#39;
    
    repositories {
        jcenter()
        mavenCentral()
    }
    
    dependencies{
    //    compile(project(&quot;:graphql-spring-boot-starter&quot;))
    //    compile(project(&quot;:graphiql-spring-boot-starter&quot;))
        compile &#39;com.graphql-java:graphql-spring-boot-starter:3.6.0&#39;
    
        // to embed GraphiQL tool
        compile &#39;com.graphql-java:graphiql-spring-boot-starter:3.6.0&#39;
    
        compile &quot;com.embedler.moon.graphql:spring-graphql-common:$LIB_SPRING_GRAPHQL_COMMON_VER&quot;
    
        compile(&quot;org.springframework.boot:spring-boot-starter-web&quot;)
        compile(&quot;org.springframework.boot:spring-boot-starter-actuator&quot;)
    
        testCompile(&quot;org.springframework.boot:spring-boot-starter-test&quot;)
    }
    
    jar.enabled = true
    uploadArchives.enabled = false
    bintrayUpload.enabled = false

After running `gradle build`, I run the generated `jar` file from the terminal. This is what I get on localhost:

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/2qGRf.png
||||||||||||||I had the same issue using Spring boot 2.0.0 (M6). Switching back to 1.5.8.RELEASE solved the problem. They're working on the issue, it will be released as soon as there is a non milestone release for Spring boot 2.x

https://github.com/graphql-java/graphql-spring-boot/issues/40

https://github.com/graphql-java/graphql-spring-boot/pull/36

--------------------------------------------------
&#39;raise&#39; inside &#39;try&#39; , when and how do I use &#39;raise&#39;?
So here&#39;s the code:
    
    def fancy_divide(list_of_numbers, index):
        try:
            try:
                raise Exception(&quot;0&quot;)
            finally:
                denom = list_of_numbers[index]
                for i in range(len(list_of_numbers)):
                    list_of_numbers[i] /= denom
        except Exception as ex:
            print(ex)

If I call :

    fancy_divide([0, 2, 4], 0)

why does it not print out &#39;0&#39; ?

and if I edit the code like this :


    def fancy_divide(list_of_numbers, index):
        try:
            try:
                raise Exception(&quot;0&quot;)
            finally:
                denom = list_of_numbers[index]
                for i in range(len(list_of_numbers)):
                    list_of_numbers[i] /= denom
        except Exception as ex:
            raise Exception(&quot;0&quot;)
            print(ex)

and then call the same thing, it prints:

    Traceback (most recent call last):

      File &quot;&lt;ipython-input-16-c1b0ac98281c&gt;&quot;, line 1, in &lt;module&gt;
        fancy_divide([0, 2, 4], 0)

      File &quot;/Users/dsn/.spyder-py3/temp.py&quot;, line 10, in fancy_divide
        raise Exception(&quot;0&quot;)

    Exception: 0

Why is that ? And what is the right way to / when should I use raise?


||||||||||||||Your `finally` block is raising an exception itself, a divide-by-zero error (because your denominator is 0). If a `finally` block executes as an exception is bubbling, and raises an exception of its own, it either:

 1. On Python 2, replaces the existing exception
 2. On Python 3, it wraps the existing exception in the new exception (creating a chain of exceptions, where the outermost one is the one that is checked, but the inner exceptions exist for context)

Your other code prints the traceback because you don't catch the second exception you raise at all (and it bypasses your `print`).

I'd suggest [reading the exception tutorial](https://docs.python.org/3/tutorial/errors.html) to understand more; your example code is so contrived/pointless it's impossible to say what misunderstandings you really have, and what is just to illustrate a specific behavior to support your question.

--------------------------------------------------
getting the total count of search results in wordpress
How to get the total count of results, in the search results page in wordpress ... i think my question is clear .i need the total number of search results that displayed in  the search results page .And also need to find the count of results from page and post separately  
what i have tried is

        &lt;?php echo count($posts); ?&gt;



by using this i got the total number of search results . but i also need the count of pages and posts in the search results
||||||||||||||Try this code,

    $allsearch = new WP_Query("s=$s&showposts=0"); 
    echo $allsearch ->found_posts.' results found.';

Hope this will helps you.

For more please visit,

[Result Count in WordPress][1]

[Display Search Result Count][2]


  [1]: http://www.wpbeginner.com/wp-tutorials/display-search-term-and-result-count-in-wordpress/
  [2]: https://wordpress.stackexchange.com/questions/108865/display-search-result-count

--------------------------------------------------
Talking to a HID
I have a sensor developed by PNI Corp called the spacepoint-fusion. I need to interface with this device in C++ and continuously read new data from the device. 

When I plug the device into my computer, I see /dev/hidraw1 and /dev/hidraw2 show up. Also /dev/usb/hiddev0 shows up. 

My problem is that I have no idea how to read these devices. I can&#39;t find any examples or documentation online. I don&#39;t even know where to start with this. I have been looking at libhid and hiddev as possible solutions, but as of yet, I can&#39;t figure out how to use either of these libraries. 

So how do I read from this human interface device in c++ on a linux machine? Examples would be greatly appreciated. 
Thanks.
||||||||||||||You've got a complicated road ahead of you.  You will first need information about the "spacepoint-fusion" (hopefully it came with documentation).

Some initial values such as Product ID/Vendor ID can be gained with the terminal command:

    lsusb

Next, you will probably need to know the Endpoints, which could again be found using:

    lsusb -v

From this, you can find what addresses on the device can be written to, and what addresses can be read from (and possibly the size of the read/write buffers).  But this is as far as you can get without proper documents.  You will need to know what values to write to the device, and what values to expect back from the device.

Assuming you DO know what values to read/write from/to the device try and follow this example:

<https://web.archive.org/web/20130902234909/http://www.lvr.com/code/generic_hid.c>

I am also making the assumption that your device is HID compliant, which does not have to be the case at all.  Anyway, I wish you well on your USB journey.

--------------------------------------------------
Form pattern validation with react-hook-form
I have been working on a react form and I need to restrict users to put special characters and allow only these ones: [A-Za-z].

I have tried the below code but I am still able to insert inside special characters such as: &#39;♥&#39;, &#39;&gt;&#39;, &#39;+&#39;, etc. 

    export default Component (props {
      ...
      return (
       &lt;input 
        pattern={props.pattern}
       /&gt; 
      )
    }

And I am sending it as a prop to my form:

    &lt;Component 
    pattern=&quot;[A-Za-z]+&quot;
    /&gt;

Can you let me know what I am missing and point out what could be the issue? Many thanks.






||||||||||||||The `pattern` attribute on `input` only works on `submit` in vanilla HTML forms.

If you're using `react-hook-form`, it should be in the ref, like this:

    <input
        name="email"
        ref={register({
          required: "Required",
          pattern: {
            value: /^[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}$/i,
            message: "invalid email address"
          }
        })}
      />

please have a check on react-hook-form doc. 

--------------------------------------------------
Bitbucket Pipeline schedule trigger
I can&#39;t see anyone talking about what I&#39;m looking to do. I&#39;m currently running a pipeline on a branch merge within the bitbucket area.

      branches:
        staging:
          - step:
              name: Clone
              script:
                - echo &quot;Clone all the things!&quot; 

What I want to do is when a branch gets merged into master, trigger an event that will enable the schedule to run for the next day. 

If there are no changes I don&#39;t want anything to run, however, if there are I want the schedule to kick in and work.

I&#39;ve read through the Pipeline triggers:

https://support.atlassian.com/bitbucket-cloud/docs/pipeline-triggers/

But I can&#39;t see anywhere that would allow me to do it. Has anyone done this sort of thing? Is it possible, or am I limited by bitbucket itself?
||||||||||||||Never done this, but there's an API for creating schedules. I think you would need to determine the date and specify the single cron task, e.g. March 30, 2022 at midnight:
0 0 30 3 * 2022

However year is an extension, not a standard CRON field; "at" is an alternative that may be accessible (but also not standard). It all depends on what Bitbucket allows for CRON schedule, so I think this is not a conclusive answer (still needs info on how to setup the schedule).

Here is the docs 
https://developer.atlassian.com/bitbucket/api/2/reference/resource/repositories/%7Bworkspace%7D/%7Brepo_slug%7D/pipelines_config/schedules/

--------------------------------------------------
Problem with spring boot graphql. Request /graphql results with 404
I&#39;m trying to run simplest graphql example. I created application with spring initializer and only added graphql dependencies. My `build.gradle`

    buildscript {
    	ext {
    		springBootVersion = &#39;2.1.1.RELEASE&#39;
    	}
    	repositories {
    		mavenCentral()
    	}
    	dependencies {
    		classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}&quot;)
    	}
    }
    
    apply plugin: &#39;java&#39;
    apply plugin: &#39;eclipse&#39;
    apply plugin: &#39;org.springframework.boot&#39;
    apply plugin: &#39;io.spring.dependency-management&#39;
    
    group = &#39;com.example&#39;
    version = &#39;0.0.1-SNAPSHOT&#39;
    sourceCompatibility = 1.8
    
    repositories {
    	mavenCentral()
    }
    
    
    dependencies {
    	implementation(&#39;org.springframework.boot:spring-boot-starter-web&#39;)
    	testImplementation(&#39;org.springframework.boot:spring-boot-starter-test&#39;)
    
        compile &#39;com.graphql-java-kickstart:graphql-spring-boot-starter:5.3.1&#39;
        compile &#39;com.graphql-java-kickstart:graphiql-spring-boot-starter:5.3.1&#39;
        compile &#39;com.graphql-java-kickstart:voyager-spring-boot-starter:5.3.1&#39;
    }

DemoApplication.java

    package com.example.demo;
    
    import org.springframework.boot.SpringApplication;
    import org.springframework.boot.autoconfigure.SpringBootApplication;
    
    @SpringBootApplication
    public class DemoApplication {
    
    	public static void main(String[] args) {
    		SpringApplication.run(DemoApplication.class, args);
    	}
    }

When I run the project and hit the endpoint `/graphql` it returns `404`. What is missing in my configuration?
||||||||||||||The docs (https://github.com/graphql-java-kickstart/graphql-spring-boot#enable-graphql-servlet) say:

> The servlet becomes accessible at /graphql if graphql-spring-boot-starter added as a dependency to a boot application and a GraphQLSchema bean is present in the application.

...and the minimum example it links to looks like this:

    @SpringBootApplication
    public class ApplicationBootConfiguration {
    
        public static void main(String[] args) {
            SpringApplication.run(ApplicationBootConfiguration.class, args);
        }

        @Bean
        GraphQLSchema schema() {
            return GraphQLSchema.newSchema()
                .query(GraphQLObjectType.newObject()
                    .name("query")
                    .field(field -> field
                        .name("test")
                        .type(Scalars.GraphQLString)
                        .dataFetcher(environment -> "response")
                    )
                    .build())
                .build();
        }
    }

So you're missing a Graphql schema to be used. It says if there is one, the API endpoint will be exposed automatically.  
Good luck!

--------------------------------------------------
How do I convert the WebVTT format to plain text?
Here is a sample of WebVTT

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-html --&gt;

    WEBVTT
    Kind: captions
    Language: en
    Style:
    ::cue(c.colorCCCCCC) { color: rgb(204,204,204);
     }
    ::cue(c.colorE5E5E5) { color: rgb(229,229,229);
     }
    ##

    00:00:00.060 --&gt; 00:00:03.080 align:start position:0%
     
    &lt;c.colorE5E5E5&gt;okay&lt;00:00:00.690&gt;&lt;c&gt; so&lt;/c&gt;&lt;00:00:00.750&gt;&lt;c&gt; this&lt;/c&gt;&lt;00:00:01.319&gt;&lt;c&gt; is&lt;/c&gt;&lt;00:00:01.469&gt;&lt;c&gt; a&lt;/c&gt;&lt;/c&gt;&lt;c.colorCCCCCC&gt;&lt;00:00:01.500&gt;&lt;c&gt; newsflash&lt;/c&gt;&lt;00:00:02.040&gt;&lt;c&gt; page&lt;/c&gt;&lt;00:00:02.460&gt;&lt;c&gt; for&lt;/c&gt;&lt;/c&gt;

    00:00:03.080 --&gt; 00:00:03.090 align:start position:0%
    &lt;c.colorE5E5E5&gt;okay so this is a&lt;/c&gt;&lt;c.colorCCCCCC&gt; newsflash page for
     &lt;/c&gt;

    00:00:03.090 --&gt; 00:00:08.360 align:start position:0%
    &lt;c.colorE5E5E5&gt;okay so this is a&lt;/c&gt;&lt;c.colorCCCCCC&gt; newsflash page for&lt;/c&gt;
    &lt;c.colorE5E5E5&gt;Meraki&lt;00:00:03.659&gt;&lt;c&gt; printing&lt;/c&gt;&lt;00:00:05.120&gt;&lt;c&gt; so&lt;/c&gt;&lt;00:00:06.529&gt;&lt;c&gt; all&lt;/c&gt;&lt;00:00:07.529&gt;&lt;c&gt; we&lt;/c&gt;&lt;00:00:08.040&gt;&lt;c&gt; need&lt;/c&gt;&lt;00:00:08.130&gt;&lt;c&gt; to&lt;/c&gt;&lt;00:00:08.189&gt;&lt;c&gt; do&lt;/c&gt;&lt;/c&gt;

    00:00:08.360 --&gt; 00:00:08.370 align:start position:0%
    &lt;c.colorE5E5E5&gt;Meraki printing so all we need to do
     &lt;/c&gt;

    00:00:08.370 --&gt; 00:00:11.749 align:start position:0%
    &lt;c.colorE5E5E5&gt;Meraki printing so all we need to do
    here&lt;00:00:08.700&gt;&lt;c&gt; is&lt;/c&gt;&lt;00:00:08.820&gt;&lt;c&gt; to&lt;/c&gt;&lt;00:00:09.000&gt;&lt;c&gt; swap&lt;/c&gt;&lt;00:00:09.330&gt;&lt;c&gt; out&lt;/c&gt;&lt;00:00:09.480&gt;&lt;c&gt; the&lt;/c&gt;&lt;00:00:09.660&gt;&lt;c&gt; logo&lt;/c&gt;&lt;00:00:09.929&gt;&lt;c&gt; here&lt;/c&gt;&lt;00:00:10.650&gt;&lt;c&gt; and&lt;/c&gt;&lt;00:00:10.830&gt;&lt;c&gt; I&lt;/c&gt;&lt;/c&gt;

    00:00:11.749 --&gt; 00:00:11.759 align:start position:0%
    here is to swap out the logo here&lt;c.colorE5E5E5&gt; and I
     &lt;/c&gt;

    00:00:11.759 --&gt; 00:00:16.400 align:start position:0%
    here is to swap out the logo here&lt;c.colorE5E5E5&gt; and I
    should&lt;00:00:11.969&gt;&lt;c&gt; also&lt;/c&gt;&lt;00:00:12.120&gt;&lt;c&gt; work&lt;/c&gt;&lt;00:00:12.420&gt;&lt;c&gt; on&lt;/c&gt;&lt;00:00:12.630&gt;&lt;c&gt; move&lt;/c&gt;&lt;00:00:12.840&gt;&lt;c&gt; out&lt;/c&gt;&lt;00:00:13.049&gt;&lt;c&gt; as&lt;/c&gt;&lt;00:00:13.230&gt;&lt;c&gt; well&lt;/c&gt;&lt;00:00:15.410&gt;&lt;c&gt; and&lt;/c&gt;&lt;/c&gt;

    00:00:16.400 --&gt; 00:00:16.410 align:start position:0%
    &lt;c.colorE5E5E5&gt;should also work on move out as well and
     &lt;/c&gt;

&lt;!-- end snippet --&gt;

I used &lt;a href=&quot;https://github.com/rg3/youtube-dl&quot;&gt;youtube-dl&lt;/a&gt; to grab it from YouTube.

I want to convert this to plain text. I can&#39;t just strip out the times and colour tags as the text repeats itself .

So I&#39;m wondering if something exists to convert this to plain text or if there is some pseudo code someone could offer so I could code that up?

I have also posted an issue about this with &lt;a href=&quot;https://github.com/rg3/youtube-dl/issues/17178&quot;&gt;youtube-dl&lt;/a&gt;.
||||||||||||||I've used [WebVTT-py](https://webvtt-py.readthedocs.io/en/latest/) to extract the plain text transcription.

<!-- language: py -->
    import webvtt
    vtt = webvtt.read('subtitles.vtt')
    transcript = ""
    
    lines = []
    for line in vtt:
        # Strip the newlines from the end of the text.
        # Split the string if it has a newline in the middle
        # Add the lines to an array
        lines.extend(line.text.strip().splitlines())
    
    # Remove repeated lines
    previous = None
    for line in lines:
        if line == previous:
           continue
        transcript += " " + line
        previous = line
    
    print(transcript)

--------------------------------------------------
Why I&#39;m not able to unwrap and serialize a Java map using the Jackson Java library?
My bean looks like this:

    class MyBean {
    	
    	private @JsonUnwrapped HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();
    	
    	private String name;
    	
    	public HashMap&lt;String, String&gt; getMap() {
    		return map;
    	}
    
    	public void setMap(HashMap&lt;String, String&gt; map) {
    		this.map = map;
    	}
    
    	public String getName() {
    		return name;
    	}
    
    	public void setName(String name) {
    		this.name = name;
    	}
    }

While I&#39;m serializing the bean using the following code: 

    MyBean bean = new MyBean();
    HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();;
    map.put(&quot;key1&quot;, &quot;value1&quot;);
    map.put(&quot;key2&quot;, &quot;value2&quot;);
    bean.setMap(map);
    bean.setName(&quot;suren&quot;);
    ObjectMapper mapper = new ObjectMapper();
    System.out.println(&quot;\n&quot;+mapper.writeValueAsString(bean));

I&#39;m getting result like this:

    {&quot;map&quot;:{&quot;key2&quot;:&quot;value2&quot;,&quot;key1&quot;:&quot;value1&quot;},&quot;name&quot;:&quot;suren&quot;}

but

    {&quot;key2&quot;:&quot;value2&quot;,&quot;key1&quot;:&quot;value1&quot;,&quot;name&quot;:&quot;suren&quot;}

is expected per the [JacksonFeatureUnwrapping documentation](http://wiki.fasterxml.com/JacksonFeatureUnwrapping). Why am I not getting the unwrapped result?
||||||||||||||`@JsonUnwrapped` doesn't work for maps, only for proper POJOs with getters and setters. For maps, You should use [`@JsonAnyGetter`][1] and [`@JsonAnySetter`][2] (available in jackson version >= 1.6).

In your case, try this:

    @JsonAnySetter 
    public void add(String key, String value) {
        map.put(key, value);
    }
    
    @JsonAnyGetter
    public Map<String,String> getMap() {
        return map;
    }

That way, you can also directly add properties to the map, like `add('abc','xyz')` will add a new key `abc` to the map with value `xyz`. 

  [1]: http://jackson.codehaus.org/1.6.0/javadoc/org/codehaus/jackson/annotate/JsonAnyGetter.html
  [2]: http://jackson.codehaus.org/1.6.0/javadoc/org/codehaus/jackson/annotate/JsonAnySetter.html

--------------------------------------------------
Call methods from Swift initializer
Let&#39;s say I have the following class in Swift (which has obvious problems)

    class MyClass {
        let myProperty: String

        init() {
            super.init()
            self.setupMyProperty()
        }

        func setupMyProperty() {
            myProperty = &quot;x&quot;
        }
    }

This is overly simplified but I&#39;m basically trying to delegate the initialization of `myProperty` into the `setupMyProperty()` method. It&#39;s a pattern I use often to break down the different parts of the setup of a class.

But of course, I can&#39;t call `self` until the super initializer has run, and I can&#39;t run the super initializer until all the properties have been set, so I&#39;m in a catch 22. On top of it since `setupMyProperty()` isn&#39;t considered an initializer, it won&#39;t be able to assign `myProperty` anyway.

Can anyone tell me how to implement this pattern in Swift?

||||||||||||||declare it as an implicitly unwrapped optional

    class MyClass : NSObject {
        var myProperty: String!
        
        init() {
            super.init()
            self.setupMyProperty()
        }
        
        func setupMyProperty() {
            self.myProperty = "x"
        }
    }

Source: [Swift documentation, Unowned References and Implicitly Unwrapped Optional Properties](https://docs.swift.org/swift-book/documentation/the-swift-programming-language/automaticreferencecounting/#Unowned-References-and-Implicitly-Unwrapped-Optional-Properties)

--------------------------------------------------
How to get full range of onEdit(e)?
How to get the whole range of the onEdit(e) by Google Sheets script? I have tried the following script:

    var sheet = spreadsheet.getActiveSheet();
    function onEdit(e){
       var row = e.range.getRow();
       var column = e.range.getColumn();
       sheet.getRange(&#39;A1&#39;).setNote(range.getRow() + &#39;,&#39; + range.getColumn());
    }
Well, it works if I edit only one cell. For example, if I edit cell B2, then it will set the note of cell A1 to be &quot;2,2&quot;. But what if I edit multiple cells simultaneously? For example, suppose the values of range(&#39;A2:B3&#39;) are all non-empty (say, all &#39;abc&#39;) and I copy range(&#39;A2:B3&#39;) and paste it to range(&#39;A10:B11&#39;). Then, the note of cell A1 will only show &quot;10,1&quot; (i.e., row and column of cell A10). How can I show the WHOLE RANGE (instead of merely the top left cell), for example, &quot;10,1,2,2&quot; or the rows and columns of &#39;A10&#39; and &#39;B11&#39;? Thank you!
||||||||||||||    var sheet = SpreadsheetApp.getActiveSheet();
    function onEdit(e){
       var row = e.range.rowStart;
       var column = e.range.columnStart;
       var endRow = e.range.rowEnd;
       var colEnd = e.range.columnEnd;
       sheet.getRange('A1').setNote(row + ',' + column+','+endRow+','+colEnd);
    }

You can use these variables to get the whole edited range

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/kaVWX.png

--------------------------------------------------
How many number of comparisons are required to find an element in an unordered list that is neither maximum nor minimum?
If we have an unordered list has n distinct elements so Now I am confused between Time complexity to be ϴ(n) or ϴ(1) , since in worst case we may end up making n comparisons ,but then if I take 3 elements at a time then I can find the 2nd largest element in ϴ(1) time, so I am confused with these two approaches ,please guide .
||||||||||||||It's pretty simple:

The first approach has a lot of completely useless overhead. Since the list only contains distinct items one of the first three items **must** fulfil the constraint of being neither the largest nor smallest element of the list. All other comparisons are completely useless for the purpose of finding an **arbitrary** element matching the constraints.

--------------------------------------------------
AttributeError: &#39;Adam&#39; object has no attribute &#39;get_updates&#39;
I&#39;m training a VAE with TensorFlow Keras backend and I&#39;m using Adam as the optimizer. the code I used is attached below.

        def compile(self, learning_rate=0.0001):
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        self.model.compile(optimizer=optimizer,
                           loss=self._calculate_combined_loss,
                           metrics=[_calculate_reconstruction_loss,
                                    calculate_kl_loss(self)])

The TensorFlow version I&#39;m using is 2.11.0. The error I&#39;m getting is

    AttributeError: &#39;Adam&#39; object has no attribute &#39;get_updates&#39;

I&#39;m suspecting the issues arise because of the version mismatch. Can someone please help me to sort out the issue? Thanks in advance.
||||||||||||||Try replacing your 2nd line 
"optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
by 
"optimizer = tf.keras.optimizers.**legacy**.Adam(learning_rate=learning_rate)"

For further information, check tf 2.11.0 Release 11/28/2022 in https://github.com/tensorflow/tensorflow/releases 
It indicates in particular that: 
"The tf.keras.optimizers.Optimizer base class now points to the new Keras optimizer, while the old optimizers have been moved to the tf.keras.optimizers.legacy namespace."


--------------------------------------------------
Git clone / pull continually freezing at &quot;Store key in cache?&quot;
I&#39;m attempting to clone a repo from my BitBucket account to my Windows 10 laptop (running GitBash). I&#39;ve completed all of the steps necessary to connect (set up my SSH key, verified by successfully SSHing git@bitbucket.org, etc). However, whenever I attempt to clone a repo, the prompt continually hangs up after confirming that I want to cache Bitbucket&#39;s key.  

    User@Laptop MINGW64 /C/Repos
    $ git clone git@bitbucket.org:mygbid/test.git
    Cloning into &#39;test&#39;...
    The server&#39;s host key is not cached in the registry. You
    have no guarantee that the server is the computer you
    think it is.
    The server&#39;s rsa2 key fingerprint is:
    ssh-rsa 2048 97:8c:1b:f2:6f:14:6b:5c:3b:ec:aa:46:46:74:7c:40
    If you trust this host, enter &quot;y&quot; to add the key to
    PuTTY&#39;s cache and carry on connecting.
    If you want to carry on connecting just once, without
    adding the key to the cache, enter &quot;n&quot;.
    If you do not trust this host, press Return to abandon the
    connection.
    Store key in cache? (y/n) y

No files are cloned, and the result is an empty repo. Trying to initiate a git pull origin master from this repo also asks to cache the key, then hangs with no feedback. Despite not asking for the key to be cached when I do a test SSH, git operations always ask for the key every time before failing.

With no error messages to work with, I&#39;m really at a loss as to what is wrong. I&#39;ve tried multiple repos, including very small ones, with no success at all.
||||||||||||||I had this problem when cloning a repo on Windows 10 too. 

I got around it by using the Putty GUI to SSH to the server in question (in your case: bitbucket.org) then clicked 'Yes' when the prompt asks if you want to save the server key to the cache. Running the clone command again then worked for me!

--------------------------------------------------
Can I type an object in such a way to allow only its keys as its value?
Simplified problem: let&#39;s say we have a graph and that each node has its unique name. If we were to have a type that describes graph edges as a map from name to list of node names you could come to from that node, how would we do that?

Logically, that would be something like:

    type EdgesT = { [name: string]: Exclude&lt;keyof EdgesT, name&gt;[]; };

But I just can&#39;t get that description working, even without exclude part, as it always sees keyof EdgesT as string.

Alternatively, if the same problem was for a function argument, we could do something like:

    type BuildEdges&lt;T&gt; = { [Name in keyof T]: Exclude&lt;keyof T, Name&gt;[]; };
    
    const fn = &lt;T&gt;(edges: BuildEdges&lt;T&gt;) =&gt; null;

And that works, for that explicit case. But if we were to get more info out of type argument (for something else), we couldn&#39;t. For type argument, `&lt;T&gt;` works, `&lt;T extends Record&lt;string, any&gt;&gt;` works, `&lt;T extends Record&lt;string, unknown&gt;&gt;` works, but anything else does not. For example, `&lt;T extends Record&lt;string, number&gt;&gt;` does not work - does anyone has any idea why?

Any idea about working around this situation, achieving a bit more, or describing this limitation?

||||||||||||||There is no specific type in TypeScript that works this way. It is fundamentally [generic](https://www.typescriptlang.org/docs/handbook/2/generics.html). You can't represent it as a large [union](https://www.typescriptlang.org/docs/handbook/2/everyday-types.html#union-types) of `BuildEdges<T>` for every possible `T`.  Conceptually it would be a special kind of generic type called an *[existentially quantified](https://en.wikipedia.org/wiki/Type_system#Existential_types) generic*... you want to say "a value is a `BuildEges` if there *exists* some `T` that works". But TypeScript doesn't directly support existential types (although there is an open feature request for them as [microsoft/TypeScript#14466](https://github.com/microsoft/TypeScript/issues/14466#)). In an alternate universe where that was implemented, maybe you could write

    // THIS IS NOT VALID TYPESCRIPT, DO NOT TRY:
    type BuildEdges = <exists T>{ [K in keyof T]: Exclude<keyof T, K> []; }

But for now you need to do something else.  

---

There are ways to encode existential types in TypeScript, but they end up being more complicated than I think you need here.  Instead, you could just leave it as a regular generic type like

    type BuildEdges<T> = { [K in keyof T]: Exclude<keyof T, K>[]; };

And then write a helper function to infer `T` given the value.  This was your approach anyway, but it seems you didn't find a good [constraint](https://www.typescriptlang.org/docs/handbook/2/generics.html#generic-constraints).  Here's how I'd do it:

    const fn = <T extends BuildEdges<T>>(edges: T) => null;
    
That's a recursive constraint. It essentially validates that `T` is a proper `BuildEdges<T>`, leading to the following desirable behavior:


    fn({ a: ["c"], b: ["a"], c: ["a", "b"] }); // okay
    fn({ a: ["c"], b: ["a"], c: ["a", "c"] }); // error!

[Playground link to code](https://www.typescriptlang.org/play?ts=5.3.3#code/PQKgUALgngDgpgAgEIFcCWAbAJgUSwczgGcEBeBAHjgA80iISAVAPgG8EBtAaQTQDsEAazhQA9gDMEjALoAuBDmoBjDCixwKwsZMYAaBF2adpAbgQBfMCGCRYiVJlwFiFFmQTtuvAVolS5Csqq6poifnoGzBymFiZgYEqifPQI4gLkrgg0EHB8WCQO2HiERK7MzAAUcM5E8owAlGRGfCgYGHFgaRXsAIbyHABESgPS+gBG-QM9I-pKk9P6A2MjFvVmwMAIooI9UJ183Qh9nEMzCBMn06MIc5cDi8PSq+ubcABOb6JvAIRgQA)

--------------------------------------------------
Select first row of each group by group in SQL Server
I would like to get the first row of each group with the smallest &quot;differenz&quot; value of the group like I tried here. How can I achieve that?

This is my query


```
SELECT 
    ae.vpid AS aeVpid,
    ae.EreignisDat AS aeEreignisDat, 
    be.StichtagDat AS beStichtagDat,
    ABS(DATEDIFF(day, ae.EreignisDat, be.StichtagDat)) AS differenz
FROM 
    AEGE AS ae 
LEFT OUTER JOIN
    BEST AS be ON ae.VPID = be.VPID
WHERE 
    ae.EreignisDat &lt;&gt; be.StichtagDat
ORDER BY
    aevpid, differenz ASC, beStichtagDat DESC
```

I get this result:

![this](https://i.stack.imgur.com/6xr7q.png)

where &quot;beStichtagDat&quot; is variable and depending on the first two attributes &quot;aeVpid&quot; and &quot;aeEreignisDat&quot;

I tried some group by&#39;s on some attributes and subqueries with a `min()` on &quot;differenz&quot; and I also tried the `row_number()` over the groups but I could not manage to get it done
||||||||||||||`row_number()` + `partition by` is probably the way to go here, but this is still incomplete because the question doesn't show anything about how to split up the groups:

```
SELECT  aeVpid, aeEreignisDat, beStichtagDat, differenz
FROM (
    select
        ae.vpid as aeVpid,
        ae.EreignisDat as aeEreignisDat, 
        be.StichtagDat as beStichtagDat,
        abs(datediff(day, ae.EreignisDat, be.StichtagDat)) as differenz,
        row_number() over (partition by ???? order by abs(datediff(day, ae.EreignisDat, be.StichtagDat))) rn

    from AEGE as ae 
    left join BEST as be on ae.VPID = be.VPID
    where ae.EreignisDat <> be.StichtagDat
) t
WHERE rn = 1
ORDER BY aevpid, differenz, beStichtagDat DESC
```

--------------------------------------------------
Quarkus Mutiny Multi and Uni memory leaks and io.vertx.core.VertxException: Thread blocked
I have this problem and have been stuck with it for a week now

I&#39;m querying a huge amount of data (millions) from a DB, and then breaks them into partition using `Multi` and for each of them, process using `Uni`

My main function code looks roughly something like this:
```java
public Uni&lt;Void&gt;processData(Request request) {
    AtomicReference&lt;List&lt;Item&gt;&gt; listItemRef = new AtomicReference&lt;&gt;();
    return getAllMatchingItem(request)
            .onItem().transformToMulti(result -&gt; Multi.createFrom().iterable(ListUtils.partition(result, 10000)))
            .onItem().transformToUni(resultBatched -&gt; Uni.createFrom().item(() -&gt; resultBatched)
                    .chain(resultBatched -&gt; {
                        listItemRef.setRelease(resultBatched);
                        return getOtherItemBasedOnItem(resultBatched);
                    })
                    .chain(otherItems -&gt; {
                        List&lt;Item&gt; items = listItemRef.getPlain();
                        return createProcessedItems(items, otherItems);
                    })
                    .chain(processedItems -&gt; {
                        return insertProcessedItems(processedItems);
                    })
                    .onTermination().invoke(() -&gt; {
                        LOGGER.info(&quot;Data process success&quot;);
                    })
                    .replaceWithVoid() // to make sure no remaining reference to upstream Unis
            )
            .merge(1)
            .collect().asList()
            .replaceWithVoid()
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}

```

and the supporting functions are roughly like this:

```java
public Uni&lt;List&lt;Item&gt;&gt; getAllMatchingItem(Request request) {
    // get data to DB, basically PanacheRepository.find(...).list
    return Uni.createFrom().item(() -&gt; repo.find(&quot; ... &quot;).list)
            .runSubscriptionOn(Infrastructure.getDefaultWorkerPool())
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}

public Uni&lt;List&lt;OtherItem&gt;&gt; getOtherItemBasedOnItem(List&lt;Item&gt; items) {
    // get data to DB, basically PanacheRepository.find(id in items.getIdSet).list()
    return Uni.createFrom().item(() -&gt; repo.find(&quot;id in ?1&quot;, items.getIDSet()).list)
            .runSubscriptionOn(Infrastructure.getDefaultWorkerPool())
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}

public Uni&lt;List&lt;ProcessedItem&gt;&gt; createProcessedItems(List&lt;Item&gt; items, List&lt;OtherItem&gt; otherItems) {
    return Uni.createFrom().item(() -&gt; doCreateProcessedItems(item, otherItems))
            .runSubscriptionOn(Infrastructure.getDefaultWorkerPool())
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}

public List&lt;ProcessedItem&gt; doCreateProcessedItems(List&lt;Item&gt; items, List&lt;OtherItem&gt; otherItems) {
    // process those items
    // basically create Map, create list, etc
    // no external function call, just plain code
}

public Uni&lt;Void&gt; insertProcessedItems(List&lt;ProcessedItem&gt; processedItems) {
    // basically just insert/update
    return Uni.createFrom().voidItem()
            .runSubscriptionOn(Infrastructure.getDefaultWorkerPool())
            .invoke(() -&gt; repo.persist(processedItems))
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}
```

I suspect there are memory leaks because the log `Data process success` appears a few times before getting this exception
```
java.lang.OutOfMemoryError: Java heap space
```

I also getting this warning:
```
(vertx-blocked-thread-checker) Thread Thread[vert.x-eventloop-thread-0,5,main] has been blocked for 3203 ms, time limit is 2000 ms: io.vertx.core.VertxException: Thread blocked
2024-02-03 21:20:10,152 WARN [io.ver.cor.imp.BlockedThreadChecker] (vertx-blocked-thread-checker) Thread Thread[vert.x-eventloop-thread-0,5,main] has been blocked for 3203 ms, time limit is 2000 ms: io.vertx.core.VertxException: Thread blocked
```

I appreciate any clue I could get on how to solve this complex issue

Note: I&#39;m using Quarkus version 2.16 if that matters
||||||||||||||A Vert.x thread block checker warning means that you are doing some blocking I/O and/or long-running work on an event-loop thread.

From what I see in your code I suspect that you are doing calls to Hibernate (not Reactive) and then put data from memory into reactive pipelines, eventually collecting everything as a list (`.collect().asList()`) does just that.

Reactive is great if you do it properly end-to-end with non-blocking I/O. If that's not the case I'd recommend rewriting the code as plain imperative code. 

--------------------------------------------------
how do I refresh a HierarchicalDataSource for a Kendo TreeView?
TreeView creation:

    function CreateNotificationTree(userId)
    {
        debugger;
        var data = new kendo.data.HierarchicalDataSource({
            transport: {
                read: {
                    url: &quot;../api/notifications/byuserid/&quot; + userId,
                    contentType: &quot;application/json&quot;
                }
            },
            schema: {
                model: {
                    children: &quot;notifications&quot;
                }
            }
        });
    
        $(&quot;#treeview&quot;).kendoTreeView({
            dataSource: data,
            loadOnDemand: true,
            dataUrlField: &quot;LinksTo&quot;,
            checkboxes: {
                checkChildren: true
            },
            dataTextField: [&quot;notificationType&quot;, &quot;NotificationDesc&quot;],
            select: treeviewSelect
        });
    
        function treeviewSelect(e)
        {
            var node = this.dataItem(e.node);
            window.open(node.NotificationLink, &quot;_self&quot;);
        }
    }

Where things get updated and I need to refresh the dataSet:

    $(&#39;#btnDelete&#39;).on(&#39;click&#39;, function()
    {
        var treeView = $(&quot;#treeview&quot;).data(&quot;kendoTreeView&quot;);
        var userId = $(&#39;#user_id&#39;).val();
    
        $(&#39;#treeview&#39;).find(&#39;input:checkbox:checked&#39;).each(function()
        {
            debugger;
            var li = $(this).closest(&quot;.k-item&quot;)[0];
            var notificationId = treeView.dataSource.getByUid(li.getAttribute(&#39;data-uid&#39;)).ID;
    
            if (notificationId == &quot;undefined&quot;)
            {
                alert(&#39;No ID was found for one or more notifications selected. These notifications will not be deleted. Please contact IT about this issue.&#39;);
            }
            else
            {
                $.ajax(
                    {
                        url: &#39;../api/notifications/deleteNotification?userId=&#39; + userId + &#39;&amp;notificationId=&#39; + notificationId,
                        type: &#39;DELETE&#39;,
                        success: function()
                        {
                            alert(&#39;Delete successful.&#39;);
                            //Here is where I try to refresh the data source.
                            CreateNotificationTree(userId);
                        },
                        failure: function()
                        {
                            alert(&#39;Delete failed.&#39;);
                        }
                    });
                treeView.remove($(this).closest(&#39;.k-item&#39;));
            }
        });
    });

The problem here is that it does refresh the tree view.... BUT NOT the CHILDREN nodes...

anyone know how to get this working?
||||||||||||||It looks like you are completely rebuilding the tree view. Any reason why you don't just refresh the tree view's data source?  

Given your code above, I would recommend this:

    treeView.dataSource.read();

Also, depending on what type of server you are getting the JSON from, it may be allowing the browser to cache the results, as Kendo data sources default to using GET statements.  This could be fixed on the server side, or you could switch to using a POST to retrieve the data:

    read: {
        url: "../api/notifications/byuserid/" + userId,
        contentType: "application/json",
        type: "POST" // Fixes issue if browser was caching GET requests
    }

--------------------------------------------------
Is it possible to overlay a marker on top of a plotly.js box plot?
Suppose I&#39;m using the [simple box plot example](https://plot.ly/javascript/box-plots/#box-plot-that-displays-the-underlying-data) in plotly&#39;s documentation:

[![simple box plot example][1]][1]

&lt;!-- language: lang-js --&gt;

    var data = [
      {
        y: [0, 1, 1, 2, 3, 5, 8, 13, 21],
        boxpoints: &#39;all&#39;,
        jitter: 0.3,
        pointpos: -1.8,
        type: &#39;box&#39;
      }
    ];
    
    Plotly.newPlot(&#39;myDiv&#39;, data);

I want to overlay a marker on top of the underlying data scatter plot that&#39;s to the left of the box plot. This marker would have its own hover text and everything. This is how I envision this looking:

[![sample with marker][2]][2]

Is there a way to do this in plotly? I&#39;ve looked all over for an example of this, and I can&#39;t find anything that looks relevant. Thanks!

  [1]: https://i.stack.imgur.com/8On0u.png
  [2]: https://i.stack.imgur.com/3buNI.png
||||||||||||||If you are plotting your points on top of the box plot (`pointpos = 0`) you can add another trace with an x value which is identical to your boxplot name, `trace 0` in this case. 

If you are plotting your points next to your boxplot, it becomes a lot more tricky because the scatter points do not have defined x-values on the axis.

You could your new point manually but then the hover info is still in the old position.

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    var data = [{
        y: [0, 1, 1, 2, 3, 5, 8, 13, 21],
        boxpoints: 'all',
        jitter: 0.3,
        pointpos: 0,
        type: 'box'
      },
      {
        y: [0, 1, 1, 2, 3, 5, 8, 13, 21],
        boxpoints: 'all',
        jitter: 0.3,
        pointpos: 1.8,
        type: 'box'
      },
      {
        x: ['trace 0'],
        y: [18],
        name: 'My special marker',
        text: 'Some really interesting hover info',
        marker: {
          size: 20
        }
      },
      {
        x: ['trace 1'],
        y: [18],
        name: 'Another special marker',
        text: 'Some really interesting hover info',
        marker: {
          size: 20
        }
      }
    ];

    Plotly.newPlot('myDiv', data);
    var boxPoint = document.getElementsByClassName('trace boxes')[1].getElementsByClassName('point')[0];
    var point = document.getElementsByClassName('scatterlayer')[0].getElementsByClassName('point')[1];
    var y = point.attributes['transform'].value.split(',')[1];
    var x = boxPoint.attributes['transform'].value.split(',')[0];
    point.setAttribute('transform', x + ', ' + y);

<!-- language: lang-html -->

    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <div id="myDiv"></div>

<!-- end snippet -->



--------------------------------------------------
React native navigation to second page
I want to create a basic `ScrollView` in which the first page will be visible and it will take the full height of the screen.
When user clicks my button I want to scroll to the second page which will also occupy the full height of the `ScrollView`
What is the best way to achieve this?
||||||||||||||To achieve a full-screen, scrollable interface where each page takes the full height of the screen, and you can navigate between pages by pressing a button, you can utilize a combination of FlatList and ScrollView. Here's a step-by-step guide:

**1. Define Your Pages:** First, outline the structure of each page you want to display. This can be done using an array of objects, each representing a page:

```
const STEPS = [
  {
    id: 1,
    title: 'Screen1 Title',
    component: <Screen1 />,
  },
  {
    id: 2,
    title: 'Screen2 Title',
    component: <Screen2 />,
  },
  // Add more pages as needed
];

```

**2. Setup FlatList:** Use a FlatList to handle the horizontal paging between each full-screen view. Ensure the FlatList is configured to snap to each page and disable scroll indicators for a cleaner look:
```
import React, { useRef, useState } from 'react';
import { Dimensions, FlatList, ScrollView, StyleSheet } from 'react-native';

const WIDTH = Dimensions.get('window').width;

const YourComponent = () => {
  const ref = useRef();
  const [currentSlideIndex, setCurrentSlideIndex] = useState(0);

  // Function to navigate to the next page
  const goToNextSlide = () => {
    const nextSlideIndex = currentSlideIndex + 1;
    if (nextSlideIndex < STEPS.length) { // Ensure we don't exceed our page array
      const offset = nextSlideIndex * WIDTH;
      ref?.current?.scrollToOffset({ offset, animated: true });
      setCurrentSlideIndex(nextSlideIndex);
    }
  };

  return (
    <FlatList
      ref={ref}
      data={STEPS}
      keyExtractor={item => item.id.toString()}
      snapToAlignment="start"
      decelerationRate="fast"
      snapToInterval={WIDTH}
      showsHorizontalScrollIndicator={false}
      horizontal
      bounces={false}
      renderItem={({ item }) => (
        <ScrollView
          style={styles.container}
          showsVerticalScrollIndicator={false}>
          {/* Conditional rendering based on current slide index */}
          {currentSlideIndex === item.id - 1 && item.component}
        </ScrollView>
      )}
      scrollEnabled={false} // Disable FlatList scroll to control it via button
    />
  );
};

const styles = StyleSheet.create({
  container: {
    width: WIDTH,
    height: '100%', // Ensure full height
  },
});

```
The **goToNextSlide** function is triggered by a button (not shown in the snippet) to navigate to the next page. Remember to replace `<Screen1 />` and `<Screen2 />` with your actual component content.

This approach provides a smooth, app-like paging experience that can be easily controlled programmatically and is highly customizable to fit your specific needs.


--------------------------------------------------
Ansible | Add line &amp; update /etc/hosts when outdated
I want to insert line or update my `/etc/hosts` if there is new data. 
My following playbook is using `lineinfile`, however, I experienced issue that when one of my servers got new IP, the module would basically add new line containing the new IP, which is good, but the old one would still be present.

I&#39;d like to do it as neat as possible, hopefully just use 1 module, not the combination of `lineinfile` &amp; `replace`. 



Example of existing playbook
```
- ansible.builtin.lineinfile:
    path: /etc/hosts
    line: &quot;{{ hostvars[item].ansible_all_ipv4_addresses.0 }} {{ hostvars[item].ansible_hostname }} {{ hostvars[item].ansible_hostname }}&quot;
  loop: &quot;{{ groups[&#39;web&#39;] }}&quot;
```

So... I basiaclly end up with:

*/etc/hosts*
```
192.168.1.1 web1.example.com web1
192.168.1.2 web2.example.com web2
192.168.1.3 web3.example.com web3
```

**Provisioned new web2:**
```
192.168.1.1 web1.example.com web1
192.168.1.2 web2.example.com web2
192.168.1.3 web3.example.com web3
192.168.1.4 web2.example.com web2
```
||||||||||||||You need to use the [`regexp` option](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/lineinfile_module.html#parameter-regexp) to tell `lineinfile` which lines to replace. That might look something like this:

```yaml
- hosts: web
  gather_facts: true

- hosts: localhost
  gather_facts: false
  tasks:
    - ansible.builtin.lineinfile:
        path: hosts
        line: "{{ hostvars[item].ansible_all_ipv4_addresses.0 }} {{ hostvars[item].ansible_fqdn }} {{ hostvars[item].ansible_hostname }}"
        regexp: "{{ hostvars[item].ansible_fqdn }}"
      loop: "{{ groups['web'] }}"
```

In my test environment, running it the first time produces:

```
172.17.0.4 web1.example.com web1
172.17.0.3 web2.example.com web2
172.17.0.2 web3.example.com web3
```

And if I update the ip address of `web2`, I get:

```
172.17.0.4 web1.example.com web1
172.17.0.5 web2.example.com web2
172.17.0.2 web3.example.com web3
```

Ansible has replaced the line for `web2.example.com`, rather than appending a new line.

--------------------------------------------------
Populating a dynamic table based on drop down selections?
I have raw data in columns A:E, I want to bring it over to columns H2:K dynamically based on whatever values are f2 and g2. If f2 = column D name then bring over the data for that row highlighted in columns H:K

[![enter image description here][1]][1]

If I were to do this in excel, I&#39;d do:

    =let(
    _lastrow,match(2/1(A:A&lt;&gt;&quot;&quot;),
    _Product,A2:INDEX(A:A,),_lastrow),
    _Comments,B2:INDEX(B:B),_lastrow),
    _Criteria1,D2:INDEX(D:D),_lastrow),
    _Criteria2,E2:INDEX(E:E),_lastrow),
    _Hstack,CHOOSE({1,2,3,4},_Product,_Comments,_Criteria1,_Criteria2),
    FILTER(_Hstack,(_Criteria1=F2)*(_Criteria2=G2)))

but it&#39;s not working in google sheets. I know function syntax is a little different, but has anyone figured this out? 

Link to sheet: 

  [1]: https://i.stack.imgur.com/A2krb.png
||||||||||||||You can use the [`FILTER`](https://support.google.com/docs/answer/3093197?hl=en) function 

```cpp
=FILTER({A:A,C:E},D:D=F2,E:E=G2)
```

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/QoM7X.png

--------------------------------------------------
Convert numbers to English strings
Websites like http://www.easysurf.cc/cnvert18.htm and http://www.calculatorsoup.com/calculators/conversions/numberstowords.php tries to convert a numerical string into an english strings, but they are giving natural sounding output.

For example, on http://www.easysurf.cc/cnvert18.htm:

    [in]: 100456
    [out]:  one hundred  thousand four hundred fifty-six

this website is a little better, http://www.calculator.org/calculate-online/mathematics/text-number.aspx:

    [in]: 100456
    [out]: one hundred thousand, four hundred and fifty-six
    
    [in]: 10123124001
    [out]: ten billion, one hundred and twenty-three million, one hundred and twenty-four thousand, one 


but it breaks at some point:

    [in]: 10000000001
    [out]: ten billion, , , one 

I&#39;ve wrote my own version but it involves lots of rules and it caps at one billion, from http://pastebin.com/WwFCjYtt:

    import codecs
     
    def num2word (num):
      ones = {1:&quot;one&quot;,2:&quot;two&quot;,3:&quot;three&quot;,4:&quot;four&quot;,
              5:&quot;five&quot;,6:&quot;six&quot;,7:&quot;seven&quot;,8:&quot;eight&quot;,
              9:&quot;nine&quot;,0:&quot;zero&quot;,10:&quot;ten&quot;}
      teens = {11:&quot;eleven&quot;,12:&quot;twelve&quot;,13:&quot;thirteen&quot;,
               14:&quot;fourteen&quot;,15:&quot;fifteen&quot;}
      tens = {2:&quot;twenty&quot;,3:&quot;thirty&quot;,4:&quot;forty&quot;,
              5:&quot;fifty&quot;,6:&quot;sixty&quot;,7:&quot;seventy&quot;,
              8:&quot;eighty&quot;,9:&quot;ninety&quot;}
      lens = {3:&quot;hundred&quot;,4:&quot;thousand&quot;,6:&quot;hundred&quot;,7:&quot;million&quot;,
              8:&quot;million&quot;, 9:&quot;million&quot;,10:&quot;billion&quot;#,13:&quot;trillion&quot;,11:&quot;googol&quot;,
              }
     
      if num &gt; 999999999:
        return &quot;Number more than 1 billion&quot;
     
      # Ones
      if num &lt; 11:
        return ones[num]
      # Teens
      if num &lt; 20:
        word = ones[num%10] + &quot;teen&quot; if num &gt; 15 else teens[num]
        return word
      # Tens
      if num &gt; 19 and num &lt; 100:
        word = tens[int(str(num)[0])]
        if str(num)[1] == &quot;0&quot;:
          return word
        else:
          word = word + &quot; &quot; + ones[num%10]
          return word
     
      # First digit for thousands,hundred-thousands.
      if len(str(num)) in lens and len(str(num)) != 3:
        word = ones[int(str(num)[0])] + &quot; &quot; + lens[len(str(num))]
      else:
        word = &quot;&quot;
       
      # Hundred to Million  
      if num &lt; 1000000:
        # First and Second digit for ten thousands.  
        if len(str(num)) == 5:
          word = num2word(int(str(num)[0:2])) + &quot; thousand&quot;
        # How many hundred-thousand(s).
        if len(str(num)) == 6:
          word = word + &quot; &quot; + num2word(int(str(num)[1:3])) + \
                &quot; &quot; + lens[len(str(num))-2]
        # How many hundred(s)?
        thousand_pt = len(str(num)) - 3
        word = word + &quot; &quot; + ones[int(str(num)[thousand_pt])] + \
                &quot; &quot; + lens[len(str(num))-thousand_pt]
        # Last 2 digits.
        last2 = num2word(int(str(num)[-2:]))
        if last2 != &quot;zero&quot;:
          word = word + &quot; and &quot; + last2
        word = word.replace(&quot; zero hundred&quot;,&quot;&quot;)
        return word.strip()
     
      left, right = &#39;&#39;,&#39;&#39;  
      # Less than 1 million.
      if num &lt; 100000000:
        left = num2word(int(str(num)[:-6])) + &quot; &quot; + lens[len(str(num))]
        right = num2word(int(str(num)[-6:]))
      # From 1 million to 1 billion.
      if num &gt; 100000000 and num &lt; 1000000000:
        left = num2word(int(str(num)[:3])) +  &quot; &quot; + lens[len(str(num))]
        right = num2word(int(str(num)[-6:]))
      if int(str(num)[-6:]) &lt; 100:
        word = left + &quot; and &quot; + right
      else:  
        word = left + &quot; &quot; + right
      word = word.replace(&quot; zero hundred&quot;,&quot;&quot;).replace(&quot; zero thousand&quot;,&quot; thousand&quot;)
      return word
     
    print num2word(int(raw_input(&quot;Give me a number:\n&quot;)))

**How can I make the script i&#39;ve wrote accept `&gt; billion`?**

**Is there any other way to get the same output?**

**Can my code be written in a less verbose way?**
||||||||||||||A more general approach to this problem uses repeated division (i.e. `divmod`) and only hardcodes the special/edge cases necessary.

For example, `divmod(1034393, 1000000) -> (1, 34393)`, so you've effectively found the number of millions and are left with a remainder for further calculations.

Possibly more illustrative example: `divmod(1034393, 1000) -> (1034, 393)` which allows you to take off groups of 3 decimal digits at a time from the right.

In English we tend to group digits in threes, and similar rules apply. This should be parameterized and not hard coded. For example, "303" could be three hundred and three million, three hundred and three thousand, or three hundred and three. The logic should be the same except for the suffix, depending on what place you're in. Edit: looks like this is sort of there due to recursion.

Here is a partial example of the kind of approach I mean, using a generator and operating on integers rather than doing lots of `int(str(i)[..])` everywhere.

    say_base = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven',
        'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen',
        'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen']
    
    say_tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy',
        'eighty', 'ninety']
    
    def hundreds_i(num):
        hundreds, rest = divmod(num, 100)
        if hundreds:
            yield say_base[hundreds]
            yield ' hundred'
        if 0 < rest < len(say_base):
            yield ' and '
            yield say_base[rest]
        elif rest != 0:
            tens, ones = divmod(rest, 10)
            yield ' and '
            yield say_tens[tens]
            if ones > 0:
                yield '-'
                yield say_base[ones]
    
    assert "".join(hundreds_i(245)) == "two hundred and forty-five"
    assert "".join(hundreds_i(999)) == 'nine hundred and ninety-nine'
    assert "".join(hundreds_i(200)) == 'two hundred'



--------------------------------------------------
Is it indeed possible to manage roles and member accounts using MS Graph?
I&#39;ve been tasked to look into using MS Graph to manage Azure AD (aka Entra) roles and members by code. Currently I am on C# NET6 for that.

Before I dive in too deep and promise my manager something I can&#39;t deliver I want to be sure this is actually possible, because I do not see anything immediately directory related in the [Graph Explorer][1].

**What I think I need**

Using this [link][2] you can find the available Directory related scopes:

[![enter image description here][3]][3]

I think I would need the highlighted scope, so Directory.ReadWrite.All.

**What I don&#39;t see**

Using Graph Explorer I can find Groups, but I think that&#39;s AD groups, likely that&#39;s teams and Office 365 related?

If my above assumption is correct it&#39;s strange that I do not see anything Directory 
related.

**What I have**

I have used the Visual Studio Blazor Server template to generate that starter app, now with MS Graph and authentication enabled so that I also get this view:

[![enter image description here][4]][4]

(That line is the actual name)

It means I can confirm it&#39;s possible to at least get basic information from MS Graph, but not much else yet.

As mentioned I don&#39;t want to dive in yet, also because I would need privileged access and that needs to be discussed with the sys admins first.

**My questions**

1. Is it indeed possible to use MS Graph for Entra Role / Members management?

2. Is what I described above the correct first steps to take?

3. I remember reading that granting privileged access for MS Graph is still overruled by Entra itself. Is that true and does that mean what you can see / update is still limited by what you&#39;re allowed at Entra level?


  [1]: https://aka.ms/ge
  [2]: https://graphpermissions.merill.net/permission/Directory.ReadWrite.All
  [3]: https://i.stack.imgur.com/1Ydg0.png
  [4]: https://i.stack.imgur.com/t2T4X.png
||||||||||||||If I understand correctly you want to manage Azure Entra Id role assignments like the Global Administrator, User Administrator, Application Developer etc. roles. In that case the answer is yes this is possible.

For example, you can get [the role definitions](https://developer.microsoft.com/en-us/graph/graph-explorer?request=roleManagement%2Fdirectory%2FroleDefinitions&method=GET&version=v1.0&GraphUrl=https://graph.microsoft.com) and [assignments](https://developer.microsoft.com/en-us/graph/graph-explorer?request=roleManagement%2Fdirectory%2FroleAssignments&method=GET&version=v1.0&GraphUrl=https://graph.microsoft.com).

For your C# code, take a look at the `graphClient.RoleManagement.Directory` object

```csharp
// Code snippets are only available for the latest version. Current version is 5.x

// To initialize your graphClient, see https://learn.microsoft.com/en-us/graph/sdks/create-client?from=snippets&tabs=csharp
var result = await graphClient.RoleManagement.Directory.RoleAssignments.GetAsync((requestConfiguration) =>
{
	requestConfiguration.QueryParameters.Filter = "roleDefinitionId eq '62e90394-69f5-4237-9190-012177145e10'";
	requestConfiguration.QueryParameters.Expand = new string []{ "principal" };
});
```

Required privilege for role assignments is `RoleManagement.ReadWrite.Directory`

--------------------------------------------------
What are reference counted variables in Delphi?
I know what normal global variables and local variables are, but what are &quot;local reference-counted&quot; variables and &quot;local non reference-counted&quot; variables?

What is it? What&#39;s the difference? How do they work?

Can someone show code examples and explain please?
||||||||||||||A reference-counted object maintains an internal counter of how many variables refer to it at runtime.  Delphi native types that have reference counting include the string types `AnsiString` and `UnicodeString`, as well as `interface`s and dynamic arrays.

Such types are automatically managed by the Delphi compiler for you. Multiple variables of these types can refer to the same object in memory.  When a managed variable refers to an existing object, the compiler increases <sup>1</sup> the object's counter.  When that variable no longer refers to the object, because it was reassigned or went out of scope, the compiler decreases <sup>1</sup> the object's counter.

<sup>1: unless the variable is `const`, then it doesn't.</sup>

When an object's counter falls to zero, meaning no more variables refer to it, then the compiler frees the object from memory.

--------------------------------------------------
TypeScript array.sort() of typed values
Let&#39;s assume we have a type and an array:
```javascript
type Fruit = &#39;apple&#39; | &#39;orange&#39; | &#39;pineapple&#39;

const initialFruits: Fruit[] = [&#39;pineapple&#39;, &#39;apple&#39;, &#39;orange&#39;]
```
If I try to sort the array before assign the value I get an error:
```javascript
const initialFruits: Fruit[] = [&#39;pineapple&#39;, &#39;apple&#39;, &#39;orange&#39;].sort()

// Type &#39;string[]&#39; is not assignable to type &#39;Fruit[]&#39;.
```

I found one way to save types after sorting, but it looks weird:
```javascript
const initialFruits: Fruit[] = [
  &#39;pineapple&#39; as const,
  &#39;apple&#39; as const,
  &#39;orange&#39; as const
].sort()
```
Is there more convenient way to fix that?

||||||||||||||Array is sorted according to `Fruit` type.

    type Fruit = 'apple' | 'orange' | 'pineapple'

    const initialFruits: Fruit[] = ['pineapple', 'apple', 'orange'].sort() as Fruit[];

    const sortedFruits: Fruit[] = ['pineapple', 'apple', 'orange'].sort((a, b) => {
      return initialFruits.indexOf(a) - initialFruits.indexOf(b);
    }) as Fruit[];

--------------------------------------------------
WSL vscode command returning error &quot;not found&quot;
I have this message whenever I try to type `code .`

[![Screen capture containing an output obtained probably at a Unix-based WSL 2 terminal running on Windows. The output text is quoted exactly in the rest of the question for accessibility.][1]][1]

```bash
/mnt/c/Users/jerom/.vscode/extensions/ms-vscode-remote.remote-wsl-0.63.13/scripts/wslCode.sh: 69: /home/jerome/.vscode-server-server/bin/f80445acd5a3dadef24aa209168452a3d97cc326/bin/code: not found
```

  [1]: https://i.stack.imgur.com/6cQOm.png

Can someone help me please?

Thanks in advance
||||||||||||||I just now had the same issue on WSL2 Kali. I tried everything given here: https://github.com/microsoft/vscode-remote-release/issues/2962.

My problem was VS Code was not on Kali's path. Here's how I fixed it :)

```bash
echo $PATH
# Confirm PATH is missing this: 
/mnt/c/Users/wcd/AppData/Local/Programs/Microsoft VS Code/bin/

# Confirm VS Code's location then export:
export PATH=$PATH:"/mnt/c/Users/wcd/AppData/Local/Programs/Microsoft VS Code/bin/" 
# It's temporarily added to path...
echo $PATH
# This should now trigger VS Code Server install,
# then open ~/ in VS Code. 
code .

# If above works, make it permanent:
echo 'export PATH=$PATH:"/mnt/c/Users/wcd/AppData/Local/Programs/Microsoft VS Code/bin/"' >> ~/.bashrc

# Restart shell + test
exec "$SHELL"
code .
```

--------------------------------------------------
Concurrent Consumers in Quarkus
Is there a way to manage concurrency for concurrent consumers while trying  to consume messages from Quarkus?

I am looking for Batch Processing with concurrency to process several messages in seconds. I tried the same in SpringBoot earlier and it works ok. Trying to find a way if Quarkus is better.
||||||||||||||Check the [SmallRye Reactive Messaging][1] tutorial and the [Kafka Guide][2]. 

Basically, you can either do batch processing in an imperative or reactive way. 

Imperative way:

```java
@Incoming("your-topic")
public void handle(List<Message> messages) {
    // process
}
```
Reactive way:

```java
@Incoming("your-topic")
public Uni<Void> handle(Multi<Message> messages) {
    // process
}
```



  [1]: https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/3.3/model/model.html#processing-streams
  [2]: https://quarkus.io/guides/kafka#receiving-kafka-records-in-batches

--------------------------------------------------
Connecting to confluent kafka cloud from flink sql
I get the below error when I try to connect to [Confluent][1] cloud.

```
Caused by: org.apache.kafka.common.KafkaException: java.security.NoSuchAlgorithmException: TLSv1.3 SSLContext not available
    at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory.createSSLContext(DefaultSslEngineFactory.java:268)
    at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory.configure(DefaultSslEngineFactory.java:173)
    at org.apache.kafka.common.security.ssl.SslFactory.instantiateSslEngineFactory(SslFactory.java:140)
    at org.apache.kafka.common.security.ssl.SslFactory.configure(SslFactory.java:97)
    at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:180)
    ... 18 more
Caused by: java.security.NoSuchAlgorithmException: TLSv1.3 SSLContext not available
    at java.base/sun.security.jca.GetInstance.getInstance(GetInstance.java:159)
    at java.base/javax.net.ssl.SSLContext.getInstance(SSLContext.java:168)
    at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory.createSSLContext(DefaultSslEngineFactory.java:243)
```

The flink SQL I am using:

```
CREATE TABLE IF NOT EXISTS some_source_table
(
    headers     VARCHAR NOT NULL,
    id          VARCHAR NOT NULL,
    `timestamp` TIMESTAMP_LTZ(3) NULL,
    type        VARCHAR NOT NULL,
    contentJson VARCHAR NOT NULL
) WITH (
    &#39;connector&#39; = &#39;kafka&#39;,
    &#39;topic-pattern&#39; = &#39;kafka_topic__.+?&#39;,
    &#39;properties.bootstrap.servers&#39; = &#39;some.aws.confluent.cloud:9092&#39;,
    &#39;properties.group.id&#39; = &#39;some-id-1&#39;,
    &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;,
    &#39;format&#39; = &#39;json&#39;,
    &#39;json.timestamp-format.standard&#39; = &#39;ISO-8601&#39;,
    &#39;scan.topic-partition-discovery.interval&#39;= &#39;60000&#39;,
    &#39;json.fail-on-missing-field&#39; = &#39;false&#39;,
    &#39;json.ignore-parse-errors&#39; = &#39;true&#39;,
    &#39;properties.security.protocol&#39; = &#39;SASL_SSL&#39;,
    &#39;properties.sasl.mechanism&#39; = &#39;PLAIN&#39;,
    &#39;properties.sasl.jaas.config&#39; = &#39;org.apache.kafka.common.security.plain.PlainLoginModule required username=*** password=***;&#39;,
    &#39;properties.ssl.endpoint.identification.algorithm&#39; = &#39;https&#39;
);
```

When I check the Java version in the [Kubernetes][2] pod that run the task manager I see `11.0.18`. I get this by running `System.getProperty(&quot;java.version&quot;)`.


UPDATE
my gradle file looks like this:
    implementation &quot;org.apache.flink:flink-streaming-java:${flinkVersion}&quot;
    implementation &quot;org.apache.flink:flink-table-api-java-bridge:${flinkVersion}&quot;
    implementation &quot;org.apache.flink:flink-table-planner_${scalaVersion}:${flinkVersion}&quot;
    implementation &quot;org.apache.flink:flink-json:${flinkVersion}&quot;
    implementation &quot;org.apache.flink:flink-clients:${flinkVersion}&quot;

    //this seems to be needed : removing this is causing error in guice JsonProperty.Naming
    implementation &quot;com.fasterxml.jackson.core:jackson-databind:${jacksonVersion}&quot;
    implementation &quot;com.fasterxml.jackson.datatype:jackson-datatype-jsr310:${jacksonVersion}&quot;

    implementation &quot;org.apache.flink:flink-statebackend-rocksdb:${flinkVersion}&quot;

    //needed for a local flink ui to show when running environment = local
    implementation &quot;org.apache.flink:flink-runtime-web:${flinkVersion}&quot;

    implementation &quot;org.apache.logging.log4j:log4j-core:${log4jVersion}&quot;
    implementation &quot;org.apache.logging.log4j:log4j-api:${log4jVersion}&quot;
    implementation &quot;org.apache.logging.log4j:log4j-slf4j-impl:${log4jVersion}&quot;

    implementation &#39;io.jsonwebtoken:jjwt:0.2&#39;
    implementation &#39;com.mashape.unirest:unirest-java:1.4.9&#39;
    implementation &#39;org.rocksdb:rocksdbjni:7.9.2&#39;

    // --------------------------------------------------------------
    // Dependencies that should be part of the shadow jar, e.g.
    // connectors. These must be in the flinkShadowJar configuration!
    // --------------------------------------------------------------
    flinkShadowJar files(&#39;libs/flink-connector-kafka-1.16.0.jar&#39;)
    flinkShadowJar &quot;org.apache.flink:flink-connector-base:${flinkVersion}&quot;

    flinkShadowJar &quot;org.apache.commons:commons-text:1.10.0&quot;
    flinkShadowJar &quot;org.projectlombok:lombok:1.18.26&quot;
    flinkShadowJar &quot;com.fasterxml.jackson.core:jackson-databind:${jacksonVersion}&quot;
    flinkShadowJar &quot;com.fasterxml.jackson.datatype:jackson-datatype-jsr310:${jacksonVersion}&quot;
    flinkShadowJar &quot;com.sailpoint:atlas:${ATLAS_VERSION}&quot;
    flinkShadowJar &quot;com.sailpoint:atlas-event:${ATLAS_VERSION}&quot;
    flinkShadowJar &quot;com.google.inject:guice:5.1.0&quot;
    flinkShadowJar &quot;org.apache.flink:flink-s3-fs-hadoop:${flinkVersion}&quot;
    flinkShadowJar &#39;io.jsonwebtoken:jjwt:0.2&#39;
    flinkShadowJar &#39;com.mashape.unirest:unirest-java:1.4.9&#39;

What could I be missing here?

  [1]: https://en.wikipedia.org/wiki/Confluent,_Inc.
  [2]: https://en.wikipedia.org/wiki/Kubernetes


||||||||||||||I'm assuming you're using [Flink's SQL connector][1], which has shaded the Kafka Clients JAR in there. Because of the shading, that means that the JAAS Config needs to be pointed to the shaded PlainLoginModule, e.g.

```
'properties.sasl.jaas.config' = 'org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="key" password="value";'
```


  [1]: https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/table/kafka/

--------------------------------------------------
Match regex except if it ends like a specified string?
I&#39;m trying to match everything before the top property except if it has style.top. This is for old browsers so negative lookbehind is not allowed. Any ideas?

Tried a lot of different strategies already. Not great at regex.
||||||||||||||You can use a negative lookahead:

    ^(?!(?:.*[^$\s])?\bstyle\W*\.top\b).*\.top\b.*

The pattern matches:

- `^` Start of the string
- `(?!` Negative lookahead
  - `(?:` Non capture group
    - `.*[^$\s]` Match optional characters followed by a non whitespace char other than `$`
  - `)?` Close the non capture group and make it optional
  - `\bstyle\W*\.top\b` Match the word `style` followed by optional non word characters and then `.top`
- `)` Close the lookahead
- `.*\btop\b.*` Match the string containing  `.top`

See the [regex tests](https://regexr.com/7ql9g).

<hr>

If it should be `style` or `(style)` then:

    ^(?!(?:.*[^$\s])?(?:\bstyle|\(style\))\.top\b).*\.top\b.*

--------------------------------------------------
How to create a Github action for labeling issues created by specific users?
I&#39;d like to create a Github action that automatically labels issues of my Patreon supporters.

I created a file with the supporters&#39; Github usernames:

#### `./github/supporters.json`

```js
{
  usernames: [
    &#39;test1&#39;, 
    &#39;test2&#39;
  ]
}
```
(I can change the file to any other format, if that would make it easier)

How do I check if the issue was created by a user on the list?

I&#39;ve been thinking about using this marketplace action:
https://github.com/marketplace/actions/super-labeler

But it seems like it only accepts a Regex pattern. Should I just list usernames in the Regex directly?
||||||||||||||I figured out a solution, using this action https://github.com/actions-ecosystem/action-add-labels:

I'm specifying usernames in the github action .yaml file directly:

```
[
  "some-username-1",
  "some-username-2"
]
```

#### `./github/workflows/label-issue.yaml`

```
name: Label supporter

on:
  issues:
    types: [opened, edited]

jobs:
  add_label_tier_1:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions-ecosystem/action-add-labels@v1.1.0
        if: 
          ${{ 
            contains(
              fromJson('[
                "some-username-1",
                "some-username-2"
              ]'), 
              github.actor
            ) 
          }}
        with:
          github_token: ${{ github.token }}
          labels: |
            supporter 💖
            priority +1
            
  add_label_tier_2:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions-ecosystem/action-add-labels@v1.1.0
        if: 
          ${{ 
            contains(
              fromJson('[
                "some-username-3"
              ]'), 
              github.actor
            ) 
          }}
        with:
          github_token: ${{ github.token }}
          labels: |
            supporter 💖
            priority +2
```


--------------------------------------------------
How to apply piecewise linear fit in Python?
I am trying to fit piecewise linear fit as shown in fig.1 for a data set

![enter image description here][1]

This figure was obtained by setting on the lines. I attempted to apply a piecewise linear fit using the code:

    from scipy import optimize
	import matplotlib.pyplot as plt
	import numpy as np


	x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15])
	y = np.array([5, 7, 9, 11, 13, 15, 28.92, 42.81, 56.7, 70.59, 84.47, 98.36, 112.25, 126.14, 140.03])


	def linear_fit(x, a, b):
	    return a * x + b
	fit_a, fit_b = optimize.curve_fit(linear_fit, x[0:5], y[0:5])[0]
	y_fit = fit_a * x[0:7] + fit_b
	fit_a, fit_b = optimize.curve_fit(linear_fit, x[6:14], y[6:14])[0]
	y_fit = np.append(y_fit, fit_a * x[6:14] + fit_b)


	figure = plt.figure(figsize=(5.15, 5.15))
	figure.clf()
	plot = plt.subplot(111)
	ax1 = plt.gca()
	plot.plot(x, y, linestyle = &#39;&#39;, linewidth = 0.25, markeredgecolor=&#39;none&#39;, marker = &#39;o&#39;, label = r&#39;\textit{y_a}&#39;)
	plot.plot(x, y_fit, linestyle = &#39;:&#39;, linewidth = 0.25, markeredgecolor=&#39;none&#39;, marker = &#39;&#39;, label = r&#39;\textit{y_b}&#39;)
	plot.set_ylabel(&#39;Y&#39;, labelpad = 6)
	plot.set_xlabel(&#39;X&#39;, labelpad = 6)
	figure.savefig(&#39;test.pdf&#39;, box_inches=&#39;tight&#39;)
	plt.close()    

But this gave me fitting of the form in fig. 2, I tried playing with the values but no change I can&#39;t get the fit of the upper line proper. The most important requirement for me is how can I get Python to get the gradient change point. 

I want the code to recognize and fit two linear fits in the appropriate range. How can this be done in Python?

![enter image description here][2]


  [1]: http://i.stack.imgur.com/Thrit.png
  [2]: http://i.stack.imgur.com/UjrF6.png
||||||||||||||You can use `numpy.piecewise()` to create the piecewise function and then use `curve_fit()`, Here is the code

    from scipy import optimize
    import matplotlib.pyplot as plt
    import numpy as np
    %matplotlib inline
    
    x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15], dtype=float)
    y = np.array([5, 7, 9, 11, 13, 15, 28.92, 42.81, 56.7, 70.59, 84.47, 98.36, 112.25, 126.14, 140.03])
    
    def piecewise_linear(x, x0, y0, k1, k2):
        return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])
    
    p , e = optimize.curve_fit(piecewise_linear, x, y)
    xd = np.linspace(0, 15, 100)
    plt.plot(x, y, "o")
    plt.plot(xd, piecewise_linear(xd, *p))

the output:

![enter image description here][1]


For an N parts fitting, please reference [segments_fit.ipynb][2]


  [1]: http://i.stack.imgur.com/xw0QH.png
  [2]: https://gist.github.com/ruoyu0088/70effade57483355bbd18b31dc370f2a

--------------------------------------------------
Can&#39;t install xdebug on Mac with Homebrew
I&#39;m kind of new to using Homebrew, but I love it.  It&#39;s so easy.  I&#39;m trying install Xdebug.  Some of the posts on the web say to do this:

    brew install xdebug

But it doesn&#39;t work.  I get: `Error, no available formula.`  

I did `brew search xdebug` and it returned:

    josegonzalez/php/php53-xdebug	 josegonzalez/php/php54-xdebug

I tried several different iterations of `brew install` with this including `brew install php53-xdebug`, but still no luck.  Can someone help me?  I can&#39;t find anything on Xdebug&#39;s site about using Homebrew, but yet posts on the web seem to indicate it&#39;s possible.
||||||||||||||Add this repository: https://github.com/josegonzalez/homebrew-php#readme

Then use `brew install php54-xdebug` for PHP 5.4

Or `brew install php53-xdebug` for PHP 5.3

Or `brew install php55-xdebug` for PHP 5.5

--------------------------------------------------
React router 6 - Avoid re-render the parent components of a nested child-route
Here is one my Routes:

    &lt;Routes&gt;
      &lt;Route path=&quot;A&quot;&gt;
        &lt;Route path=&quot;:date&quot; element={
    	  &lt;Wrapper&gt;
    	    &lt;ProviderB&gt;
    		  &lt;Summary /&gt;
      	  &lt;/ProviderB&gt;
      	&lt;/Wrapper&gt;
        }
        /&gt;
      &lt;/Route&gt;
    &lt;/Routes&gt;

This is **Wrapper**:

    export const Wrapper: React.FC&lt;Props&gt; = ({children}) =&gt; {
       return (
           &lt;ProviderA&gt;
             {children}
           &lt;/ProviderA&gt;
       );
    };

This is component **ProviderA**: 

    const ContextAction = createContext({
      ...
    });
    
    const ContextValue = createContext({
      ...
    });
    
    export const ProviderA: React.FC&lt;Props&gt; = ({children}) =&gt; {
       const [state, setState] = useState(&#39;&#39;);
    
      return (
        &lt;ContextAction.Provider value={setState}&gt;
          &lt;ContextValue.Provider value={state}&gt;
            {children}
          &lt;/ContextValue.Provider&gt;
        &lt;/ContextAction.Provider&gt;
      );
    };

And this is component **ProviderB**: 

    const ContextAction = createContext({
      ...
    });
    
    const ContextValue = createContext({
      ...
    });
    
    export const ProviderB: React.FC&lt;Props&gt; = ({children}) =&gt; {
       const [state, setState] = useState(&#39;&#39;);
    
      return (
        &lt;ContextAction.Provider value={setState}&gt;
          &lt;ContextValue.Provider value={state}&gt;
            {children}
          &lt;/ContextValue.Provider&gt;
        &lt;/ContextAction.Provider&gt;
      );
    };


What I want to achieve is to render only **Summary** when the date in the URL does change. For example, a URL change from A/26-01-2024 to A/25-01-2014 should not trigger the re-rendering of **Wrapper**; instead it should re-render only the child-nested route **Summary**. How may I achieve that by using `&lt;Outlet /&gt;`?
Hopefully, the above makes sense even if it is tricky.
||||||||||||||> What I want to achieve is to render only Summary when the date in the
> URL does change.

This is just how React works. If you don't want `ProviderA` and `ProviderB` to rerender when the route rerenders then move them higher up the ReactTree.

I suggest creating a layout route that renders the providers and an `Outlet` for the nested routes.

Example:

```jsx
import { Outlet } from 'react-router-dom';

export const ProviderLayout = () => (
  <ProviderA>
    <ProviderB>
      <Outlet />
    </ProviderB>
  </ProviderA>
);
```

```jsx
<Routes>
  <Route path="A">
    <Route element={<ProviderLayout />}>
      <Route path=":date" element={<Summary />} />
    </Route>
  </Route>
</Routes>
```

If rerendering is still an issue then continue pushing the providers higher and higher up the ReactTree until they are being rendered from a stable component that doesn't rerender when the route(s) rerender(s).

--------------------------------------------------
Flutter change color when button gets clicked
I am working on an app where the user is able to expand and collapse text.
I want to text at the bottom to be blurred. As soon as the user expands the text the text should be displayed normal. When the text gets collapsed again, I want the text at the bottom again to be blurred.
I solved it the following way. My problem is that when the user collapse the text, the text at the bottom is not blurred again.


Here is my code:

    class ExpandableText extends StatefulWidget {
      final String text;
      const ExpandableText({Key? key, required this.text}) : super(key: key);
    
      @override
      State&lt;ExpandableText&gt; createState() =&gt; _ExpandableTextState();
    }
    
    class _ExpandableTextState extends State&lt;ExpandableText&gt; {
      late String firstHalf;
      late String secondHalf;
    
      bool hiddenText = true;
    
      Color fadeStrong = Colors.black.withOpacity(0.8);
      Color fadeWeak = Colors.black.withOpacity(0.6);
    
      // double textHeight = Dimensions.screenHight / 5.63;
      double textHeight = 180;
    
      @override
      void initState() {
        super.initState();
        if (widget.text.length &gt; textHeight) {
          firstHalf = widget.text.substring(0, textHeight.toInt());
          secondHalf =
              widget.text.substring(textHeight.toInt() + 1, widget.text.length);
        } else {
          firstHalf = widget.text;
          secondHalf = &quot;&quot;;
        }
      }
    
      @override
      Widget build(BuildContext context) {
        return Stack(children: [
          Container(
            child: secondHalf.isEmpty
                ? Text(
                    firstHalf,
                    style: const TextStyle(color: Colors.white),
                  )
                : Column(
                    children: [
                      Text(
                        hiddenText ? (&quot;$firstHalf...&quot;) : (firstHalf + secondHalf),
                        style: const TextStyle(color: Colors.white),
                      ),
                      InkWell(
                        onTap: () {
                          setState(() {
                            hiddenText = !hiddenText;
                            hiddenText
                                ? fadeStrong = fadeStrong
                                : fadeStrong = Colors.transparent;
                        hiddenText
                            ? fadeWeak = fadeWeak
                            : fadeWeak = Colors.transparent;
                          });
                        },
                        child: Row(
                          children: [
                            hiddenText
                                ? const Text(
                                    &#39;Read more&#39;,
                                    style: TextStyle(color: Colors.blue),
                                  )
                                : const Text(&#39;Read less&#39;,
                                    style: TextStyle(color: Colors.blue)),
                            Icon(
                              hiddenText
                                  ? Icons.arrow_drop_down
                                  : Icons.arrow_drop_up_outlined,
                              color: Colors.blue,
                            )
                          ],
                        ),
                      )
                    ],
                  ),
          ),
          Container(
            margin: const EdgeInsets.only(top: 52),
            height: 20,
            color: fadeStrong,
          ),
          Container(
            margin: const EdgeInsets.only(top: 32),
            height: 20,
            color: fadeWeak,
          ),
        ]);
      }
    }

[![Text collapsed][1]][1]

[![Text expanded][2]][2]

[![Text collapsed again][3]][3]


  [1]: https://i.stack.imgur.com/fBj8y.png
  [2]: https://i.stack.imgur.com/P6hqX.png
  [3]: https://i.stack.imgur.com/gsWjd.png
||||||||||||||When you overrides the color on tap event, that lose the initial reference

                   onTap: () {
                      setState(() {
                        hiddenText = !hiddenText;
                        hiddenText
                            ? fadeStrong = fadeStrong
                            : fadeStrong = Colors.transparent;
                    hiddenText
                        ? fadeWeak = fadeWeak
                        : fadeWeak = Colors.transparent;
                      });
                    },


maybe you should try something like that will help you

                   onTap: () {
                      setState(() {
                        hiddenText = !hiddenText;
                        hiddenText
                            ? fadeStrong = Colors.black.withOpacity(0.8)
                            : fadeStrong = Colors.transparent;
                    hiddenText
                        ? fadeWeak = Colors.black.withOpacity(0.6)
                        : fadeWeak = Colors.transparent;
                      });
                    },

--------------------------------------------------
Sending outlook email using win32com on python from a secondary mailbox account

I&#39;m having an issue [like this one][1]

```
        outlook = win32com.client.Dispatch(&#39;outlook.application&#39;)
        accounts = win32com.client.Dispatch(&quot;outlook.Application&quot;).Session.Accounts
        print(accounts[1])
        mail = outlook.CreateItem(0)
        mail.SentOnBehalfOfName  =  accounts[1]
        mail.SendUsingAccount  =  accounts[1]
        mail.To = to
        mail.Subject = &#39;Subject TEST&#39;
        mail.HTMLBody = emailBody
        mail.display()
        mail.Send()
```
if I comment ```mail.Send()``` the window that shows up will show everything correctly but if I send it, i&#39;ll get reply ```This message could not be sent. You do not have the permission to send the message on behalf of the specified user.``` which is obviously not true since if instead of sending directly and choose the exact same email from the dropdown menu in From, and then click SEND, it will send the email with no issues.

  [1]: https://social.msdn.microsoft.com/Forums/en-US/cc25d70a-eb6d-4bca-bcae-33c05380e61e/outlook-macro-sentonbehalfofname?forum=outlookdev
||||||||||||||So with the help of [@Eugene Astafiev][1] and [this post][2] I fixed the issue like this:

```
        outlook = win32com.client.Dispatch('outlook.application')
        for account in outlook.Session.Accounts:
            if account.DisplayName == "email@email.om":
                print(account)
                mail = outlook.CreateItem(0)
                mail._oleobj_.Invoke(*(64209, 0, 8, 0, account))
                mail.To = to
                mail.Subject = 'Vodafone Support'
                mail.HTMLBody = emailBody
                #mail.display()
                mail.Send()
```


  [1]: https://stackoverflow.com/a/74195997/10747598
  [2]: https://stackoverflow.com/questions/57826082/python-win32-client-sent-email-from-a-secondary-outlook-account-from-its-own-out

--------------------------------------------------
JTextArea setText() &amp; UndoManager
I&#39;m using an `UndoManager` to capture changes in my `JTextArea`.

The method `setText()` however deletes everything and then pastes the text. When I undo I firstly see an empty area and then it would show which text it had before.

How to reproduce:

1. Run the following code
2. Click the `setText()` button
3. Press &lt;kbd&gt;CTRL+Z&lt;/kbd&gt; to undo (you&#39;ll see an empty textarea!)
4. Press &lt;kbd&gt;CTRL+Z&lt;/kbd&gt; to undo (you&#39;ll see the actual previous text)

I want to skip 3).

    import javax.swing.AbstractAction;
    import javax.swing.JFrame;
    import javax.swing.JTextArea;
    import javax.swing.KeyStroke;
    import javax.swing.event.UndoableEditEvent;
    import javax.swing.event.UndoableEditListener;
    import javax.swing.text.Document;
    import javax.swing.undo.CannotRedoException;
    import javax.swing.undo.CannotUndoException;
    import javax.swing.undo.UndoManager;
    
    import java.awt.event.ActionEvent;
    import javax.swing.JButton;
    import java.awt.event.ActionListener;
    
    @SuppressWarnings(&quot;serial&quot;)
    public class JTextComponentSetTextUndoEvent extends JFrame
    {
    	JTextArea area = new JTextArea();
    
    	public JTextComponentSetTextUndoEvent()
    	{
    		setSize(300, 300);
    		setDefaultCloseOperation(EXIT_ON_CLOSE);
    		getContentPane().setLayout(null);
    
    		area.setText(&quot;Test&quot;);
    		area.setBounds(0, 96, 146, 165);
    		getContentPane().add(area);
    
    		JButton btnSettext = new JButton(&quot;setText()&quot;);
    		btnSettext.addActionListener(new ActionListener()
    		{
    			public void actionPerformed(ActionEvent arg0)
    			{
    				area.setText(&quot;stackoverflow.com&quot;);
    			}
    		});
    		btnSettext.setBounds(0, 28, 200, 50);
    		getContentPane().add(btnSettext);
    
    		final UndoManager undoManager = new UndoManager();
    		Document doc = area.getDocument();
    
    		doc.addUndoableEditListener(new UndoableEditListener()
    		{
    			public void undoableEditHappened(UndoableEditEvent evt)
    			{
    				undoManager.addEdit(evt.getEdit());
    			}
    		});
    
    		area.getActionMap().put(&quot;Undo&quot;, new AbstractAction(&quot;Undo&quot;)
    		{
    			public void actionPerformed(ActionEvent evt)
    			{
    				try
    				{
    					if (undoManager.canUndo())
    					{
    						undoManager.undo();
    					}
    				} catch (CannotUndoException e)
    				{
    				}
    			}
    		});
    
    		area.getInputMap().put(KeyStroke.getKeyStroke(&quot;control Z&quot;), &quot;Undo&quot;);
    
    		area.getActionMap().put(&quot;Redo&quot;, new AbstractAction(&quot;Redo&quot;)
    		{
    			public void actionPerformed(ActionEvent evt)
    			{
    				try
    				{
    					if (undoManager.canRedo())
    					{
    						undoManager.redo();
    					}
    				} catch (CannotRedoException e)
    				{
    				}
    			}
    		});
    
    		area.getInputMap().put(KeyStroke.getKeyStroke(&quot;control Y&quot;), &quot;Redo&quot;);
    	}
    
    	public static void main(String[] args)
    	{
    		new JTextComponentSetTextUndoEvent().setVisible(true);
    	}
    }
||||||||||||||You can try something like this:

	//Works fine for me on Windows 7 x64 using JDK 1.7.0_60:
	import java.awt.*;
	import java.awt.event.*;
	import java.util.*;
	import javax.swing.*;
	import javax.swing.event.*;
	import javax.swing.text.*;
	import javax.swing.undo.*;
	
	public final class UndoManagerTest {
	  private final JTextField textField0 = new JTextField("default");
	  private final JTextField textField1 = new JTextField();
	  private final UndoManager undoManager0 = new UndoManager();
	  private final UndoManager undoManager1 = new UndoManager();
	
	  public JComponent makeUI() {
	    textField1.setDocument(new CustomUndoPlainDocument());
	    textField1.setText("aaaaaaaaaaaaaaaaaaaaa");
	
	    textField0.getDocument().addUndoableEditListener(undoManager0);
	    textField1.getDocument().addUndoableEditListener(undoManager1);
	
	    JPanel p = new JPanel();
	    p.add(new JButton(new AbstractAction("undo") {
	      @Override public void actionPerformed(ActionEvent e) {
	        if (undoManager0.canUndo()) {
	          undoManager0.undo();
	        }
	        if (undoManager1.canUndo()) {
	          undoManager1.undo();
	        }
	      }
	    }));
	    p.add(new JButton(new AbstractAction("redo") {
	      @Override public void actionPerformed(ActionEvent e) {
	        if (undoManager0.canRedo()) {
	          undoManager0.redo();
	        }
	        if (undoManager1.canRedo()) {
	          undoManager1.redo();
	        }
	      }
	    }));
	    p.add(new JButton(new AbstractAction("setText(new Date())") {
	      @Override public void actionPerformed(ActionEvent e) {
	        String str = new Date().toString();
	        textField0.setText(str);
	        textField1.setText(str);
	      }
	    }));
	
	    Box box = Box.createVerticalBox();
	    box.setBorder(BorderFactory.createEmptyBorder(5, 5, 5, 5));
	    box.add(makePanel("Default", textField0));
	    box.add(Box.createVerticalStrut(5));
	    box.add(makePanel("replace ignoring undo", textField1));
	
	    JPanel pp = new JPanel(new BorderLayout());
	    pp.add(box, BorderLayout.NORTH);
	    pp.add(p, BorderLayout.SOUTH);
	    return pp;
	  }
	  private static JPanel makePanel(String title, JComponent c) {
	    JPanel p = new JPanel(new BorderLayout());
	    p.setBorder(BorderFactory.createTitledBorder(title));
	    p.add(c);
	    return p;
	  }
	  public static void main(String[] args) {
	    EventQueue.invokeLater(new Runnable() {
	      @Override public void run() {
	        createAndShowGUI();
	      }
	    });
	  }
	  public static void createAndShowGUI() {
	    JFrame f = new JFrame();
	    f.setDefaultCloseOperation(WindowConstants.EXIT_ON_CLOSE);
	    f.getContentPane().add(new UndoManagerTest().makeUI());
	    f.setSize(320, 240);
	    f.setLocationRelativeTo(null);
	    f.setVisible(true);
	  }
	}
	
	class CustomUndoPlainDocument extends PlainDocument {
	  private CompoundEdit compoundEdit;
	  @Override protected void fireUndoableEditUpdate(UndoableEditEvent e) {
	    if (compoundEdit == null) {
	      super.fireUndoableEditUpdate(e);
	    } else {
	      compoundEdit.addEdit(e.getEdit());
	    }
	  }
	  @Override public void replace(
	      int offset, int length,
	      String text, AttributeSet attrs) throws BadLocationException {
	    if (length == 0) {
	      System.out.println("insert");
	      super.replace(offset, length, text, attrs);
	    } else {
	      System.out.println("replace");
	      compoundEdit = new CompoundEdit();
	      super.fireUndoableEditUpdate(new UndoableEditEvent(this, compoundEdit));
	      super.replace(offset, length, text, attrs);
	      compoundEdit.end();
	      compoundEdit = null;
	    }
	  }
	}

--------------------------------------------------
Can I have persistent sessions for VS Code Remote Development?
Visual Studio Code offers a really nice way to do remote development over SSH with its Remote Development extension. Does this extension and its terminal has support for permanent sessions that would survive disconnects?

- You can achieve this by running UNIX `screen` within Visual Studio Code terminal

- However, Visual Studio Code itself installs its remote agent, so maybe this remote agent offers similar functionality built-in, leading to easier user experience (no need to hassle with `screen`)
||||||||||||||# Introduction
> Does this extension and its terminal has support for permanent sessions that would survive disconnects?

No, it seems that, currently, the [«Remote - SSH» Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh) does not have such a feature.

# Straightforward solution
It seems to be a known «Remote - SSH» Visual Studio Code extension issue:

* GitHub issue: [Persistent SSH session · Issue #3096 · microsoft/vscode-remote-release · GitHub](https://github.com/microsoft/vscode-remote-release/issues/3096).

Roughly speaking, the solution is:

1. To communicate with the maintainers (for example, by posting comments on the GitHub issue).
2. If applicable, to wait for a next «Remote - SSH» Visual Studio Code extension release with the resolved issue.

--------------------------------------------------
FastAPI: Performance results differ between run_in_threadpool() and run_in_executor() with ThreadPoolExecutor

Here&#39;s a minimal reproducible example of my FastAPI app. I have a strange behavior and I&#39;m not sure I understand the reason. 

I&#39;m using ApacheBench (`ab`) to send multiple requests as follows:
```
ab -n 1000 -c 50 -H &#39;accept: application/json&#39; -H &#39;x-data-origin: source&#39; &#39;http://localhost:8001/test/async&#39;
```

**FastAPI app**

    import time
    import asyncio
    import enum
    from typing import Any

    from fastapi import FastAPI, Path, Body
    from starlette.concurrency import run_in_threadpool

    app = FastAPI()
    loop = asyncio.get_running_loop()
    def sync_func() -&gt; None:
        time.sleep(3)
        print(&quot;sync func&quot;)
    
    async def sync_async_with_fastapi_thread() -&gt; None:
        await run_in_threadpool( time.sleep, 3)
        print(&quot;sync async with fastapi thread&quot;)
    
    async def sync_async_func() -&gt; None:
        await loop.run_in_executor(None, time.sleep, 3)
    
    async def async_func() -&gt; Any:
        await asyncio.sleep(3)
        print(&quot;async func&quot;)
    
    @app.get(&quot;/test/sync&quot;)
    def test_sync() -&gt; None:
        sync_func()
        print(&quot;sync&quot;)
    
    @app.get(&quot;/test/async&quot;)
    async def test_async() -&gt; None:
        await async_func()
        print(&quot;async&quot;)
    
    @app.get(&quot;/test/sync_async&quot;)
    async def test_sync_async() -&gt; None:
        await sync_async_func()
        print(&quot;sync async&quot;)
    
    @app.get(&quot;/test/sync_async_fastapi&quot;)
    async def test_sync_async_with_fastapi_thread() -&gt; None:
        await sync_async_with_fastapi_thread()
        print(&quot;sync async with fastapi thread&quot;)

Here&#39;s the ApacheBench results:

**async with (asyncio.sleep)** : 
*Concurrency Level:      50
- Time taken for tests:   63.528 seconds
- Complete requests:      1000
- Failed requests:        0
- Total transferred:      128000 bytes
- HTML transferred:       4000 bytes
- Requests per second:    15.74 [#/sec] (mean)
- **Time per request:       3176.407 [ms] (mean)**
- Time per request:       63.528 [ms] (mean, across all concurrent requests)
Transfer rate:          1.97 [Kbytes/sec] received*




**sync (with time.sleep):**
Concurrency Level:      50
- *Time taken for tests:   78.615 seconds
- Complete requests:      1000
- Failed requests:        0
- Total transferred:      128000 bytes
- HTML transferred:       4000 bytes
- Requests per second:    12.72 [#/sec] (mean)
- **Time per request:       3930.751 [ms] (mean)**
- Time per request:       78.615 [ms] (mean, across all concurrent requests)
Transfer rate:          1.59 [Kbytes/sec] received*


**sync_async (time sleep with run_in_executor) :** *Concurrency Level:      50
- Time taken for tests:   256.201 seconds
- Complete requests:      1000
- Failed requests:        0
- Total transferred:      128000 bytes
- HTML transferred:       4000 bytes
- Requests per second:    3.90 [#/sec] (mean)
- **Time per request:       12810.038 [ms] (mean)**
- Time per request:       256.201 [ms] (mean, across all concurrent requests)
Transfer rate:          0.49 [Kbytes/sec] received*


**sync_async_fastapi (time sleep with run_in threadpool):**
*Concurrency Level:      50
- Time taken for tests:   78.877 seconds
- Complete requests:      1000
- Failed requests:        0
- Total transferred:      128000 bytes
- HTML transferred:       4000 bytes
- Requests per second:    12.68 [#/sec] (mean)
- **Time per request:       3943.841 [ms] (mean)**
- Time per request:       78.877 [ms] (mean, across all concurrent requests)
Transfer rate:          1.58 [Kbytes/sec] received*


In conclusion, I&#39;m experiencing a surprising disparity in results, especially when using run_in_executor, where I&#39;m encountering significantly higher average times (12 seconds). I don&#39;t understand this outcome.

--- EDIT ---
**After AKX answer.**

    Here the code working as expected: 
    import time
    import asyncio
    from anyio import to_thread
    
    to_thread.current_default_thread_limiter().total_tokens = 200
    loop = asyncio.get_running_loop()
    executor = ThreadPoolExecutor(max_workers=100)
    def sync_func() -&gt; None:
        time.sleep(3)
        print(&quot;sync func&quot;)
    
    async def sync_async_with_fastapi_thread() -&gt; None:
        await run_in_threadpool( time.sleep, 3)
        print(&quot;sync async with fastapi thread&quot;)
    
    async def sync_async_func() -&gt; None:
        await loop.run_in_executor(executor, time.sleep, 3)
    
    async def async_func() -&gt; Any:
        await asyncio.sleep(3)
        print(&quot;async func&quot;)
    
    @app.get(&quot;/test/sync&quot;)
    def test_sync() -&gt; None:
        sync_func()
        print(&quot;sync&quot;)
    
    @app.get(&quot;/test/async&quot;)
    async def test_async() -&gt; None:
        await async_func()
        print(&quot;async&quot;)
    
    @app.get(&quot;/test/sync_async&quot;)
    async def test_sync_async() -&gt; None:
        await sync_async_func()
        print(&quot;sync async&quot;)
    
    @app.get(&quot;/test/sync_async_fastapi&quot;)
    async def test_sync_async_with_fastapi_thread() -&gt; None:
        await sync_async_with_fastapi_thread()
        print(&quot;sync async with fastapi thread&quot;)
||||||||||||||## Using [`run_in_threadpool()`][1]
Starlette's [`run_in_threadpool()`][1] uses [`anyio.to_thread.run_sync()`][2], behind the scenes, which "will run the *sync* blocking function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked"&mdash;see [this answer][3] and AnyIO's [Working with threads][4] documentation for more details. Calling `anyio.to_thread.run_sync()`&mdash;which internally calls [`AsyncIOBackend.run_sync_in_worker_thread()`][5]&mdash;will return a coroutine that can be `await`ed to get the eventual result of the *sync* function (e.g., `result = await run_in_threadpool(...)`), and hence, FastAPI will still work *asynchronously*. As can be seen in Starlette's source code (link is given above), `run_in_threadpool()` simply looks like this (supporting both *sequence* and *keyword* arguments):
```python
async def run_in_threadpool(
    func: typing.Callable[P, T], *args: P.args, **kwargs: P.kwargs
) -> T:
    if kwargs:  # pragma: no cover
        # run_sync doesn't accept 'kwargs', so bind them in here
        func = functools.partial(func, **kwargs)
    return await anyio.to_thread.run_sync(func, *args)
```

As described in [AnyIO's documentation][6]:

> #### Adjusting the default maximum worker thread count
> 
> The **default** AnyIO worker thread limiter has a value of `40`, meaning
> that any calls to `to_thread.run_sync()` without an explicit `limiter`
> argument will cause a **maximum of `40` threads** to be spawned. You can
> adjust this limit like this: 
> ``` 
> from anyio import to_thread
>
> async def foo():
>     # Set the maximum number of worker threads to 60
>     to_thread.current_default_thread_limiter().total_tokens = 60
> ```
> **Note**
>
> AnyIO’s default thread pool limiter does not affect the default thread pool executor on `asyncio`.

Since [FastAPI uses Startlette's `concurrency` module][7] to run requests/blocking functions in an external threadpool, the default value of the thread limiter is also applied, i.e., `40` threads maximum&mdash;see the relevant [`AsyncIOBackend.current_default_thread_limiter()`][8] method that returns the [`CapacityLimiter`][9] with the default number of threads. As described above, one can adjust that value, thus **increasing the number of threads**, which might lead to an improvement in performance results&mdash;always **depending** on the number of requests your API is expected to serve concurrently. For instance, if you expect the API to serve no more than 50 requests at a time, then set the maximum number of threads to 50&mdash;if you have *synchronous*/*blocking* background tasks/`StreamingResponse`'s generators (i.e., functions defined with normal `def` instead of `async def`), or use `UploadFile`'s operations as well, you could add more threads as required, as FastAPI actually runs all those in an external threadpool, using `run_in_threadpool`&mdash;it is all explained in [this answer][3] in details. 

Note that using the approach below, which was described [here][10], would have the same effect on adjusting the number of worker threads:
```python
from anyio.lowlevel import RunVar
from anyio import CapacityLimiter

RunVar("_default_thread_limiter").set(CapacityLimiter(60))
```

But, it would be best to follow the approach provided by AnyIO's official documentation (as shown earlier). It is also a good idea to have this done when the application starts up, using a `lifespan` event handler, as demonstrated [here][11]. 

#### Working Example 1
```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from anyio import to_thread
import time


@asynccontextmanager
async def lifespan(app: FastAPI):    
    to_thread.current_default_thread_limiter().total_tokens = 60
    yield


app = FastAPI(lifespan=lifespan)


@app.get("/sync")
def test_sync() -> None:
    time.sleep(3)
    print("sync")


@app.get('/get_available_threads')
async def get_available_threads():
    return to_thread.current_default_thread_limiter().available_tokens
```

Using ApacheBench, you could test the example above as follows, which will send `1000` requests in total with `50` being sent simultaneously at a time (`-n`: Number of requests, `-c` : Number of concurrent requests):
```
ab -n 1000 -c 50 "http://localhost:8000/sync"
```

Since the `/sync` endpoint above is defined with normal `def` instead of `async def`, FastAPI will use `run_in_threadpool()`, behind the scenes, to run it in a separate thread and `await` it, thus ensuring that event loop (and hence, the main thread) does not get blocked due to the blocking operations (either blocking IO-bound or CPU-bound) that will be performed inside that endpoint. 

While running a performance test on the example above, if you call the `/get_available_threads` endpoint from your browser, e.g., `http://localhost:8000/get_available_threads`, you would see that the amount of threads **available** is always 10 or above (since only 50 threads are used at a time in this test, but the thread limiter was set to `60`), meaning that setting the maximum number of threads on AnyIO's thread limiter to a number that is well above your needs, like `200` as shown in some other answer and in your recent example, wouldn't bring about any improvements in the performance; on the contrary, you would end up with a number of threads "sitting" there without being used. As explained earlier, the number of maximum threads should depend on the number of requests your API is expected to serve concurrently, as well as any other blocking tasks/functions that would run in the threadpool by FastAPI itself, under the hood (and of course, on the server machine's resources available).

The example below is the **same** as the one above, but instead of letting FastAPI itself to handle the blocking operation(s) inside the `def` endpoint (by running the `def` endpoint in the external threadpool and `await`ing it), the endpoint is now defined with `async def` (meaning that FastAPI will run it directly in the event loop), but inside the endpoint, `run_in_threadpool()` is used (which returns an `await`able) to run the blocking operation. Performing a benchmark test on the example below would yield similar results to the previous example.

#### Working Example 2
```python
from fastapi import FastAPI
from fastapi.concurrency import run_in_threadpool
from contextlib import asynccontextmanager
from anyio import to_thread
import time


@asynccontextmanager
async def lifespan(app: FastAPI):    
    to_thread.current_default_thread_limiter().total_tokens = 60
    yield


app = FastAPI(lifespan=lifespan)


@app.get("/sync_async_run_in_tp")
async def test_sync_async_with_run_in_threadpool() -> None:
    await run_in_threadpool(time.sleep, 3)
    print("sync_async using FastAPI's run_in_threadpool")


@app.get('/get_available_threads')
async def get_available_threads():
    return to_thread.current_default_thread_limiter().available_tokens
```

Using ApacheBench, you could test the example above as follows:
```
ab -n 1000 -c 50 "http://localhost:8000/sync_async_run_in_tp"
```

## Using [`loop.run_in_executor()`][12] with [`ThreadPoolExecutor`][13]
When using `asyncio`'s [`loop.run_in_executor()`][12]&mdash;after obtaining the running event loop using [`asyncio.get_running_loop()`][14]&mdash;one could pass `None` to the `executor` argument, which would lead to the *default* executor being used; that is, a [`ThreadPoolExecutor`][13]. **Note** that when calling `loop.run_in_executor()` and passing `None` to the `executor` argument, this **does not** create a new instance of a `ThreadPoolExecutor` every time you do that; instead, a `ThreadPoolExecutor` is only initialised once the first time you do that, but for subsequent calls to `loop.run_in_executor()` with passing `None` to the `executor` argument, Python **reuses** that very same instance of `ThreadPoolExecutor` (hence, the *default* executor). This can been seen in the [source code of `loop.run_in_executor()`][15]. That means, the number of threads that can be created, when calling `await loop.run_in_executor(None, ...)`, is **limited** to the default number of thread workers in the `ThreadPoolExecutor` class.

As described in the documentation of `ThreadPoolExecutor`&mdash;and as shown in its implementation [here][16]&mdash;by default, the `max_workers` argument is set to `None`, in which case, the number of worker threads is set based on the following equation: `min(32, os.cpu_count() + 4)`. The [`os.cpu_count()`][17] function reutrns the number of *logical* CPUs in the current system. As explained in [this article][18], *physical* cores refers to the number of CPU cores provided in the hardware (e.g., the chips), while *logical* cores is the number of CPU cores **after** hyperthreading is taken into account. If, for instance, your machine has 4 physical cores, each with hyperthreading (most modern CPUs have this), then Python will see 8 CPUs and will allocate 12 threads (8 CPUs + 4) to the pool by default (Python limits the number of threads to 32 to "avoid consuming surprisingly large resources on multi-core machines"; however, one could always adjust the `max_workers` argument on their own when using a custom `ThreadPoolExecutor`, instead of using the *default* one). You could check the default number of worker threads on your system as follows:
```python
import concurrent.futures

# create a thread pool with the default number of worker threads
pool = concurrent.futures.ThreadPoolExecutor()

# report the number of worker threads chosen by default
# Note: `_max_workers` is a protected variable and may change in the future
print(pool._max_workers)
```

Now, as shown in your original example, you are not using a custom `ThreadPoolExecutor`, but instead using the *default* `ThreadPoolExecutor` every time a request arrives, by calling `await loop.run_in_executor(None, time.sleep, 3)` (inside the `sync_async_func()` function, which is triggered by the `/test/sync_async` endpoint). Assuming your machine has 4 physical cores with hyperthreading enabled (as explained in the example earlier), then the default number of worker threads for the *default* `ThreadPoolExecutor` would be 12. That means, based on your original example and the `/test/sync_async` endpoint that triggers the `await loop.run_in_executor(None, time.sleep, 3)` function, your application could only handle 12 concurrent requests at a time. That is the **main reason** for the difference observed in the performance results when compared to using `run_in_threadpool()`, which comes with `40` allocated threads by default.

One way to solve this is to create a new instance of `ThreadPoolExecutor` (on your own, instead of using the *default* executor) every time a request arrives and have it terminated once the task is completed (using the `with` statement), as shown below:
```python
import concurrent.futures
import asyncio

loop = asyncio.get_running_loop()
with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
    await loop.run_in_executor(pool, time.sleep, 3)
```

While this should wok just fine, it would be best to instantiate a `ThreadPoolExecutor` once at the application startup, adjust the number of worker threads as needed, and re-use that executor when required. Having said that, depending on the blocking task and/or external libraries you might be using for that task, if you ever encounter a memory leak after tasks are completed when re-using a `ThreadPoolExecutor`&mdash;i.e., memory that is no longer needed, but is not released&mdash;you might find creating a new instance of `ThreadPoolExecutor` each time, as shown above, more suitable (Note, however, that if this was a [`ProcessPoolExecutor`][19] instead, creating and destroying many processes over and over could become **computationally expensive**).

Below is a complete working example, demonstrating how to create a re-usable custom `ThreadPoolExecutor`. Calling  the `/get_active_threads` endpoint from your browser, e.g., `http://localhost:8000/get_active_threads`, while running a performance test with ApacheBench (using `50` concurrent requests, as described in your question and as shown below), you would see that the number of **active** threads never goes above `51` (50 concurrent threads + 1, which is the main thread), despite setting the `max_workers` argument to `60` in the example below. This is simply because, in this performance test, the application is never required to serve more than `50` requests at the same time. Also, `ThreadPoolExecutor` won't spin new threads, if idle threads are available (thus saving resources)&mdash;see the [relevant implementation part][20]. Hence, again, initialising the `ThreadPoolExecutor` with `max_workers=100`, as shown in your recent update, would be unecessary, if you never expect your FastAPI application to serve more than 50 requests at a time.

#### Working Example
```python
from fastapi import FastAPI, Request
from contextlib import asynccontextmanager
import concurrent.futures
import threading
import asyncio
import time


@asynccontextmanager
async def lifespan(app: FastAPI):    
    pool = concurrent.futures.ThreadPoolExecutor(max_workers=60)
    yield {'pool': pool}
    pool.shutdown()


app = FastAPI(lifespan=lifespan)


@app.get("/sync_async")
async def test_sync_async(request: Request) -> None:
    loop = asyncio.get_running_loop()
    await loop.run_in_executor(request.state.pool, time.sleep, 3)  
    print("sync_async")


@app.get('/get_active_threads')
async def get_active_threads():
    return threading.active_count()
```

Using ApacheBench, you could test the example above as follows:
```
ab -n 1000 -c 50 "http://localhost:8000/sync_async"
```

## Final Notes
In general, you should always aim for using *asynchronous* code (i.e., using `async`/`await`), wherever is possible, as `async` code, also known as coroutines, run in the event loop, which runs in the main thread and executes all tasks in that thread. That means there is only **one** thread that can take a lock on the interpreter. When dealing with *sync* blocking IO-bound tasks though, you could either (1) define your endpoint with `def` and let FastAPI handle it behind the scenes as described earlier and in [this answer][3], or (2) define your endpoint with `async def` and use `run_in_threadpool()` on your own to run that blocking task in a separate thread and `await` it, or (3) use `asyncio`'s `loop.run_in_executor()` with a custom (preferably re-usable) `ThreadPoolExecutor`, adjusting the number of workers as required. When required to perform blocking CPU-bound tasks, while running such tasks in an external thread and `await`ing them would successfully prevent the event loop from getting blocked, it wouldn't, however, provide the performance improvement you would expect from running code in parallel. Thus, for CPU-bound tasks, one may choose to use a `ProcessPoolExecutor` instead (**Note:** when using processes in general, you need to explicitly protect the entry point with `if __name__ == '__main__'`)&mdash;example on using a `ProcessPoolExecutor` can be found in [this answer][21]. 

To run tasks in the background, without waiting for them to complete in order to proceed with executing the rest of the code in an endpoint, you could use FastAPI's [`BackgroundTasks`][22], as shown [here][23] and [here][24]. If the background task function is defined with `async def`, FastAPI will run it directly in the event loop, whereas if it is defined with normal `def`, FastAPI will use `run_in_threadpool()` and `await` the returned coroutine (same concept as API endpoints). Another option when you need to run an `async def` function in the background, but not necessarily having it trigerred after returning a FastAPI response (which is the case in `BackgroundTasks`), is to use [`asyncio.create_task()`][25], as shown in [this answer][26] and [this answer][27]. If you need to [perform heavy background computation][28] and you don't necessarily need it to be run by the same process, you may benefit from using other bigger tools such as Celery.

Finally, regarding the **optimal/maximum number of worker threads**, I would suggest reading [this article][29] (have a look at [this article][30] as well for more details on `ThreadPoolExecutor` in general). As explained in the article:

> It is important to **limit the number** of worker threads in the thread
> pools to the number of asynchronous tasks you wish to complete, **based
> on** the resources in your system, or on the number of resources you
> intend to use within your tasks.
> 
> Alternately, you may wish to **increase the number** of worker threads
> dramatically, **given the greater capacity** in the resources you intend
> to use.
>
> [...]
>
> It is common to have **more threads than CPUs** (physical or logical) in
> your system. The reason for this is that threads are used for IO-bound tasks, not
> CPU-bound tasks. This means that threads are used for tasks that wait
> for relatively slow resources to respond, like hard drives, DVD
> drives, printers, network connections, and much more.
> 
> Therefore, **it is not uncommon** to have tens, hundreds and even
> thousands of threads in your application, **depending on your specific
> needs**. It is unusual to have more than one or a few thousand threads.
> If you require this many threads, then alternative solutions may be
> preferred, such as `AsyncIO`.

Also, in the same article:

> #### Does the Number of Threads in the `ThreadPoolExecutor` Match the Number of CPUs or Cores?
> 
> The number of worker threads in the `ThreadPoolExecutor` is **not
> related** to the number of CPUs or CPU cores in your system.
> 
> You can configure the number of threads **based on** the number of
> tasks you need to execute, the amount of local system resources you
> have available (e.g., memory), and the limitations of resources you
> intend to access within your tasks (e.g., connections to remote
> servers).
> 
> #### How Many Threads Should I Use?
> 
> If you have hundreds of tasks, you should probably set the number of
> threads to be equal to the number of tasks.
> 
> If you have thousands of tasks, you should probably cap the number of
> threads at hundreds or 1,000.
> 
> If your application is intended to be executed multiple times in the
> future, you can test different numbers of threads and compare overall
> execution time, then choose a number of threads that gives
> approximately the best performance. You may want to mock the task in
> these tests with a random sleep operation.
> 
> #### What Is the Maximum Number of Worker Threads in the `ThreadPoolExecutor`?
> 
> There is no maximum number of worker threads in the
> `ThreadPoolExecutor`.
> 
> Nevertheless, your system will have an upper limit of the number of
> threads you can create based on **how much main memory (RAM) you have
> available**.
> 
> Before you exceed main memory, you will reach a point of diminishing
> returns in terms of adding new threads and executing more tasks. This
> is because your operating system must switch between the threads,
> called *context switching*. With too many threads active at once, your
> program may spend more time context switching than actually executing
> tasks.
> 
> A sensible upper limit for many applications is hundreds of threads to
> perhaps a few thousand threads. More than a few thousand threads on a
> modern system may result in too much context switching, depending on
> your system and on the types of tasks that are being executed.


  [1]: https://github.com/encode/starlette/blob/ec417f7f84f3533f928a4bc2b8dd0c6c51cdbbad/starlette/concurrency.py#L36
  [2]: https://anyio.readthedocs.io/en/stable/api.html#anyio.to_thread.run_sync
  [3]: https://stackoverflow.com/a/71517830/17865804
  [4]: https://anyio.readthedocs.io/en/stable/threads.html#working-with-threads
  [5]: https://github.com/agronholm/anyio/blob/137de708f40262e959d9d43178dda600432da56b/src/anyio/_backends/_asyncio.py#L2088
  [6]: https://anyio.readthedocs.io/en/stable/threads.html#adjusting-the-default-maximum-worker-thread-count
  [7]: https://github.com/tiangolo/fastapi/blob/3f3ee240dd8656962e94e89eceb3838508982068/fastapi/concurrency.py#L7
  [8]: https://github.com/agronholm/anyio/blob/137de708f40262e959d9d43178dda600432da56b/src/anyio/_backends/_asyncio.py#L2435
  [9]: https://github.com/agronholm/anyio/blob/137de708f40262e959d9d43178dda600432da56b/src/anyio/_backends/_asyncio.py#L1657
  [10]: https://github.com/tiangolo/fastapi/issues/4221#issuecomment-982260467
  [11]: https://stackoverflow.com/a/76322910/17865804
  [12]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor
  [13]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
  [14]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop
  [15]: https://github.com/python/cpython/blob/39ec7fbba84663ab760853da2ac422c2e988d189/Lib/asyncio/base_events.py#L872
  [16]: https://github.com/python/cpython/blob/d466052ad48091a00a50c5298f33238aff591028/Lib/concurrent/futures/thread.py#L145
  [17]: https://docs.python.org/3/library/os.html#os.cpu_count
  [18]: https://superfastpython.com/number-of-cpus-python/
  [19]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
  [20]: https://github.com/python/cpython/blob/9afc6d102d16080535325f645849cd84eb04d57d/Lib/concurrent/futures/thread.py#L181
  [21]: https://stackoverflow.com/a/77862153/17865804
  [22]: https://fastapi.tiangolo.com/tutorial/background-tasks/
  [23]: https://stackoverflow.com/a/76280152
  [24]: https://stackoverflow.com/a/73283272
  [25]: https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task
  [26]: https://stackoverflow.com/a/70873984/17865804
  [27]: https://stackoverflow.com/a/76148361/17865804
  [28]: https://stackoverflow.com/a/74508996/17865804
  [29]: https://superfastpython.com/threadpoolexecutor-number-of-threads/
  [30]: https://superfastpython.com/threadpoolexecutor-in-python/

--------------------------------------------------
How to make child div scrollable when it exceeds parent height?
I have 2 child divs nested in a parent div in row-column pattern: the parent is a column, and the children are rows.![enter image description here][1]

The upper child div is of variable height, but is guaranteed to be less than the height of the parent div.

The lower child div is also of variable height.  In some cases, the heights of the child divs will make the lower child div exceed the parent.  In this case, I need to make the lower div scrollable. Note that I want only the lower div to be scrollable, not the whole parent div.

How do I handle this?

See attached jsfiddle for case example: http://jsfiddle.net/0yxnaywu/5/

**HTML:**

  

     &lt;div class=&quot;parent&quot;&gt;
        &lt;div class=&quot;child1&quot;&gt;
            hello world filler
        &lt;/div&gt;
        &lt;div class=&quot;child2&quot;&gt;
            this div should overflow and scroll down
        &lt;/div&gt;
    &lt;/div&gt;

**CSS:**

&lt;!-- language: css --&gt;

    .parent {
        width: 50px;
        height: 100px;
        border: 1px solid black;
    }
    
    .child1 {
        background-color: red;
    }
    
    .child2 {
        background-color: blue;
    }

  [1]: http://i.stack.imgur.com/9YxIb.png
||||||||||||||Overflow only works when you give it a value to overflow when greater than. Your value is relative to how big the top is, so using jQuery, grab that value then subtract from the parent.

<!-- language: js -->

    $(document).ready(function() {
      $(".child2").css("max-height", ($(".parent").height()-$(".child1").height()));
    });

and add `overflow`'s to the children

<!-- language: css -->

    .child1 {
        background-color: red;
        overflow: hidden;
    }

    .child2 {
        background-color: blue;
        overflow: auto;
    }

http://jsfiddle.net/m9goxrbk/

--------------------------------------------------
Create a column based on multiple columns for certain rows Power BI
I have the next table in Power BI

    | Ubicaci&#243;n.Name     | Fecha_entrega__c | Sector_entrante__c |
    | ------------------ | ---------------- | -------------------| 
    | PAD FP.c-1050      | 5/31/2021        | Perforaci&#243;n        |
    | PAD LAnch.x-2(h)   | 4/30/2022        | Terminaci&#243;n        | 
    | PAD LAnch.x-2(h)   | 2/28/2022        | Perforaci&#243;n        |
    | PAD LAnch.x-2(h)   | 7/13/2022        | Well Testing       | 
    | PAD de Pozos 1003  | 4/23/2022        | Terminaci&#243;n        |  
    | PAD de Pozos 1003  | 8/11/2022        | Perforaci&#243;n        | 

I would like to create a column based on the next logic

For a certain group in &quot;Ubicaci&#243;n.Name&quot;, for example, &quot;PAD LAnch.x-2(h)&quot;, there are three rows of this kind, I want to check whether the newest of these rows, based on &quot;Fecha_entrega__c&quot; column, has in &quot;Sector_entrante__c&quot; column the string &quot;Well Testing&quot;. In this case we can see in row 4 which has the newest date of rows 2,3 and 4 (all of them part of &quot;PAD LAnch.x-2(h)&quot;) that in column &quot;Sector_entrante__c&quot; it says &quot;Well Testing&quot;, so I want a column that gives the number 1 for rows 2,3 and 4. If it didn&#39;t have &quot;Well Testing&quot; I would like to give the value 0 for rows 2,3 and 4. 

    | Ubicaci&#243;n.Name     | Fecha_entrega__c | Sector_entrante__c | Column |
    | ------------------ | ---------------- | -------------------| -------|
    | PAD FP.c-1050      | 5/31/2021        | Perforaci&#243;n        | 0      |
    | PAD LAnch.x-2(h)   | 4/30/2022        | Terminaci&#243;n        | 1      |
    | PAD LAnch.x-2(h)   | 2/28/2022        | Perforaci&#243;n        | 1      |
    | PAD LAnch.x-2(h)   | 7/13/2022        | Well Testing       | 1      |
    | PAD de Pozos 1003  | 4/23/2022        | Terminaci&#243;n        | 0      |  
    | PAD de Pozos 1003  | 8/11/2022        | Perforaci&#243;n        | 0      |

Thanks. 

The only thing I have been able to do is to use Rank and Filter to identify with numbers which are the newest and oldest of these group

`RANK Column = RANKX(FILTER(&#39;Form  NQN PAD Handover (3)&#39;,&#39;Form  NQN PAD Handover (3)&#39;[Ubicaci&#243;n.Name]=EARLIER(&#39;Form  NQN PAD Handover (3)&#39;[Ubicaci&#243;n.Name])),&#39;Form  NQN PAD Handover (3)&#39;[Fecha_entrega__c],,DESC)`

But I really have no idea how to apply logic to certain groups within columns.
||||||||||||||[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/edZMW.png


    Column = 
    VAR x = CALCULATE(MAX('Table'[Fecha_entrega__c]), ALLEXCEPT('Table', 'Table'[Ubicación.Name]))
    VAR y = CALCULATE(MAX('Table'[Sector_entrante__c]), 'Table'[Fecha_entrega__c] = x, ALLEXCEPT('Table', 'Table'[Ubicación.Name]))
    
    RETURN IF(y = "Well Testing", 1,0)

--------------------------------------------------
$ flutter pub run build_runner build in project with hive not responding
When run $ flutter pub run build_runner build in project with hive, it just stops here(i have even waited 2 hours and its not going any further),
i have tried creating a new project specifically for hive implementation. but its the same issue
```
[INFO] Generating build script...
[INFO] Generating build script completed, took 528ms
[WARNING] Deleted previous snapshot due to missing asset graph.
[INFO] Creating build script snapshot......
[INFO] Creating build script snapshot... completed, took 21.3s
[INFO] Initializing inputs
[INFO] Building new asset graph...
[INFO] Building new asset graph completed, took 1.2s
[INFO] Checking for unexpected pre-existing outputs....
[INFO] Checking for unexpected pre-existing outputs. completed, took 2ms
[INFO] Running build...
[INFO] Generating SDK summary...
```

this is my class:

```
import &#39;package:hive/hive.dart&#39;;

part &#39;person.g.dart&#39;;

@HiveType(typeId: 0)
class Person extends HiveObject {
  @HiveField(0)
  int id;
  @HiveField(1)
  String name;
  @HiveField(2)
  DateTime birthDate;
  Person(this.id, this.name, this.birthDate);
}
```
and my pubspec.yaml file:

```
environment:
  sdk: &quot;&gt;=2.7.0 &lt;3.0.0&quot;

dependencies:
  flutter:
    sdk: flutter
  hive:
  hive_flutter:
  path_provider:

  # The following adds the Cupertino Icons font to your application.
  # Use with the CupertinoIcons class for iOS style icons.
  cupertino_icons: ^0.1.3

dev_dependencies:
  flutter_test:
    sdk: flutter
  build_runner:
  hive_generator:


flutter:

  
```
||||||||||||||*I Was Also facing the same issue and solved it with,*

    flutter pub upgrade

*If that doesn't help you, then try these steps too*

    flutter clean
    
    flutter pub get
    
    flutter packages pub run build_runner build --delete-conflicting-outputs  

--------------------------------------------------
How to access localStorage in node.js?
I tried searching the web for a node module that can access the client&#39;s localStorage but wasn&#39;t able to find anything. Anyone know of one?
||||||||||||||You can use :

`node-localstorage` npm module to use `localStorage` at the [tag:NodeJS] server side.

    var LocalStorage = require('node-localstorage').LocalStorage,
    localStorage = new LocalStorage('./scratch');

--------------------------------------------------
Get all runes with id and name from League of legends API
When performing requests to the riot API endpoint [/lol/match/v4/matches/{matchId}](https://developer.riotgames.com/apis#match-v4) the response contains rune data for each player in the match .e.g

```
&quot;perk0&quot;: 8005,
&quot;perk0Var1&quot;: 2107,
&quot;perk0Var2&quot;: 1319,
&quot;perk0Var3&quot;: 788,
```

These are only the Id values for the runes. Where can I get the corresponding name for the rune ?

I&#39;ve tried the following request : https://euw1.api.riotgames.com/lol/static-data/v1/runes , but returns the following response :

```
{
    &quot;status&quot;: {
        &quot;message&quot;: &quot;Forbidden&quot;,
        &quot;status_code&quot;: 403
    }
}
```
||||||||||||||I found the resource that stores all the needed data here : http://ddragon.leagueoflegends.com/cdn/10.16.1/data/en_US/runesReforged.json

--------------------------------------------------
How to efficiently make a recursive table in graphql?
I am writing a graphql schema using amplify. I have a categories table in which I want to relate each category with other categories. Every Category can have multiple parents or children.
This is what I have right now.

```
type Category @model {
  id: ID! @primaryKey
  name: String!
  parents: [Category] @hasMany
  subCategories:[Category] @hasMany
  categoryParentsId: [String] @index(name: &quot;byParent&quot;)
  categorySubCategoriesId: [String] @index(name: &quot;byChild&quot;)
}

type Query {
  getAllCategories: [Category!]!
}

input CreateCategoryInput {
  name: String!
  categoryParentsId: [String]
  categorySubCategoriesId: [String]
}
```
I am using the mutation generated by amplify: 
```
export const createCategory = /* GraphQL */
  mutation CreateCategory( $input: CreateCategoryInput! $condition: ModelCategoryConditionInput ) {
    createCategory(input: $input, condition: $condition) {
      id
      name
      parents {
        nextToken
        __typename
      }
      items {
        nextToken
        __typename
      }
      subCategories {
        nextToken
        __typename
      }
      categoryParentsId
      categorySubCategoriesId
      createdAt
      updatedAt
      __typename
    }
  }
```
I am using a react app in the frontend. Whenever I try to input this error shows up from Amplify.
&gt; &quot;One or more parameter values were invalid: Type mismatch for Index Key categoryParentsId Expected: S Actual: L IndexName: gsi-Category.parents (Service: DynamoDb, Status Code: 400&quot;

I understand that it is generating a GSI which are categoryParentsId and categorySubCategoriesId which can&#39;t be other than string, number or boolean. So it kind of conflicts with what I stated as an array of strings.

Should I just add another field that stores that array of strings? Is there a better approach?
||||||||||||||You don't need to refer to both the parent and child types **and** the corresponding ids in your GraphQL type:
```
type Category @model {
  id: ID! @primaryKey
  name: String!
  parents: [Category] @hasMany
  subCategories:[Category] @hasMany
 }
```
should suffice.

> Normally references to related objects are done by referring to the object *type* and not the id (you can always get the id(s) of the related objects by including them in your query). In your case you have both.

Also please include the definition of the mutation you are attempting. You showed the `input type` but not the mutation.

--------------------------------------------------
Heikin Ashi candle code in pine script V5
In pinescript version 4, the Heikin Ashi candle open is calculated as:

```
ha_close = (open + high + low + close)/4
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```
However, it shows compalilation error in pinescript version 5:
```
ha_open = na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2
```

The compilation error &quot;Undeclared identifier &#39;ha_open&#39;&quot;.

I have no idea what to do to solve this. 
||||||||||||||In pinescript, you must declare variables before using them.  
You should use (for version 5) :

    var float ha_open = na
    ha_close = (open + high + low + close)/4
    ha_open := na(ha_open[1]) ? (open + close)/2 : (ha_open[1] + ha_close[1]) / 2

`var float ha_open = na` declare the variable as a float and initialize it to `na`

--------------------------------------------------
fatal error: opencv2/opencv_modules.hpp: No such file or directory #include &quot;opencv2/opencv_modules.hpp&quot;
Hello all I am trying to use opencv-c++ API (version 4.4.0) which I have built from source. It is installed in /usr/local/ and I was simply trying to load and display an image using the following code - 
```
#include &lt;iostream&gt;
#include &lt;opencv4/opencv2/opencv.hpp&gt;
#include &lt;opencv4/opencv2/core.hpp&gt;
#include &lt;opencv4/opencv2/imgcodecs.hpp&gt;
#include &lt;opencv4/opencv2/highgui.hpp&gt;
#include &lt;opencv4/opencv2/core/cuda.hpp&gt;

using namespace cv;

int main()
{
    std::string image_path = &quot;13.jpg&quot;;
    cv::Mat img = cv::imreadmulti(image_path, IMREAD_COLOR);
    if(img.empty())
    {
        std::cout&lt;&lt;&quot;COULD NOT READ IMAGE&quot;&lt;&lt;std::endl;
        return 1;
    }
    imshow(&quot;Display Window&quot;, img);
    return 0;
}
```
And when I compile it throws the following error during compilation - 
```
In file included from /CLionProjects/opencvTest/main.cpp:2:
/usr/local/include/opencv4/opencv2/opencv.hpp:48:10: fatal error: opencv2/opencv_modules.hpp: No such file or directory
 #include &quot;opencv2/opencv_modules.hpp&quot;
```
My Cmake is as follows - 

```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
include_directories(&quot;/usr/local/include/opencv4/opencv2/&quot;)
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest PUBLIC &quot;/usr/local/lib/&quot;)
```
I do not know what am I doing wrong here.. This might be a noob question, But I ahev just started using opencv in C++
||||||||||||||The solution is to just include_directories path till `/usr/local/opencv4` and it works perfectly.

However, the best way I believe is to use the `find_package` function. I updated my Cmake to the following and it takes care of linking during build. 
```
cmake_minimum_required(VERSION 3.15)
project(opencvTest)

set(CMAKE_CXX_STANDARD 17)
find_package(OpenCV REQUIRED)
include_directories(${OpenCV_INCLUDE_DIRS})
add_executable(opencvTest main.cpp)
target_link_libraries(opencvTest ${OpenCV_LIBS})
``` 


--------------------------------------------------
how to get the url parameters from http
I am working in a very rudimentary &quot;routing&quot; system for small CMS in nodejs without express or any framework. My aim is to have very few dependencies. 
For templating I found jrender that works fine in the sample route &quot;hey&quot; below: 

    var http = require(&#39;http&#39;)
    var jsrender = require (&#39;jsrender&#39;);    
    
    var html = jsrender.renderFile(&#39;./templates/hey.html&#39;, {name: &quot;Jim&quot;, age: &quot;22&quot;});
        
    
    http.createServer(function (req, res) {
    	res.writeHead(200, {&#39;Content-Type&#39;: &#39;text/html&#39;}); // http header
    
    	var url = req.url;
    	if(url ===&#39;/about&#39;){
            console.log (req.url)
      		res.write(&quot;hey&quot;); //write a response
      		res.end(); //end the response
            
    	}else if(url ===&#39;/contact&#39;){
      		res.write(&#39;&lt;h1&gt;contact us page&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
            
        }else if(url ===&#39;/hey&#39;){
      		res.write(html); //write a response
      		res.end(); //end the response    
            
    	}else{
      		res.write(&#39;&lt;h1&gt;Hello World!&lt;h1&gt;&#39;); //write a response
      		res.end(); //end the response
    	}
    
    }).listen(3000, function(){
    	console.log(&quot;Judge Dress live on port 3000&quot;); //the server object listens on port 3000
    }); 


My problem is to get a parameter for a page e.g. /?pages=pagename to have dynamic routes. Is there any way to extact this parameter from req.url ? 

||||||||||||||You can use the node.js built-in 'querystring' module. To get "me" from "http://localhost:3000/about/?pages=me"

    const querystring = require('querystring');     
    console.log(querystring.parse(req.url)["/about/?pages"])

--------------------------------------------------
Regex to match all strings of given format with given exceptions
I&#39;m really struggling with this one. I tried to search from left to right, but still can&#39;t figure this out.

I have a list of strings with random amount of tags, each placed in brackets, randomly positioned within each string. Few examples may look as follows.


```
[tag1][tag4] Desired string - with optional dash [tag10]
[tag1][tag2][tag3] Desired string [tag10]
[tag3][tag1][tag2][tag5] Desired - string (with suffix)
[tag2][tag5][tag4] [Animation] Target string [tag10]
[tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
```

What I&#39;m trying to achieve is to extract from each string the content without tags, which are enclosed in brackets. The only exception is tag **[Animation]** or **[Animations]**. In case, one of these tags appear, I want to extract them as well together with the desired string.

So in case of list above, the desired output would be following. (I don&#39;t care about the whitespace around extracted strings, it will be trimmed afterwards.)

```
Desired string - with optional dash
Desired string
Desired - string (with suffix)
[Animation] Target string
[Animations](prefix)Desired - string (and suffix)
```


Originally, I was using as simple regex as `\[.*?\]`. Which matched all tags in brackets, and I simply replaced everything with empty string.

```python
re_pattern = r&quot;\[.*?\]&quot;
re.sub(re_pattern, &#39;&#39;, dirty_string).strip()
```

However, now I found a need to have an exception for tags **[Animation]** and **[Animations]**, and really can&#39;t figure it out. Your help would be much appreciated.
Thanks.
||||||||||||||You could use the better `regex` module with the following expression:

    \[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*

In `Python`, this could be

    import regex as re
    
    data = """
    [tag1][tag4] Desired string - with optional dash [tag10]
    [tag1][tag2][tag3] Desired string [tag10]
    [tag3][tag1][tag2][tag5] Desired - string (with suffix)
    [tag2][tag5][tag4] [Animation] Target string [tag10]
    [tag3][tag1][tag5][tag10][Animations](prefix)Desired - string (and suffix)
    """
    
    pattern = re.compile(r'\[Animations?\](*SKIP)(*FAIL)|\[[^][]+\]\h*')
    
    print(pattern.sub("", data))

And would yield

    Desired string - with optional dash 
    Desired string 
    Desired - string (with suffix)
    [Animation] Target string 
    [Animations](prefix)Desired - string (and suffix)



--------------------------------------------------
Node.js server that accepts POST requests
I&#39;m trying to allow javascript to communicate with a Node.js server. 

**POST request (web browser)**

    var	http = new XMLHttpRequest();
    var params = &quot;text=stuff&quot;;
    http.open(&quot;POST&quot;, &quot;http://someurl.net:8080&quot;, true);
    
    http.setRequestHeader(&quot;Content-type&quot;, &quot;application/x-www-form-urlencoded&quot;);
    http.setRequestHeader(&quot;Content-length&quot;, params.length);
    http.setRequestHeader(&quot;Connection&quot;, &quot;close&quot;);
    
    alert(http.onreadystatechange);
    http.onreadystatechange = function() {
      if (http.readyState == 4 &amp;&amp; http.status == 200) {
        alert(http.responseText);
      }
    }
    
    http.send(params);

Right now the Node.js server code looks like this. Before it was used for GET requests. I&#39;m not sure how to make it work with POST requests.

**Server (Node.js)**

    var server = http.createServer(function (request, response) {
      var queryData = url.parse(request.url, true).query;
    
      if (queryData.text) {
        convert(&#39;engfemale1&#39;, queryData.text, response);
    	response.writeHead(200, {
    	  &#39;Content-Type&#39;: &#39;audio/mp3&#39;, 
    	  &#39;Content-Disposition&#39;: &#39;attachment; filename=&quot;tts.mp3&quot;&#39;
    	});
      } 
      else {
        response.end(&#39;No text to convert.&#39;);
      }
    }).listen(8080);
||||||||||||||The following code shows how to read values from an HTML form. As @pimvdb said you need to use the request.on('data'...) to capture the contents of the body.
```
const http = require('http')

const server = http.createServer(function(request, response) {
  console.dir(request.param)

  if (request.method == 'POST') {
    console.log('POST')
    var body = ''
    request.on('data', function(data) {
      body += data
      console.log('Partial body: ' + body)
    })
    request.on('end', function() {
      console.log('Body: ' + body)
      response.writeHead(200, {'Content-Type': 'text/html'})
      response.end('post received')
    })
  } else {
    console.log('GET')
    var html = `
			<html>
				<body>
					<form method="post" action="http://localhost:3000">Name: 
						<input type="text" name="name" />
						<input type="submit" value="Submit" />
					</form>
				</body>
			</html>`
    response.writeHead(200, {'Content-Type': 'text/html'})
    response.end(html)
  }
})

const port = 3000
const host = '127.0.0.1'
server.listen(port, host)
console.log(`Listening at http://${host}:${port}`)


```

If you use something like [Express.js][1] and [Bodyparser](https://www.npmjs.com/package/body-parser) then it would look like this since Express will handle the request.body concatenation


```
var express = require('express')
var fs = require('fs')
var app = express()

app.use(express.bodyParser())

app.get('/', function(request, response) {
  console.log('GET /')
  var html = `
    <html>
        <body>
            <form method="post" action="http://localhost:3000">Name: 
                <input type="text" name="name" />
                <input type="submit" value="Submit" />
            </form>
        </body>
    </html>`
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end(html)
})

app.post('/', function(request, response) {
  console.log('POST /')
  console.dir(request.body)
  response.writeHead(200, {'Content-Type': 'text/html'})
  response.end('thanks')
})

const port = 3000
app.listen(port)
console.log(`Listening at http://localhost:${port}`)

```

  [1]: http://expressjs.com/


--------------------------------------------------
How can I validate an email address using a regular expression?
Over the years I have slowly developed a [regular expression][1] that validates *most* email addresses correctly, assuming they don&#39;t use an IP address as the server part.

I use it in several PHP programs, and it works most of the time.  However, from time to time I get contacted by someone that is having trouble with a site that uses it, and I end up having to make some adjustment (most recently I realized that I wasn&#39;t allowing four-character [TLDs][2]).

*What is the best regular expression you have or have seen for validating emails?*

I&#39;ve seen several solutions that use functions that use several shorter expressions, but I&#39;d rather have one long complex expression in a simple function instead of several short expression in a more complex function.

  [1]: http://en.wikipedia.org/wiki/Regular_expression
  [2]: https://en.wikipedia.org/wiki/Top-level_domain



||||||||||||||The [fully RFC 822 compliant regex][1] is inefficient and obscure because of its length.  Fortunately, RFC 822 was superseded twice and the current specification for email addresses is [RFC 5322][2].  RFC 5322 leads to a regex that can be understood if studied for a few minutes and is efficient enough for actual use.

One RFC 5322 compliant regex can be found at the top of the page at http://emailregex.com/ but uses the IP address pattern that is floating around the internet with a bug that allows `00` for any of the unsigned byte decimal values in a dot-delimited address, which is illegal.  The rest of it appears to be consistent with the RFC 5322 grammar and passes several tests using `grep -Po`, including cases domain names, IP addresses, bad ones, and account names with and without quotes.

Correcting the `00` bug in the IP pattern, we obtain a working and fairly fast regex.  (Scrape the rendered version, not the markdown, for actual code.)

 > (?:[a-z0-9!#$%&'\*+/=?^\_\`{|}~-]+(?:\\.[a-z0-9!#$%&'\*+/=?^_\`{|}~-]+)\*|"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])\*")@(?:(?:\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?\\.)+\[a-z0-9\](?:\[a-z0-9-\]\*\[a-z0-9\])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])

or:

```
(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])
```

Here is [diagram][3] of [finite state machine][4] for above regexp which is more clear than regexp itself
[![enter image description here][5]][5]


The more sophisticated patterns in Perl and PCRE (regex library used e.g. in PHP) can [correctly parse RFC 5322 without a hitch][6]. Python and C# can do that too, but they use a different syntax from those first two. However, if you are forced to use one of the many less powerful pattern-matching languages, then it’s best to use a real parser.

It's also important to understand that validating it per the RFC tells you absolutely nothing about whether that address actually exists at the supplied domain, or whether the person entering the address is its true owner. People sign others up to mailing lists this way all the time. Fixing that requires a fancier kind of validation that involves sending that address a message that includes a confirmation token meant to be entered on the same web page as was the address. 

Confirmation tokens are the only way to know you got the address of the person entering it. This is why most mailing lists now use that mechanism to confirm sign-ups. After all, anybody can put down `president@whitehouse.gov`, and that will even parse as legal, but it isn't likely to be the person at the other end.

For PHP, you should *not* use the pattern given in [Validate an E-Mail Address with PHP, the Right Way][7] from which I quote:

> There is some danger that common usage and widespread sloppy coding will establish a de facto standard for e-mail addresses that is more restrictive than the recorded formal standard.

That is no better than all the other non-RFC patterns. It isn’t even smart enough to handle even [RFC 822][8], let alone RFC 5322. [This one][6], however, is.

If you want to get fancy and pedantic, [implement a complete state engine][9]. A regular expression can only act as a rudimentary filter. The problem with regular expressions is that telling someone that their perfectly valid e-mail address is invalid (a false positive) because your regular expression can't handle it is just rude and impolite from the user's perspective. A state engine for the purpose can both validate and even correct e-mail addresses that would otherwise be considered invalid as it disassembles the e-mail address according to each RFC. This allows for a potentially more pleasing experience, like

>The specified e-mail address 'myemail@address,com' is invalid. Did you mean 'myemail@address.com'?

See also [Validating Email Addresses][10], including the comments. Or [Comparing E-mail Address Validating Regular Expressions][11].

[![Regular expression visualization](https://i.stack.imgur.com/SrUwP.png)](https://i.stack.imgur.com/SrUwP.png)

[Debuggex Demo][12]


  [1]: http://ex-parrot.com/~pdw/Mail-RFC822-Address.html
  [2]: https://datatracker.ietf.org/doc/html/rfc5322
  [3]: https://regexper.com/#(%3F%3A%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B(%3F%3A%5C.%5Ba-z0-9!%23%24%25%26'*%2B%2F%3D%3F%5E_%60%7B%7C%7D~-%5D%2B)*%7C%22(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21%5Cx23-%5Cx5b%5Cx5d-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)*%22)%40(%3F%3A(%3F%3A%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%5C.)%2B%5Ba-z0-9%5D(%3F%3A%5Ba-z0-9-%5D*%5Ba-z0-9%5D)%3F%7C%5C%5B(%3F%3A(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D))%5C.)%7B3%7D(%3F%3A(2(5%5B0-5%5D%7C%5B0-4%5D%5B0-9%5D)%7C1%5B0-9%5D%5B0-9%5D%7C%5B1-9%5D%3F%5B0-9%5D)%7C%5Ba-z0-9-%5D*%5Ba-z0-9%5D%3A(%3F%3A%5B%5Cx01-%5Cx08%5Cx0b%5Cx0c%5Cx0e-%5Cx1f%5Cx21-%5Cx5a%5Cx53-%5Cx7f%5D%7C%5C%5C%5B%5Cx01-%5Cx09%5Cx0b%5Cx0c%5Cx0e-%5Cx7f%5D)%2B)%5C%5D)
  [4]: https://en.wikipedia.org/wiki/Finite-state_machine
  [5]: https://i.stack.imgur.com/YI6KR.png
  [6]: https://stackoverflow.com/questions/201323/what-is-the-best-regular-expression-for-validating-email-addresses/1917982#1917982
  [7]: http://www.linuxjournal.com/article/9585
  [8]: https://datatracker.ietf.org/doc/html/rfc822
  [9]: http://cubicspot.blogspot.com/2012/06/correct-way-to-validate-e-mail-address.html
  [10]: http://worsethanfailure.com/Articles/Validating_Email_Addresses.aspx
  [11]: http://fightingforalostcause.net/misc/2006/compare-email-regex.php
  [12]: https://www.debuggex.com/r/aH_x42NflV8G-GS7

--------------------------------------------------
JPA generating broken SQL when using native query and pageable
Using Spring-Boot 2.7.7, when I attempt to create a native PostgreSQL query that receives a Pageable and outputs a Page, it seems to generate a broken SQL for one of the page attributes

This is the Query I used for the function:

```
    @Query(value =&quot;SELECT * &quot; +
            &quot;FROM propriedade &quot; +
            &quot;INNER JOIN proprietario &quot; +
            &quot;  ON proprietario.id = propriedade.proprietario_id &quot; +
            &quot;WHERE proprietario.nome ILIKE %:proprietario% &quot;,
            nativeQuery = true
    )
    Page&lt;Propriedade&gt; findByProprietarioLikePage(Pageable pageable, @Param(&quot;proprietario&quot;) String proprietario);
```
When I try to call the function, it generates a few SQL commands, but breaks in this one:
```
Hibernate: select count(INNER) FROM propriedade INNER JOIN proprietario   ON proprietario.id = propriedade.proprietario_id WHERE proprietario.nome ILIKE ?   AND condominio_id = ? 
2023-08-07 10:34:57.361  WARN 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42601
2023-08-07 10:34:57.361 ERROR 319426 --- [nio-8082-exec-4] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: syntax error at or near &quot;)&quot;

```
If I try to run this count on PSQL, I get the same error:
```
ERROR:  syntax error at or near &quot;)&quot;
LINE 1: select count(INNER) FROM propriedade INNER JOIN proprietario...
                          ^
```
The problem seems to be with the generated query to count the entries in the table
||||||||||||||Please, try to add an alias, like "p", after "propriedade". It'll be like this: 

    @Query(value ="SELECT * " +
            "FROM propriedade p " +
            "INNER JOIN proprietario " +
            "  ON proprietario.id = p.proprietario_id " +
            "WHERE proprietario.nome ILIKE %:proprietario% ",
            nativeQuery = true
    )

--------------------------------------------------
Get child node index
In straight up javascript (i.e., no extensions such as jQuery, etc.), is there a way to determine a child node&#39;s index inside of its parent node without iterating over and comparing all children nodes?

E.g.,

    var child = document.getElementById(&#39;my_element&#39;);
    var parent = child.parentNode;
    var childNodes = parent.childNodes;
    var count = childNodes.length;
    var child_index;
    for (var i = 0; i &lt; count; ++i) {
      if (child === childNodes[i]) {
        child_index = i;
        break;
      }
    }

Is there a better way to determine the child&#39;s index?
||||||||||||||you can use the `previousSibling` property to iterate back through the siblings until you get back `null` and count how many siblings you've encountered:

    var i = 0;
    while( (child = child.previousSibling) != null ) 
      i++;
    //at the end i will contain the index.

Please note that in languages like Java, there is a `getPreviousSibling()` function, however in JS this has become a property -- `previousSibling`.

Use [previousElementSibling][2] or [nextElementSibling][1] to ignore text and comment nodes.


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element/nextElementSibling
  [2]: https://developer.mozilla.org/en-US/docs/Web/API/Element/previousElementSibling

--------------------------------------------------
Is there a way to inherit the parent __init__ arguments?
Suppose I have a basic class inheritance:

```
class A:
    def __init__(self, filepath: str, debug=False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, **kwargs):
        super(B, self).__init__(**kwargs)
        self.portnumber = portnumber
```

For typing and completion purposes, I would like to somehow &quot;forward&quot; the list of arguments from `A.__init__()` to `B.__init__()`.


Is there a way to do this? To have a type checker correctly infer the signature for `B.__init__(...)` and have an IDE be able to provide meaningful completions or checks?

---

[edit] after searching a little bit more, here is something that is perhaps closer to what I look:

if I declared `A` and `B` as _dataclasses_ :

```
from dataclasses import dataclass

@dataclass
class A:
    filepath: str
    debug: bool = False

@dataclass
class B(A):
    portnumber: int = 42
```

I can get the following hints in vscode with the standard pylance extension:
[![screen capture of vscode autocompletion][1]][1]

Could there be something similar to target just the `__init__()` method?  
perhaps by explicitly naming the base method that gets &quot;extended&quot; (e.g: a special `@extends(A.__init__)` decorator)?

  [1]: https://i.stack.imgur.com/Xv9L0m.png
||||||||||||||Yes this is possible, but personally I wouldn't recommend it - see below:

```py

from typing import TypedDict, Unpack

class AInterface(TypedDict):
    filepath: str
    debug: bool

class A:
    def __init__(self, **kwargs: Unpack[AInterface]):
        self.filepath = kwargs["filepath"]
        self.debug = kwargs["debug"]

class B(A):
    def __init__(self, portnumber: int, **kwargs: Unpack[AInterface]):
        super().__init__(**kwargs)
        self.portnumber = portnumber
```

By using the `TypedDict` we can structure the kwargs argument giving it a type, and allowing us to pass it through. If you have multiple inheritance you could even combine the interfaces together to produce the current kwargs type. When you use the `__init__` for A and B you still get warned if you miss parts of the `TypedDict`.

I would instead just pass the arguments down to the next layer manually:

```py
class A:
    def __init__(self, filepath: str, debug: bool =False):
        self.filepath = filepath
        self.debug = debug

class B(A):
    def __init__(self, portnumber: int, filepath: str, debug: bool =False):
        super().__init__(filepath=filepath, debug = debug)
        self.portnumber = portnumber
```


--------------------------------------------------
MySQL / Laravel structure: monolith tables, or thousands of small tables?
**Short Version:**

Our webapp works with sets of scoped `project` data - that is, model relationships that will always relate to models in the same `project`, and strictly never cross over into other `project`s. Security / opacity between projects is a key requirement. The whole scoped relational ecosystem spans ~20 database tables so far.

We are currently managing this ecosystem with ~20 monolith tables, and enforcing opacity / security through code - but we&#39;re losing our grasp on it. We&#39;re considering adopting a structure where each `project` deploys its own clone of these ~20 tables into the same database. Are there any known fundamental drawbacks to having thousands of tables in one database, like increased storage size, slower performance, higher indexing overhead? Our team just doesn&#39;t have the database expertise to speak to flaws that might be introduced by this ourselves. 

If it makes any difference, we&#39;re using Laravel 10 - all models and relationships take advantage of Laravel / Eloquent structures. We&#39;re anticipating at least 200 projects active at a time, with a few being added or nuked each month.

-----

**Long Version:** We have an internal-use project org / management webapp, with some complicated requirements.

In broad strokes, there are `projects`, and each `project` has several related entities - `permissions`, `reports`, `tickets`, `labels`, `ticket_label`, `media`, `notifications`, many more. All related entities are scoped to their project - reports, labels, tickets, etc created inside a project by definition will never be transitioned to another, and no entities are generalized to multiple projects. Our project shareholders are adamant that a project&#39;s information is totally secure and opaque from other projects - no project should know any other project exists, and when a project is deleted there shouldn&#39;t be a trace of it left.

It&#39;s a little messy, because all these entities are binned into single tables with each other, regardless of project. There&#39;s extra work and middleware going into, for example, making sure some bad actor can&#39;t reassign a ticket ID in from an external project to gain access to its data. It&#39;s also complicated deleting a project and ensuring all nested / related data has been wiped - we get pretty far with appropriate foreign_keys and `-&gt;onDelete( &#39;cascade&#39; )`, but as the app gets more developed it&#39;s more and more difficult to **guarantee** that all data has been scrubbed when a project is deleted. There have been a few incidents where orphans containing sensitive information have been discovered. We&#39;ve been improving our tests and code when each is discovered, but it&#39;s becoming clear that we can&#39;t guarantee fallthroughs won&#39;t happen again - shareholders are expressing doubts.

Someone brought up that we can reduce a lot of complexity if we&#39;re able to generate a group of tables each time a new project is created. So, when project `0f9ebA2` is created, it creates tables `0f9ebA2_reports`, `0f9ebA2_tickets`, and so on as well. These tables will be identical structurally, so can all be created from the same migrations. In terms of convenience and cleanliness, the advantages are clear - foreign keys will be pointed to tables with the same prefix, and guarantee IDs outside of the project can&#39;t be assigned. It&#39;s also trivial to ensure all data has been scrubbed - just delete the tables. Many of the cross-pollination protections become obsolete, reduced to just access permissions.

The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach - and it doesn&#39;t seem like a common practice in MySQL, so there aren&#39;t a lot of articles or forums on the subject. We&#39;d like to get another perspective on this, and see if we&#39;re overlooking any fundamental flaws before we pull the trigger - like increased storage size, slower performance, higher indexing overhead. Matters of MySQL architecture and performance, over best practice and opinion.
||||||||||||||A few years ago I managed the databases at a company that had about 10,000 schemas, each schema had the same set of ~120 tables. They did this for similar reasons that you have, to make sure data for different clients is kept separate, for privacy and security reasons.

This was on MySQL 5.1 at the time. We found that after a few tens of thousands of tables on a given server, performance became a problem. It turns out that MySQL has internal data structures corresponding to each table, and they didn't architect this to handle so many tables. So eventually scanning lists of open tables becomes a bottleneck.

We split the schemas over seven servers, so each server didn't have so many tables. About 160,000 tables per server was our maximum.

I gave feedback to the MySQL product manager about this bottleneck, as they were developing a revamped implementation of the data dictionary for MySQL 8.0. He passed this along to the engineers, and they made sure to test scalability up to 1 million tables per server.

So definitely make sure that you use MySQL 8.0 or later to get this improvement.

But even with MySQL 8.0, this doesn't give you unlimited scalability. Eventually if you intend to keep growing, you must develop the capability to store data on more than one database server, and your applications need to have code to switch between database servers.

For example, in our case, one of the db servers had a simple table that stored a list of all the clients and which db server their data was stored on. This list was read at the startup of the app, and held in cache. It was a simple mapping list. Then on any request, the app could quickly tell which of seven db servers it should send the query. All the functions to run queries had an argument which was the db connection to use.

So in a way, "are there any performance limitations" is the wrong question. The assumption should be that there _are_ performance limitations, it's just a matter of how large can you grow until you hit those limits. Assume that you will.

Then the question is how to keep growing beyond those limits, and that means scaling out to multiple servers. Build this into your application design.

---

> The main rub is that noone on our team has deep enough database experience to speak to potential performance or integrity flaws with this approach

Well, now's your opportunity to exercise your general software engineering skills, and develop some tests. 

You — or _someone_ on your team — hopefully have a degree in Computer Science? Well, approach the problem like a scientist. What type of tests would measure scalability of this kind? Presumably you'd need a lab where you could build a database with lots of tables. You'd need some scripts that can populate those tables, probably with a parameter so you can re-run the test at different scale. You'd need some way to drive query traffic in a repeatable fashion, and measure performance.

Then you need to develop requirements for scalability. What performance is acceptable? What rate of degradation is acceptable? Who gets to decide this?

No one starts with these skills. _They learn as they work._ "I don't have those skills" is not an excuse. They try something, they make mistakes, learn from them, improve their processes. 

You also need to learn that optimization and scalability is not about choosing the right technology. No technology scales if you use it improperly. Scalability comes from architecture, which should be where you have software engineering skills.

--------------------------------------------------
Cannot read properties of undefined (reading [api.reducerPath]) at Object.extractRehydrationInfo after clearing browser data
I have used redux persist with RTK query and redux toolkit. After clearing browser data manually from browser settings,
it could not rehydrate RTK query reducer and showing 

    Uncaught TypeError: Cannot read properties of undefined (reading &#39;notesApi&#39;)
        at Object.extractRehydrationInfo (notesApi.js:18:1)
        at createApi.ts:234:1
        at memoized (defaultMemoize.js:123:1)
        at createApi.ts:260:1
        at memoized (defaultMemoize.js:123:1)
        at createReducer.ts:239:1
        at Array.filter (&lt;anonymous&gt;)
        at reducer (createReducer.ts:236:1)
        at reducer (createSlice.ts:325:1)
        at combination (redux.js:560:1).

Here is the [screenshot of my problem][1].

Official Documentation says 

 - RTK Query supports rehydration via the extractRehydrationInfo option
   on createApi. This function is passed every dispatched action, and
   where it returns a value other than ***undefined***, that value is used to
   rehydrate the API state for fulfilled &amp; errored queries.

But what about ***undefined*** value like in my case?

This is my store




    const reducers = combineReducers({
      userReducer,
      [notesApi.reducerPath]: notesApi.reducer,
    });
    
    const persistConfig = {
      key: &quot;root&quot;,
      storage,
    };
    
    const persistedReducer = persistReducer(
      persistConfig,
      reducers
    );
    
    const store = configureStore({
      reducer: persistedReducer,
      middleware: (getDefaultMiddleware) =&gt;
        getDefaultMiddleware({
          serializableCheck: {
            ignoredActions: [FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER],
          },
        }).concat(notesApi?.middleware),
    });    
    
    export default store;




This is the notesApi



    export const notesApi = createApi({
     reducerPath: &quot;notesApi&quot; ,
      baseQuery: fetchBaseQuery({
        baseUrl: &quot;http://localhost:5000/api/notes/&quot;,
        prepareHeaders: (headers, { getState }) =&gt; {
          const token = getState().userReducer.userInfo.token;
          console.log(token);
          if (token) {
            headers.set(&quot;authorization&quot;, `Bearer ${token}`);
          }
          return headers;
        },
      }),
      extractRehydrationInfo(action, { reducerPath }) {
        if (action.type === REHYDRATE) {
            return action.payload[reducerPath]
        }
      },
      tagTypes: [&quot;notes&quot;],
    
      endpoints: (builder) =&gt; ({
        createNote: builder.mutation({
          query: (data) =&gt; ({
            url: `/create`,
            method: &quot;POST&quot;,
            body: data,
          }),
          invalidatesTags: [&quot;notes&quot;],
        }),
        getSingleNote: builder.query({
          query: (id) =&gt; ({
            url: `/${id}`,
          }),
          providesTags: [&quot;notes&quot;],
        })
    });
    export const {  useGetSingleNoteQuery,
      useCreateNoteMutation,
    } = notesApi;



  [1]: https://i.stack.imgur.com/jqawj.png
||||||||||||||I've run into this issue a few times and it seems to manifest when attempting to rehydrate the store when there isn't anything in localStorage to hydrate from.

The error is saying it can't read `"notesApi"` of undefined when running `extractRehydrationInfo`. `"notesApi"` is the API slice's `reducerPath` value. The action's payload is undefined.

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload[reducerPath]; // <-- action.payload undefined
      }
    },

To resolve this issue I've simply used the Optional Chaining operator on the action payload.

Example:

    extractRehydrationInfo(action, { reducerPath }) {
      if (action.type === REHYDRATE) {
        return action.payload?.[reducerPath];
      }
    },

--------------------------------------------------
Pyspark. spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, java.net.SocketException: Connection reset
I am new to pyspark, and i&#39;m trying to run multiple time series in prophet with pyspark (as distributed computing because i have 100s of times series to predict) but i have error as below. 


```
import time 
start_time = time.time()
sdf = spark.createDataFrame(data)
print(&#39;%0.2f min: Lags&#39; % ((time.time() - start_time) / 60))
sdf.createOrReplaceTempView(&#39;Quantity&#39;)
spark.sql(&quot;select Reseller_City, Business_Unit, count(*) from Quantity group by Reseller_City, Business_Unit order by Reseller_City, Business_Unit&quot;).show()
query = &#39;SELECT Reseller_City, Business_Unit, conditions, black_week, promos, Sales_Date as ds, sum(Rslr_Sales_Quantity) as y FROM Quantity GROUP BY Reseller_City, Business_Unit, conditions, black_week, promos, ds ORDER BY Reseller_City, Business_Unit, ds&#39;
spark.sql(query).show()
sdf.rdd.getNumPartitions()
store_part = (spark.sql(query).repartition(spark.sparkContext.defaultParallelism[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;])).cache()

store_part.explain()

from pyspark.sql.types import *

result_schema =StructType([
  StructField(&#39;ds&#39;,TimestampType()),
  StructField(&#39;Reseller_City&#39;,StringType()),
  StructField(&#39;Business_Unit&#39;,StringType()),
  StructField(&#39;y&#39;,DoubleType()),
  StructField(&#39;yhat&#39;,DoubleType()),
  StructField(&#39;yhat_upper&#39;,DoubleType()),
  StructField(&#39;yhat_lower&#39;,DoubleType())
  ])
from pyspark.sql.functions import pandas_udf, PandasUDFType

@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )
def forecast_sales( store_pd ):
    
    model = Prophet(interval_width=0.95, holidays = lock_down)
    model.add_country_holidays(country_name=&#39;DE&#39;)
    model.add_regressor(&#39;conditions&#39;)
    model.add_regressor(&#39;black_week&#39;)
    model.add_regressor(&#39;promos&#39;)
    
    train = store_pd[store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;]
    future_pd = store_pd[store_pd[&#39;ds&#39;]&gt;=&#39;2021-10-01 00:00:00&#39;]
    model.fit(train[[&#39;ds&#39;, &#39;y&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])


    forecast_pd = model.predict(future_pd[[&#39;ds&#39;, &#39;conditions&#39;, &#39;black_week&#39;, &#39;promos&#39;]])  

    f_pd = forecast_pd[ [&#39;ds&#39;,&#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ].set_index(&#39;ds&#39;)

    #store_pd = store_pd.filter(store_pd[&#39;ds&#39;]&lt;&#39;2021-10-01 00:00:00&#39;)

    st_pd = future_pd[[&#39;ds&#39;,&#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;]].set_index(&#39;ds&#39;)

    results_pd = f_pd.join( st_pd, how=&#39;left&#39; )
    results_pd.reset_index(level=0, inplace=True)

    results_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]] = future_pd[[&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]].iloc[0]

    return results_pd[ [&#39;ds&#39;, &#39;Reseller_City&#39;,&#39;Business_Unit&#39;,&#39;y&#39;, &#39;yhat&#39;, &#39;yhat_upper&#39;, &#39;yhat_lower&#39;] ]
results = (store_part.groupBy([&#39;Reseller_City&#39;,&#39;Business_Unit&#39;]).apply(forecast_sales).withColumn(&#39;training date&#39;, current_date() ))
results.cache()
results.show()
``` 

All the lines are executed perfectly but error the come from **results.show()**  line  I dont understand where i have done wrong, Much appreciated if someone helps me 

```
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-46-8c647e8bf4d9&gt; in &lt;module&gt;
----&gt; 1 results.show()

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\dataframe.py in show(self, n, truncate, vertical)
    438         &quot;&quot;&quot;
    439         if isinstance(truncate, bool) and truncate:
--&gt; 440             print(self._jdf.showString(n, 20, vertical))
    441         else:
    442             print(self._jdf.showString(n, int(truncate), vertical))

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

C:\spark-3.0.3-bin-hadoop2.7\python\pyspark\sql\utils.py in deco(*a, **kw)
    126     def deco(*a, **kw):
    127         try:
--&gt; 128             return f(*a, **kw)
    129         except py4j.protocol.Py4JJavaError as e:
    130             converted = convert_exception(e.java_exception)

C:\spark-3.0.3-bin-hadoop2.7\python\lib\py4j-0.10.9-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
--&gt; 328                     format(target_id, &quot;.&quot;, name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o128.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 1243, Grogu.profiflitzer.local, executor driver): java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(Unknown Source)
	at java.net.SocketInputStream.read(Unknown Source)
	at java.io.BufferedInputStream.fill(Unknown Source)
	at java.io.BufferedInputStream.read(Unknown Source)
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:311)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	... 1 more

```  
||||||||||||||You can also set the os env variables by following the below steps,
run this before SparkSession/SparkContext

    import os
    import sys
    
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

It worked for me

--------------------------------------------------
Calculate difference between 2 date / times in Oracle SQL
I have a table as follows:

    Filename - varchar
    Creation Date - Date format dd/mm/yyyy hh24:mi:ss
    Oldest cdr date - Date format dd/mm/yyyy hh24:mi:ss

How can I calcuate the difference in hours minutes and seconds (and possibly days) between the two dates in Oracle SQL?

Thanks


||||||||||||||You can substract dates in Oracle. This will give you the difference in days. Multiply by 24 to get hours, and so on.

    SQL> select oldest - creation from my_table;


If your date is stored as character data, you have to convert it to a date type first.


    SQL> select 24 * (to_date('2009-07-07 22:00', 'YYYY-MM-DD hh24:mi') 
                 - to_date('2009-07-07 19:30', 'YYYY-MM-DD hh24:mi')) diff_hours 
           from dual;
    
    DIFF_HOURS
    ----------
           2.5

---
*Note*:

This answer applies to dates represented by the Oracle data type `DATE`.
Oracle also has a data type `TIMESTAMP`, which can also represent a date (with time). If you subtract `TIMESTAMP` values, you get an `INTERVAL`; to extract numeric values, use the `EXTRACT` function.

--------------------------------------------------
Excel VBA Macro Returning &quot;Subscript Out of Range&quot; for Incremental Function Converting Hyperlinks to Raw URLs
I am writing an Excel VBA macro that is working fine except for one specific function, and I cannot figure out why it is failing; I&#39;m not a programmer, although I do have some understanding of the basic logic, just little/no experience at writing it, so apologies in advance if there is any confusion imparted by my attempted explanations. The error returned when attempting to execute the code in question is &quot;Run-time error code &#39;9&#39;: subscript out of range&quot;, and I&#39;ve copied the relevant code snippets below:

    &#39; Define variable for worksheet in question
    Dim wsSales As Worksheet
    Set wsSales = ThisWorkbook.Sheets(&quot;Sales&quot;)

    &#39; Find last row with data in it
    Dim lastRowSales As Long
    lastRowSales = wsSales.Cells(Rows.Count, &quot;J&quot;).End(xlUp).Row

    &#39; Loop through column J and convert hyperlinks to raw URLs
    For i = 2 To lastRowSales
        If wsSales.Cells(i, &quot;J&quot;).Hyperlinks.Count &gt; 0 Then
            wsSales.Cells(i, &quot;J&quot;).Value = wsSales.Hyperlinks(i).Address
        End If
    Next i`

- For extra info/context, column J of the Sales sheet referenced contains hyperlinked text (e.g., &quot;Object Name&quot; that points to a URL in a sales-related webpage), and I&#39;m trying to get the actual URL for each row in the range so I can output it elsewhere. Row 1 is a header row, so I&#39;m starting with &#39;i = 2&#39; to ignore it accordingly.
- What the above code ends up doing is partially successful, but specifically fails on the last row for some reason. So if I have, for example, 100 rows in column J of the Sales sheet (99 rows with data and 1 header row), it will successfully convert any hyperlinked values to a URL for the first 99 rows, but row 100 does not convert and Excel spits out the &#39;subscript out of range&#39; error. When looking at the highlighted code that failed after clicking &#39;Debug&#39; on the error pop-up in the VBA Editor, it is specifically the &#39;wsSales.Hyperlinks(i).Address&#39; part that returns a value of &#39;&lt;subscript out of range&gt;&#39;.
- Additionally, it does not actually convert things quite properly; for example, say that row 50 has a hyperlinked text string in it. Rather than converting cell J50 to show the URL that was in J50, it actually shows the URL for J51, and it does this for the entire range (where it&#39;s showing the URL of the cell below it, not the cell itself).
- If I start with &#39;i = 1&#39; instead to include checking the header row (which will never have a hyperlink, but I figured was worth testing), the function works identically - same behavior, same error, no difference at all relative to starting with &#39;i = 2&#39;. That seems to imply to me the error is somewhere either in the logic before the function actually executes or my references in the function itself.
- I have also tested the above code with &quot;wsSales.Hyperlinks(1).address&quot; (1 instead of i) and it ends up completing successfully but using the same URL for the entire column J, so there seems to be a flaw with that logic as an alternative (presumably the static reference for the Hyperlinks object).  The same is true if I use &#39;2&#39; instead of &#39;1&#39;, so I suspect that using any digit will give me the same core problem.
- I feel like there must be something wrong with either my function or some variable I&#39;ve defined that is causing this, but after looking extensively through my code and attempting to &#39;rubber ducky&#39; troubleshoot it, I&#39;m still coming up blank.


I&#39;ve used essentially the exact same logic for multiple other formulas that compose the rest of the larger macro and they all work properly, but this function specifically fails to work as expected; every other &#39;for i = # To [value]&#39; iterates successfully and commenting out the above code snippet from the larger macro enables the full macro to work exactly as expected, just not this function. Does anyone have any thoughts or suggestions for why this may be failing to function as expected? Any ideas for what logic I should check, what may be failing, or a better way to do this? Any advice would be greatly appreciated, thanks!
||||||||||||||Refer to the hyperlink in *each specific cell*, not the [`Worksheet.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.worksheet.hyperlinks) collection:
```
For i = 2 To lastRowSales
    If wsSales.Cells(i, "J").Hyperlinks.Count > 0 Then
        wsSales.Cells(i, "J").Value = wsSales.Cells(i, "J").Hyperlinks(1).Address
    End If
Next i`
```
In other words, you want to use the [`Range.Hyperlinks`](https://learn.microsoft.com/en-us/office/vba/api/excel.range.hyperlinks) property.

If you did want to use the `Worksheet.Hyperlinks` approach:

```
Dim h As Hyperlink
For Each h In wsSales.Hyperlinks
    h.Range.Value = h.Address
Next
```

--------------------------------------------------
How to send email attachments?
I am having problems understanding how to email an attachment using Python. I have successfully emailed simple messages with the `smtplib`. Could someone please explain how to send an attachment in an email. I know there are other posts online but as a Python beginner I find them hard to understand. 

    
||||||||||||||Here's another:

    import smtplib
    from os.path import basename
    from email.mime.application import MIMEApplication
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    from email.utils import COMMASPACE, formatdate
    
    
    def send_mail(send_from, send_to, subject, text, files=None,
                  server="127.0.0.1"):
        assert isinstance(send_to, list)
    
        msg = MIMEMultipart()
        msg['From'] = send_from
        msg['To'] = COMMASPACE.join(send_to)
        msg['Date'] = formatdate(localtime=True)
        msg['Subject'] = subject
 
        msg.attach(MIMEText(text))

        for f in files or []:
            with open(f, "rb") as fil:
                part = MIMEApplication(
                    fil.read(),
                    Name=basename(f)
                )
            # After the file is closed
            part['Content-Disposition'] = 'attachment; filename="%s"' % basename(f)
            msg.attach(part)

    
        smtp = smtplib.SMTP(server)
        smtp.sendmail(send_from, send_to, msg.as_string())
        smtp.close()


It's much the same as the first example... But it should be easier to drop in.

  [1]: http://snippets.dzone.com/posts/show/2038

--------------------------------------------------
Get handle to desktop / shell window
In one of my programs, I need to test if the user is currently focusing the desktop/shell window. Currently, I&#39;m using `GetShellWindow()` from *user32.dll* and compare the result to `GetForegroundWindow()`.

This approach is working until someone changes the desktop wallpaper, but as soon as the wallpaper is changed the handle from `GetShellWindow()` doesn&#39;t match the one from `GetForegroundWindow()` anymore and I don&#39;t quite get why that is. (**OS:** Windows 7 32bit)

Is there a better approach to check if the desktop is focused? Preferably one that won&#39;t be broken if the user changes the wallpaper?

**EDIT:** I designed a workaround: I&#39;m testing the handle to have a child of class `SHELLDLL_DefView`. If it has, the desktop is on focus. Whilst, it&#39;s working at my PC that doesn&#39;t mean it will work all the time.
||||||||||||||The thing changed a little bit since there are slideshows as wallpaper available in Windows 7.
You are right with WorkerW, but this works only with wallpaper is set to slideshow effect. 

When there is set the wallpaper mode to slideshow, you have to search for a window of class `WorkerW` and check the children, whether there is a `SHELLDLL_DefView`.
If there is no slideshow, you can use the good old `GetShellWindow()`.

I had the same problem some months ago and I wrote a function for getting the right window. Unfortunately I can't find it. But the following should work. Only the Win32 Imports are missing:

    public enum DesktopWindow
    {
        ProgMan,
        SHELLDLL_DefViewParent,
        SHELLDLL_DefView,
        SysListView32
    }
    
    public static IntPtr GetDesktopWindow(DesktopWindow desktopWindow)
    {
        IntPtr _ProgMan = GetShellWindow();
        IntPtr _SHELLDLL_DefViewParent = _ProgMan;
        IntPtr _SHELLDLL_DefView = FindWindowEx(_ProgMan, IntPtr.Zero, "SHELLDLL_DefView", null);
        IntPtr _SysListView32 = FindWindowEx(_SHELLDLL_DefView, IntPtr.Zero, "SysListView32", "FolderView");
    
        if (_SHELLDLL_DefView == IntPtr.Zero)
        {
            EnumWindows((hwnd, lParam) =>
            {
                if (GetClassName(hwnd) == "WorkerW")
                {
                    IntPtr child = FindWindowEx(hwnd, IntPtr.Zero, "SHELLDLL_DefView", null);
                    if (child != IntPtr.Zero)
                    {
                        _SHELLDLL_DefViewParent = hwnd;
                        _SHELLDLL_DefView = child;
                        _SysListView32 = FindWindowEx(child, IntPtr.Zero, "SysListView32", "FolderView"); ;
                        return false;
                    }
                }
                return true;
            }, IntPtr.Zero);
        }
    
        switch (desktopWindow)
        {
            case DesktopWindow.ProgMan:
                return _ProgMan;
            case DesktopWindow.SHELLDLL_DefViewParent:
                return _SHELLDLL_DefViewParent;
            case DesktopWindow.SHELLDLL_DefView:
                return _SHELLDLL_DefView;
            case DesktopWindow.SysListView32:
                return _SysListView32;
            default:
                return IntPtr.Zero;
        }
    }

In your case you would call `GetDesktopWindow(DesktopWindow.SHELLDLL_DefViewParent);` to get the top-level window for checking whether it is the foreground window.

--------------------------------------------------
Keil compiler v5 to v.6
I&#39;m forced to switch from ARMCC v5 to CLANG(v.6). Here is the problem. 
I have some struct that includes a pointer to the function which gets as a parameter pointer to the same structure. 
So I do 

```
struct _some_struct_s;
typedef void (*callback_f)(struct _some_struct *p);
 
typedef struct {
  callback_f fn;
  int        x; 
} some_type_s;

// init function
void init_some_struct (some_struct *p, callback_f f) {
  p-&gt;fn = f;
  p-&gt;x = 0;
}
```
In another file I&#39;m writing the callback() and calling init_some_struct()
```
some_type_s  my_struc;
void callback (some_type_s *p) {
  p-&gt;x++;
}
init_some_struct (&amp;my_struc, callback);
```
I had no issues with compiler 5 but a warning with version 6.
***
```
warning: incompatible function pointer types passing &#39;void (some_struct_s *)&#39; to parameter of type &#39;callback_f&#39; (aka &#39;void (*)(struct _some_struct_s *)&#39;) [-Wincompatible-function-pointer-types]
```
What can I do to avoid having this warning?


What can I do to avoid having this warning?

||||||||||||||1. `typedef (*callback_f)(struct _some_struct *p);` - you alias the type as function of pointer which returns `int`.

It should be `typedef void (*callback_f)(struct some_struct *p);`

2.     typedef struct {
            callback_f *fn;
   `fn` is a pointer to pointer to function. It should be `callback_f fn;`

```
typedef struct some_struct _some_struct_s;
typedef void callback_f(struct _some_struct *p);
 
typedef struct some_struct{
  callback_f *fn;
  int        x; 
} some_struct_s;

// init function
void init_some_struct (some_struct *p, callback_f *f) {
  p->fn = f;
  p->x = 0;
}
```



--------------------------------------------------
How to create mutually exclusive fields in Pydantic
I am using Pydantic to model an object. How can I make two fields mutually exclusive?

For instance, if I have the following model:

    class MyModel(pydantic.BaseModel):
        a: typing.Optional[str]
        b: typing.Optional[str]

I want field `a` and field `b` to be mutually exclusive. I want only one of them to be set. Is there a way to achieve that?
||||||||||||||You can use pydantic.validator decorator to add custom validations.

```lang-python
from typing import Optional
from pydantic import BaseModel, validator

class MyModel(BaseModel):
    a: Optional[str]
    b: Optional[str]

    @validator("b", always=True)
    def mutually_exclusive(cls, v, values):
        if values["a"] is not None and v:
            raise ValueError("'a' and 'b' are mutually exclusive.")

        return v
```

--------------------------------------------------
Web automation with Selenium + python and google chrome 115.x &gt;
How to use selenium and chrome CFT for web automation from chrome version 115.x using python?

I have an automation script that worked fine until chrome version 114.x. From version 115.x it stopped working due to the version update, but also due to the new method with chrome cft.
||||||||||||||After upgrading to Chrome version 115.x, my automation stopped working, and chrome driver versions were no longer released, because from chrome version 115.x, automations are performed by CFT (chrome for test ), which as I understand this browser remains static until user action, preventing automations from stopping due to automatic chrome updates and need for crhome driver replacement.
The problem was solved with the solution below:


```
# using selenium 4.8 and python 3.9

from selenium import webdriver
from selenium.webdriver.chrome.options import Options


options = Options()
options.binary_location = 'path to chrome.exe'
## this is the chromium for testing which can be downloaded from the link given below

driver = webdriver.Chrome(chrome_options = options, executable_path = 'path to chromedriver.exe')
## must be the same as the downloaded version of chrome cft.
```
As of today, the files can be downloaded from: https://googlechromelabs.github.io/chrome-for-testing/

Prefer the stable version and download the compatible browser and chromedriver.

The rest of the code continues to work.

source: https://stackoverflow.com/questions/45500606/set-chrome-browser-binary-through-chromedriver-in-python

--------------------------------------------------
How can I list the taints on Kubernetes nodes?
The [docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint) are great about explaining how to set a taint on a node, or remove one. And I can use `kubectl describe node` to get a verbose description of one node, including its taints. But what if I&#39;ve forgotten the name of the taint I created, or which nodes I set it on? Can I list all of my nodes, with any taints that exist on them?
||||||||||||||<!-- language-all: lang-bash -->

    kubectl get nodes -o json | jq '.items[].spec'

which will give the complete spec with node name, or:

    kubectl get nodes -o json | jq '.items[].spec.taints'

will produce the list of the taints per each node

--------------------------------------------------
How to write unitTest for methods using a stream as a parameter
I have class `ImportProvider` , and I want write unit test for Import method.

But this should be unit test, so I don&#39;t want to read from file to stream.
Any idea?

  

    public class ImportProvider : IImportProvider
    { 
         public bool Import(Stream stream)
         {
             //Do import
        
             return isImported;
         }
    }
        
    public interface IImportProvider
    {
          bool Import(Stream input);
    }

This is unit test:

    [TestMethod]
    public void ImportProvider_Test()
    {
        // Arrange           
        var importRepository = new Mock&lt;IImportRepository&gt;(); 
        var imp = new ImportProvider(importRepository.Object);
        //Do setup...

        // Act
        var test_Stream = ?????????????
        // This working but not option:
        //test_Stream = File.Open(&quot;C:/ExcelFile.xls&quot;, FileMode.Open, FileAccess.Read);
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }
||||||||||||||Use a MemoryStream. Not sure what your function expects, but to stuff a UTF-8 string into it for example:

    //Act
    using (var test_Stream = new MemoryStream(Encoding.UTF8.GetBytes("whatever")))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }

EDIT: If you need an Excel file, and you are unable to read files from disk, could you add an Excel file as an embedded resource in your test project? See [How to embed and access resources by using Visual C#][1]

You can then read as a stream like this:

    //Act
    using (var test_Stream = this.GetType().Assembly.GetManifestResourceStream("excelFileResource"))
    {
        var result = imp.Import(test_Stream);
    
        // Assert    
        Assert.IsTrue(result);
    }


  [1]: https://support.microsoft.com/en-us/kb/319292

--------------------------------------------------
Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
What is causing this build error:

```

- Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
- Plugin Repositories (could not resolve plugin artifact &#39;com.android.application:com.android.application.gradle.plugin:7.0.3&#39;)
  Searched in the following repositories:
    Gradle Central Plugin Repository
    Google
```

in `build.gradle` file

Expecting a successful android build
||||||||||||||In my case `settings.gradle` file was missing. You can create a file and place into project root folder.

**settings.gradle**:

    pluginManagement {
        repositories {
            gradlePluginPortal()
            google()
            mavenCentral()
        }
    }
    dependencyResolutionManagement {
        repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)
        repositories {
            google()
            mavenCentral()
        }
    }
    rootProject.name = "android-geocode"
    include ':app'

--------------------------------------------------
How do I use an API key/secret on Binance&#39;s TestNet?
Following the instructions here, https://docs.binance.org/smart-chain/wallet/arkane.html, I created a Binance SmartChain account with its &quot;0x&quot; prefixed wallet address.  I then added funds.  What I can&#39;t figure out is how I get a TestNet API key and secret so that I can test my Python API calls.  I create the client like so

	from binance.client import Client
	...
	auth_client = Client(key, b64secret)
     if account.testing:
     	auth_client.API_URL = &#39;https://testnet.binance.vision/api&#39;
 
How do I get an API key tied to my Binance SmartChain address?
||||||||||||||You have to create your API credentials from [here][1] and pass the testnet variable into the Client constructor. See the
[documentation][2].

```python
auth_client = Client(key, b64secret, testnet=True)
```

does the job.


  [1]: https://testnet.binance.vision/
  [2]: https://python-binance.readthedocs.io/en/latest/binance.html?highlight=client#binance.client.Client.__init__

--------------------------------------------------
org.gradle.kotlin.kotlin-dsl was not found
I am getting the following error while running the build

    FAILURE: Build failed with an exception.
    
    * Where:
    Build file &#39;/home/charming/mainframer/bigovlog_android/buildSrc/build.gradle.kts&#39; line: 4
    
    * What went wrong:
    Plugin [id: &#39;org.gradle.kotlin.kotlin-dsl&#39;, version: &#39;1.2.6&#39;] was not found in any of the following sources:
    
    - Gradle Core Plugins (plugin is not in &#39;org.gradle&#39; namespace)
    - Plugin Repositories (could not resolve plugin artifact &#39;org.gradle.kotlin.kotlin-dsl:org.gradle.kotlin.kotlin-dsl.gradle.plugin:1.2.6&#39;)
      Searched in the following repositories:
        Gradle Central Plugin Repository

my buildSrc/build.gradle.kts

    repositories {
        jcenter()
    }
    plugins {
        `kotlin-dsl`
        id(&quot;groovy&quot;)
    }
    dependencies{
        gradleApi()
        localGroovy()
    }

I tried everything but still not working
||||||||||||||Did you check that Android Studio wasn't running in Offline Mode? Take a look at `Preferences/Build, Execution, Deployment/Gradle/Global Gradle settings` and see if Offline Work is checked.

--------------------------------------------------
Typesetting New Functions in LaTeX
So, I just have a little question:

What is the &quot;best way&quot; to typeset new functions in LaTeX which aren&#39;t already included in the various packages?  Right now I&#39;m just using `\mbox` as my go-to method,  but I just was wondering if there was a more &quot;acceptable way of doing it (as with mbox, I have to make sure to include spaces around the text of the functions in order for it to not look too strange)

Here is an example:

    $y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$

which comes out looking like:

![$y(t)=2e^{1/2}\sqrt{\pi}\mbox{Erfi }(t)$][1]

Don&#39;t get me wrong... I think it looks fine, but I was just looking for some opinions (as far as best practices go).

  [1]: http://adamnbowen.com/images/error_function.jpg
||||||||||||||Use `\DeclareMathOperator` from package `amsmath`. For example,

```tex
\documentclass{article}
\usepackage{amsmath}
\DeclareMathOperator\erfi{Erfi}

\begin{document}
Consider $x + y + \erfi(t) = z$ for example.
\end{document}
```

produces

[![result][1]][1]

If you only need it once, you can also use `\operatorname`: you get the same output as above with

```tex
\documentclass{article}
\usepackage{amsmath}
\begin{document}
Consider $x + y + \operatorname{Erfi}(t) = z$ for example.
\end{document}
```

If you cannot use the `amsmath` package for some reason, you can manually do `\mathop{\mathrm{Erfi}}` like:

```
\documentclass{article}
\begin{document}
Consider $x + y + \mathop{\mathrm{Erfi}}(t) = z$ for example.
\end{document}
```

See the always-useful TeX FAQ, specifically [Defining a new log-like function in LaTeX](https://texfaq.org/FAQ-newfunction).

  [1]: https://i.stack.imgur.com/9GCor.png

--------------------------------------------------
C# change a string variable with List or Array
I have some static strings 

    static string   Robocopy_Mirror = &quot;[Robocopy_Mirror]&quot;; 
    static string   Robocopy_Copy = &quot;[Robocopy_Copy]&quot;;
    static string   Network_Path_1 = &quot;[Network_Path_1]&quot;;    // \\NAS\Sync\
    static string   Lokal_Path_1 = &quot;[Lokal_Path_1]&quot;;      // X:\Sync\

And I thought I could save some lines of code if I put them in a List and change the values in the List with a loop.

    List&lt;string&gt; variableListe = new List&lt;string&gt;()  
    {   
        Robocopy_Mirror , Robocopy_Copy , Network_Path_1, Lokal_Path_1, 
        File_Network_Sync_1, File_Lokal_Sync_1, File_Network_Sync_2, File_Lokal_Sync_2
    };


But I can&#39;t change the static variables. I guess the List object does not change the static variable? Is there a quick way to change it? 

    for (int i = 0; i &lt; variableListe.Count-1; i++)
    {
        variableListe[i] = AppConfig.ElementAt(configPathPosition);
    }
    Console.WriteLine(Robocopy_Mirror); 

    // prints  &quot;[Robocopy_Mirror]&quot; instead of like C:\robocopy


||||||||||||||The static variables store references to string objects in memory. The elements in the list also store references to the same string objects in memory, but _each element is it's own variable_ and those elements _do not store references to the static variables;_ they refer to the string objects directly. 

When you change an element in the list, you're changing the variable in the list to point to a new object in a new memory location. The static variables do not change and continue to refer to the same unchanged strings as they did before.


--------------------------------------------------
How do I run curl command from within a Kubernetes pod
I have the following questions:

1. I am logged into a Kubernetes pod using the following command:

        ./cluster/kubectl.sh exec my-nginx-0onux -c my-nginx -it bash

    The &#39;ip addr show&#39; command shows its assigned the ip of the pod. Since pod is a logical concept, I am assuming I am logged into a docker container and not a pod, In which case, the pod IP is same as docker container IP. Is that understanding correct?

2. from a Kubernetes node, I do `sudo docker ps` and then do the following:-

        sudo docker exec  71721cb14283 -it &#39;/bin/bash&#39;

    This doesn&#39;t work. Does someone know what I am doing wrong?

3. I want to access the nginx service I created, from within the pod using curl. How can I install curl within this pod or container to access the service from inside. I want to do this to understand the network connectivity.
||||||||||||||Here is how you get a curl command line within a kubernetes network to test and explore your internal REST endpoints.

To get a prompt of a busybox running inside the network, execute the following command. (A tip is to use one unique container per developer.)

```sh
kubectl run curl-<YOUR NAME> --image=radial/busyboxplus:curl -i --tty --rm
```

You may omit the --rm and keep the instance running for later re-usage. To reuse it later, type:

```sh
kubectl attach <POD ID> -c curl-<YOUR NAME> -i -t
```

Using the command `kubectl get pods` you can see all running POD's. The `<POD ID>` is something similar to `curl-yourname-944940652-fvj28`.

**EDIT:** Note that you need to login to google cloud from your terminal (once) before you can do this! Here is an example, make sure to put in your zone, cluster and project: 
```sh
gcloud container clusters get-credentials example-cluster --zone europe-west1-c --project example-148812
```

--------------------------------------------------
Execute CURL with kubectl
I am trying to execute `curl` command with `kubectl` like 

    kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;

Gives belob error

	OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused &quot;exec: \&quot;kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39;\&quot;: 
	stat kubectl exec POD_NAME &quot;curl -X PUT http://localhost:8080/abc -H \&quot;Content-Type: application/json\&quot; -d &#39;{\&quot;name\&quot;:\&quot;aaa\&quot;,\&quot;no\&quot;:\&quot;10\&quot;}&#39; &quot;: no such file or directory&quot; 
    :unknown command terminated with exit code 126
I have tried to escape the quotes but no luck. Then I tried simple curl 

    kubectl exec -it POD_NAME curl http://localhost:8080/xyz

This gives proper output as excepted. Any help with this 

Update: 

But when I run interactive (`kubectl exec -it POD_NAME /bin/bash`) mode of container and then run the curl inside the container works like champ
||||||||||||||i think you need to do something like this:

```
kubectl exec POD_NAME curl "-X PUT http://localhost:8080/abc -H \"Content-Type: application/json\" -d '{\"name\":\"aaa\",\"no\":\"10\"}' "
```

what the error suggests is that its trying to interpret everything inside `""` as a single command, not as a command with parameters. so its essentially looking for an executable called that

--------------------------------------------------
Open in Safari with UIActivityViewController?
I&#39;m sharing a URL via UIActivityViewController. I&#39;d like to see &quot;Open in Safari&quot; or &quot;Open in browser&quot; appear on the share sheet, but it doesn&#39;t. Is there a way to make this happen?

Note: I am not interested in solutions that involve adding somebody else&#39;s library to my app. I want to understand how to do this, not just get it to happen.

||||||||||||||Yes, you could add your custom action to Share sheet in iOS


You would have to copy this class.

    class MyActivity: UIActivity {
        var _activityTitle: String
        var _activityImage: UIImage?
        var activityItems = [Any]()
        var action: ([Any]) -> Void
        
        init(title: String, image: UIImage?, performAction: @escaping ([Any]) -> Void) {
            _activityTitle = title
            _activityImage = image
            action = performAction
            super.init()
        }
        override var activityTitle: String? {
            return _activityTitle
        }
    
        override var activityImage: UIImage? {
            return _activityImage
        }
        override var activityType: UIActivity.ActivityType {
            return UIActivity.ActivityType(rawValue: "com.someUnique.identifier")
        }
    
        override class var activityCategory: UIActivity.Category {
            return .action
        }
        override func canPerform(withActivityItems activityItems: [Any]) -> Bool {
            return true
        }
        override func prepare(withActivityItems activityItems: [Any]) {
            self.activityItems = activityItems
        }
        override func perform() {
            action(activityItems)
            activityDidFinish(true)
        }
    }

Please go through the class you might need to change a few things.

This is how you use it.

        let customItem = MyActivity(title: "Open in Safari", image: UIImage(systemName: "safari")  ) { sharedItems in
            guard let url = sharedItems[0] as? URL else { return }
            UIApplication.shared.open(url)
        }

        let items = [URL(string: "https://www.apple.com")!]
        let ac = UIActivityViewController(activityItems: items, applicationActivities: [customItem])
        ac.excludedActivityTypes = [.postToFacebook]
        present(ac, animated: true)

I have done this for one action, and tested it, it works.

Similarly you could do it for other custom actions.

For more on it refer this link.
[Link To Detailed Post][1]


  [1]: https://www.hackingwithswift.com/articles/118/uiactivityviewcontroller-by-example


--------------------------------------------------
How to cache playwright-python contexts for testing?
I am doing some web scraping using [`playwright-python&gt;=1.41`][1], and have to launch the browser in a headed mode (e.g. `launch(headless=False)`.

For CI testing, I would like to somehow cache the headed interactions with Chromium, to enable offline testing:

- First invocation: uses Chromium to make real-world HTTP transactions
- Later invocations: uses Chromium, but all HTTP transactions read from a cache

How can this be done? I can&#39;t find any clear answers on how to do this.

  [1]: https://github.com/microsoft/playwright-python
||||||||||||||It might solve your problem using HAR-file recording:
1. Run the first test while [recording a HAR-file][1]
2. Storing the HAR-file as an artifact, in your repo or similar in your CI environment
3. Running test again [with recorded HAR-file][2]

Here is how to do that with `playwright==1.41.1` and `pytest-playwright==0.3.3`:

```python
import pathlib

import pytest
from playwright.sync_api import Browser, Playwright

CACHE_DIR = pathlib.Path(__file__).parent / "cache"


@pytest.fixture(name="example_har", scope="session")
def fixture_example_har(playwright: Playwright) -> pathlib.Path:
    har_file = CACHE_DIR / "example.har"
    with (
        playwright.chromium.launch(headless=False) as browser,
        browser.new_page() as page,
    ):
        page.route_from_har(har_file, url="*/**", update=True)
        page.goto("https://example.com/")
    return har_file


def test_caching(browser: Browser, example_har: pathlib.Path) -> None:
    with browser.new_context(offline=True) as context:
        page = context.new_page()
        page.route_from_har(example_har, url="*/**")
        page.goto("https://example.com/")
```

  [1]: https://playwright.dev/python/docs/mock#recording-a-har-file
  [2]: https://playwright.dev/python/docs/mock#replaying-from-har

--------------------------------------------------
Python: Using .format() on a Unicode-escaped string
I am using Python 2.6.5. My code requires the use of the &quot;more than or equal to&quot; sign. Here it goes:  

    &gt;&gt;&gt; s = u&#39;\u2265&#39;
    &gt;&gt;&gt; print s
    &gt;&gt;&gt; ≥
    &gt;&gt;&gt; print &quot;{0}&quot;.format(s)
    Traceback (most recent call last):
         File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; 
    UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode character u&#39;\u2265&#39;
      in position 0: ordinal not in range(128)`  

Why do I get this error? Is there a right way to do this? I need to use the `.format()` function.

||||||||||||||Just make the second string also a unicode string

    >>> s = u'\u2265'
    >>> print s
    ≥
    >>> print "{0}".format(s)
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    UnicodeEncodeError: 'ascii' codec can't encode character u'\u2265' in position 0: ordinal not in range(128)
    >>> print u"{0}".format(s)
    ≥
    >>> 



--------------------------------------------------
Only Content controls are allowed directly in a content page that contains Content controls in ASP.NET
I have an application which has a master page and child pages. My application is working fine on local host (on my intranet). But as soon as I put it on a server that is on the internet, I get the error shown below after clicking on any menus.

&gt; Only Content controls are allowed directly in a content page that contains Content controls.

![screenshot][1]




  [1]: http://i.stack.imgur.com/b21sZ.png
||||||||||||||
Double and triple check your opening and closing Content tags throughout your child pages.

**Confirm that they** 

 - are in existence
 - are spelled correctly
 - have an ID
 - have runat="server"
 - have the correct ContentPlaceHolderID

--------------------------------------------------
Apollo Client is not reading variables passed in using useQuery hook
Having a weird issue passing variables into the useQuery hook.

The query:
```
const GET_USER_BY_ID= gql`
  query($id: ID!) {
    getUser(id: $id) {
      id
      fullName
      role
    }
  }
`;
```
Calling the query:
```
const DisplayUser: React.FC&lt;{ id: string }&gt; = ({ id }) =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID, {
    variables: { id },
  });

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Rendering the component:
```
&lt;DisplayUser id=&quot;5e404fa72b819d1410a3164c&quot; /&gt;
```

This yields the error: 
```
&quot;Argument \&quot;id\&quot; of required type \&quot;ID!\&quot; was provided the variable \&quot;$id\&quot; which was not provided a runtime value.&quot;
```

Calling the query from GraphQL Playground returns the expected result:
```
{
  &quot;data&quot;: {
    &quot;getUser&quot;: {
      &quot;id&quot;: &quot;5e404fa72b819d1410a3164c&quot;,
      &quot;fullName&quot;: &quot;Test 1&quot;,
      &quot;role&quot;: &quot;USER&quot;
    }
  }
}
```
And calling the query without a variable but instead hard-coding the id:
```
const GET_USER_BY_ID = gql`
  query {
    getUser(id: &quot;5e404fa72b819d1410a3164c&quot;) {
      id
      fullName
      role
    }
  }
`;

const DisplayUser: React.FC = () =&gt; {
  const { data, error } = useQuery(GET_USER_BY_ID);

  return &lt;div&gt;{JSON.stringify({ data, error })}&lt;/div&gt;;
};
```
Also returns the expected result.

I have also attempted to test a similar query that takes `firstName: String!` as a parameter which also yields an error saying that the variable was not provided a runtime value. This query also works as expected when hard-coding a value in the query string.

This project was started today and uses `&quot;apollo-boost&quot;: &quot;^0.4.7&quot;`, `&quot;graphql&quot;: &quot;^14.6.0&quot;`, and `&quot;react-apollo&quot;: &quot;^3.1.3&quot;`.
||||||||||||||[Solved]

In reading through the stack trace I noticed the issue was referencing `graphql-query-complexity` which I was using for validationRules. I removed the validation rules and now everything works! Granted I don't have validation at the moment but at least I can work from here. Thanks to everyone who took the time to respond!

--------------------------------------------------
Why it is a StackOverFlow Exception?
Why following code throws `StackoverflowException`? 

    class Foo
    {
        Foo foo = new Foo();
    }
    class Program
    {
        static void Main(string[] args)
        {
            new Foo();
        }
    }
||||||||||||||In `Main` you create a new `Foo` object, invoking its constructor.
Inside the `Foo` constructor, you create a different `Foo` instance, again invoking the `Foo` constructor.

This leads to infinite recursion and a `StackOverflowException` being thrown.

--------------------------------------------------
Function to aggregate json
Assume I have a gcs bucket with json files with the following structure:

```
[
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeid&quot;: &quot;Y1&quot;,
    &quot;storeName&quot;: &quot;alibaba1&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.8/3.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y2&quot;,
     &quot;storeName&quot;: &quot;alibaba2&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;1.7/2.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y3&quot;,
     &quot;storeName&quot;: &quot;alibaba3&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;2.7/4.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  },
  {
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;storeUuid&quot;: &quot;Y4&quot;,
     &quot;storeName&quot;: &quot;alibaba4&quot;,
    &quot;a&quot;: &quot;1/2/3&quot;,
    &quot;b&quot;: &quot;1.0/1.0/3&quot;,
    &quot;c&quot;: &quot;0/0/0&quot;,
    &quot;d&quot;: &quot;0/0/0&quot;,
    &quot;e&quot;: &quot;3.7/5.4&quot;,
    &quot;f&quot;: &quot;1/2/3&quot;,
    &quot;g&quot;: &quot;1/2/3&quot;,
  }
]
```

What I want to do is to aggregate the different values by summing ```a, b,c, d, f,g``` and taking the average of ```e``` to return one single ```json``` like

```
[
{
    &quot;Id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;Name&quot;: &quot;alibaba&quot;,
    &quot;a&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;b&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;c&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;d&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;e&quot;: &quot;average over all first instance/average over all second instance&quot;,
    &quot;f&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
    &quot;g&quot;: &quot;sum over all first instance/sum over all second instances/sum aover all third instance&quot;,
  }
]
``` 

Not that any of the values in ```*/*/*``` could be NaN and that the data in ```e``` could be a string ```data unvavailable```.

In have created this function 

```
def format_large_numbers_optimized(value):
    abs_values = np.abs(value)
    mask = abs_values &gt;= 1e6
    formatted_values = np.where(mask, 
                                np.char.add(np.round(value / 1e6, 2).astype(str), &quot;M&quot;), 
                                np.round(value, 2).astype(str))
    return formatted_values

def process_json_data_optimized(json_list):
    result = {}
    keys = set(json_list[0].keys()) - {&#39;Id&#39;, &#39;Name&#39;, &#39;storeid&#39;, &#39;storeName&#39;}
    for key in keys:
        result[key] = {&#39;values&#39;: []}
    for json_data in json_list:
        for key in keys:
            value = json_data.get(key, &#39;0&#39;)  
            result[key][&#39;values&#39;].append(value)
    for key in keys:
        all_values_processed = []
        for value in result[key][&#39;values&#39;]:
            if isinstance(value, str) and &#39;/&#39; in value:
                processed_values = [float(v) if v != &#39;data unavailable&#39; else 0 for v in value.split(&#39;/&#39;)]
            elif isinstance(value, float) or isinstance(value, int):
                processed_values = [value]
            else:
                processed_values = [0.0]  
            all_values_processed.append(processed_values)
        numeric_values = np.array(all_values_processed)
        if numeric_values.ndim == 1:
            numeric_values = numeric_values[:, np.newaxis]
        summed_values = np.sum(numeric_values, axis=0)
        formatted_summed_values = &#39;/&#39;.join(format_large_numbers_optimized(summed_values))
        result[key][&#39;summed&#39;] = formatted_summed_values
    processed_result = {key: data[&#39;summed&#39;] for key, data in result.items()}
    processed_result[&#39;Id&#39;] = json_list[0][&#39;Id&#39;]
    processed_result[&#39;Name&#39;] = json_list[0][&#39;Name&#39;]
    return processed_result
```

But it does not create what I expect. I am a at a total loss. Would really appreciate any help.
||||||||||||||Note that you are placing the values as lists `all_values_processed`.
Assuming that the `/` character is just a separator, and that what you want by replacing `all_values_processed.append(processed_values)` by `all_values_processed += processed_values`. Or even better you could just aggregate the values.

For instance you could have a function to aggregate like this

```lang-py
import math
def agg_func(value, initial):
  v_count, v_sum = initial
  if isinstance(value, str) and '/' in value:
    for v in value.split('/'):
      if v != 'data unavailable' :
         v = float(v)
         if not math.isnan(v):
           v_sum += v
           v_count += 1
  elif isinstance(value, float) or isinstance(value, int):
    if not math.isnan(value):
      v_sum += value
      v_count += 1
  return v_count, v_sum
```

A function that aggregate the given keys in the json

```lang-py
def agg_json(v_list, fields):
  state = {k: (0, 0) for k in fields}
  for item in v_list:
    for k in fields:
      if k in item:
        state[k] = agg(item[k], state[k])
  return state
```


Now
```lang-py
state = agg_json(json_list, ['a', 'b', 'c', 'd', 'e', 'f', 'g'])
```

will give you a dictionary with tuples containing the count and the sum for each field. To get your final answer you could do

```lang-py
result = {k: v[1] / v[0] if k == 'e' else v[1] for k, v in state.items()}
```

--------------------------------------------------
color text in divs with two colors using css only - tricky
OK, let me rewrite my question in another words so it looks clear and interesting: [jsFiddle][1]


  [1]: http://jsfiddle.net/xY6T3/1/

I need a pure css solution that colorizes the lines of text in the color depending whether the line is odd or even.

The example of code could be :

    &lt;div class=&quot;main&quot;&gt;
        &lt;div class=&quot;zipcode12345&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode23456&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode90033&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;zipcode11321&quot;&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;blue with css&lt;/div&gt;
            &lt;div class=&quot;myclass&quot;&gt;red with css&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;

Is it possible to make it with css? As you see [@ jsFiddle][1], it is not colorized as expected.

So, the main div is &quot;main&quot;.
The inner `div`s always have class names in format &quot;zipcodeXXXXX&quot;, as you see.
The number of zipcodeXXXXX is variable, the number of `myclass` is variable.
However, the odd lines should be always red and the even lines should be always blue.
Does pure css solution exist?

That would be kind of 

    .myclass:nth-child(2n+1){
     color:red;
    }
    .myclass:nth-child(2n){
     color:blue;
    }

if we could igonre `&quot;zipcodeXXXXX&quot;` divs, right?

Thank you.
||||||||||||||Simply apply different odd/even rules to the parent elements as well as the child elements:

<!-- language: lang-css -->

    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(odd),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(even) {
        color: red;
    }
    
    div[class^="zipcode"]:nth-of-type(odd) .myclass:nth-of-type(even),
    div[class^="zipcode"]:nth-of-type(even) .myclass:nth-of-type(odd) {
        color: blue;
    }

[**JSFiddle demo**][1].


  [1]: http://jsfiddle.net/xY6T3/9/

--------------------------------------------------
How to populate columns in a table using JavaScript
I need to create a simple table using JavaScript based on an array with nested objects, which should have only two columns. In the first cell of the first column of the table, the Processor header is specified, after which the corresponding processor models are written to the lower cells. In the first cell of the second column, the Processor frequency header is indicated, after which the frequencies are written to the lower cells. I was able to generate code for this task, but it doesn&#39;t work correctly. Instead of writing keys to column cells after the first iteration, for some reason, it takes into account unnecessary objects. That&#39;s why you get an undefined value in the table headers and a re-duplication. Please tell me how to solve this problem. 

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    let processorFrequency = [
        {
            titleOne : &#39;Processor&#39;, values : [
                {name : &#39;80386LC (1988г.)&#39;},
                {name : &#39;80486DX4 (1994г.)&#39;},
                {name : &#39;Pentium MMX (1997г.)&#39;},
                {name : &#39;Pentium II (1998г.)&#39;},
                {name : &#39;Pentium III (1999г.)&#39;},
                {name : &#39;Pentium IV&#39;},
                {name : &#39;Athlon-Athlon XP&#39;},
                {name : &#39;Athlon 64&#39;},

            ]
        },
        {
            titleTwo : &#39;Processor frequency&#39;, values : [
                {name : &#39;33-60&#39;},
                {name : &#39;80-133&#39;},
                {name : &#39;160-233&#39;},
                {name : &#39;260-550&#39;},
                {name : &#39;300-1400&#39;},
                {name : &#39;1600-3800&#39;},
                {name : &#39;1400-3200&#39;},
                {name : &#39;2600-3800&#39;},
            ]
    },
    ]



    function Test(){
        let table = document.getElementsByTagName(&#39;table&#39;)[0];
        for (let i = 0; i &lt; processorFrequency.length; i++) {
            var pf = processorFrequency[i];
            let tableRow = document.createElement(&#39;tr&#39;);
            let tdOne = document.createElement(&#39;td&#39;);
            let tdTwo = document.createElement(&#39;td&#39;);
            let txtOne = document.createTextNode(pf.titleOne);
            let txtTwo = document.createTextNode(pf.titleTwo);
            
            tdOne.className = &#39;head&#39;;
            tdTwo.className = &#39;head&#39;;

            tdOne.appendChild(txtOne);
            tdTwo.appendChild(txtTwo);

            tableRow.appendChild(tdOne);
            tableRow.appendChild(tdTwo);
            table.appendChild(tableRow);

            var values = pf.values;
            for (let j = 0; j &lt; values.length; j++) {
                let value = values[j];
                let tableRow = document.createElement(&#39;tr&#39;);
                let td = document.createElement(&#39;td&#39;);
                let txt = document.createTextNode(value.name);
                td.appendChild(txt);
                tableRow.appendChild(td);
                table.appendChild(tableRow);
            }
        }
    } 

    Test();

&lt;!-- language: lang-css --&gt;

    table td, table th {
      border: 1px solid black;
      padding: 5px;
    }

&lt;!-- language: lang-html --&gt;

    &lt;table&gt;&lt;!-- Contents will be created via JavaScript --&gt;
    &lt;/table&gt;

&lt;!-- end snippet --&gt;


||||||||||||||Seems like the issue is that you are creating a new row for each processor AND frequency value, which results in extra rows being added to the table. The correct way should be create a single row for each processor, with two cells (one for the processor model and one for the processor frequency):

    let processorFrequency = [
        {
            titleOne: 'Processor', values: [
                { name: '80386LC (1988г.)' },
                { name: '80486DX4 (1994г.)' },
                { name: 'Pentium MMX (1997г.)' },
                { name: 'Pentium II (1998г.)' },
                { name: 'Pentium III (1999г.)' },
                { name: 'Pentium IV' },
                { name: 'Athlon-Athlon XP' },
                { name: 'Athlon 64' },
            ]
        },
        {
            titleTwo: 'Processor frequency', values: [
                { name: '33-60' },
                { name: '80-133' },
                { name: '160-233' },
                { name: '260-550' },
                { name: '300-1400' },
                { name: '1600-3800' },
                { name: '1400-3200' },
                { name: '2600-3800' },
            ]
        },
    ];
    
    function Test() {
        let table = document.getElementsByTagName('table')[0];
    
        for (let i = 0; i < processorFrequency[0].values.length; i++) {
            let tableRow = document.createElement('tr');
            
            // Processor Name Cell
            let tdOne = document.createElement('td');
            let txtOne = document.createTextNode(processorFrequency[0].values[i].name);
            tdOne.appendChild(txtOne);
            tableRow.appendChild(tdOne);
    
            // Processor Frequency Cell
            let tdTwo = document.createElement('td');
            let txtTwo = document.createTextNode(processorFrequency[1].values[i].name);
            tdTwo.appendChild(txtTwo);
            tableRow.appendChild(tdTwo);
    
            table.appendChild(tableRow);
        }
    }
    
    Test();

--------------------------------------------------
System.UnauthorizedAccessException: Access to the path &quot;...&quot; is denied
  I have C# wpf installation done with .net using click once installation. All works fine. Then I have the following code which is part of the installed program:

    String destinationPath = System.Windows.Forms.Application.StartupPath + &quot;\\&quot; + fileName;
    File.Copy(path, destinationPath, true);
    this.DialogResult = true;
    this.Close();

But I get this error:

&gt;System.UnauthorizedAccessException: Access to the path C:\user\pc\appdata\local\apps\2.0.......  is denied.
&gt;
&gt;at System.IO.File.InternalCopy(String sourceFileName, String destFileName, Boolean overwrite, Boolean checkHost)
&gt;       at System.IO.File.Copy(String sourceFileName, String destFileName, Boolean overwrite)

Is it a permission error or do I need to tweak something in my code?

What puzzles me is why the user is able to install the program using click once into that directory without any issues, but uploading a file to it doesn&#39;t work?
||||||||||||||When installing an application the installer usually asks for administrative privileges. If the user chooses "Yes" the program will run and have read and write access to a larger variety of paths than what a normal user has. If the case is such that the installer did not ask for administrative privileges, it might just be that ClickOnce automatically runs under some sort of elevated privileges.

I'd suggest you write to the local appdata folder instead, but if you feel you really want to write to the very same directory as your application you must first run your app with administrator privileges.

To make your application always ask for administrator privileges you can modify your app's manifest file and set the `requestedExecutionLevel` tag's `level` attribute to `requireAdministrator`:

    <requestedExecutionLevel level="requireAdministrator" uiAccess="false" />

You can read a bit more in [**How do I force my .NET application to run as administrator?**](https://stackoverflow.com/questions/2818179/how-do-i-force-my-net-application-to-run-as-administrator)

--------------------------------------------------
Create a NuGet package for .NET8 MAUI with Azure DevOps
I have created a `.NET8 MAUI Class Library` to use in MAUI projects. The repo is in `Azure DevOps` and I was trying to build and publish the package via NuGet.

For that, I wrote a YAML file

    trigger:
    - main
    
    pool:
      vmImage: ubuntu-latest
    
    steps:
    - task: UseDotNet@2
      displayName: &#39;Use dotnet 8&#39;
      inputs:
        version: &#39;8.0.x&#39;
    - task: CmdLine@2
      inputs:
        script: &#39;dotnet workload install maui&#39;
    - task: DotNetCoreCLI@2
      displayName: Restore packages
      inputs:
        command: &#39;restore&#39;
        feedsToUse: &#39;select&#39;
        vstsFeed: &#39;c800d0d7-e2af-4567-997f-de7cf7888e6c&#39;
    - task: DotNetCoreCLI@2
      displayName: Build project
      inputs:
        command: &#39;build&#39;
        projects: &#39;**/PSC.Maui.Components.BottomSheet.csproj&#39;
        arguments: &#39;--configuration $(buildConfiguration)&#39;

When the pipeline runs, I get this error

    Generating script.
    Script contents:
    dotnet workload install maui
    ========================== Starting Command Output ===========================
    /usr/bin/bash --noprofile --norc /home/vsts/work/_temp/42901c0d-f407-4f75-912b-f93132efa865.sh
    Workload ID maui isn&#39;t supported on this platform.
    
    ##[error]Bash exited with code &#39;1&#39;.
    Finishing: CmdLine


[![enter image description here][1]][1]

Then, I tried to create the NuGet package locally, but it was not recognized by the NuGet website when I uploaded it.

How can I change the pipeline?

  [1]: https://i.stack.imgur.com/loBv9.png
||||||||||||||.Net MAUI does not support Linux, therefore you can neither build to it or from it.  

See [here](https://learn.microsoft.com/en-us/dotnet/maui/supported-platforms?view=net-maui-8.0)  

--------------------------------------------------
How to specify multiple locators for Selenium web element using the FindBy and PageFactory mechanisms
I like to use `PageFactory` with `@FindBy` annotations in my automation framework to auto-locate elements in my page object classes. 

I have one WebElement for which I need to be able to specify a couple of different locators. I thought FindBys was my solution, but apparently, that is not how it works. It&#39;s the equivalent of `driver.findElement(option1).findelement.(option2)`. That&#39;s not what I need. I need something that will find an element by one or the other locators. If one doesn&#39;t work, then use the other locator. Is there a way to do this in Selenium using FindBy annotations?
||||||||||||||There is apparently a new feature in Selenium as of May this year -- the @FindAll annotation that does exactly what I need;

http://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/support/FindAll.html
http://selenium.10932.n7.nabble.com/Pull-Request-62-Add-a-FindAll-annotation-to-the-Java-Page-Factory-td24814.html

--------------------------------------------------
Store cout from function as string
I have a function that takes in a vector of integers and outputs them via `std::cout`. 

    #include &lt;iostream&gt;
    #include &lt;vector&gt;
    
    void final_sol(std::vector&lt;int&gt; list){
        for (int i ; i &lt; list.size() ; i++){
            std::cout &lt;&lt; list[i] &lt;&lt; &quot; &quot;;
        }
    }
    
    int main(){
        std::vector&lt;int&gt; list = {1, 2, 3, 4, 5};
        final_sol(list);
        return 0;
    }
However, from this point I would like to have a way to quickly obtain the outputs of `final_sol(vector)` as a string. One way to do this would be to modify the original function to also create the string. However, I am not interested in modifying `final_sol(vector)`. Is there another way I could store the outputs as a string?
||||||||||||||Provide overload:
```
void final_sol(std::ostream& out, const std::vector<int>& list){
    for (int i = 0; i < list.size() ; i++){
        out << list[i] << " ";
    }
}

void final_sol(const std::vector<int>& list){
    final_sol(std::cout, list);
}
```
This way you existing calling code will not be impacted - most probably this is what you want: not modifying function signature. Not what you described: not do not modifying implementation of final_sol.

Then you can do:
```cpp
std::ostringstream str;
final_sol(str, list);
auto s = str.str()
```


--------------------------------------------------
FastAPI runs api-calls in serial instead of parallel fashion
I have the following code:

```python
import time
from fastapi import FastAPI, Request
    
app = FastAPI()
    
@app.get(&quot;/ping&quot;)
async def ping(request: Request):
        print(&quot;Hello&quot;)
        time.sleep(5)
        print(&quot;bye&quot;)
        return {&quot;ping&quot;: &quot;pong!&quot;}
```
If I run my code on localhost - e.g., `http://localhost:8501/ping` - in different tabs of the same browser window, I get:
```
Hello
bye
Hello
bye
```
instead of:
```
Hello
Hello
bye
bye
```
I have read about using `httpx`, but still, I cannot have a true parallelization. What&#39;s the problem?
||||||||||||||As per [FastAPI's documentation][1]:

> When you declare a path operation function with normal `def` instead
> of `async def`, it is run in an external threadpool **that is then
> `await`ed**, instead of being called directly (as it would block the
> server).

also, as described [here][2]:

> If you are using a third party library that communicates with
> something (a database, an API, the file system, etc.) and doesn't have
> support for using `await`, (this is currently the case for most
> database libraries), then declare your path operation functions as
> normally, with just `def`.
> 
> If your application (somehow) doesn't have to communicate with
> anything else and wait for it to respond, use `async def`.
> 
> If you just don't know, use normal `def`.
> 
> **Note**: You can mix `def` and `async def` in your path operation functions as much as you need and define each one using the best
> option for you. FastAPI will do the right thing with them.
> 
> Anyway, in any of the cases above, FastAPI **will still work
> asynchronously** and be extremely fast.
> 
> But by following the steps above, it will be able to do some
> performance optimizations.



Thus, `def` endpoints (in the context of asynchronous programming, a function defined with just `def` is called *synchronous* function), in FastAPI, run in a separate thread from an external threadpool that is then `await`ed, and hence, FastAPI will still work *asynchronously*. In other words, the server will process requests to such endpoints *concurrently*. Whereas, `async def` endpoints run in the [`event loop`][3]&mdash;on the main (single) thread&mdash;that is, the server will also process requests to such endpoints *concurrently*/*asynchronously*, **as long as there is** an [`await`][4] call to non-blocking I/O-bound operations inside such `async def` endpoints/routes, such as *waiting* for (1) data from the client to be sent through the network, (2) contents of a file in the disk to be read, (3) a database operation to finish, etc., (have a look [here][5]). If, however, an endpoint defined with `async def` does not `await` for something inside, in order to give up time for other tasks in the `event loop` to run (e.g., requests to the same or other endpoints, background tasks, etc.), each request to such an endpoint will have to be completely finished (i.e., exit the endpoint), before returning control back to the `event loop` and allow other tasks to run. In other words, in such cases, the server will process requests *sequentially*. **Note** that the same concept not only applies to FastAPI endpoints, but also to [`StreamingResponse`'s generator function][6] (see [`StreamingResponse`][7] class implementation), as well as [`Background Tasks`][8] (see [`BackgroundTask`][9] class implementation); hence, after reading this answer to the end, you should be able to decide whether you should define a FastAPI endpoint, `StreamingResponse`'s generator, or background task function with `def` or `async def`. 

The keyword `await` (which works only within an `async def` function) passes function control back to the `event loop`. In other words, it suspends the execution of the surrounding [coroutine][10] (i.e., a coroutine object is the result of calling an `async def` function), and tells the `event loop` to let some other task run, until that `await`ed task is completed. **Note** that just because you may define a custom function with `async def` and then `await` it inside your `async def` endpoint, it doesn't mean that your code will work asynchronously, if that custom function contains, for example, calls to `time.sleep()`, CPU-bound tasks, non-async I/O libraries, or any other blocking call that is incompatible with asynchronous Python code. In FastAPI, for example, when using the `async` methods of [`UploadFile`][11], such as `await file.read()` and `await file.write()`, FastAPI/Starlette, behind the scenes, actually runs such *synchronous* [File objects' methods][12] in a separate thread from the external threadpool (using the `async` [`run_in_threadpool()`][13] function) and `await`s it; otherwise, such methods/operations would block the `event loop`&mdash;you can find out more by looking at the [implementation of the `UploadFile` class][14]. The number of worker threads of that external threadpool can be adjusted as required&mdash;please have a look at [this answer][15] for more details.

**Note**  that `async` does not mean *parallel*, but *concurrently*. Asynchronous code with [`async` and `await` is many times summarised as using coroutines][16]. **Coroutines** are collaborative (or [cooperatively multitasked][17]), meaning that "at any given time, a program with coroutines is running **only** one of its coroutines, and this running coroutine suspends its execution only when it explicitly requests to be suspended" (see [here][18] and [here][19] for more info on coroutines). As described in [this article][20]:

> Specifically, whenever execution of a currently-running coroutine
> reaches an `await` expression, the coroutine may be suspended, and
> another previously-suspended coroutine may resume execution if what it
> was suspended on has since returned a value. Suspension can also
> happen when an `async for` block requests the next value from an
> asynchronous iterator or when an `async with` block is entered or
> exited, as these operations use `await` under the hood.

If, however, a blocking I/O-bound or CPU-bound operation was directly executed/called inside an `async def` function/endpoint, it would **block the main thread**, and hence, the `event loop` (as the `event loop` runs in the main thread). Hence, a blocking operation such as `time.sleep()` in an `async def` endpoint would block the entire server (as in the code example provided in your question). Thus, if your endpoint is not going to make any `async` calls, you could declare it with normal `def` instead, in which case, FastAPI would run it in a separate thread from the external threadpool and `await` it, as explained earlier (more solutions are given in the following sections). Example:
```python
@app.get("/ping")
def ping(request: Request):
	#print(request.client)
	print("Hello")
	time.sleep(5)
	print("bye")
	return "pong"
```

Otherwise, if the functions that you had to execute inside the endpoint are `async` functions that you had to `await`, you should define your endpoint with `async def`. To demonstrate this, the example below uses the [`asyncio.sleep()`][21] function (from the [`asyncio`][22] library), which provides a non-blocking sleep operation. The `await asyncio.sleep()` method will suspend the execution of the surrounding coroutine (until the sleep operation is completed), thus allowing other tasks in the `event loop` to run. Similar examples are given [here][23] and [here][24] as well.
```python
import asyncio
 
@app.get("/ping")
async def ping(request: Request):
	#print(request.client)
	print("Hello")
	await asyncio.sleep(5)
	print("bye")
	return "pong"
```

**Both** the endpoints above will print out the specified messages to the screen in the same order as mentioned in your question&mdash;if two requests arrived at around the same time&mdash;that is:
```text
Hello
Hello
bye
bye
```

### Important Note
When you call your endpoint for the second (third, and so on) time, please remember to do that from **a tab that is isolated from the browser's main session**; otherwise, succeeding requests (i.e., coming after the first one) will be blocked by the browser (on **client side**), as the browser will be waiting for response from the server for the previous request before sending the next one. You can confirm that by using `print(request.client)` inside the endpoint, where you would see the `hostname` and `port` number being the same for all incoming requests&mdash;if requests were initiated from tabs opened in the same browser window/session)&mdash;and hence, those requests would be processed sequentially, because of the browser sending them sequentially in the first place. To **solve** this, you could either:
1. Reload the same tab (as is running), or
2. Open a new tab in an Incognito Window, or
3. Use a different browser/client to send the request, or
4. Use the `httpx` library to [make asynchronous HTTP requests][25], along with the [*awaitable*][26] [`asyncio.gather()`][27], which allows executing multiple asynchronous operations concurrently and then returns a list of results in the **same** order the awaitables (tasks) were passed to that function (have a look at [this answer][28] for more details).

   **Example**:
   ```python
   import httpx
   import asyncio

   URLS = ['http://127.0.0.1:8000/ping'] * 2

   async def send(url, client):
       return await client.get(url, timeout=10)

   async def main():
       async with httpx.AsyncClient() as client:
           tasks = [send(url, client) for url in URLS]
           responses = await asyncio.gather(*tasks)
           print(*[r.json() for r in responses], sep='\n')

   asyncio.run(main())
   ```
   In case you had to call different endpoints that may take different time to process a request, and you would like to print the response out on client side as soon as it is returned from the server&mdash;instead of waiting for `asyncio.gather()` to gather the results of all tasks and print them out in the same order the tasks were passed to the `send()` function&mdash;you could replace the `send()` function of the example above with the one shown below:
   ```
   async def send(url, client):
       res = await client.get(url, timeout=10)
       print(res.json())
       return res
   ```

`Async`/`await` and Blocking I/O-bound or CPU-bound Operations
--------------------------------------

If you are required to use `async def` (as you might need to `await` for coroutines inside your endpoint), but also have some _synchronous_ blocking I/O-bound or CPU-bound operation (long-running computation task) that will block the `event loop` (essentially, the entire server) and won't let other requests to go through, for example:
```python 
@app.post("/ping")
async def ping(file: UploadFile = File(...)):
    print("Hello")
	try:
		contents = await file.read()
		res = cpu_bound_task(contents)  # this will block the event loop
	finally:
		await file.close()
	print("bye")
    return "pong"
```

then:

1. You should check whether you could change your endpoint's definition to normal `def` instead of `async def`. For example, if the only method in your endpoint that has to be awaited is the one reading the file contents (as you mentioned in the comments section below), you could instead declare the type of the endpoint's parameter as `bytes` (i.e., `file: bytes = File()`) and thus, FastAPI would read the file for you and you would receive the contents as `bytes`. Hence, there would be no need to use `await file.read()`. Please note that the above approach should work for small files, as the enitre file contents would be stored into memory (see the [documentation on `File` Parameters][29]); and hence, if your system does not have enough RAM available to accommodate the accumulated data (if, for example, you have 8GB of RAM, you can’t load a 50GB file), your application may end up crashing. Alternatively, you could call the `.read()` method of the [`SpooledTemporaryFile`][30] directly (which can be accessed through the `.file` attribute of the `UploadFile` object), so that again you don't have to `await` the `.read()` method&mdash;and as you can now declare your endpoint with normal `def`, each request will run in a **separate thread** (example is given below). For more details on how to upload a `File`, as well how Starlette/FastAPI uses `SpooledTemporaryFile` behind the scenes, please have a look at [this answer][31] and [this answer][32].

   ```python 
   @app.post("/ping")
   def ping(file: UploadFile = File(...)):
       print("Hello")
	   try:
		   contents = file.file.read()
		   res = cpu_bound_task(contents)
	   finally:
		   file.file.close()
       print("bye")
       return "pong"
   ```

2. Use FastAPI's (Starlette's) [`run_in_threadpool()`][13] function from the `concurrency` module&mdash;as @tiangolo suggested [here][33]&mdash;which "will run the function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked" (see [here][34]). As described by @tiangolo [here][35], "`run_in_threadpool` is an `await`able function; the first parameter is a normal function, the following parameters are passed to that function directly. It supports both *sequence* arguments and *keyword* arguments".

   ```python
   from fastapi.concurrency import run_in_threadpool

   res = await run_in_threadpool(cpu_bound_task, contents)
   ```

3. Alternatively, use `asyncio`'s [`loop.run_in_executor()`][36]&mdash;after obtaining the running `event loop` using [`asyncio.get_running_loop()`][37]&mdash;to run the task, which, in this case, you can `await` for it to complete and return the result(s), before moving on to the next line of code. Passing `None` to the *executor* argument, the *default* executor will be used; which is a [`ThreadPoolExecutor`][38]:

   ```python
   import asyncio

   loop = asyncio.get_running_loop()
   res = await loop.run_in_executor(None, cpu_bound_task, contents)
   ```
   or, if you would like to [pass keyword arguments][39] instead, you could use a `lambda` expression (e.g., `lambda: cpu_bound_task(some_arg=contents)`), or, preferably, [`functools.partial()`][40], which is specifically recommended in the documentation for [`loop.run_in_executor()`][36]:
   ```python
   import asyncio
   from functools import partial

   loop = asyncio.get_running_loop()
   res = await loop.run_in_executor(None, partial(cpu_bound_task, some_arg=contents))
   ```

   In Python 3.9+, you could also use [`asyncio.to_thread()`][41] to asynchronously run a synchronous function in a separate thread&mdash;which, essentially, uses `await loop.run_in_executor(None, func_call)` under the hood, as can been seen in the [implementation of `asyncio.to_thread()`][42]. The `to_thread()` function takes the name of a blocking function to execute, as well as any arguments (`*args` and/or `**kwargs`) to the function, and then returns a coroutine that can be `await`ed. Example:
   ```
   import asyncio

   res = await asyncio.to_thread(cpu_bound_task, contents)
   ```
    
   **Note** that as explained in [**this answer**][15], passing `None` to the `executor` argument **does not** create a new `ThreadPoolExecutor` every time you call `await loop.run_in_executor(None, ...)`, but instead re-uses the *default* executor with the *default* number of worker threads (i.e., `min(32, os.cpu_count() + 4)`). Thus, depending on the requirements of your application, that number might be quite low. In that case, you should rather use a custom [`ThreadPoolExecutor`][38]. For instance:
   ```python
   import asyncio
   import concurrent.futures

   loop = asyncio.get_running_loop()
   with concurrent.futures.ThreadPoolExecutor() as pool:
	   res = await loop.run_in_executor(pool, cpu_bound_task, contents)
   ```
   I would strongly recommend having a look at the linked answer above to learn about the difference between using `run_in_threadpool()` and `run_in_executor()`, as well as how to create a re-usable custom `ThreadPoolExecutor` at the application startup, and adjust the number of maximum worker threads as needed.

4. `ThreadPoolExecutor` will successfully prevent the `event loop` from being blocked, but won't give you the **performance improvement** you would expect from running **code in parallel**; especially, when one needs to perform `CPU-bound` tasks, such as the ones described [here][43] (e.g., audio or image processing, machine learning, and so on). It is thus preferable to **run CPU-bound tasks in a separate process**&mdash;using [`ProcessPoolExecutor`][44], as shown below&mdash;which, again, you can integrate with `asyncio`, in order to `await` it to finish its work and return the result(s). As described [here][45], it is important to protect the entry point of the program to avoid recursive spawning of subprocesses, etc. Basically, your code must be under [`if __name__ == '__main__'`][46]. 

   ```python
   import concurrent.futures
   
   loop = asyncio.get_running_loop()
   with concurrent.futures.ProcessPoolExecutor() as pool:
       res = await loop.run_in_executor(pool, cpu_bound_task, contents) 
   ```
   Again, I'd suggest having a look at the linked answer earlier on how to create a re-usable `ProcessPoolExecutor` at the application startup. You might find [this answer][47] helpful as well.

5. Use **more [workers][48]** to take advantage of multi-core CPUs, in order to run multiple processes in parallel and be able to serve more requests. For example, `uvicorn main:app --workers 4` (if you are using [Gunicorn as a process manager with Uvicorn workers][49], please have a look at [**this answer**][50]). When using 1 worker, only one process is run. When using multiple workers, this will spawn multiple processes (all single threaded). Each process has a separate Global Interpreter Lock (GIL), as well as its own `event loop`, which runs in the main thread of each process and executes all tasks in its thread. That means, there is only one thread that can take a lock on the interpreter of each process; unless, of course, you employ additional threads, either outside or inside the `event loop`, e.g., when using a `ThreadPoolExecutor` with `loop.run_in_executor`, or defining endpoints/background tasks/`StreamingResponse`'s generator with normal `def` instead of `async def`, as well as when calling `UploadFile`'s methods (see the first two paragraphs of this answer for more details).

   **Note:** Each worker ["has its own things, variables and memory"][51]. This means that `global` variables/objects, etc., won't be shared across the processes/workers. In this case, you should consider using a database storage, or  Key-Value stores (Caches), as described [here][52] and [here][53]. Additionally, note that "if you are consuming a large amount of memory in your code, **each process** will consume an equivalent amount of memory".


6. If you need to perform **heavy background computation** and you don't necessarily need it to be run by the same process (for example, you don't need to share memory, variables, etc), you might benefit from using other bigger tools like [Celery][54], as described in [FastAPI's documentation][55].


  [1]: https://fastapi.tiangolo.com/async/#path-operation-functions
  [2]: https://fastapi.tiangolo.com/async/#concurrency-and-async-await
  [3]: https://docs.python.org/3/library/asyncio-eventloop.html
  [4]: https://stackoverflow.com/questions/38865050/is-await-in-python3-cooperative-multitasking
  [5]: https://fastapi.tiangolo.com/async/#asynchronous-code
  [6]: https://stackoverflow.com/a/75760884/17865804
  [7]: https://github.com/encode/starlette/blob/31164e346b9bd1ce17d968e1301c3bb2c23bb418/starlette/responses.py#L235
  [8]: https://fastapi.tiangolo.com/tutorial/background-tasks/
  [9]: https://github.com/encode/starlette/blob/33f46a13625bcca4b7520e33be299a23b2e2b26c/starlette/background.py#L15
  [10]: https://docs.python.org/3/library/asyncio-task.html#coroutines
  [11]: https://fastapi.tiangolo.com/tutorial/request-files/#uploadfile
  [12]: https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects
  [13]: https://github.com/encode/starlette/blob/b8ea367b4304a98653ec8ce9c794ad0ba6dcaf4b/starlette/concurrency.py#L35
  [14]: https://github.com/encode/starlette/blob/048643adc21e75b668567fc6bcdd3650b89044ea/starlette/datastructures.py#L426
  [15]: https://stackoverflow.com/a/77941425/17865804
  [16]: https://fastapi.tiangolo.com/async/#coroutines
  [17]: https://en.wikipedia.org/wiki/Cooperative_multitasking
  [18]: https://stackoverflow.com/questions/553704/what-is-a-coroutine
  [19]: https://stackoverflow.com/questions/1934715/difference-between-a-coroutine-and-a-thread
  [20]: https://jwodder.github.io/kbits/posts/pyasync-fundam/
  [21]: https://docs.python.org/3/library/asyncio-task.html#asyncio.sleep
  [22]: https://docs.python.org/3/library/asyncio.html
  [23]: https://docs.python.org/3/library/asyncio-task.html#coroutine
  [24]: https://stackoverflow.com/a/56730924
  [25]: https://www.python-httpx.org/async/#making-async-requests
  [26]: https://docs.python.org/3/library/asyncio-task.html#awaitables
  [27]: https://docs.python.org/3/library/asyncio-task.html#asyncio.gather
  [28]: https://stackoverflow.com/a/74239367/17865804
  [29]: https://fastapi.tiangolo.com/tutorial/request-files/#define-file-parameters
  [30]: https://docs.python.org/3/library/tempfile.html#tempfile.SpooledTemporaryFile
  [31]: https://stackoverflow.com/a/70657621/17865804
  [32]: https://stackoverflow.com/a/70667530/17865804
  [33]: https://github.com/tiangolo/fastapi/issues/1066#issuecomment-612940187
  [34]: https://bocadilloproject.github.io/guide/async.html#common-patterns
  [35]: https://gitter.im/tiangolo/fastapi?at=5ce550f675d9a575a625feb7
  [36]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor
  [37]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop
  [38]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
  [39]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio-pass-keywords
  [40]: https://docs.python.org/3/library/functools.html#functools.partial
  [41]: https://docs.python.org/3/library/asyncio-task.html#asyncio.to_thread
  [42]: https://github.com/python/cpython/blob/c5660ae96f2ab5732c68c301ce9a63009f432d93/Lib/asyncio/threads.py#L12
  [43]: https://fastapi.tiangolo.com/async/#is-concurrency-better-than-parallelism
  [44]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
  [45]: https://stackoverflow.com/q/15900366
  [46]: https://stackoverflow.com/questions/419163/what-does-if-name-main-do
  [47]: https://stackoverflow.com/a/77862153/17865804
  [48]: https://fastapi.tiangolo.com/deployment/server-workers/
  [49]: https://fastapi.tiangolo.com/deployment/server-workers/#gunicorn-with-uvicorn-workers
  [50]: https://stackoverflow.com/a/71613757/17865804
  [51]: https://fastapi.tiangolo.com/deployment/concepts/#memory-per-process
  [52]: https://stackoverflow.com/a/71537393/17865804
  [53]: https://stackoverflow.com/a/65699375/17865804
  [54]: https://docs.celeryq.dev/
  [55]: https://fastapi.tiangolo.com/tutorial/background-tasks/#caveat

--------------------------------------------------
Is it possible to access Svelte store from external js files?
I am wondering if i would be able to access my *Svelte* store values from a plain .js file.

I am trying to write functions returning a dynamic value based on a store value, to import them in any component.
But in a plain .js file I can&#39;t just access the store value with the $ sign..

Quick exemple of a basic function that uses a store value and could be used on multiple components: 

```js
//in .svelte

function add() {
    $counter = $counter + 1;
}
```

*EDIT: rephrasing a bit*

*EDIT:*
Found a solution but i don&#39;t really know if it&#39;s really optimized..

```js
//in .js file

import { get } from &quot;svelte/store&quot;;
import { counter } from &quot;./stores&quot;;

export function add() {
    var counterRef = get(counter);
    counter.set(counterRef + 1);
}
```
||||||||||||||In addition to rixo's answer, a better way to implement `add` is to use the store's `update` method:

```js
import { counter } from "./stores";

export function add() {
    counter.update(n => n + 1);
}
```

You could also create a [custom store](https://svelte.dev/tutorial/custom-stores) that implemented that logic.

--------------------------------------------------
Is CP437 decoding broken for control characters?
According to the [Wikipedia page for Code Page 437](https://en.wikipedia.org/wiki/Code_page_437) the byte values `\x01` through `\x1f` should decode to graphic characters, e.g. `b&#39;\x01&#39;` equates to ☺ `&#39;\u263A&#39;`. But that&#39;s not what `decode` produces:

    &gt;&gt;&gt; b&#39;\x01&#39;.decode(&#39;cp437&#39;)
    &#39;\x01&#39;

That was Python 3.6 but 2.7 does the same, for all 31 byte values.
||||||||||||||While there were graphics associated with the byte range `\x01` through `\x1f`, those graphics were only used in some contexts. In other contexts, those code points would be interpreted as control characters, as in ASCII. Quoting an [IBM page on CP437][1]:

> Code points X'01' through X'1F' and X'7F' may be controls or graphics depending on context. For displays the hexadecimal code in a memory-mapped 
video display buffer is a graphic. For printers the graphics context is established by a preceding control sequence in the data stream. There are two 
such control sequences: ESC X'5C' and ESC X'5E' named Print All Characters and Print Single Character respectively. In other situations the code 
points in question are used as controls.



Python's CP437 decoding is based on the [Unicode mappings on Unicode.org][2], which use the control character interpretation.

The [Unicode FAQ implies][3] that "The correct Unicode mappings for the special graphic characters (01-1F, 7F) of CP437 and other DOS-type code pages" should be available at https://www.unicode.org/Public/MAPPINGS, but digging down there only turns up the mappings with the control characters, and a [page][4] linking to several IBM websites. Digging through IBM's sites turns up ftp://ftp.software.ibm.com/software/globalization/gcoc/attachments/CP00437.txt, which gives graphical mappings for `\x01`-`\x1f` in terms of IBM's [GCGID system][5], but not in terms of Unicode.

I don't know if there actually *is* an official mapping, from either IBM or Unicode, that gives canonical Unicode mappings for `\x01`-`\x1f` in terms of the graphical interpretation of CP437.


  [1]: http://www-01.ibm.com/software/globalization/cp/cp00437.html
  [2]: ftp://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/PC/CP437.TXT
  [3]: http://unicode.org/faq/char_combmark.html#5
  [4]: http://www.unicode.org/Public/MAPPINGS/VENDORS/IBM/IBM_conversions.html
  [5]: https://www-01.ibm.com/software/globalization/gcgid/gcgid.html

--------------------------------------------------
Converting from Python-Polars to Rust-Polars
I have the following working Python polars code. I am learning Rust and am interested in converting Python to Rust.

```
df = df.with_columns(pl.concat([pl.col(base).slice(0, period).rolling_mean(period), pl.col(base).slice(period,None)]).alias(&#39;con&#39;))
```
How to convert the same in Rust? It might be very trivial, still not sure where I am going wrong.

```
let rolling_options = RollingOptions {
        window_size : Duration::parse(duration_str.as_str()),
        ..Default::default()
    };
let a = col(base).slice(0,period).rolling_mean(rolling_options);
let b = col(base).slice(period,lit(Null {}));

let temp_df = df.with_column(concat([a, b], UnionArgs::default()));
```
I keep getting the following error

&gt;mismatched types
expected enum `Expr`
   found enum `Result&lt;LazyFrame, PolarsError&gt;`

When i checked the data types of **a** and **b**, to my surprise they are **LazyFrame** and not **Expr**

[rolling_mean][1] as per the document returns **Expr** and so does [slice][2]. Not sure what I am missing.


  [1]: https://docs.rs/polars/latest/polars/prelude/enum.Expr.html#method.rolling_mean
  [2]: https://docs.rs/polars/latest/polars/prelude/enum.Expr.html#method.slice
||||||||||||||The result datatype you are getting is an enum meant to represent whether the operation was successful (returns a lazyFrame) or it failed (returns polarserror).

you should be able to 'uwnrap' the result

This link should cover the different ways to handle errors/results in rust:
https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html

--------------------------------------------------
Is Element.tagName always uppercase?
Reading at [MDN about Element.tagName][1] it states:

&gt;On HTML elements in DOM trees flagged as HTML documents, tagName returns the element name in the uppercase form.

My question is: is this trustable? Does IE (old and modern) behave as expected? Is this likely to change? or is it better to always work with `el.tagName.toLowerCase()`?


  [1]: https://developer.mozilla.org/en-US/docs/Web/API/Element.tagName
||||||||||||||You don't have to `toLowerCase` or whatever, browsers do behave the same on this point (surprisingly huh?).

About the rationale, once I had discussion with a colleague who's very professional on W3C standards. One of his opinions is that using uppercase TAGNAME would be much easier to recognize them out of user content. That's quite persuasive for me.

-------------
**Edit:** As @adjenks says, XHTML doctype returns mixed-case tagName *if the document is served as `Content-Type: application/xhtml+xml`*. Test page: http://programming.enthuses.me/tag-node-case.php?doc=x

Technically, please read this spec for more info: http://www.w3.org/TR/REC-DOM-Level-1/level-one-core.html#ID-745549614

> Note that this (tagName) is **case-preserving in XML**, as are all of the operations of the DOM. The HTML DOM returns the tagName of an HTML element in the canonical uppercase form, regardless of the case in the source HTML document.

As of asker's question: this is trustable. Breaking change is not likely to happen in HTML spec.

--------------------------------------------------
How to validate more than one field of a Pydantic model?
I want to validate three model Fields of a Pydantic model. To do this, I am importing [`root_validator`][1] from pydantic, however I am getting the error below:
```py3
from pydantic import BaseModel, ValidationError, root_validator
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ImportError: cannot import name &#39;root_validator&#39; from &#39;pydantic&#39; (C:\Users\Lenovo\AppData\Local\Programs\Python\Python38-32\lib\site-packages\pydantic\__init__.py)
```

I tried this:
```python
@validator
def validate_all(cls, v, values, **kwargs):
    ...
```

I am inheriting my pydantic model from some common fields parent model. Values showing only parent class fields, but not my child class fields. For example:

```py3
class Parent(BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str
    
    @validator
    def validate_all(cls, v, values, **kwargs):
        #here values showing only (name and comment) but not address and phone.
        ...
```


  [1]: https://pydantic-docs.helpmanual.io/usage/validators/#root-validators
||||||||||||||To extend on the answer of `Rahul R`, this example shows in more detail how to use the `pydantic` validators.

This example contains all the necessary information to answer your question.

Note, that there is also the option to use a `@root_validator`, as mentioned by `Kentgrav`, see the example at the bottom of the post for more details.

```python
import pydantic

class Parent(pydantic.BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str

    # If you want to apply the Validator to the fields "name", "comments", "address", "phone"
    @pydantic.validator("name", "comments", "address", "phone")
    @classmethod
    def validate_all_fields_one_by_one(cls, field_value):
        # Do the validation instead of printing
        print(f"{cls}: Field value {field_value}")

        return field_value  # this is the value written to the class field

    # if you want to validate to content of "phone" using the other fields of the Parent and Child class
    @pydantic.validator("phone")
    @classmethod
    def validate_one_field_using_the_others(cls, field_value, values, field, config):
        parent_class_name = values["name"]
        parent_class_address = values["address"] # works because "address" is already validated once we validate "phone"
        # Do the validation instead of printing
        print(f"{field_value} is the {field.name} of {parent_class_name}")

        return field_value 

Customer(name="Peter", comments="Pydantic User", address="Home", phone="117")
```
**Output**
```cmd
<class '__main__.Customer'>: Field value Peter
<class '__main__.Customer'>: Field value Pydantic User
<class '__main__.Customer'>: Field value Home
<class '__main__.Customer'>: Field value 117
117 is the phone number of Peter
Customer(name='Peter', comments='Pydantic User', address='Home', phone='117')
```

To answer your question in more detail:

Add the fields to validate to the `@validator` decorator directly above the validation function.
- `@validator("name")` uses the field value of `"name"` (e.g. `"Peter"`) as input to the validation function. All fields of the class and its parent classes can be added to the `@validator` decorator.
- the validation function (`validate_all_fields_one_by_one`) then uses the field value as the second argument (`field_value`) for which to validate the input. The return value of the validation function is written to the class field. The signature of the validation function is `def validate_something(cls, field_value)` where the function and variable names can be chosen arbitrarily (but the first argument should be `cls`). According to Arjan (https://youtu.be/Vj-iU-8_xLs?t=329), also the `@classmethod` decorator should be added.


If the goal is to validate one field by using other (already validated) fields of the parent and child class, the full signature of the validation function is `def validate_something(cls, field_value, values, field, config)` (the argument names `values`,`field` and `config` **must** match) where the value of the fields can be accessed with the field name as key (e.g. `values["comments"]`).

**Edit1**: If you want to check only input values of a certain type, you could use the following structure:
```python
@validator("*") # validates all fields
def validate_if_float(cls, value):
    if isinstance(value, float):
        # do validation here
    return value
```

**Edit2**: Easier way to validate all fields together using `@root_validator`:
```python
import pydantic

class Parent(pydantic.BaseModel):
    name: str
    comments: str

class Customer(Parent):
    address: str
    phone: str

    @pydantic.root_validator()
    @classmethod
    def validate_all_fields_at_the_same_time(cls, field_values):
        # Do the validation instead of printing
        print(f"{cls}: Field values are: {field_values}")
        assert field_values["name"] != "invalid_name", f"Name `{field_values['name']}` not allowed."
        return field_values
```

**Output**:

```python
Customer(name="valid_name", comments="", address="Street 7", phone="079")
<class '__main__.Customer'>: Field values are: {'name': 'valid_name', 'comments': '', 'address': 'Street 7', 'phone': '079'}
Customer(name='valid_name', comments='', address='Street 7', phone='079')
```

```python
Customer(name="invalid_name", comments="", address="Street 7", phone="079")
ValidationError: 1 validation error for Customer
__root__
  Name `invalid_name` not allowed. (type=assertion_error)
```

--------------------------------------------------
What is Python&#39;s bytes type actually used for?
Could somebody explain the general purpose of [the bytes type in Python 3](https://docs.python.org/3/library/stdtypes.html#bytes-objects), or give some examples where it is preferred over other data types? 

I see that the advantage of [bytearrays](https://docs.python.org/3/library/stdtypes.html#bytearray-objects) over strings is their mutability, but what about bytes? So far, the only situation where I actually needed it was sending and receiving data through sockets; is there something else? 
||||||||||||||Possible duplicate of [what is the difference between a string and a byte string][1]

In short, the bytes type is a sequence of bytes that have been encoded and are ready to be stored in memory/disk. There are many types of encodings (utf-8, utf-16, windows-1255), which all handle the bytes differently. The bytes object can be decoded into a str type.

The str type is a sequence of unicode characters. The str needs to be encoded to be stored, but is mutable and an abstraction of the bytes logic. 

There is a strong relationship between `str` and `bytes`. `bytes` can be decoded into a `str`, and `str`s can be encoded into bytes. 

You typically only have to use `bytes` when you encounter a string in the wild with a unique encoding, or when a library requires it. `str` , especially in python3, will handle the rest. 

More reading [here][2] and [here][3]



  [1]: https://stackoverflow.com/questions/6224052/what-is-the-difference-between-a-string-and-a-byte-string
  [2]: https://eli.thegreenplace.net/2012/01/30/the-bytesstr-dichotomy-in-python-3
  [3]: https://betterprogramming.pub/strings-unicode-and-bytes-in-python-3-everything-you-always-wanted-to-know-27dc02ff2686

--------------------------------------------------
Process terminated. Couldn&#39;t find a valid ICU package installed on the system in Asp.Net Core 3 - ubuntu
I am trying to run a Asp.Net Core 3 application in Ubuntu 19.10 thru terminal using `dotnet run` command but it does not seem to work. I get this error.

&gt; ```none
&gt; Process terminated. Couldn&#39;t find a valid ICU package installed on the system.
&gt; Set the configuration flag System.Globalization.Invariant to true if you want
&gt; to run with no globalization support.   
&gt;  at System.Environment.FailFast(System.String)   
&gt;  at System.Globalization.GlobalizationMode.GetGlobalizationInvariantMode()
&gt;  at System.Globalization.GlobalizationMode..cctor()   
&gt;  at System.Globalization.CultureData.CreateCultureWithInvariantData()   
&gt;  at System.Globalization.CultureData.get_Invariant()   
&gt;  at System.Globalization.CultureInfo..cctor()   
&gt;  at System.StringComparer..cctor()   
&gt;  at System.StringComparer.get_OrdinalIgnoreCase()   
&gt;  at Microsoft.Extensions.Configuration.ConfigurationProvider..ctor()   
&gt;  at Microsoft.Extensions.Configuration.EnvironmentVariables.EnvironmentVariablesConfigurationSource.Build(Microsoft.Extensions.Configuration.IConfigurationBuilder)
&gt;  at Microsoft.Extensions.Configuration.ConfigurationBuilder.Build()   
&gt;  at Microsoft.AspNetCore.Hosting.GenericWebHostBuilder..ctor(Microsoft.Extensions.Hosting.IHostBuilder)
&gt;  at Microsoft.Extensions.Hosting.GenericHostWebHostBuilderExtensions.ConfigureWebHost(Microsoft.Extensions.Hosting.IHostBuilder, System.Action&#39;1&lt;Microsoft.AspNetCore.Hosting.IWebHostBuilder&gt;)   
&gt;  at Microsoft.Extensions.Hosting.GenericHostBuilderExtensions.ConfigureWebHostDefaults(Microsoft.Extensions.Hosting.IHostBuilder, System.Action&#39;1&lt;Microsoft.AspNetCore.Hosting.IWebHostBuilder&gt;)   
&gt;  at WebApplication.Program.CreateHostBuilder(System.String[])   
&gt;  at WebApplication.Program.Main(System.String[])
&gt; ```

I installed the dotnet core sdk using the ubuntu store and after that I also installed Rider IDE.

The weird thing here is that when I run the app using Rider it runs fine, the only issue is using terminal dotnet core commands.

Does anybody know what might be the issue ?

The application is created using Rider. I don&#39;t think that this plays a role but just as a side fact.

I know there are also other ways to install dotnet core in ubuntu but since the sdk is available in the ubuntu story I thought it should work out of the box and of course its an easier choice.

Also tried this [one](https://stackoverflow.com/questions/58132275/ci-cannot-build-net-project-fails-with-couldnt-find-a-valid-icu-package-ins) but does not seem to work for me. Still the same issue happens after running the commands.

||||||||||||||The alternative solution as described in [Microsoft documentation][1] is to set environment variable before running your app 

    export DOTNET_SYSTEM_GLOBALIZATION_INVARIANT=1


  [1]: https://learn.microsoft.com/en-us/dotnet/core/run-time-config/globalization

--------------------------------------------------
Python: Should I save PyPi packages offline as a backup?
**My Python projects heavily depends on PyPi packages**.&lt;br&gt;
I want to make sure that: in any time in the future: the packages required by my apps will always be available online on PyPi.&lt;br&gt;
For example:-&lt;br&gt;
I found a project on Github that requires PyQt4.&lt;br&gt;
when I tried to run it on my Linux machine,&lt;br&gt;
it crashed on startup because it can&#39;t find PyQt4 package on PyPi.&lt;br&gt;
&gt; NB: I know that PyQt4 is deprecated

I searched a lot to find an archive for PyPi that still holds PyQt4 package, but I couldn&#39;t find them anywhere.&lt;br&gt;

so I had to rewrite that app to make it work on PyQt5.&lt;br&gt;
I only changed the code related to the UI (ie: PyQt4).&lt;br&gt;
other functions were still working.&lt;br&gt;

so the only problem with that app was that PyQt4 package was removed from PyPi.&lt;br&gt;
&lt;hr&gt;&lt;br&gt;
so, my question is: should I save a backup of the PyPi packages I use ?
||||||||||||||**TL;DR**

YES if you want availability... The next big question is **how** best to keep a backup version of the dependencies? There are some suggestions at the end of the answer.

**Long Verion:**

Your questions touches on the concept of "Availability" which one of the three pillars of Information Assurance (or Information Security). The other two pillars are Confidentiality and Integrity... The CIA triad.

PyPi packages are maintained by the owners of those packages, a project that depends on a package and list it as a dependency must take into account the possibility that the owner of the package will pull the package or a version of the package out of PyPi at any moment.

Important python packages with many dependencies usually are maintained by foundations or organisations that are more responsible with dealing with downstream dependants packages and projects. However keeping support for old packages is very costly and requires extra effort and usually maintainers sets a date for end of support, or publish a the package lifecycle where they state when a specific version will be removed from the public PyPi server.

Once that happens, the dependant have to update the code (as you did), or provide the original dependency via alternative means.

This topic is very important for procurement in Libraries, Universities, Labs, Companies, and Government Agencies where a software tool might have dependencies on other software packages (or ecosystem), and where "availability" should be addressed adequately. Addressing might mean ensuring high availability, but it could also mean living with the risk of losing availability of the packages... The choices you make for "security" of your project should be informed by a risk analysis.

Now to make sure that dependencies are always available... I quickly compiled the following list. Note that each option has pros and cons. You should evaluate these and other options based on your needs:

 1. Store the virtual environment along with the code. Once you create a virtual environment and install the packages you require for the project in that virtual environment, you can keep the virtual environment for posterity.
 2. Host your own PyPi instance (or mirror) and keep a copy of packages you depend upon hosted on it https://packaging.python.org/en/latest/guides/hosting-your-own-index/ 
 3. Use an "artifact management tool" such as Artifactory from https://jfrog.com/artifact-management/, where you can not only host python packages but also docker images, nmap packages, and other kinds of artifacts.
 4. Get the source code of all dependencies, and always build from source.

Let me know if you think of other ideas so that I add them to the list...

--------------------------------------------------
How to create a windowless application in C#?
I am new to C# and want to make a program that runs without a console/GUI, but can&#39;t figure out how. Is that even possible?

The only options I found were minimizing the window or hiding it AFTER start, but I want it to start without a window/visible in the task bar. I am aware of the fact that I (of course) won&#39;t be able to write to the console...
||||||||||||||What you're looking for is a windows service. You can create one, or at least I see the option to (in VS2022), when creating a new project.

Just create new project and search `Windows Service` and check the one that says `C#`. Pay attention though, one says VB.

Creating a window form and then hiding it.. unless you actually want to bring it up at some point, would be something I would advise against.

    using System;
    using System.Collections.Generic;
    using System.Linq;
    using System.ServiceProcess;
    using System.Text;
    using System.Threading.Tasks;
    
    namespace TestService
    {
        internal static class Program
        {
            /// <summary>
            /// The main entry point for the application.
            /// </summary>
            static void Main()
            {
                ServiceBase[] ServicesToRun;
                ServicesToRun = new ServiceBase[]
                {
                    new Service1()
                };
                ServiceBase.Run(ServicesToRun);
            }
        }
    }



--------------------------------------------------
How to draw tiled image with QT
I&#39;m writing interface with C++/Qt in QtCreator&#39;s designer. What element to chose to make as a rect with some background image?

And the second question: how to draw tiled image? I have and image with size (1&#215;50) and I want to render it for the parent width. Any ideas?

-----------------

    mTopMenuBg = QPixmap(&quot;images/top_menu_bg.png&quot;);
    mTopMenuBrush = QBrush(mTopMenuBg);
    mTopMenuBrush.setStyle(Qt::TexturePattern);
    mTopMenuBrush.setTexture(mTopMenuBg);
    
    ui-&gt;graphicsView-&gt;setBackgroundBrush(mTopMenuBrush);

&gt; QBrush: Incorrect use of
&gt; TexturePattern
||||||||||||||If you just want to show an image you can use [QImage][1].  To make a background with the image tiled construct a [QBrush][2] with the QImage.  Then, if you were using [QGraphicsScene][3] for example, you could set the bursh as the background brush.

Here is an example which fills the entire main window with the tiled image "document.png":

    int main(int argc, char *argv[]) {
    	QApplication app(argc, argv);
    	QMainWindow *mainWindow = new QMainWindow();
    
    	QGraphicsScene *scene = new QGraphicsScene(100, 100, 100, 100);
    	QGraphicsView *view = new QGraphicsView(scene);
    	mainWindow->setCentralWidget(view);
    
    	QImage *image = new QImage("document.png");
    	if(image->isNull()) {
    		std::cout << "Failed to load the image." <<std::endl;
    	} else {
    		QBrush *brush = new QBrush(*image);
    		view->setBackgroundBrush(*brush);
    	}
    
    	mainWindow->show();
    	return app.exec();
    }

The resulting app:  
![screen shot][4]


Alternatively, it seems that you could use [style sheets][5] with any widget and change the [background-image][6] property on the widget.  This has more integration with QtDesigner as you can set the style sheet and image in [QtDesigner][7].


  [1]: https://doc.qt.io/qt-6/qimage.html
  [2]: https://doc.qt.io/qt-6/qbrush.html
  [3]: https://doc.qt.io/qt-6/qgraphicsscene.html
  [4]: http://i.stack.imgur.com/1LK7W.jpg
  [5]: https://doc.qt.io/qt-6/stylesheet-reference.html
  [6]: https://doc.qt.io/qt-6/stylesheet-reference.html#background-image-prop
  [7]: https://doc.qt.io/qt-6/designer-stylesheet.html

--------------------------------------------------
Autofilter and set field using variable
I&#39;m using a serch criteria to get the number/index location of a column to use in the field section of the autofilter. Getting an error &quot;runtime err 1004: Autofilter method of range class failed&quot; not sure if it&#39;s possible. I can see in the degug the variable is holding the correct number

```

Private Sub cmdExtract1_Click()
Dim ws As Worksheet
    Dim lngLastRow As Long
    Dim rngData As Range
    Dim iColNumber As Integer
    
    
     Dim strSearch As String
    Dim aCell As Range
  
    Set ws = Worksheets(&quot;Detail Excel&quot;)
    ws.Activate


    &#39;Identify the last row and use that info to set up the Range
    With ws
    ws.Range(&quot;1:1&quot;).Select
        lngLastRow = ActiveSheet.Cells.Find(&quot;*&quot;, SearchOrder:=xlByRows, SearchDirection:=xlPrevious).Row

strSearch = &quot;Deleted App&quot;

    Set aCell = Sheet1.Rows(1).Find(What:=strSearch, LookIn:=xlValues, _
    LookAt:=xlWhole, SearchOrder:=xlByRows, SearchDirection:=xlNext, _
    MatchCase:=False, SearchFormat:=False)
    
     iColNumber = aCell.Column
         
    End With
    

&#39;Offer Date: include dates, remove blanks
Application.DisplayAlerts = False &#39;switching off the alert button
ws.Range(&quot;A1&quot; &amp; &quot;:y&quot; &amp; lngLastRow).AutoFilter Field:=iColNumber, Criteria1:=&quot;&quot;
ws.Range(&quot;A2&quot; &amp; &quot;:y&quot; &amp; lngLastRow).SpecialCells(xlCellTypeVisible).Delete
Application.DisplayAlerts = True &#39;switching on the alert button

On Error Resume Next
ws.ShowAllData
```




search variable then set the cell column number to a variable
||||||||||||||If you only want to delete rows with blanks you could do that without AutoFilter:

```
Private Sub cmdExtract1_Click()
    Dim ws As Worksheet, strSearch As String, m As Variant
  
    Set ws = Worksheets("Detail Excel")
    strSearch = "Deleted App"
    
    m = Application.match(strSearch, ws.rows(1), 0)
    
    If IsError(m) Then 'no header match?
        MsgBox "Header '" & strSearch & "' not found!", vbExclamation
        Exit Sub
    Else
        With ws.Range(ws.Cells(2, m), ws.Cells(rows.count, m).End(xlUp))
            On Error Resume Next 'ignore error in case no blanks
            .SpecialCells(xlCellTypeBlanks).EntireRow.Delete
            On Error GoTo 0      'stop ignoring errors
        End With
    End If

End Sub
```

EDIT: using autofilter

```
Private Sub cmdExtract1_Click()
    Dim ws As Worksheet, strSearch As String, m As Variant
    Dim lr As Long, lc As Long
  
    Set ws = Worksheets("Detail Excel")
    strSearch = "Deleted App"
    
    m = Application.match(strSearch, ws.rows(1), 0)
    
    If IsError(m) Then 'no header match?
        MsgBox "Header '" & strSearch & "' not found!", vbExclamation
        Exit Sub
    Else
        lr = LastOccupiedRow(ws)
        lc = ws.Cells(1, ws.Columns.count).End(xlToLeft).Column 'last header
        With ws.Range("A1", ws.Cells(lr, lc))
            .AutoFilter Field:=m, Criteria1:=""
            .Offset(1).SpecialCells(xlCellTypeVisible).EntireRow.Delete
        End With
        ws.ShowAllData
    End If
End Sub

Function LastOccupiedRow(ws As Worksheet) As Long
    Dim f As Range
    Set f = ws.Cells.Find("*", SearchOrder:=xlByRows, SearchDirection:=xlPrevious)
    If Not f Is Nothing Then LastOccupiedRow = f.row
End Function
```

--------------------------------------------------
How do I decrypt email from the href value
I am trying to decrypt email from the href value.
I encountered this problem while doing a web scraping task using python.

```html
&lt;a href=&quot;javascript:linkTo_UnCryptMailto(%27ocknvq%2CkphqBngjocpp0ej%27);&quot;&gt;
```

https://i.stack.imgur.com/yB8vo.png

 Whenever I click on *Email*, it directs me to Outlook mail application.From there I can get the email.However, it want the email without actually going to the mail application and in the console.

I have tried various decryption methods discussed on this website but it didn&#39;t work.Can someone give me a hint about what type of method for encryption is used?

||||||||||||||If you are looking to reverse engineer the encryption, you will need to go through the site's source and look for the JS function `linkTo_UnCryptMailto`.

More simply though, if you open dev tools (Ctrl + Shift + I) on chrome and click on the console, you should just be able to type `linkTo_UnCryptMailto(%27ocknvq%2CkphqBngjocpp0ej%27)` and view the result since the function appears to be global.

--------------------------------------------------
Show Bootstrap modal using (#myModal).modal(&#39;show&#39;) from Angular component
I have an Angular 8 project that is using bootstrap. I am trying to show the modal dialog inside my component using the `$(&#39;myModal&#39;).modal(&#39;show&#39;)` inside my component&#39;s *Typescript* file.

Here&#39;s my component&#39;s file:

    import {Component, OnInit} from &#39;@angular/core&#39;;
    import {Router} from &#39;@angular/router&#39;;
    // import * as $ from &#39;jquery&#39;;
    import * as bootstrap from &#39;bootstrap&#39;;
    
    @Component({
      selector: &#39;app-xyz&#39;,
      templateUrl: &#39;./xyz.component.html&#39;,
      styleUrls: [&#39;./xyz.css&#39;]
    })
    export class XyzComponent implements OnInit {
    
      constructor(private router: Router) {
      }
    
      ngOnInit() {
      }
    
      submit() {
        $(&#39;#confirm&#39;).modal(&#39;show&#39;);
      }
    
    }

Upon invoking the submit() funciton on click I get the following error: `ERROR TypeError: $(...).modal is not a function`

I installed bootstrap and jquery using `npm install bootstrap` --save and `npm install jquery --save`.

I even installed *ngx-bootstrap*.

However, when I uncomment the line importing *jQuery* I get a different error: `ERROR TypeError: jquery__WEBPACK_IMPORTED_MODULE_3__(...).modal is not a function`
||||||||||||||Check your angular.json file to make sure that the Jquery js file is included in your scripts array.

`"scripts": [
    "node_modules/jquery/dist/jquery.min.js",
]`

Then in your component TS file declare var $ instead of trying to import it:

`declare var $: any;`

That should allow you to trigger the modal via

`$('#confirm').modal();`

Also make sure that the HTML is correct. Example Modal:


    <div class="modal fade" id="confirm" tabindex="-1" role="dialog" data-backdrop="static" aria-labelledby="noDataModalCenterTitle" aria-hidden="true">
        <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
                <div class="modal-header" style="background-color:lightgrey">
                    <h5 class="modal-title" id="noDataModalLongTitle">No Data</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                    </button>
                </div>
                <div class="modal-body">
                    <i class="fas fa-check fa-4x mb-3 animated rotateIn"></i>
                    No Data Found. Please expand your search criteria and try again.
                </div>
                <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Ok</button>
                </div>
            </div>
        </div>
    </div>

Hope this helps!

--------------------------------------------------
cypher: Count distinct paths between two nodes disregarding link type
I&#39;m use cypher to count the number of paths between two nodes.  I have this query:

    MATCH p=(a)-[*1..2]-(b)
    where ID(a) = &quot;node1&quot;
    and ID(b) = &quot;node2&quot;
    return COUNT(p)

However, many of my nodes have multiple links to the same node with different relationship types.  I&#39;d like to ONLY count the distinct paths regardless of the relationship type.  

For example, the paths returned may be as follows:

    (node1)-[rel_type_a]-(node3)-[rel_type_b]-(node2) 
    (node1)-[rel_type_c]-(node3)-[rel_type_d]-(node2) 
    (node1)-[rel_type_e]-(node3)-[rel_type_f]-(node2) 
The query above counts this as 3 paths, but I only want to count this as a single path since all of the nodes are are the same, I&#39;m not interested in the relationship types.

Thanks in advance!

||||||||||||||I have found the following query that works:

    MATCH p=(a)-[*1..2]-(b)
    where ID(a) = "node1"
    and ID(b) = "node2"
    return count(distinct(nodes(p)))

If there is a better way to do this I'm all ears!

--------------------------------------------------
OffsetDateTime date object not getting stored in db the way date is set in object
I have a model which has startDateTime field which is storing DateTime in OffsetDateTime format.

```
startDateTime: 2023-07-25T04:40:46.143-08:00
```

However when we store the above object in our PostgreSQL db on GCP, it is getting stored in below format:  
`2023-07-25 12:40:46.143+00`

It looks like it is adjusting the above object to UTC time.  
But my requirement is, it sohuld store in the same format which is there in object and **should not adjust** to UTC time.

I explored the methods given [here](https://docs.oracle.com/javase/8/docs/api/java/time/OffsetDateTime.html), but none of the methods is satisfying my requirement.

Can someone please suggest if there is a way to acheive this. Any help would be appreciated.

Code I used and input is ```startDateTime: 2023-07-25T12:40:46.143Z```
and ``` &quot;timeZoneOffset&quot;: &quot;UTC-08:00&quot; ```. I am converting input to expected OffsetDateTime based on timezoneOffset value.
```
 val actualDateTime: OffsetDateTime = OffsetDateTime.parse(startDateTime)
            val zoneOffset: ZoneOffset = ZoneOffset.of(timeZoneOffset.replace(&quot;UTC&quot;, &quot;&quot;))
            val expectedDateTime: OffsetDateTime = actualDateTime.withOffsetSameInstant(zoneOffset)
            return expectedDateTime.toString()
```

DDL:
```
CREATE TABLE IF NOT EXISTS TRANSACTION
(
    id                uuid                     DEFAULT,
    start_date_time   TIMESTAMP WITH TIME ZONE NOT NULL,
    end_date_time     TIMESTAMP WITH TIME ZONE NOT NULL,
    currency          VARCHAR(5)               NOT NULL,
    country           VARCHAR(50)              NOT NULL,
    created_at        TIMESTAMP WITH TIME ZONE NOT NULL,
    created_by        VARCHAR (255)            NOT NULL,
    startDateTime     TIMESTAMP WITH TIME ZONE NOT NULL
)
```
||||||||||||||Your best practice is to think of time zone as a presentation attribute and the timestamp itself as an instant in time.  If your business need requires that you preserve the timezone of the input field, then you must store that in another field outside of the timestamp.

The type "timestamp with timezone" is used by postgresql only to interpret the timezone offset from an input string.  That timezone data is not preserved by the database.  The timestamp is always stored in UTC, and the time zone is immediately lost.

--------------------------------------------------
Chrome Devtools formatter for javascript proxy
I&#39;ve recently started using proxies in one of my projects.  The one downside of this has been that when inspecting the object in a debugger, it&#39;s now wrapped by the proxy [javascript proxy](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy).

[![enter image description here][1]][1]

Intead of seeing `[[Handler]],[[Target]],[[isRevoked]]` I would prefer to just see the object referenced by `[[Target]]`.

It&#39;s a minor inconvenience but I think that it could be solved with a [Chrome Devtools custom formatter](https://www.mattzeunert.com/2016/02/19/custom-chrome-devtools-object-formatters.html).

Seems like this would be fairly common, but I can&#39;t find any existing formatters.  Just wanted to double check that there wasn&#39;t already one out there before I go down the road of writing my own.


  [1]: https://i.stack.imgur.com/alW5J.png
||||||||||||||So it turns out this is quite difficult to achieve.  The first problem is that it's [impossible to identify a Proxy][1] without:

[A:][2] Adding a custom symbol to your proxy implementation (if you control the Proxy init code)

[B:][3] Overriding the `window.Proxy` prototype and using a Weakset to basically track every proxy init 

On top of that, there is no way to access to original `[[Target]]` object.  However, running `JSON.parse(JSON.stringify(obj))` does seems to work well for just `console.log` purposes.

Assuming you don't have control to modify the Proxy handler, this is what your solution would look like:

```
// track all proxies in weakset (allows GC)
const proxy_set = new WeakSet();
window.Proxy = new Proxy(Proxy, {
      construct(target, args) {
        const proxy = new target(args[0], args[1]);
        proxy_set.add(proxy);
        return proxy;
      },
});

window.devtoolsFormatters = [{
  header(obj: any) {
    try {
      if (!proxy_set.has(obj)) {
        return null;
      }
      return ['object', {object: JSON.parse(JSON.stringify(obj))}]; //hack... but seems to work
    } catch (e) {
      return null;
    }
},
  hasBody() {
      return false;
  },
}];
```

  [1]: https://stackoverflow.com/questions/36372611/how-to-test-if-an-object-is-a-proxy
  [2]: https://stackoverflow.com/a/37198132/800619
  [3]: https://stackoverflow.com/a/53463589/800619

--------------------------------------------------
How to select n columns from a matrix minimizing a given function
I must buy **one of each product**, but I can visit **no more then n shops**. Which n shops should I choose to spend the least amount of money? Products are not divisible, every shop have full inventory.

|           | Shop A | Shop B | Shop C |
| --------- | ------ | ------ | ------ |
| Product 1 | $10.00 | $12.00 | $9.99  |
| Product 2 | $8.50  | $9.99  | $7.99  |
| Product 3 | $15.00 | $14.50 | $16.99 |


So I need to  minimize
``df.min(axis=1).sum() ``, where df represents any combination of n columns.

Can I do better than check all the combinations by brute force? Greedy approach, or dynamic programming don&#39;t work here. Sorting columns also doesn&#39;t help, because a shop with half of its products prohibitively expensive and the other half almost free, can have a biggest total sum of its products, but still be the best candidate.
||||||||||||||This code solves the problem. Sadly I am not interested in the output itself, but in complexity. How many steps should I do when trying to simulate underlying algorithm on paper?


    import pulp
    
    # You could formulate this as in integer linear program with binary variables:
    problem = pulp.LpProblem("Product Purchase Problem", pulp.LpMinimize)
    
    # Define the decision variables
    stores = ["Store 1", "Store 2", "Store 3"]
    products = ["Product 1", "Product 2", "Product 3"]
    
    # Xij = 1 if product j is purchased at store i, else = 0;
    X = pulp.LpVariable.dicts("X", [(i, j) for i in stores for j in products], cat="Binary")
    # Si = 1 if store i is available, else = 0.
    S = pulp.LpVariable.dicts("S", stores, cat="Binary")
    
    # Define the objective function
    C = {
        ("Store 1", "Product 1"): 1,
        ("Store 1", "Product 2"): 3,
        ("Store 1", "Product 3"): 4,
        ("Store 2", "Product 1"): 3,
        ("Store 2", "Product 2"): 1,
        ("Store 2", "Product 3"): 3,
        ("Store 3", "Product 1"): 2,
        ("Store 3", "Product 2"): 3,
        ("Store 3", "Product 3"): 1,
    }
    
    # Minimize Σij CijXij, where Cij is the cost of purchasing product j from store i
    problem += pulp.lpSum([C[(i, j)] * X[(i, j)] for i in stores for j in products])
    
    # Subject to constraints
    for j in products:
    # Σi Xij = 1 for all products j (each product is purchased at some store)
        problem += pulp.lpSum([X[(i, j)] for i in stores]) == 1
        for i in stores:
    # Xij <= Si for all i,j (can only purchase at store i if store i is available)
            problem += pulp.lpSum(X[(i, j)]) <= S[i]
    
    # Subject to constraint Σi Si <= n (at most n stores available)
    problem += pulp.lpSum([S[i] for i in stores]) <= 2
    
    # Solve the problem
    problem.solve()
    
    # Print the results
    for v in problem.variables():
        print(v.name, "=", v.varValue)
    print("Total Cost =", pulp.value(problem.objective))

--------------------------------------------------
Django - Search Query Results Loading Incorrectly
On my search results page for some reason when a logged in user makes a search query in my gaming application all the results display instead of the query itself.

What needs to get fixed with what I currently have?

I’m building a gaming application where logged in users have access to play different games based on their rank in our application… so when a user makes a search for lets say the letter ``d`` all the unlocked games and locked games that have the letter ``d`` should display. I&#39;m currently getting that but the search query shows all results that don&#39;t inlcude letter ``d``. 


This is only happening when a user is logged in. When a user is logged out the search works correctly, a search query for ``d`` will show results of every game with the character ``d`` in it. 

Any help is gladly appreciated!

Thanks!

Below is my code. 

**models.py**

    class Game_Info(models.Model):
        id = models.IntegerField(primary_key=True, unique=True, blank=True, editable=False)
        game_title = models.CharField(max_length=100, null=True)
        game_rank = models.IntegerField(default=1)
        game_image = models.ImageField(default=&#39;default.png&#39;, upload_to=&#39;game_covers&#39;, null=True, blank=True)
    
    class User_Info(models.Model):
        id = models.IntegerField(primary_key=True, blank=True)
        image = models.ImageField(default=&#39;/profile_pics/default.png&#39;, upload_to=&#39;profile_pics&#39;, null=True, blank=True)
        user = models.OneToOneField(settings.AUTH_USER_MODEL,blank=True, null=True, on_delete=models.CASCADE)
        rank = models.IntegerField(default=1)    


**views.py**

    def is_valid_queryparam(param):
        return param != &#39;&#39; and param is not None
    
    def search_filter_view(request):
        user_profile_games_filter = Game_Info.objects.all()
        user_profile = User_Info.objects.all()
        title_query = request.POST.get(&#39;q&#39;)  
    
        if is_valid_queryparam(title_query):
            user_profile_games_filter = user_profile_games_filter.filter(game_title__icontains=title_query)
    
        if request.user.is_authenticated:
            user_profile = User_Info.objects.filter(user=request.user)
            user_profile_game_obj = User_Info.objects.get(user=request.user)
            user_profile_rank = int(user_profile_game_obj.rank)
    
    
            user_profile_games_filter = Game_Info.objects.annotate(
                user_unlocked_game=Case(
                    When(game_rank__lte=user_profile_rank, then=Value(True)),
                    default=Value(False),
                    output_field=BooleanField()
                )
            )
    
            context = {
                &#39;user_profile&#39;: user_profile,  
                &#39;user_profile_games_filter&#39;: user_profile_games_filter,
                &#39;title_query&#39; : title_query
            }
    
        else:
            context = {
                &#39;user_profile&#39;: user_profile,
                &#39;user_profile_games_filter&#39;: user_profile_games_filter,
                &#39;title_query&#39; : title_query
           }
    
        return render(request, &quot;search_results.html&quot;, context)


**search.html**

    &lt;h1&gt;Results for &amp;#34;{{ title_query }}&amp;#34;&lt;/h1&gt;
    
    
                {% for content in user_profile_games_filter %}
                    {% if content.user_unlocked_game %}
                            &lt;!-- unlocked games logic --&gt;
                            &lt;a class=&quot;game-tile-container&quot; href=&quot;{% url &#39;detail&#39; content.pk %}&quot;&gt;
                                &lt;li class=&quot;results_info&quot;&gt;
                                    &lt;img src= &quot;{{ content.game_image.url }}&quot;&gt; 
                                    &lt;span class=&quot;results_title&quot;&gt;{{ content.game_title }}&lt;/span&gt;
                                &lt;/li&gt;
                            &lt;/a&gt;
            
                            {% else %}
                            &lt;!-- locked games logic --&gt;
                            &lt;a class=&quot;game-tile-container&quot; href=&quot;{% url &#39;detail&#39; content.pk %}&quot;&gt;
                                &lt;li class=&quot;results_info&quot;&gt;
                                    &lt;div class=&quot;locked_game&quot;&gt;
                                        &lt;img class=&quot;lock-img&quot; src={% static &#39;images/treasure-chest-closed-alt.png&#39; %} /&gt;
                                        &lt;img src= &quot;{{ content.game_image.url }}&quot;&gt; 
                                        &lt;button class=&quot;level-up&quot;&gt;Reach level {{ content.game_rank }} to unlock&lt;/button&gt;
                                    &lt;/div&gt;
                                    &lt;span class=&quot;results_title&quot;&gt;{{ content.game_title }}&lt;/span&gt;
                
                                &lt;/li&gt;
                            &lt;/a&gt;
                    {% endif %}
                {% endfor %}


  

||||||||||||||Issue:

    user_profile_games_filter = Game_Info.objects.annotate(
                user_unlocked_game=Case(
                    When(game_rank__lte=user_profile_rank, then=Value(True)),
                    default=Value(False),
                    output_field=BooleanField()
                )
            )

**SOLUTION** for anyone new; found after a long search and talk in the comments!:

Create a new variable where you store a list of the ID's of the filtered objects, in this case is the variable `user_profile_games_filter`:

    NEW_VARIABLE = user_profile_games_filter.values_list('id', flat=True)

Next, with the filtered objects, assign it to the desired object, making sure that you filter the id using `id__in` and it should be equal to the list of IDs. No loop needed!

    user_profile_games_filter = Game_Info.objects.filter(id__in=NEW_VARIABLE).annotate(...)

--------------------------------------------------
How to extract the Vector from an OpenAI Embeddings Call?
I use nearly the same code as here in this Git Repo to get Embeddings from OpenAI:
https://gist.github.com/limcheekin/997de2ae0757cd46db796f162c3dd58c

    oai = OpenAI(
    # This is the default and can be omitted
    api_key=&quot;sk-.....&quot;,
    )

    def get_embedding(text_to_embed, openai):
       
        response = openai.embeddings.create(
            model= &quot;text-embedding-ada-002&quot;,
            input=[text_to_embed]
        )
        
        return response
    
    embedding_raw = get_embedding(text,oai)

According to the Git Repo the Vector should be in `response[&#39;data&#39;][0][&#39;embedding&#39;]`. But it isn&#39;t in my case.

When I print the response Variable, I got this:

    print(embedding_raw)

Output: 

    CreateEmbeddingResponse(data=[Embedding(embedding=[0.009792150929570198, -0.01779201813042164, 0.011846082285046577, -0.0036859565880149603, -0.0013213189085945487, 0.00037509595858864486,..... -0.0121011883020401, -0.015751168131828308], index=0, object=&#39;embedding&#39;)], model=&#39;text-embedding-ada-002&#39;, object=&#39;list&#39;, usage=Usage(prompt_tokens=360, total_tokens=360))


Sorry, I&#39;m new to python, but how can I access the vector data?
||||||||||||||Simply return just the embedding vector as follows:

    def get_embedding(text_to_embed, openai):
       
        response = openai.embeddings.create(
            model= "text-embedding-ada-002",
            input=[text_to_embed]
        )
        
        return response.data[0].embedding # Change this

    embedding_raw = get_embedding(text,oai)

--------------------------------------------------
React Redux Reducer: &#39;this.props.tasks.map is not a function&#39; error
I am making a React Redux example; however, I ran into an issue and get the error below:

&gt; TypeError: this.props.tasks.map is not a function
[Learn More]

I have tried many things and I cannot seem to understand why this is not working. I believe it is when the allReducers maps the tasks from the Tasks function. I have fixed this error back and forth but then it would complain it was undefined. I would fix that and loop back to this issue. Any help would be appreciated. Im sure I am making a simple mistake. Below are my following files

**App.js**
&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    import React from &#39;react&#39;;
    import TaskBoard from &quot;../containers/task-board&quot;;
    require(&#39;../../scss/style.scss&#39;);

    const App = () =&gt; (
        &lt;div&gt;
            &lt;h2&gt;Task List&lt;/h2&gt;
            &lt;hr /&gt;
            &lt;TaskBoard/&gt;
        &lt;/div&gt;
    );

    export default App;


&lt;!-- end snippet --&gt;

**index.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

        import {combineReducers} from &#39;redux&#39;;
        import {Tasks} from &#39;./reducer-tasks&#39;;
        const allReducers = combineReducers({
            tasks: Tasks
        });

        export default allReducers

&lt;!-- end snippet --&gt;

**task-board.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

        import React, {Component} from &#39;react&#39;;
        import {bindActionCreators} from &#39;redux&#39;;
        import {connect} from &#39;react-redux&#39;;
        import {deleteTaskAction} from &#39;../actions/ActionIndex&#39;;
        import {editTaskAction} from &#39;../actions/ActionIndex&#39;;
        class TaskBoard extends Component {
            renderList() {
                return this.props.tasks.map((task) =&gt; {
                    if(task.status == &quot;pending&quot;){
                        return (&lt;li key={task.id}&gt;
                            {task.id} {task.description}
                            &lt;button type=&quot;button&quot;&gt;Finish&lt;/button&gt;
                            &lt;button type=&quot;button&quot;&gt;Edit&lt;/button&gt;
                            &lt;button onClick={() =&gt; this.props.deleteTask(task)} type=&quot;button&quot;&gt;Delete&lt;/button&gt;
                        &lt;/li&gt;
                    );
                }
            });
        }
        render() {
            if (!this.props.tasks) {
                console.log(this.props.tasks);
                return (&lt;div&gt;You currently have no tasks, please first create one...&lt;/div&gt;);
            }
            return (
                &lt;div&gt;
                    {this.renderList()}
                &lt;/div&gt;
            );
        }
    }
        function mapStateToProps(state) {
            return {
                tasks: state.tasks
            };
        }
        function matchDispatchToProps(dispatch){
            return bindActionCreators(
            {
                deleteTask: deleteTaskAction,
                editTask: editTaskAction
            }, dispatch)
        }
        export default connect(mapStateToProps,matchDispatchToProps)(TaskBoard);

&lt;!-- end snippet --&gt;

**reducer-tasks.js**

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    const initialState = {
    	tasks: [
            {
                id: 1,
                description: &quot;This is a task&quot;,
                status: &quot;pending&quot;
            },
            {
                id: 2,
                description: &quot;This is another task&quot;,
                status: &quot;pending&quot;
            },
            {
                id: 3,
                description: &quot;This is an easy task&quot;,
                status: &quot;pending&quot; 

            }
    	]
    }

    export function Tasks (state = initialState, action) {
        switch (action.type) {
            case &#39;ADD_TASK&#39;:
                return Object.assign({}, state, {
                	tasks: [
                		...state.tasks,
                		{
                			description: action.text,
                			status: action.status
                		}
                	]
                })
                break;

            case &#39;EDIT_TASK&#39;:
                return action.payload;
                break;

            case &#39;DELETE_TASK&#39;:
                return Object.assign({}, state, {
                	status: action.status
                })
                break;
        }

        return state;
    }

&lt;!-- end snippet --&gt;

**actionindex.js**
&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;


        export const addTaskAction = (task) =&gt; {
            return {
                type: &#39;ADD_TASK&#39;,
                text: &quot;Here is a sample description&quot;,
                status: &quot;pending&quot;
            }
        };
        export const deleteTaskAction = (task) =&gt; {
            return {
                type: &#39;DELETE_TASK&#39;,
                status: &quot;deleted&quot;
            }
        };
        export const editTaskAction = (task) =&gt; {
            return {
                type: &#39;EDIT_TASK&#39;,
                payload: task
            }
        };

&lt;!-- end snippet --&gt;


||||||||||||||It's because the function 'map' can only be used for arrays, not for objects.

If you print out this.props.tasks in the render function of task-board.js you'll see that it's an OBJECT which contains the tasks array, not the actual tasks array itself.

So to fix this it's quite easy, instead of:

        return this.props.tasks.map((task) => {

it's 

        return this.props.tasks.tasks.map((task) => {

Then it works

--------------------------------------------------
Google Chrome Extension - waiting until page loads
In my Google Chrome Extension, I have a [Content Script][1] (content.js) and a [Background Page][2] (background.html). I have context.js checking for a keyword that appears on the page. However, I want to wait until the page is fully loaded until I search the page, because the keyword may occur at the bottom of the page. 

See [Page action by content][3] sandwich example ([files][4]), this is basically what I am doing. If you load the extension you&#39;ll see the extension only works when the word &quot;sandwich&quot; appears at the top of the page.


  [1]: http://code.google.com/chrome/extensions/content_scripts.html
  [2]: http://code.google.com/chrome/extensions/background_pages.html
  [3]: http://code.google.com/chrome/extensions/samples.html
  [4]: http://src.chromium.org/viewvc/chrome/trunk/src/chrome/common/extensions/docs/examples/api/pageAction/pageaction_by_content/
||||||||||||||Try to add this to the "content_scripts" part of your manifest.json file.
```json
"run_at": "document_end"
```
https://developer.chrome.com/docs/extensions/mv3/content_scripts/

--------------------------------------------------
No matching constructor for initialization of &#39;v8::ScriptOrigin&#39; || candidate constructor (the implicit move constructor) not viable
I am using invoking a  class inside my project src c++ file as
```
ScriptOrigin script_origin(
        isolate_,
        script_name,
        Integer::New(isolate_, 0),                            // line offset
        Integer::New(isolate_, 0),                            // column offset
        False(isolate_),                                      // isCrossOrigin
        Local&lt;Integer&gt;(),                                     // scriptId
        Local&lt;Value&gt;(),                                       // sourceMapURL
        False(isolate_),                                      // isOpaque
        False(isolate_),                                      // isWASM
        su.IsNoModule() ? False(isolate_) : True(isolate_));
  ```
Tried hardcoding the Local&lt;Integer&gt;() as a random integer value also . Still getting the below error

    error: no matching constructor for initialization of &#39;v8::ScriptOrigin&#39;
          ScriptOrigin script_origin(
    note: candidate constructor (the implicit copy constructor) not viable: requires 1 argument, but 10 were provided 
    note: candidate constructor (the implicit move constructor) not viable: requires 1 argument, but 10 were provided


Am i invoking the class wrongly? i do not understand the mismatch arguments note


This is the source code of the class inside a v8 include file

```
 class V8_EXPORT ScriptOrigin {
 public:
  V8_INLINE ScriptOrigin(Isolate* isolate, Local&lt;Value&gt; resource_name,
                         int resource_line_offset = 0,
                         int resource_column_offset = 0,
                         bool resource_is_shared_cross_origin = false,
                         int script_id = -1,
                         Local&lt;Value&gt; source_map_url = Local&lt;Value&gt;(),
                         bool resource_is_opaque = false, bool is_wasm = false,
                         bool is_module = false,
                         Local&lt;Data&gt; host_defined_options = Local&lt;Data&gt;())
      : v8_isolate_(isolate),
        resource_name_(resource_name),
        resource_line_offset_(resource_line_offset),
        resource_column_offset_(resource_column_offset),
        options_(resource_is_shared_cross_origin, resource_is_opaque, is_wasm,
                 is_module),
        script_id_(script_id),
        source_map_url_(source_map_url),
        host_defined_options_(host_defined_options) {
    VerifyHostDefinedOptions();
  }
};
```


||||||||||||||Many of the parameters you're passing have the wrong type. For example, `Integer::New(isolate_, 0)` produces a `v8::Local<v8::Integer>`, not a C++ `int`. Similarly, `False(isolate_)` produces a `v8::Local<v8::Boolean>`, not a C++ `bool`.

This isn't really related to V8: whenever working with C++, you need to care about the types of your values.

The part where the compiler says ...
```none
note: candidate constructor (the implicit copy constructor) not 
viable: requires 1 argument, but 10 were provided 

note: candidate constructor (the implicit move constructor) not 
viable: requires 1 argument, but 10 were provided
```
... is just about that it _tried_ to match the 10 arguments you supplied with the copy constructor and move constructor (both of which expect 1 argument only), but that failed.

--------------------------------------------------
SSIS Excel Destination Editor closes unexpectedly
I&#39;m fairly new to SSIS, but what I&#39;m trying to do should be simple:

I have a Data Flow task that has an OLE DB Source feeding into an Excel Destination. The issue though, is I can&#39;t configure the Excel Destination correctly. I&#39;m able to connect my Excel connection manager, but when I hit the &quot;New...&quot; button next to the &quot;Name of the Excel sheet&quot; dropdown, the Excel Destination Editor window just closes instead of opening a different dialog. In the image below, I highlighted the button that&#39;s closing the window.

![SSIS Button](https://i.stack.imgur.com/BR2TV.png)

In general, I&#39;m following this guide [How to use SSIS to Export to Excel][1] (current step is just above the second to last image in the article).


  [1]: http://knowlton-group.com/using-ssis-to-export-data-to-excel/
||||||||||||||I think the issue lies with the Excel connection instead. Once I changed the output file to .xls instead of .xlsx and changed the connection to Excel 97-2003, I was able to create a new Excel Sheet for the file.

--------------------------------------------------
Is it possible to display toolbar options below textarea in Quilljs editor?
How to display toolbar below `textarea`.

My code: 
    

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    var quill = new Quill(&#39;#txtMessage&#39;, {
      theme: &#39;snow&#39;,
      modules: {
        toolbar: {
          container: [
            [&#39;bold&#39;, &#39;italic&#39;, &#39;underline&#39;],
            [{
              &#39;list&#39;: &#39;ordered&#39;
            }, {
              &#39;list&#39;: &#39;bullet&#39;
            }],
            [&#39;clean&#39;],
            [&#39;code-block&#39;],
            [{
              &#39;variables&#39;: [&#39;{Name}&#39;, &#39;{Email}&#39;]
            }],
          ],
          handlers: {
            &quot;variables&quot;: function(value) {
              if (value) {
                const cursorPosition = this.quill.getSelection().index;
                this.quill.insertText(cursorPosition, value);
                this.quill.setSelection(cursorPosition + value.length);
              }
            }
          }
        }
      }
    });

    // Variables
    const placeholderPickerItems = Array.prototype.slice.call(document.querySelectorAll(&#39;.ql-variables .ql-picker-item&#39;));
    placeholderPickerItems.forEach(item =&gt; item.textContent = item.dataset.value);
    document.querySelector(&#39;.ql-variables .ql-picker-label&#39;).innerHTML = &#39;Variables&#39; + document.querySelector(&#39;.ql-variables .ql-picker-label&#39;).innerHTML;

&lt;!-- language: lang-html --&gt;

    &lt;script src=&quot;//cdn.quilljs.com/1.3.6/quill.js&quot;&gt;&lt;/script&gt;
    &lt;link href=&quot;//cdn.quilljs.com/1.3.6/quill.snow.css&quot; rel=&quot;stylesheet&quot;/&gt;
    &lt;link href=&quot;//cdn.quilljs.com/1.3.6/quill.bubble.css&quot; rel=&quot;stylesheet&quot;/&gt;

    &lt;div id=&quot;txtMessage&quot;&gt;&lt;/div&gt;

&lt;!-- end snippet --&gt;

Output for the above code:
[![enter image description here][1]][1]

I want output as follows:
[![enter image description here][2]][2]
  [1]: https://i.stack.imgur.com/PQtJv.png
  [2]: https://i.stack.imgur.com/bQlmC.png

How to accomplish above result.
||||||||||||||I can't see why not use only css.

Something like this:

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    var quill = new Quill('#editor-container', {
      modules: {
        toolbar: [
          [{
            header: [1, 2, false]
          }],
          ['bold', 'italic', 'underline'],
          ['image', 'code-block']
        ]
      },
      placeholder: 'Compose an epic...',
      theme: 'snow' // or 'bubble'
    });

<!-- language: lang-css -->

    #editor-container {
      height: 375px;
    }

    .editor-wrapper {
      position: relative;
    }

    .ql-toolbar {
      position: absolute;
      bottom: 0;
      width: 100%;
      transform: translateY(100%);
    }

<!-- language: lang-html -->

    <script src="//cdn.quilljs.com/1.3.6/quill.js"></script>
    <link href="//cdn.quilljs.com/1.3.6/quill.snow.css" rel="stylesheet"/>
    <link href="//cdn.quilljs.com/1.3.6/quill.bubble.css" rel="stylesheet"/>
    <div class="editor-wrapper">
      <div id="editor-container">
      </div>
    </div>

<!-- end snippet -->

https://codepen.io/moshfeu/pen/wXwqmg

--------------------------------------------------
Can I install Visual Studio without Admin rights?
I use a machine where I don&#39;t have administrator rights. I&#39;ve been able to run programs without admin rights by extracting the program&#39;s .zip file to a directory I have created on my desktop. However, I can&#39;t find such a .zip file for Visual Studio.

Is there a way to install Visual Studio Community Edition without administrator rights?


||||||||||||||Practically no. Visual Studio (Express and above, excluding VS Code) consists of multiple components that must be installed as admin, and will be required for the app you're debugging to be available as system-wide component. It *might* be possible to use [ThinApp](https://www.vmware.com/products/thinapp.html) or its equivalent, but ThinApp can't even work with [VS 2010](https://www.vmware.com/support/thinapp4/doc/releasenotes_thinapp52.html) and it was by far the best of its class.

A (resource intensive) alternative to get VS on any PC will be packaging a VM with VS installed, either creating one yourself or get a [ready-made](https://developer.microsoft.com/en-us/windows/downloads/virtual-machines) ones. VirtuaBox is available as [portable fork](http://www.vbox.me/) if you can't even get Hyper-V tools installed. But this still require kernel drivers installation, which means at least one-time admin access. Depending on your internet connection & budget, it might be more practical to setup a VPS with VS installed, then remote there.

--------------------------------------------------
ggplot2: how to produce smaller points
I have a large dataset that I am plotting using a scatter plot. These points have a unique combination of x,y and therefore they don&#39;t overlap, but some of them are very close to each other therefore I&#39;m plotitng them with small size.  

1- How to produce smaller point symbols (smaller `size`) so that the areas are proportional. In this example, the last point does not have an area proportional to the `size`. I was expecting it 10 smaller than the middle one e.g.:

    df &lt;- data.frame(c1 = 1:3, c2 = c(1,1,1))
    ggplot(df) + geom_point(aes(x= c1, y = c2), size = c(1, 0.1, 0.01)) 

2- How does the `size` in ggplot2 matches the R graphics `cex` argument e.g.:  `plot(df$c2 ~ df$c1, cex = c(1, 0.1, 0.01))`. 
Thanks
||||||||||||||There is a `size = ` argument to `geom_point`, but you either specify a size for all points:

    + geom_point(size = 0.5)

Or you map the size to one of the columns in your data using `aes`:

    + geom_point(aes(size = c2))

In the latter case, you can control the range of sizes using `scale_size_continuous`. The default is min = 1, max = 6. To get _e.g._ min = 2, max = 8:

    + geom_point(aes(size = c2)) + scale_size_continuous(range = c(2, 8))

- Note that the "ggplot2 way" is to map data to geoms, not to assign values to each observation
- and no, size here is different to `cex`

--------------------------------------------------
Can&#39;t close Excel completely using win32com on Python
This is my code, and I found many answers for [VBA][1], .NET framework and is pretty strange. When I execute this, Excel closes.

    from win32com.client import DispatchEx
    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wbs.Close()
    excel.Quit()
    wbs = None
    excel = None # &lt;-- Excel Closes here

But when I do the following, it does not close.

    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wb = wbs.Open(&#39;D:\\Xaguar\\A1.xlsm&#39;)
    wb.Close(False)
    wbs.Close()
    excel.Quit()
    wb = None
    wbs = None
    excel = None  # &lt;-- NOT Closing !!!

I found some possible answer in Stack Overflow question *[Excel process remains open after interop; traditional method not working][2]*. The problem is that is not Python, and I don&#39;t find `Marshal.ReleaseComObject` and `GC`. I looked over all the demos on `...site-packages/win32com` and others.

Even it does not bother me if I can just get the PID and kill it.

I found a workaround in *[Kill process based on window name (win32)][3]*.

May be not the proper way, but a workround is:

    def close_excel_by_force(excel):
        import win32process
        import win32gui
        import win32api
        import win32con

        # Get the window&#39;s process id&#39;s
        hwnd = excel.Hwnd
        t, p = win32process.GetWindowThreadProcessId(hwnd)
        # Ask window nicely to close
        win32gui.PostMessage(hwnd, win32con.WM_CLOSE, 0, 0)
        # Allow some time for app to close
        time.sleep(10)
        # If the application didn&#39;t close, force close
        try:
            handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, p)
            if handle:
                win32api.TerminateProcess(handle, 0)
                win32api.CloseHandle(handle)
        except:
            pass

    excel = DispatchEx(&#39;Excel.Application&#39;)
    wbs = excel.Workbooks
    wb = wbs.Open(&#39;D:\\Xaguar\\A1.xlsm&#39;)
    wb.Close(False)
    wbs.Close()
    excel.Quit()
    wb = None
    wbs = None
    close_excel_by_force(excel) # &lt;--- YOU #@#$# DIEEEEE!! DIEEEE!!!

  [1]: http://en.wikipedia.org/wiki/Visual_Basic_for_Applications
  [2]: https://stackoverflow.com/questions/8977571/excel-process-remains-open-after-interop-traditional-method-not-working
  [3]: http://python.6.n6.nabble.com/Kill-process-based-on-window-name-win32-td1042063.html

||||||||||||||Try this:

    wbs.Close()
    excel.Quit()
    del excel # this line removed it from task manager in my case


--------------------------------------------------
Log4j2 createOnDemand=&quot;true&quot; does not allow creation of new file on a daily basis
`Log4j2 createOnDemand=&quot;true&quot;` does not allow creation of new file on a daily basis in-spite of using `RollingFile Appenders` with `TimeBasedTriggeringPolicy`.

Below is my `log4j2.xml` file.

I have two `appenders`, one is for all logs, another is for a custom purpose, which needs to be generated only on demand, but the `createOnDemand` is overriding the Rolling nature of the log and it is not allowing to create new log file for the custom log.

    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;Configuration status=&quot;WARN&quot;&gt;
    	&lt;Appenders&gt;
    		&lt;RollingFile name=&quot;App&quot; 
    				fileName=&quot;app.log&quot; 
    				filePattern=&quot;app.%d{yyyy-MM-dd}.log&quot;&gt;
    			&lt;PatternLayout pattern=&quot;[%t] %d{yyyy-MM-dd HH:mm:ss,SSS zzz} %-5p %l - %m%n&quot; /&gt;
    			&lt;Policies&gt;
    				&lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;
    			&lt;/Policies&gt;
    		&lt;/RollingFile&gt;
    		&lt;RollingFile name=&quot;custom&quot;
    				 fileName=&quot;appCustom.log&quot;
    				 filePattern=&quot;appCustom.%d{yyyy-MM-dd-HH-mm}.log&quot;
    				 createOnDemand=&quot;true&quot;&gt;
    		&lt;PatternLayout pattern=&quot;[%t] %d{yyyy-MM-dd HH:mm:ss,SSS zzz} %-5p %l - %m%n&quot; /&gt;
    		&lt;Policies&gt;
    			&lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt;
    		&lt;/Policies&gt;
    		&lt;/RollingFile&gt;
    	&lt;/Appenders&gt;
    	&lt;Loggers&gt;
    		&lt;Logger name=&quot;AppLogger&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
    				&lt;AppenderRef ref=&quot;App&quot;/&gt;
    			&lt;/Logger&gt;
    		&lt;Logger name=&quot;customLogger&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt;
    			&lt;AppenderRef ref=&quot;custom&quot;/&gt;
    		&lt;/Logger&gt;
    		&lt;Root level=&quot;info&quot;&gt;
    				&lt;AppenderRef ref=&quot;file&quot; /&gt;
    		&lt;/Root&gt;
    	&lt;/Loggers&gt;
    &lt;/Configuration&gt;
||||||||||||||I have found the fix for the above issue.
This was an existing bug in lo4j2 which is fixed in the version - [2.13.1][1]

Below are the links :

https://issues.apache.org/jira/browse/LOG4J2-2759

https://logging.apache.org/log4j/2.x/changes-report.html#a2.13.3

I was using 2.11.0

Upgrading resolved my issue.


  [1]: https://blogs.apache.org/logging/entry/log4j-2-13-1-released

--------------------------------------------------
How to get the Slack DM channel ID of the Slack App
I have created a simple Slack App app where the only purpose is to send a message to a channel. I understand that there is the `conversations.list` API to list all public channels to get the correct ID. 

However, as a first step, I just want to send the message to the app channel itself. If I use the D... ID it works as expected. No invite by the channel is needed. But how do I get this ID? `conversations.list` only returns publich channels, but no the app channel itself.
||||||||||||||In Slack, there is no such thing as an app's channel. There is a DM channel between every user and your app/bot. In these terms, to send a DM message from your app/bot to the user, you need to know `ID` of this user and specify it as a `channel` argument of the `postMessage` API request.

--------------------------------------------------
How do I make a custom class that&#39;s serializable with dataclasses.asdict()?
I&#39;m trying to use a dataclass as a (more strongly typed) dictionary in my application, and found this strange behavior when using a custom type subclassing `list` within the dataclass. I&#39;m using Python 3.11.3 on Windows.

```python
from dataclasses import dataclass, asdict

class CustomFloatList(list):
    def __init__(self, args):
        for i, arg in enumerate(args):
            assert isinstance(arg, float), f&quot;Expected index {i} to be a float, but it&#39;s a {type(arg).__name__}&quot;

        super().__init__(args)

    @classmethod
    def from_list(cls, l: list[float]):
        return cls(l)

@dataclass
class Poc:
    x: CustomFloatList

p = Poc(x=CustomFloatList.from_list([3.0]))
print(p)  # Prints Poc(x=[3.0])
print(p.x)  # Prints [3.0]
print(asdict(p))  # Prints {&#39;x&#39;: []}
```

This does not occur if I use a regular list[float], but I&#39;m using a custom class here to enforce some runtime constraints.

How do I do this correctly?

I&#39;m open to just using `.__dict__` directly, but I thought `asdict()` was the more &quot;official&quot; way to handle this

A simple modification makes the code behave as expected, but is slightly less efficient:

```python
from dataclasses import dataclass, asdict

class CustomFloatList(list):
    def __init__(self, args):
        dup_args = list(args)
        for i, arg in enumerate(dup_args):
            assert isinstance(arg, float), f&quot;Expected index {i} to be a float, but it&#39;s a {type(arg).__name__}&quot;

        super().__init__(dup_args)

    @classmethod
    def from_list(cls, l: list[float]):
        return cls(l)

@dataclass
class Poc:
    x: CustomFloatList

p = Poc(x=CustomFloatList.from_list([3.0]))
print(p)
print(p.x)
print(asdict(p))
```
||||||||||||||If you look at the [source code of `asdict`](https://github.com/python/cpython/blob/d334122d2295a4863384676a3ce313a831b12335/Lib/dataclasses.py#L1364), you'll see that passes a generator expression that recursively calls itself on the elements of a list when it encounters a list:
```
    elif isinstance(obj, (list, tuple)):
        # Assume we can create an object of this type by passing in a
        # generator (which is not true for namedtuples, handled
        # above).
        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)
```



 But *your implementation depletes any iterator it gets in `__init__` before the `super` call*. 

Don't do that. You'll have to "cache" the values if you want to use the superclass constructor. Something like:

```
class CustomFloatList(list):
    def __init__(self, args):
        args = list(args)
        for i, arg in enumerate(args):
            assert isinstance(arg, float), f"Expected index {i} to be a float, but it's a {type(arg).__name__}"

        super().__init__(args)
```

Or perhaps:

```
class CustomFloatList(list):
    def __init__(self, args):
        super().__init__(args)
        for i, arg in enumerate(self):
            if not isinstance(arg, float):
                raise TypeError(f"Expected index {i} to be a float, but it's a {type(arg).__name__}")
```

--------------------------------------------------
How would I run PHP code when input box changes?
I&#39;m trying to do something with PHP when a text box changes. I can do this with JavaScript with onchange but that doesn&#39;t work with PHP.


I already have a PHP function in my already existing PHP code. When a text box value changes, I want it to run the function.

||||||||||||||The reason you can do it in JavaScript is because you're in the same scope...the client side. PHP is a server-side language, so it has no notion of what is happening on the client-side, unless you explicitly tell it.

To tell PHP to evaluate, and possible return the response of a function call, you have to pass it the value of the input using a network call, such as a fetch or ajax request.

Your question shows that you really don't understand PHP, and you need to learn the fundamentals of it. PHP does not run client-side, and JavaScript does not run server-side (again, unless you're using Node, in which case you wouldn't be using PHP).

--------------------------------------------------
Listener method using Spring and ActiveMQ throws &quot;Property name cannot be null&quot; warnings repeatedly
I&#39;ve attempted to implement ActiveMQ using Spring in two places. Both implementations have had this issue. Sending either an HTTP Request using postman or directly entering a message in the ActiveMQ console causes the following Error to be repeated infinitely:

```
2024-02-02T17:08:56.317-06:00 ERROR 2264 --- [ntContainer#0-1] c.j.a.config.JmsConfig$JMSErrorHandler   : Error in listener
```

```
org.springframework.jms.listener.adapter.ListenerExecutionFailedException: Listener method &#39;public void com.jackhodge.activemqlearning.consumer.component.MessageConsumer.messageListener(com.jackhodge.activemqlearning.model.SystemMessage)&#39; threw exception
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:118) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.onMessage(MessagingMessageListenerAdapter.java:84) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:783) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:741) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:719) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:333) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:270) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1258) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1248) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:1141) ~[spring-jms-6.1.3.jar:6.1.3]
  at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]
Caused by: java.lang.NullPointerException: Property name cannot be null
  at org.apache.activemq.command.ActiveMQMessage.getObjectProperty(ActiveMQMessage.java:575) ~[activemq-client-jakarta-5.18.3.jar:5.18.3]
  at org.apache.activemq.command.ActiveMQMessage.getStringProperty(ActiveMQMessage.java:683) ~[activemq-client-jakarta-5.18.3.jar:5.18.3]
  at org.springframework.jms.support.converter.MappingJackson2MessageConverter.getJavaTypeForMessage(MappingJackson2MessageConverter.java:456) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.support.converter.MappingJackson2MessageConverter.fromMessage(MappingJackson2MessageConverter.java:241) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener.extractMessage(AbstractAdaptableMessageListener.java:250) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter.extractPayload(AbstractAdaptableMessageListener.java:472) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter$LazyResolutionMessage.unwrapPayload(AbstractAdaptableMessageListener.java:539) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.AbstractAdaptableMessageListener$MessagingMessageConverterAdapter$LazyResolutionMessage.getPayload(AbstractAdaptableMessageListener.java:521) ~[spring-jms-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.annotation.support.PayloadMethodArgumentResolver.resolveArgument(PayloadMethodArgumentResolver.java:122) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:118) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:147) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:115) ~[spring-messaging-6.1.3.jar:6.1.3]
  at org.springframework.jms.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:110) ~[spring-jms-6.1.3.jar:6.1.3]
  ... 10 common frames omitted
```

What stands out is `Error in listener [...] Property name cannot be null`.

The error still occurs when I don&#39;t do anything in my listener, and Logs/Breakpoints in the `messageListener` aren&#39;t sent nor activated.

Here&#39;s the simple app in which the error is occurring:

**JmsConfig**

```java
@Configuration
@EnableJms
public class JmsConfig {

    Logger logger = LoggerFactory.getLogger(JMSErrorHandler.class);
    
    @Bean
    public DefaultJmsListenerContainerFactory jmsListenerContainerFactory(
            ConnectionFactory connectionFactory,
            DefaultJmsListenerContainerFactoryConfigurer configurer,
            JMSErrorHandler defaultErrorHandler){
        DefaultJmsListenerContainerFactory jmsListenerContainerFactory = new DefaultJmsListenerContainerFactory();
    
        jmsListenerContainerFactory.setConnectionFactory(connectionFactory);
        jmsListenerContainerFactory.setConcurrency(&quot;1&quot;); // start w/ 5 consumers; auto-scale to 10 consumers as necessary
    
        jmsListenerContainerFactory.setErrorHandler(defaultErrorHandler);
        jmsListenerContainerFactory.setMessageConverter(this.jacksonJmsMessageConverter());
    
        configurer.configure(jmsListenerContainerFactory, connectionFactory);
        return jmsListenerContainerFactory;
    
    }
    
    
    @Service
    public class JMSErrorHandler implements ErrorHandler {
        @Override
        public void handleError(Throwable t) {
            logger.error(&quot;Error in listener &quot;, t);
        }
    }
    
    @Bean
    public MessageConverter jacksonJmsMessageConverter() {
        MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();
        converter.setTargetType(MessageType.TEXT);
        converter.setObjectMapper(new ObjectMapper());
        return converter;
    }

**PublishController**

```java
package com.jackhodge.activemqlearning.controller;

import com.jackhodge.activemqlearning.model.SystemMessage;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.jms.core.JmsTemplate;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RestController;

@RestControllerpublic class PublishController {// helperclass for sending/receiving messages
    // Spring JMS abstraction API: Distills and simplifies process; abstracts away boilerplate code
    private JmsTemplate jmsTemplate;

    @Autowired
    public PublishController(JmsTemplate jmsTemplate) {
        this.jmsTemplate = jmsTemplate;
    }

    // post method to trigger publishing of messages
    // Requests to here at sent to the Messaging Broker
    @PostMapping(&quot;/publishMessage&quot;)
    public ResponseEntity&lt;String&gt; publishMessage(@RequestBody SystemMessage systemMessage){
        try{
            jmsTemplate.convertAndSend(&quot;jackhodge-queue&quot;, systemMessage.toString());
            return new ResponseEntity&lt;&gt;(&quot;I, Jack Hodge, sent your message.&quot;, HttpStatus.OK);
        } catch (Exception e){
            return new ResponseEntity&lt;&gt;(e.getMessage(), HttpStatus.INTERNAL_SERVER_ERROR);
        }

    }
}
```

**SystemMessage**

```java
package com.jackhodge.activemqlearning.model;

import lombok.Setter;
import org.springframework.beans.factory.annotation.Autowired;
//import java.io.Serializable;

@Setter
public class SystemMessage {
    private String source;
    private String message;
    
    public SystemMessage(String source, String message) {
    
        this.source = source;
        this.message = message;
    }
    
    
    @Override
    public String toString() {
        return &quot;SystemMessage{&quot; +
                &quot;source=&#39;&quot; + source + &#39;\&#39;&#39; +
                &quot;, message=&#39;&quot; + message + &#39;\&#39;&#39; +
                &#39;}&#39;;
    }

}
```

**MessageConsumer**

```java
package com.jackhodge.activemqlearning.consumer.component;

import com.jackhodge.activemqlearning.model.SystemMessage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.jms.annotation.JmsListener;
import org.springframework.stereotype.Component;

@Componentpublic class MessageConsumer {

    public static final Logger LOGGER = LoggerFactory.getLogger(MessageConsumer.class);
    
    // Consumes from the Messaging broker
    @JmsListener(destination = &quot;jackhodge-queue&quot;)
    public void messageListener(SystemMessage systemMessage){
        LOGGER.info(&quot;Message Received {}&quot;, systemMessage);
    }

}
```

I&#39;m somewhat new to Swing and ActiveMQ and this problem has stumped me -- every avenue of breakpoints/logging/message sources as far as I&#39;m capable of has been tried. Thank you!

Sending this request via both Postman and the ActiveMQ console both resulted in the same Error regardless:

```
{     
    &quot;source&quot;:&quot;jeff bezo&quot;,     
    &quot;message&quot;:&quot;hello&quot; 
}
```
||||||||||||||I fixed this by setting setTypeIdPropertyName in my MessageConverter Bean:


    @Bean
    public MessageConverter jacksonJmsMessageConverter(){
            MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();
            converter.setTargetType(MessageType.TEXT);
            converter.setTypeIdPropertyName("_type");
            converter.setObjectMapper(new ObjectMapper());
            return converter;
        }




--------------------------------------------------
Start and kill background process within one Makefile recipe
Within one `make` recipe, I am trying to:
1. Run a server process in the background
2. Run a command that uses the server, in the foreground
3. Kill the background server process after foreground task completes

The below is where I am at (inspired by https://stackoverflow.com/a/30171236), but it doesn&#39;t quite work because `SERVER_PID` is currently empty (possibly related to this empty PID: https://stackoverflow.com/q/5768034).

```make
run-server:	## Run the server in the foreground.
	run server

run-kill-server:	## Run the server while using it.
	@$(MAKE) run-server&amp;
	export SERVER_PID=$! &amp;&amp; cmd-that-uses-server &amp;&amp; kill $(SERVER_PID)
```

https://stackoverflow.com/questions/7668311/makefile-run-processes-in-background is also related, except I believe it &quot;leaks&quot; the background processes, the recipes there don&#39;t try to `kill` spawned background processes
||||||||||||||> Within one make recipe, I am trying to:
> 
>  1.  Run a server process in the background
>  2.  Run a command that uses the server, in the foreground
>  3.  Kill the background server process after foreground task completes

But you're *not* doing all that in one recipe.  You're using two.

Bringing it all into one recipe would be a step in the right direction, though It will not in itself provide a complete solution.

> `SERVER_PID` is currently empty

Yes, because your recipe is setting *shell* variable `SERVER_PID`, but trying to reference a *`make`* variable of that name.  And also trying to reference a `make` variable named `!`, where you appear to want the shell variable of that name.  You need to escape your `$` by doubling it when you want to pass it through to the shell.

Additionally, each logical line of your recipe runs in a separate shell.  In the one where you define and later user `SERVER_PID`, no background process is ever run.

You probably want something more like this:
```
run-job:
        run server & export SERVER_PID=$$!; cmd-that-uses-server; kill $${SERVER_PID}
```

--------------------------------------------------
python while loop if all conditions are equal then do another random choice from list
This is my python code:
```python
import secrets
from time import sleep

ids = [{&#39;id&#39;: number} for number in range(1, 5+1)]

rand1 = secrets.choice(ids)
rand2 = secrets.choice(ids)
rand3 = secrets.choice(ids)

n = 0
while rand1[&#39;id&#39;] == rand2[&#39;id&#39;] == rand3[&#39;id&#39;]:
        n += 1
        print(&#39;Before&#39;)
        print(rand1[&#39;id&#39;], rand2[&#39;id&#39;], rand3[&#39;id&#39;])
        sleep(1)
        rand1 = secrets.choice(ids)
        rand2 = secrets.choice(ids)
        rand3 = secrets.choice(ids)
        print(&#39;After&#39;)
        print(rand1[&#39;id&#39;], rand2[&#39;id&#39;], rand3[&#39;id&#39;])
```
I&#39;m going to reach this:

&gt; do the while loop and choose a random id until none of the
&gt; rand1[&#39;id&#39;], rand2[&#39;id&#39;] and rand3[&#39;id&#39;] are equal.
&gt; 
&gt; Even two of them are equal, then do another for loop.
||||||||||||||Looping is not the right way to do this.  Just shuffle and deal:
```
import random

nums = list(range(1,5+1))
random.shuffle(nums)
ids = [{'id': n} for n in nums[:3]]
```

--------------------------------------------------
Avoiding duplicate tasks in celery broker
I want to create the following flow using celery configuration\api: 

 - Send TaskA(argB) Only if celery queue has no TaskA(argB) already pending

Is it possible? how?
||||||||||||||I cannot think of a way but to 

 1. Retrieve all executing and scheduled tasks via [`celery inspect`](http://docs.celeryproject.org/en/latest/userguide/workers.html#inspecting-workers)
    
 2. Iterate through them to see if your task is there.

check [this](https://stackoverflow.com/questions/5544629/retrieve-list-of-tasks-in-a-queue-in-celery) SO question to see how the first point is done.

good luck

--------------------------------------------------
How do I late-resolve * in .csproj files?
I have a `.csproj` file that looks like this:

````
&lt;Project Sdk=&quot;Microsoft.NET.Sdk&quot;&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;net8.0&lt;/TargetFramework&gt;
    &lt;LangVersion&gt;12.0&lt;/LangVersion&gt;
    &lt;DebugType&gt;portable&lt;/DebugType&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
	&lt;None Remove=&quot;*.dat&quot; /&gt;
    &lt;None Include=&quot;*.dat&quot; CopyToOutputDirectory=&quot;Always&quot; /&gt;
  &lt;/ItemGroup&gt;

  &lt;Target Name=&quot;PrecompileScript&quot; BeforeTargets=&quot;BeforeCompile&quot;&gt;
    &lt;Exec Command=&quot;dotnet run -c $(Configuration) --no-build --project ../DatGenerator/DatGenerator.csproj -directory $(ProjectDir)&quot; /&gt;
  &lt;/Target&gt;
&lt;/Project&gt;
````

The problem is `*.dat` is expanded too soon and doesn&#39;t actually pick up any files. How do I expand `*.dat` after the `&lt;Exec` directive that emits the `*.dat` runs? No, `-directory $(ProjectDir)/bin/$(Configuration)/$(TargetFramework)` isn&#39;t right. That doesn&#39;t work at all; see how there isn&#39;t a `&lt;OutputType&gt;exe&lt;/OutputType&gt;`. The copy build outputs built-in functionality needs to work.

Listing each `.dat` file manually is pretty bad. I need to pick up changes automatically here.
||||||||||||||To late-resolve a wildcard, move the item `Include` inside a target.

e.g. Change your code to

```C#
<Project Sdk="Microsoft.NET.Sdk">
  <PropertyGroup>
    <TargetFramework>net8.0</TargetFramework>
    <LangVersion>12.0</LangVersion>
    <DebugType>portable</DebugType>
  </PropertyGroup>

  <Target Name="PrecompileScript" BeforeTargets="BeforeBuild">
    <Exec Command="dotnet run -c $(Configuration) --no-build --project ../DatGenerator/DatGenerator.csproj -directory $(ProjectDir)" />
    <ItemGroup>
      <None Remove="*.dat" />
      <None Include="*.dat" CopyToOutputDirectory="PreserveNewest" />
    </ItemGroup>
  </Target>
</Project>
```

"[How MSBuild builds projects][1]" explains the evaluation and execution phases but, briefly:

 - When building a project, MSBuild has an evaluation phase followed by an executuion phase.
 - 'Top level' `PropertyGroup` and `ItemGroup` elements are evaluated in the evalution phase.
 - Target order is determined in the evaluation phase.
 - Targets are run in the execution phase.
 - `PropertyGroup` and `ItemGroup` elements within a target are evaluated when the target is run.

**Note** I updated the answer to change from using `BeforeCompile` to using `BeforeBuild`.

  [1]: https://learn.microsoft.com/en-us/visualstudio/msbuild/build-process-overview?view=vs-2022

--------------------------------------------------
react-google-maps/api DirectionsService keeps rerendering itself
I have written this code in react JS to using &quot;react-google-maps/api&quot; to calculate route between two points. Now my google map keeps rerendering itself until it gives &quot;DIRECTIONS_ROUTE: OVER_QUERY_LIMIT&quot; error. I don&#39;t know what&#39;s the issue. Help would be appreciated because I am a beginner in react and google-API and also I haven&#39;t found a lot of guides of google API in react.

Here is my code:

    import React from &quot;react&quot;;
    import {
      GoogleMap,
      useLoadScript,
      DirectionsService,
      DirectionsRenderer,
    } from &quot;@react-google-maps/api&quot;;
    
    const libraries = [&quot;places&quot;, &quot;directions&quot;];
    const mapContainerStyle = {
      width: &quot;100%&quot;,
      height: &quot;50vh&quot;,
    };
    const center = {
      lat: 31.582045,
      lng: 74.329376,
    };
    const options = {};
    
    const MainMaps = () =&gt; {
      const { isLoaded, loadError } = useLoadScript({
        googleMapsApiKey: &quot;********&quot;,
        libraries,
      });
    
      const [origin2, setOrigin2] = React.useState(&quot;lahore&quot;);
      const [destination2, setDestination2] = React.useState(&quot;gujranwala&quot;);
      const [response, setResponse] = React.useState(null);
    
      const directionsCallback = (response) =&gt; {
        console.log(response);
    
        if (response !== null) {
          if (response.status === &quot;OK&quot;) {
            setResponse(response);
          } else {
            console.log(&quot;response: &quot;, response);
          }
        }
      };
    
      const mapRef = React.useRef();
      const onMapLoad = React.useCallback((map) =&gt; {
        mapRef.current = map;
      }, []);
      if (loadError) return &quot;Error loading maps&quot;;
      if (!isLoaded) return &quot;loading maps&quot;;
    
      const DirectionsServiceOption = {
        destination: destination2,
        origin: origin2,
        travelMode: &quot;DRIVING&quot;,
      };
    
      return (
        &lt;div&gt;
          &lt;GoogleMap
            mapContainerStyle={mapContainerStyle}
            zoom={8}
            center={center}
            onLoad={onMapLoad}
          &gt;
            {response !== null &amp;&amp; (
              &lt;DirectionsRenderer
                options={{
                  directions: response,
                }}
              /&gt;
            )}
    
            &lt;DirectionsService
              options={DirectionsServiceOption}
              callback={directionsCallback}
            /&gt;
          &lt;/GoogleMap&gt;
        &lt;/div&gt;
      );
    };
    
    export default MainMaps;




||||||||||||||The rendering issue appears to be with the library itself. One alternative I can suggest is to instead use/load Google Maps API script instead of relying on 3rd party libraries. This way, you can just follow the [official documentation](https://developers.google.com/maps/documentation) provided by Google.

By loading the script, we can now follow their [Directions API documentation](https://developers.google.com/maps/documentation/javascript/directions):

Here is a sample app for your reference: https://stackblitz.com/edit/react-directions-64165413

`App.js`
```

    import React, { Component } from 'react';
    import { render } from 'react-dom';
    import Map from './components/map';
    import "./style.css";
    
    class App extends Component {
     
      render() {
        return (
           <Map 
            id="myMap"
            options={{
              center: { lat: 31.582045, lng: 74.329376 },
              zoom: 8
            }}
          />
        );
      }
    }
    
    export default App;

```

`map.js`
```

    import React, { Component } from "react";
    import { render } from "react-dom";
    
    class Map extends Component {
      constructor(props) {
        super(props);
        this.state = {
          map: "",
          origin: "",
          destination: ""
        };
        this.handleInputChange = this.handleInputChange.bind(this); 
        this.onSubmit = this.onSubmit.bind(this);
      }
    
      onScriptLoad() {
        this.state.map = new window.google.maps.Map(
          document.getElementById(this.props.id),
          this.props.options
        );
      }
    
      componentDidMount() {
        if (!window.google) {
          var s = document.createElement("script");
          s.type = "text/javascript";
          s.src = `https://maps.google.com/maps/api/js?key=YOUR_API_KEY`;
          var x = document.getElementsByTagName("script")[0];
          x.parentNode.insertBefore(s, x);
          // Below is important.
          //We cannot access google.maps until it's finished loading
          s.addEventListener("load", e => {
            this.onScriptLoad();
          });
        } else {
          this.onScriptLoad();
        }
      }
    
      onSubmit(event) {    
        this.calculateAndDisplayRoute();
        event.preventDefault();
      }
    
      calculateAndDisplayRoute() {
        var directionsService = new google.maps.DirectionsService();
        var directionsRenderer = new google.maps.DirectionsRenderer();
        directionsRenderer.setMap(this.state.map);
        directionsService.route(
          {
            origin: { query: this.state.origin },
            destination: { query: this.state.destination },
            travelMode: "DRIVING"
          },
          function(response, status) {
            if (status === "OK") {
              directionsRenderer.setDirections(response);
            } else {
              window.alert("Directions request failed due to " + status);
            }
          }
        );
        
      }
    
      handleInputChange(event) {
        const target = event.target;
        const value = target.type === "checkbox" ? target.checked : target.value;
        const name = target.name;
    
        this.setState({
          [name]: value
        });
      }
      addMarker(latLng) {
        var marker = new window.google.maps.Marker({
          position: { lat: -33.8569, lng: 151.2152 },
          map: this.state.map,
          title: "Hello Sydney!"
        });
        var marker = new google.maps.Marker({
          position: latLng,
          map: this.state.map
        });
      }
    
      render() {
        return (
          <div>
            <input
              id="origin"
              name="origin"
              value={this.state.origin}
              placeholder="Origin"
              onChange={this.handleInputChange}
            />
            <input
              id="destination"
              name="destination"
              value={this.state.destination}
              placeholder="Destination"
              onChange={this.handleInputChange}
            />
            <button id="submit" onClick={this.onSubmit}>
              Search
            </button>
            <div className="map" id={this.props.id} />
          </div>
        );
      }
    }
    
    export default Map;

```

--------------------------------------------------
At what point does binary search become more efficient than sequential search?
I&#39;ve been learning a lot about algorithms lately, and the binary searched is hailed for its efficiency in finding an item in large amounts of **sorted** data. But what if the data is not sorted to begin with? at what points does a binary search provide an efficiency boost against sequential search, with binary search having to sort the given array first off THEN search. I&#39;m interested in seeing at what points binary search passes over sequential search, if anyone has tested this before i would love to see some results.

Given an array foo[BUFF] with 14 elements

    1 3 6 3 1 87 56 -2 4 61 4 9 81 7

I would assume a sequential sort would be more efficient to find a given number, let&#39;s say... 3, because binary search would need to first sort the array **THEN** search for the number 3. BUT:

Given an array bar[BUFF] with one thousand elements held

    1 2 4 9 -2 3 8 9 4 12 4 56 //continued

A call to sort then binary search should in theory be more efficiently if i am not mistaken.


||||||||||||||In an unsorted array where no information is known, you are going to have to do linear time search.

Linear time search checks each element once, so it's complexity is `O(n)`. Comparing that to sorting. Sorting algorithms which must check each element more than once and have a complexity of `O(n * log n)`. So to even get it sorted is slower than a sequential search. Even though binary search is `O(log n)` it's pretty useless when you just have arbitrarily ordered data.

If your going to search for stuff multiple times though, consider sorting first as it'll increase your efficiency in the long run.

--------------------------------------------------
GPG error: http://archive.debian.org lenny/updates Release: The following signatures were invalid: KEYEXPIRED 1356982504
I am getting following error while update my source lists

    $ sudo apt-get update
    
    Reading package lists... Done
    
    W: GPG error: http://archive.debian.org lenny/updates Release: The following signatures were invalid: KEYEXPIRED 1356982504
    
    W: You may want to run apt-get update to correct these problems



**How to resolve this issue?**

||||||||||||||To find any expired repository keys and their IDs, use apt-key as follows:

      apt-key list | grep expired

You will get a result similar to the following:

      pub   4096R/BE1DB1F1 2011-03-29 [expired: 2014-03-28]

The key ID is the bit after the / i.e. BE1DB1F1 in this case.

To update the key, run

      sudo apt-key adv --recv-keys --keyserver keys.gnupg.net BE1DB1F1



--------------------------------------------------
Select a Dictionary&lt;T1, T2&gt; with LINQ
I have used the &quot;select&quot; keyword and extension method to return an `IEnumerable&lt;T&gt;` with LINQ, but I have a need to return a generic `Dictionary&lt;T1, T2&gt;` and can&#39;t figure it out.  The example I learned this from used something in a form similar to the following: 

    IEnumerable&lt;T&gt; coll = from x in y 
        select new SomeClass{ prop1 = value1, prop2 = value2 };

I&#39;ve also done the same thing with extension methods.  I assumed that since the items in a  `Dictionary&lt;T1, T2&gt;` can be iterated as `KeyValuePair&lt;T1, T2&gt;` that I could just replace &quot;SomeClass&quot; in the above example with &quot;`new KeyValuePair&lt;T1, T2&gt; { ...`&quot;, but that didn&#39;t work (Key and Value were marked as readonly, so I could not compile this code).

Is this possible, or do I need to do this in multiple steps?

Thanks.


||||||||||||||The extensions methods also provide a [ToDictionary][1] extension.  It is fairly simple to use, the general usage is passing a lambda selector for the key and getting the object as the value, but you can pass a lambda selector for both key and value.

    class SomeObject
    {
        public int ID { get; set; }
        public string Name { get; set; }
    }

    SomeObject[] objects = new SomeObject[]
    {
        new SomeObject { ID = 1, Name = "Hello" },
        new SomeObject { ID = 2, Name = "World" }
    };

    Dictionary<int, string> objectDictionary = 
        objects.ToDictionary(
            o => o.ID, 
            o => o.Name);

Then `objectDictionary[1]` Would contain the value "Hello"

  [1]: http://msdn.microsoft.com/en-us/library/system.linq.enumerable.todictionary.aspx

--------------------------------------------------
How do I get this code to accommodate any given number by only using boolean logic (no conditionals/ functions)
Here is the problem:

```
number = 1101
#You may modify the lines of code above, but don&#39;t move them!
#When you Submit your code, we&#39;ll change these lines to
#assign different values to the variables.
#
#The number above represents a binary number. It will always
#be up to eight digits, and all eight digits will always be
#either 1 or 0.
#
#The string gives the binary representation of a number. In
#binary, each digit of that string corresponds to a power of
#2. The far left digit represents 128, then 64, then 32, then
#16, then 8, then 4, then 2, and then finally 1 at the far right.
#
#So, to convert the number to a decimal number, you want to (for
#example) add 128 to the total if the first digit is 1, 64 if the
#second digit is 1, 32 if the third digit is 1, etc.
#
#For example, 00001101 is the number 13: there is a 0 in the 128s
#place, 64s place, 32s place, 16s place, and 2s place. There are
#1s in the 8s, 4s, and 1s place. 8 + 4 + 1 = 13.
#
#Note that although we use &#39;if&#39; a lot to describe this problem,
#this can be done entirely boolean logic and numerical comparisons.
#
#Print the number that results from this conversion.
```


---



Here is my code

```
##Add your code here!

number_str = str(number) # &quot;1101&quot;
first_num = int(number_str[-1]) * 1
#print(&quot;first num:&quot;, first_num)
second_num = int(number_str[-2]) * 2
#print(&quot;second num:&quot;, second_num) 
third_num = int(number_str[-3]) * 4
#print(&quot;Third num:&quot;, third_num)
fourth_num = int(number_str[-4]) * 8
#print(&quot;fourth num:&quot;, fourth_num)
fifth_num = int(number_str[-5]) * 16
sixt_num = int(number_str[-6]) * 32
seventh_num = int(number_str[-7]) * 64
decimal = first_num + second_num + third_num + fourth_num + fifth_num + sixt_num + seventh_num
print(decimal)
```
The error I got was:
We found a few things wrong with your code. The first one is shown below, and the rest can be found in full_results.txt in the dropdown in the top left:
We tested your code with number = 1010111. We expected your code to print this:
87
However, it printed this:
7

------------------------
I understand that I hard-coded this problem to accommodate 4 digits. I would like this to work for any given numbers without throwing an IndexError: string index out of range.

I appreciate your help.
||||||||||||||I don't like doing people's homework for them, but in this case I think the example says more than an explanation.

You do this conversion one character at a time, from left to right.  At each step, you shift the result left by one, and if the digit is '1', you add it in.
```
number = 1011
decimal = 0
for c in str(number):
    decimal = decimal * 2 + (c=='1')
print(decimal)
```
If that's too clever, replace `(c=='1')` with `int(c)`.

--------------------------------------------------
How can I calculate a hash for a filesystem-directory using Python?
I&#39;m using this code to calculate hash value for a file: 

    m = hashlib.md5()
	with open(&quot;calculator.pdf&quot;, &#39;rb&#39;) as fh:
		while True:
			data = fh.read(8192)
			if not data:
				break
			m.update(data)
		hash_value = m.hexdigest()
		
		print  hash_value

when I tried it on a folder &quot;folder&quot;I got 

    IOError: [Errno 13] Permission denied: folder


How could I calculate the hash value for a folder ?
||||||||||||||This [Recipe][1] provides a nice function to do what you are asking. I've modified it to use the MD5 hash, instead of the SHA1, as your original question asks

	def GetHashofDirs(directory, verbose=0):
	  import hashlib, os
	  SHAhash = hashlib.md5()
	  if not os.path.exists (directory):
		return -1
		
	  try:
		for root, dirs, files in os.walk(directory):
		  for names in files:
			if verbose == 1:
			  print 'Hashing', names
			filepath = os.path.join(root,names)
			try:
			  f1 = open(filepath, 'rb')
			except:
			  # You can't open the file for some reason
			  f1.close()
			  continue

		    while 1:
    		  # Read file in as little chunks
	    	  buf = f1.read(4096)
		      if not buf : break
    		  SHAhash.update(hashlib.md5(buf).hexdigest())
			f1.close()

	  except:
		import traceback
		# Print the stack traceback
		traceback.print_exc()
		return -2

	  return SHAhash.hexdigest()


You can use it like this:

    print GetHashofDirs('folder_to_hash', 1)

The output looks like this, as it hashes each file:

<!-- language: lang-none -->

    ...
    Hashing file1.cache
    Hashing text.txt
    Hashing library.dll
    Hashing vsfile.pdb
    Hashing prog.cs
    5be45c5a67810b53146eaddcae08a809

The returned value from this function call comes back as the hash. In this case, `5be45c5a67810b53146eaddcae08a809`

  [1]: http://code.activestate.com/recipes/576973-getting-the-sha-1-or-md5-hash-of-a-directory/

--------------------------------------------------
Browser-side JS: File System API vs File System Access API?
There was a File System API but shown as deprecated now:&lt;br/&gt; https://developer.mozilla.org/en-US/docs/Web/API/Window/requestFileSystem

There is now another, File System Access API:&lt;br/&gt; https://developer.mozilla.org/en-US/docs/Web/API/File_System_Access_API

What happened to the old API and why was it discontinued? Should the new File System Access API be stable in all common browsers?
||||||||||||||It turned out that File System Access API is not deprecated, it's just not standardised (May 2021); the deprecated one is the function `window.requestFileSystem`; the same function on Chromium-based browsers is `window.webkitRequestFileSystem`.

`File System API` is for creating a virtual drive (temporary or persistent) for each website when using browser-based db (IndexedDB) is not necessary especially for the purpose of storing files.<br/>
https://developer.mozilla.org/en-US/docs/Web/API/FileSystem

`File System Access API` is different, it is for accessing the real file system of the OS. This API is now standardised and available on Chromium-based browsers (May 2021). Firefox has not yet adapted this API.<br/>
https://developer.mozilla.org/en-US/docs/Web/API/File_System_Access_API

Status of these APIs: https://developer.mozilla.org/en-US/docs/Web/API


--------------------------------------------------
How to simulate a loop&#39;s &#39;break&#39; statement inside an array-iterating, custom implemented, &#39;forEach&#39; function/method?
What is the best way to implement a simulation of a loop&#39;s `break` statement, when one does iterate through a user/engine defined function?

    forEach([0, 1, 2, 3, 4], function (n) {
      console.log(n);
      
      if (n === 2) {
        break;
      }
    });

I&#39;ve thought of implementing `forEach` in a way that would break when the function returns `false`. But I would like to hear thoughts on how that is normally done.

||||||||||||||`return`ing `false` is the most common way to do it. That's what jQuery's iterator function [`.each()`][1] does:

> We can break the $.each() loop at a particular iteration by making the
> callback function return **false**. Returning non-false is the same as a
> continue statement in a for loop; it will skip immediately to the next
> iteration.

And its *very* simplified implementation:

    each: function( object, callback ) {
      var i = 0, length = object.length,
      for ( var value = object[0]; 
            i < length && callback.call( value, i, value ) !== false; // break if false is returned by the callback 
            value = object[++i] ) {}
      return object;
    }

  [1]: http://api.jquery.com/jQuery.each/

--------------------------------------------------
Understanding OpenMP shortcomings regarding fork
***I wish to understand what do they mean here. Why would this program &quot;hang&quot;?***

From https://bisqwit.iki.fi/story/howto/openmp/  

&gt; OpenMP and `fork()` It is worth mentioning that using OpenMP in a
&gt; program that calls `fork()` requires special consideration. This
&gt; problem only affects GCC; ICC is not affected.   If your program
&gt; intends to become a background process using `daemonize()` or other
&gt; similar means, you must not use the OpenMP features before the fork.
&gt; After OpenMP features are utilized, a fork is only allowed if the
&gt; child process does not use OpenMP features, or it does so as a
&gt; completely new process (such as after `exec()`).
&gt; 
&gt; This is an example of an erroneous program:
&gt; 
&gt;     #include &lt;stdio.h&gt;   
&gt;     #include &lt;sys/wait.h&gt;   
&gt;     #include &lt;unistd.h&gt;
&gt;     
&gt;     void a(){
&gt;         #pragma omp parallel num_threads(2)
&gt;         {
&gt;             puts(&quot;para_a&quot;); // output twice
&gt;         }
&gt;         puts(&quot;a ended&quot;); // output once   
&gt;     }
&gt;        
&gt;     void b(){
&gt;         #pragma omp parallel num_threads(2)
&gt;         {
&gt;             puts(&quot;para_b&quot;);
&gt;         }
&gt;         puts(&quot;b ended&quot;);   
&gt;     }
&gt;     
&gt;     int main(){    
&gt;         a();   // Invokes OpenMP features (parent process)   
&gt;         int p = fork();    
&gt;         if(!p){
&gt;             b(); // ERROR: Uses OpenMP again, but in child process
&gt;             _exit(0);    
&gt;         }    
&gt;         wait(NULL);    
&gt;         return 0;   
&gt;     }
&gt; 
&gt; When run, this program hangs, never reaching the line that outputs &quot;b
&gt; ended&quot;. There is currently no workaround as the libgomp API does not
&gt; specify functions that can be used to prepare for a call to `fork()`.


||||||||||||||The code as posted violates the POSIX standard.

The [POSIX `fork()` standard states][1]:

> A process shall be created with a single thread. If a multi-threaded
> process calls fork(), the new process shall contain a replica of the
> calling thread and its entire address space, possibly including the
> states of mutexes and other resources. **Consequently, to avoid
> errors, the child process may only execute async-signal-safe
> operations until such time as one of the `exec` functions is called.**

Running OMP-parallelized code is clearly violating the above restriction.
  [1]: http://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html

--------------------------------------------------
How to replace reference to a file with its contents?
Supose that I have a directory containing among others two markdown files 

```
a note.md
another one.md
```

with the `a note.md` containing

```
Here is a description of an idea.

![[another one.md]]
```

and the `another one.md` containing

```
Here is another idea related to it.
```

I am looking for a command in bash that would 

1. take `a note.md`, 
2. replace that the reference `![[another one.md]]` in the `a note.md` with the actual contents of the `another one.md`, and 
3. return the result (so that I could pipe it to Pandoc).

The output in this example would contain

```
Here is a description of an idea.

Here is another idea related to it.
```

---

Why? Obsidian markdown note-taking app allows [embedding file contents](https://help.obsidian.md/Linking+notes+and+files/Embed+files#Embed+a+note+in+another+note) into markdown files  using `![[]]` as described above. However, when converting such files using Pandoc, the references are treated as text. So I am looking for a way to add the embedded content prior to Pandoc conversion.

||||||||||||||If you can use perl :

```
perl -i -pe 's/!\[\[(.*?)]]/`cat "$1"`/eg' "a note.md"
```

`-i` option changes "a note.md", so you may want to do a backup before running the command.

@jhnc provided a much safer version :

```
perl -0777pe 's/!\[\[([^]]+)]]/ -f $1 && `cat \Q$1` =~ s#^\s*(.*?)\s*$#$1#sr || $& /eg' 'a note.md' > 'result.md'
```

--------------------------------------------------
Regex to match key with optional quotes and optional separator
I&#39;m trying to build a regex where it should be able to get all the values of the key mentioned in the log satisfying the following conditions

 1. It can be either in single quote, double quote or no quote.
 2. Key will be either followed by `: - `
 3. Value can either be in single quote, double quote or no quote.

Here is the example I have tried out.

    [&#39;&quot;](?:accountNumber|subNo)[&#39;&quot;][:-]\s*[&#39;&quot;](\d+)[&#39;&quot;]

This regex is finding out the `accountNumber` in the second part of the log

    23:22:12.127 DEBUG Getting Service Details for: accountNumber-&quot;525012078&quot;, subNo 5488870689 
    23:22:12.403 INFO  /subscriptions, [{&quot;accountNumber&quot;:&quot;1233&quot;,&quot;subNo&quot;:&quot;123&quot;,&quot;type&quot;:...}}] 

Qn: Need to find out the `accountNumber` or `subNo` from the log, in both JSON format and statement log.
||||||||||||||You can use
```none
(['"]|)\b(?:accountNumber|subNo)\1[:-]\s*(['"])(\d+)\2
```
See the [regex demo][1].

See the [Java code][2]:
```java
String s = "23:22:12.127 DEBUG Getting Service Details for: accountNumber-\"525012078\", subNo 5488870689 \n23:22:12.403 INFO  /subscriptions, [{\"accountNumber\":\"1233\",\"subscriptionNo\":\"123\",\"type\":...}}] ";
Pattern pattern = Pattern.compile("(['\"]|)\\b(?:accountNumber|subNo)\\1[:-]\\s*(['\"])(\\d+)\\2");
Matcher matcher = pattern.matcher(s);
while (matcher.find()){
	System.out.println(matcher.group(3)); 
} 
```
*Details*:

 - `(['"]|)\b(?:accountNumber|subNo)` - either `"` or `'` captured into Group 1  or an empty space + word boundary + `accountNumber` or `subNo`
 - `\1` - Same value as in Group 1
 - `[:-]` - a `:` or `-`
 - `\s*` - zero or more whitespaces
 - `(['"])` - Group 2: a `'` or `"` char
 - `(\d+)` - Group 3 (the result): one or more digits
 - `\2` - same value as in Group 2.

  [1]: https://regex101.com/r/QNQ3NX/2
  [2]: https://ideone.com/oGhQxI

--------------------------------------------------
Check the total number of parameters in a PyTorch model
How do I count the total number of parameters in a PyTorch model? Something similar to `model.count_params()` in Keras.
||||||||||||||PyTorch doesn't have a function to calculate the total number of parameters as Keras does, but it's possible to sum the number of elements for every parameter group:

    pytorch_total_params = sum(p.numel() for p in model.parameters())

If you want to calculate only the _trainable_ parameters:

    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

---
_Answer inspired by [this answer](https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9) on PyTorch Forums_.

--------------------------------------------------
Pylint warn the usage of print statement
I am using `pylint_django` for my django project. And I want to disable print statement usage or warn about it at least. Because I am using custom logger class. But there is no any warn about usage of print.


```[MASTER]

extension-pkg-whitelist=

ignore=CVS

ignore-patterns=

jobs=1


limit-inference-results=100

load-plugins=

persistent=yes

suggestion-mode=yes

unsafe-load-any-extension=no


[MESSAGES CONTROL]
confidence=

disable=missing-docstring,
        invalid-name,
        astroid-error,
        protected-access,
        broad-except

enable=c-extension-no-member, print-statement


[REPORTS]
evaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)

output-format=text

reports=no

score=yes


[REFACTORING]

max-nested-blocks=5

never-returning-functions=sys.exit


[LOGGING]

logging-format-style=old

logging-modules=logging

....
```

How can i solve this issue?

VsCode settings.json
```
{
  &quot;python.linting.pylintEnabled&quot;: true,
  &quot;python.linting.enabled&quot;: true,
  &quot;python.linting.flake8Enabled&quot;: false,
  &quot;python.linting.prospectorEnabled&quot;: false,
  &quot;python.linting.pylintArgs&quot;: [
    &quot;--load-plugins=pylint_django&quot;,
    &quot;--rcfile=.pylintrc&quot;,
    &quot;--enable=print-statement&quot;
  ]
}
```
||||||||||||||You can do that using [the deprecated checkers ``bad-functions`` options](https://pylint.pycqa.org/en/latest/user_guide/configuration/all-options.html#deprecated-builtins-options):

    [tool.pylint]
    bad-functions = ["map", "filter", "print"]

--------------------------------------------------
How to convert Top-and-Bottom 3d video to side-by-side 3d video with FFmpeg
I have a top-and-bottom 3d video, and i want to look at it with Gear VR, but Gear VR only support side-by-side video, so i need to convert it to side-by-side, while i don&#39;t know how to use ffmpeg to achieve it,does anyone knows ? thanks very much.

||||||||||||||See [stereo3d](http://ffmpeg.org/ffmpeg-filters.html#stereo3d) filter documentation:

`ffmpeg -i top-and-bottom.mov -vf stereo3d=abl:sbsl -c:a copy side-by-side.mov`

--------------------------------------------------
Docker: How to solve the public key error in ubuntu while installing docker
I am getting the below error message when running the below command for installing docker and kubernetes in Ubuntu server. 


    root@master:/home/ubuntu# add-apt-repository \
    &gt;   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
    &gt;   $(lsb_release -cs) \
    &gt;   stable&quot;
    Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease
    Get:2 https://download.docker.com/linux/ubuntu bionic InRelease [64.4 kB]
    Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease
    Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease
    Hit:5 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease
    **Err:2 https://download.docker.com/linux/ubuntu bionic InRelease
      The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY 7EA0A9C3F273FCD8**
    Reading package lists... Done
    W: GPG error: https://download.docker.com/linux/ubuntu bionic InRelease: The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY 7EA0A9C3F273FCD8
    **E: The repository &#39;https://download.docker.com/linux/ubuntu bionic InRelease&#39; is not signed.**
    N: Updating from such a repository can&#39;t be done securely, and is therefore disabled by default.
    N: See apt-secure(8) manpage for repository creation and user configuration details.
    root@master:/home/ubuntu#




I have also ran the below command but no luck

    root@master:/# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 7EA0A9C3F273FCD8
    Executing: /tmp/apt-key-gpghome.rDOuMCVLF2/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 7EA0A9C3F273FCD8
    gpg: keyserver receive failed: No keyserver available


||||||||||||||# EDIT: This answer apparently does not work any more

Run this to add the correct key:

    # Does not work any more
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

Source: https://docs.docker.com/install/linux/docker-ce/ubuntu/

--------------------------------------------------
Create an arc between two points in matplotlib
I am trying to recreate the chart below using matplotlib:
[![enter image description here][1]][1]

I have most of it done but, I just cant figure out how to create the arcs between the years:

    import matplotlib.pyplot as plt
    from scipy.interpolate import interp1d
    import numpy as np
    import pandas as pd
    
    colors = [&quot;#CC5A43&quot;,&quot;#2C324F&quot;,&quot;#5375D4&quot;,]
    
    data = {
        &quot;year&quot;: [2004, 2022, 2004, 2022, 2004, 2022],
        &quot;countries&quot; : [ &quot;Denmark&quot;, &quot;Denmark&quot;, &quot;Norway&quot;, &quot;Norway&quot;,&quot;Sweden&quot;, &quot;Sweden&quot;,],
        &quot;sites&quot;: [4,10,5,8,13,15]
    }
    df= pd.DataFrame(data)
    df = df.sort_values([ &#39;year&#39;], ascending=True ).reset_index(drop=True)
    df[&#39;ctry_code&#39;] = df.countries.astype(str).str[:2].astype(str).str.upper()
    df[&#39;year_lbl&#39;] =&quot;&#39;&quot;+df[&#39;year&#39;].astype(str).str[-2:].astype(str)
    sites = df.sites
    lbl1 = df.year_lbl
    
    
    fig, ax = plt.subplots( figsize=(6,6),sharex=True, sharey=True, facecolor = &quot;#FFFFFF&quot;, zorder= 1)
    
    
    ax.scatter(sites, sites, s= 340, c= colors*2 , zorder = 1)
    ax.set_xlim(0, sites.max()+3)
    ax.set_ylim(0, sites.max()+3)
    ax.axline([ax.get_xlim()[0], ax.get_ylim()[0]], [ax.get_xlim()[1], ax.get_ylim()[1]], zorder = 0, color =&quot;#DBDEE0&quot; )
    
    for i, l1 in zip(range(0,6), lbl1) :
        ax.annotate(l1, (sites[i], sites[i]), color = &quot;w&quot;,va= &quot;center&quot;, ha = &quot;center&quot;)
    
 
    ax.set_axis_off()

Which gives me this:
[![enter image description here][2]][2]

I have tried both [mpatches.arc][3] and [patches and path][4] but cant make it work.


  [1]: https://i.stack.imgur.com/P9NGe.png
  [2]: https://i.stack.imgur.com/UaONB.png
  [3]: https://stackoverflow.com/questions/30642391/how-to-draw-a-filled-arc-in-matplotlib
  [4]: https://stackoverflow.com/questions/50346166/draw-an-arc-as-polygon-using-start-end-center-and-radius-using-python-matplotl
||||||||||||||# Semicircle arc between two points
To draw a semicircle between two points:
- the center of the two points will be the center of the circle
- for a circular arc, `width` and `height` both need to be set to the diameter; that diameter is the distance between the two points (square root of sum of squares of the x and y differences)
- the starting angle can be calculated by the arc tangent of the vector from one point to the other
- the final angle will be 180º further

Encapsulated in a function, together with a little test:
```python
import matplotlib.pyplot as plt
from matplotlib. Patches import Arc
import numpy as np

def draw_semicircle(x1, y1, x2, y2, color='black', lw=1, ax=None):
    '''
    draw a semicircle between the points x1,y1 and x2,y2
    the semicircle is drawn to the left of the segment
    '''
    ax = ax or plt.gca()
    # ax. Scatter([x1, x2], [y1, y2], s=100, c=color)
    startangle = np.degrees(np.arctan2(y2 - y1, x2 - x1))
    diameter = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)  # Euclidian distance
    ax.add_patch(Arc(((x1 + x2) / 2, (y1 + y2) / 2), diameter, diameter, theta1=startangle, theta2=startangle + 180,
                     edgecolor=color, facecolor='none', lw=lw, zorder=0))

angle = np.linspace(0, 38, 80)
x = angle * np.cos(angle)
y = - angle * np.sin(angle)
fig, ax = plt.subplots()
for x1, y1, x2, y2 in zip(x[:-1], y[:-1], x[1:], y[1:]):
    draw_semicircle(x1, y1, x2, y2, color='fuchsia', lw=2)

ax.set_aspect('equal')  # show circles without deformation
ax.autoscale_view()  # fit the arc into the data limits
ax. Axis('off')
plt.show()
```
[![matplotlib semicircles between two points][1]][1]


# Specific code for the data in the question
Here is an adaption of the code for your case (180º arc on a 45º line).  The text can be positioned using the x coordinate of the first and the y coordinate of the second point.
```python
import matplotlib.pyplot as plt
from matplotlib. Patches import Arc
import pandas as pd
import math

colors = ["#CC5A43", "#2C324F", "#5375D4"]
data = {
    "year": [2004, 2022, 2004, 2022, 2004, 2022],
    "countries": ["Denmark", "Denmark", "Norway", "Norway", "Sweden", "Sweden"],
    "sites": [4, 10, 5, 8, 13, 15]
}
df = pd.DataFrame(data)
df = df.sort_values(['year'], ascending=True).reset_index(drop=True)
df['ctry_code'] = df.countries.astype(str).str[:2].astype(str).str.upper()
df['year_lbl'] = "'" + df['year'].astype(str).str[-2:].astype(str)
sites = df.sites
lbl1 = df.year_lbl
countries = df.ctry_code

fig, ax = plt.subplots(figsize=(6, 6), sharex=True, sharey=True, facecolor="#FFFFFF", zorder=1)

ax. Scatter(sites, sites, s=340, c=colors * 2, zorder=1)
ax.set_xlim(0, sites.max() + 3)
ax.set_ylim(0, sites.max() + 3)
ax.set_aspect('equal')
ax.axline([ax.get_xlim()[0], ax.get_ylim()[0]], [ax.get_xlim()[1], ax.get_ylim()[1]], zorder=0, color="#DBDEE0")

for site, l1 in zip(sites, lbl1):
    ax.annotate(l1, (site, site), color="w", va="center", ha="center")

for x1, x2, color, country in zip(sites[:len(sites) // 2], sites[len(sites) // 2:], colors, countries):
    center = (x1 + x2) / 2
    diameter = math.sqrt((x2 - x1) ** 2 + (x2 - x1) ** 2)  # Euclidian distance
    ax.add_patch(Arc((center, center), diameter, diameter, theta1=45, theta2=225,
                     edgecolor=color, facecolor='none', lw=2))
    ax.annotate(country, (x1, x2), color=color, va="center", ha="center",
                bbox=dict(boxstyle="round, pad=0.5", facecolor="aliceblue", edgecolor=color, lw=2))
ax.set_axis_off()
plt.show()
``` 
[![matplotlib arcs between points][2]][2]


  [1]: https://i.stack.imgur.com/GY9Hl.png
  [2]: https://i.stack.imgur.com/A5Spr.png

--------------------------------------------------
TypeScript error on context value type mismatch
I am implementing React context using TypeScript and I seem to be getting and error on the `value` prop of my context.

Here is what the error says:
```
Type &#39;{ createReferral: (referralData: { actionStepId: number; appTypeId: string; comment: string; createdDate: Date; createdBy: string; createdByDisplayName: string; currentAssigneeDisplayName: string; ... 4 more ...; submissionId: string; }) =&gt; void; getCurrentUser: () =&gt; AccountInfo; }&#39; is not assignable to type &#39;SelectedContextType&#39;.
```

So it looks like there is a mismatch between my expected context value types and the actual value types.

I declare my context as follows:
```
interface SelectedContextType {
  createReferral: (referralData: {
    actionStepId: number
    appTypeId: string
    comment: string
    createdDate: Date
    createdBy: string
    createdByDisplayName: string
    currentAssigneeDisplayName: string
    currentAssigneeGRNID: string
    reasonTypeId: number | string
    referralTypeId: number | string
    statusTypeId: number
    submissionId: string
  }) =&gt; void
  getCurrentUser: () =&gt; { name: string; username: string }
}

export const AppContext = createContext&lt;SelectedContextType | undefined&gt;(
  undefined
)
```

and I declare these functions in my context:
```
const AppProvider = ({ children, msalContext }: AppProps) =&gt; {
  function getCurrentUser() {
    return msalContext.accounts[0]
  }

  function createReferral(referralData: {
    actionStepId: number
    appTypeId: string
    comment: string
    createdDate: Date
    createdBy: string
    createdByDisplayName: string
    currentAssigneeDisplayName: string
    currentAssigneeGRNID: string
    reasonTypeId: number | string
    referralTypeId: number | string
    statusTypeId: number
    submissionId: string
  }) {
    referralAxios
      .post(&#39;/api/ReferralMasters&#39;, {
        actionStepID: referralData.actionStepId,
        appTypeID: referralData.appTypeId,
        comment: referralData.comment,
        createdDate: referralData.createdDate,
        createdBy: referralData.createdBy,
        createdByDisplayName: referralData.createdByDisplayName,
        currentAssigneeDisplayName: referralData.currentAssigneeDisplayName,
        currentAssigneeGRNID: referralData.currentAssigneeGRNID,
        reasonTypeID: referralData.reasonTypeId,
        referralTypeID: referralData.referralTypeId,
        statusTypeID: referralData.statusTypeId,
        submissionID: Number(referralData.submissionId),
      })
      .then(function (response) {
        console.log(response)
        if (response.data.actionStepID === 2) {
          sendReferralEmail(response.data)
        }
      })
      .catch(function (error) {
        console.log(error, &#39;testError&#39;)
      })
  }

  const appContext = { createReferral, getCurrentUser }
  return (
    &lt;AppContext.Provider value={appContext}&gt;{children}&lt;/AppContext.Provider&gt;
  )
}

export default withMsal(AppProvider)
```

I am not sure what exactly the error is saying. I see it referencing `AccountInfo` which does not exist as a type anywhere in my app.


The `msalContext.accounts[0]` is an object that looks like:
```
authorityType: &quot;MSSTS&quot;
environment: &quot;login.windows.net&quot;
homeAccountId: &quot;89887r89e&quot;
idTokenClaims: {aud: &#39;1799}
localAccountId: &quot;60000&quot;
name: &quot;User&quot;
username: &quot;User@STAR.COM&quot;
```

example: [example][1]


  [1]: https://www.typescriptlang.org/play?#code/JYWwDg9gTgLgBAJQKYEMDGMA0cDec1SoxIDCEAdsQB7wC+cAZlBCHAESHoxsBQoksXHACSAWQDOKADZlKSGtgDuwGAAsJ0uPSYs4AcgACKAF4BXQgHoQkqQFpOGPTz5yoDdEjgBBMGAAKzGDiuDxw+KrAUgAmhOQAXIioGAB0yFwAchBRSKFw1tKy1DAJYjaF8jA8tM7Aru5ongDKSFJIGEhR5TQAKgCeYJ44uWEEREjIDEhQUNIJABSEk9PSACIoMCgJQ2E7O1zAFI3EYMJRCeSmIABGU8O7cCi+fQOnCeIwULUA5ne7aCwgJCUN4fb6-HajdYdNbEBIwnL3CGcYhRABCvRBn3IP0RI2RHXRK2A4jAUhQvXSKEBmLBuPw5liMC84nEwC+5CQSCJJLJFKpSBp2PBIwZQKZLLZHKQAHEEOlhCtBTjcZxxBRnkhXnALtcpnAAD5wd5Y5WIxZTGZSDVanU3KAGo2goV097rUzia1nbWXO3Co2mK4gYmsiha420na0ACUcAAvAA+OAANwgwCidy+SBgJFFlAAquIpvMYwmhOR+UqANxwd1TcvUx0mrRVZzyATwf7kd7eXxdeCx-D4vsAHmarXanQoRQ1DtM5GyDFqHXjczuc4XS-TUecne7Pn8zCTab1A7meDQEWisWw+RkU4qWgS+4CECCJcT20Yc4wB3IcEz2a5jABZTHMMafiMFBqq0yRSBAXxzLefbJOg-xzjA4gANoAAwALrbrshAwOYf5IfeNAoWgaGUFheG5NUdwMN+MC-oOYwTBa0gLEgSyWjCmwhHS+yHMcNo+rcQlPP0mpeuGzq4v8ICAsCjYRoikIovCcJQn6GkEhiqnyep+Jor03KkuSlINnJpr3GgQHMqy7KcuZvJWQKhm2X8DkSs5MpygqSp+qq6rSWJur2oaNnBTxnFWmFXq2nqUVOl5OyusRHoJec4lQH64gBkGEqhrJqV3NGgm4hYFhwMkdXlc4fxQfAjxgH2cZCHpHHLFI2AATm0xiiB9rVGEREkXAq67MO+7IS+R7ZPaSbSKYSCxjgrV9rQ8Y4BekQxECtDDhYs3kTAyTzceUDxrk260EAA
||||||||||||||[the AccountInfo.name value is nullable](https://github.com/AzureAD/microsoft-authentication-library-for-js/blob/e7a55511680aec602310f63db0bb5b7d2b07fab3/lib/msal-common/src/account/AccountInfo.ts#L20)

just make it nullable either in your code:
```ts
  getCurrentUser: () => { name?: string; username: string }
```

--------------------------------------------------
Allow multiple CORS domain in express js
How do I allow multiple domains for CORS in express in a simplified way.

I have

     cors: {
            origin: &quot;www.one.com&quot;;
        }
    
        app.all(&#39;*&#39;, function(req, res, next) {
                res.header(&quot;Access-Control-Allow-Origin&quot;, cors.origin);
                res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept&quot;);
                next();
            });

This works when there is only one domain mentioned in `origin`

But if I want to have `origin` as an array of domains and I want to allow CORS for all the domains in the origin array, I would have something like this - 

    cors: {
                origin: [&quot;www.one.com&quot;,&quot;www.two.com&quot;,&quot;www.three.com&quot;];
            }
    
But then the problem is this below code would not work - 

    app.all(&#39;*&#39;, function(req, res, next) {
                    res.header(&quot;Access-Control-Allow-Origin&quot;, cors.origin);
                    res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept&quot;);
                    next();
                });

How do I make `res.header` take an array of domains via `cors.origin` ?
||||||||||||||The value of `Access-Control-Allow-Origin` must be a string, not a list. So to make it dynamic you need to get the requesting origin from the `Origin` HTTP request header, check it against your array of authorized origins. If it's present, then add that origin as the value of the `Access-Control-Allow-Origin` header; otherwise, use a default value, which would prohibit unauthorized domains from accessing the API.

There is no native implementation for this. You can do it yourself using the code below. 

    cors: {
      origin: ["www.one.com","www.two.com","www.three.com"],
      default: "www.one.com"
    }

    app.all('*', function(req, res, next) {
      const origin = cors.origin.includes(req.header('origin').toLowerCase()) ? req.headers.origin : cors.default;
      res.header("Access-Control-Allow-Origin", origin);
      res.header("Access-Control-Allow-Headers", "Origin, X-Requested-With, Content-Type, Accept");
      next();
    });


--------------------------------------------------
Python 3.Kivy. Is there any way to limit entered text in TextInput widget?
I&#39;m writing kivy app and resently I faced with a  problem of unlimited inputing text in TextInput widget. Is there any solution to this problem?
||||||||||||||A possible solution is to create a new property and overwrite the insert_text method:

    
    from kivy.app import App
    from kivy.uix.textinput import TextInput
    from kivy.properties import NumericProperty
    
    
    class MyTextInput(TextInput):
        max_characters = NumericProperty(0)
        def insert_text(self, substring, from_undo=False):
            if len(self.text) > self.max_characters and self.max_characters > 0:
                substring = ""
            TextInput.insert_text(self, substring, from_undo)
    
    class MyApp(App):
        def build(self):
            return MyTextInput(max_characters=4)
    
    
    if __name__ == '__main__':
        MyApp().run()

--------------------------------------------------
Generating random numbers over a range in Go
All the integer functions in [`math/rand`](http://golang.org/pkg/math/rand/#Int) generate non-negative numbers.

    rand.Int() int              // [0, MaxInt]
    rand.Int31() int32          // [0, MaxInt32]
    rand.Int31n(n int32) int32  // [0, n)
    rand.Int63() int64          // [0, MaxInt64]
    rand.Int63n(n int64) int64  // [0, n)
    rand.Intn(n int) int        // [0, n)

I would like to generate random numbers in the range **[-m, n)**. In other words, I would like to generate a mix of positive and negative numbers.
||||||||||||||I found this example at [Go Cookbook](http://golangcookbook.blogspot.com/2012/11/generate-random-number-in-given-range.html), which is equivalent to `rand.Range(min, max int)` (if that function existed):

```go
rand.Intn(max - min) + min
```

--------------------------------------------------
C#: Create a virtual drive in Computer
Is there any way to create a virtual drive in &quot;(My) Computer&quot; and manipulate it, somewhat like JungleDisk does it?

It probably does something like:

    override OnRead(object sender, Event e) {
        ShowFilesFromAmazon();
    }

Are there any API:s for this? Maybe to write to an XML-file or a database, instead of a real drive.

---

The [Dokan Library][1] seems to be the answer that mostly corresponds with my question, even though [System.IO.IsolatedStorage][2] seems to be the most standardized and most Microsoft-environment adapted.


  [1]: http://dokan-dev.net/en/
  [2]: http://msdn.microsoft.com/en-us/library/system.io.isolatedstorage.aspx
||||||||||||||You can use the <a href="https://dokan-dev.github.io/">Dokan library</a> to create a virtual drive. There is a .Net wrapper for interfacing with C#.

--------------------------------------------------
In Excel how could I change font colour according to content of a cell
I have a list of playing cards with each card in its own cell e.g.
3h
2s
Kd
Ah
Jc....
To help with visualisation I wanted to change the font colour of the hearts and diamonds to red.
Of course manually would be an option, but very tedious

I have tried (unsuccessfully) to write a vba script to do a search and replace using
https://stackoverflow.com/questions/17684155/excel-vba-find-and-replace as an example.
However as I am NOT changing the text it just looped continually. I also could not make the search format specific.

```
Sub Main()
Dim c As Range
Dim redCell As String

With Worksheets(1).Range(&quot;A1:A52&quot;)
Application.FindFormat.Clear
Application.FindFormat.Font.Color = rgbBlack &#39;Automatic was a problem
    Set c = .Find(&quot;h&quot;, LookIn:=xlValues, Searchformat:=True)
    If Not c Is Nothing Then
        redCell = c.Address
        Do
            Range(redCell).Font.Color = -16776961
            Set c = .FindNext(c)
        Loop While Not c Is Nothing
    End If
End With
End Sub
```

I also tried to use conditional formatting but could not find a criteria that worked.

    =OR(FIND(&quot;h&quot;,A1)=2,FIND(&quot;d&quot;,A1)=2) 

generates a #VALUE! error
Any pointers would be gratefully received.

||||||||||||||You can use this formula for your format condition:

    =OR(ISNUMBER(FIND("h",A1)),ISNUMBER(FIND("d",A1)))

Your attempt returns an error as `FIND("d",A1)` returns an error (no d found in A1). By using `ISNUMBER` `FIND("d",A1)` will return `false` instead of an error.


--------------------------------------------------
vs code not opening up in windows
whenever i try to open my vs code editor, nothing happens it doesn&#39;t launch and even there are no errors..!! And i am confused what&#39;s wrong here in my vs code. Please anyone help me fix it..!!

Below are the verbose command i typed in the terminal..

```
C:\Users\Avinash&gt;code . --verbose

[main 2020-05-10T05:17:56.317Z] Error: UNKNOWN: unknown error, mkdir
[main 2020-05-10T05:17:56.318Z] Lifecycle#kill()
[main 2020-05-10T05:17:56.320Z] [File Watcher (node.js)] Error: UNKNOWN: unknown error, stat &#39;c:\Users\Avinash Maurya\AppData\Roaming\Code\User&#39;
```
||||||||||||||no need of Unistalling, just go to your vscode-setup and reinstall it. (by this procedure all of your's settings, files , extensions etc.. will be restored as it is.)

--------------------------------------------------
How to copy a file from one folder to another using VBScript
How can I copy a file from one folder to another using VBScript?

I had tried this below one from the information provide on the internet:

    dim filesys
    
    set filesys=CreateObject(&quot;Scripting.FileSystemObject&quot;)
    
    If filesys.FileExists(&quot;c:\sourcefolder\anyfile.txt&quot;) Then
    
    filesys.CopyFile &quot;c:\sourcefolder\anyfile.txt&quot;, &quot;c:\destfolder\&quot;

When I execute this, I get a &#39;permission denied&#39; error. 
||||||||||||||Try this.  It will check to see if the file already exists in the destination folder, and if it does will check if the file is read-only.  If the file is read-only it will change it to read-write, replace the file, and make it read-only again.

    Const DestinationFile = "c:\destfolder\anyfile.txt"
    Const SourceFile = "c:\sourcefolder\anyfile.txt"
    
    Set fso = CreateObject("Scripting.FileSystemObject")
    	'Check to see if the file already exists in the destination folder
    	If fso.FileExists(DestinationFile) Then
    		'Check to see if the file is read-only
    		If Not fso.GetFile(DestinationFile).Attributes And 1 Then 
    			'The file exists and is not read-only.  Safe to replace the file.
    			fso.CopyFile SourceFile, "C:\destfolder\", True
    		Else 
    			'The file exists and is read-only.
    			'Remove the read-only attribute
    			fso.GetFile(DestinationFile).Attributes = fso.GetFile(DestinationFile).Attributes - 1
    			'Replace the file
    			fso.CopyFile SourceFile, "C:\destfolder\", True
    			'Reapply the read-only attribute
    			fso.GetFile(DestinationFile).Attributes = fso.GetFile(DestinationFile).Attributes + 1
    		End If
    	Else
    		'The file does not exist in the destination folder.  Safe to copy file to this folder.
    		fso.CopyFile SourceFile, "C:\destfolder\", True
    	End If
    Set fso = Nothing

--------------------------------------------------
How does Spring auto convert objects to json for @RestController
I&#39;m looking at code in which I&#39;m assuming spring decides to use Jackson behind the scenes to auto convert an object to json for a @RestController
```
@RestController 
@RequestMapping(&quot;/api&quot;)
public class ApiController {

    private RoomServices roomServices;

    @Autowired
    public ApiController(RoomServices roomServices) {
        this.roomServices = roomServices;
    }

    @GetMapping(&quot;/rooms&quot;)
    public List&lt;Room&gt; getAllRooms() {
        return this.roomServices.getAllRooms();
    }
}
```
The Room class is just a plain java class with some fields, getters/setters. There is no Jackson or any other explicit serialization going on in the code. Although this does return json when checking the url. I tried looking through the spring documentation but I&#39;m not quite sure what I&#39;m looking for. What is the name for this process in spring / how does it work? I tried with just @Controller and it broke. Is this functionality coming from @RestController? 
||||||||||||||If you are using [Spring Boot Starter Web][1], you can see that it's using [Spring Boot Starter JSON][2] through the compile dependencies, and Jackson is the dependency of the Start JSON library. So, you're assumption is right (Spring is using Jackson for JSON conversion by default)

Spring use it's AOP mechanism to intercept the mapping methods in `@Controller` (you can see that [`@RestController`][3] is actually a `@Controller` with `@ResponseBody`), spring create a proxy object (using JDK proxy or through cglib) for the class that annotated with `@Controller`.

When the request flow is processing, the program who really call the mapping method will be lead to the proxy first, the proxy will invoke the real `@Controller` object's method and convert it's returning value to JSON String using Jackson Library (if the method is annotated with `@ResponseBody`) and then return the JSON String back to the calling program.


  [1]: https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-web/2.4.1
  [2]: https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-json/2.4.1
  [3]: https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/bind/annotation/RestController.html

--------------------------------------------------
mount: unknown filesystem type &#39;vmhgsf&#39;
I&#39;m trying to mount my Windows shared folder in CentOS using command: 

    ~mount -t vmhgfs .host:/shared-folder /var/www/html/

Unfortunately I get :

    ~mount: unknown filesystem type &#39;vmhgfs&#39;

error. I tried to use:

    ~/usr/bin/vmhgfs-fuse /mnt

but mountpoint is not empty...

Is there any way to mount this folder on VMware player?
||||||||||||||Cyb

Try this:

    vmhgfs-fuse .host:/shared-folder /var/www/html/
you might need to use **sudo** on this

--------------------------------------------------
Why do we require “requires requires”?
One of the corners of C++20 constraints is that there are certain situations in which you have to write `requires requires`. For instance, this example from [\[expr.prim.req\]/3](http://eel.is/c++draft/expr#prim.req-3):

&gt; A _requires-expression_ can also be used in a _requires-clause_ ([temp]) as a way of writing ad hoc constraints on template arguments such as the one below:
&gt;
&gt; 
    template&lt;typename T&gt;
      requires requires (T x) { x + x; }
        T add(T a, T b) { return a + b; }

&gt; The first requires introduces the _requires-clause_, and the second introduces the _requires-expression_. 

What is the technical reason behind needing that second `requires` keyword? Why can&#39;t we just allow writing:

    template&lt;typename T&gt;
      requires (T x) { x + x; }
        T add(T a, T b) { return a + b; }

&lt;sub&gt;(Note: please don&#39;t answer that the grammar `requires` it)&lt;/sub&gt;
||||||||||||||It is because the grammar requires it. It does.

A `requires` constraint does not *have to* use a `requires` expression. It can use any more-or-less arbitrary boolean constant expression. Therefore, `requires (foo)` must be a legitimate `requires` constraint.

A `requires` *expression* (that thing that tests whether certain things follow certain constraints) is a distinct construct; it's just introduced by the same keyword. `requires (foo f)` would be the beginning of a valid `requires` expression.

What you want is that if you use `requires` in a place that accepts constraints, you should be able to make a "constraint+expression" out of the `requires` clause.

So here's the question: if you put `requires (foo)` into a place that is appropriate for a requires constraint... how far does the parser have to go before it can realize that this is a requires *constraint* rather than a constraint+expression the way you want it to be?

Consider this:

````
void bar() requires (foo)
{
  //stuff
}
````

If `foo` is a type, then `(foo)` is a parameter list of a requires expression, and everything in the `{}` is not the body of the function but the body of that `requires` expression. Otherwise, `foo` is an expression in a `requires` clause.

Well, you could say that the compiler should just figure out what `foo` is first. But C++ *really* doesn't like it when the basic act of parsing a sequence of tokens requires that the compiler figure out what those identifiers mean before it can make sense of the tokens. Yes, C++ is context-sensitive, so this does happen. But the committee prefers to avoid it where possible.

So yes, it's grammar.

--------------------------------------------------
Jquery clone row and its all elements with different id
HTML Table whose 2nd row which I want to clone is&lt;br/&gt;

    &lt;table id=&quot;tblDoc&quot; class=&quot;doc-Table&quot;&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;label&gt;Document Description&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Custom&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;File Type&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Ref&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;label&gt;Document&lt;/label&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr id=&quot;uploadrow_0&quot;&gt;
        &lt;td&gt;
            &lt;asp:DropDownList ID=&quot;ddlDocumentDescription_0&quot; runat=&quot;server&quot;&gt;&lt;/asp:DropDownList&gt;
        &lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;txtCustomFileName_0&quot; type=&quot;text&quot; class=&quot;upload-TextBoxes&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;select id=&quot;ddlFileType_0&quot; class=&quot;upload-Dropdowns&quot;&gt;
                &lt;option value=&quot;0&quot;&gt;--Select--&lt;/option&gt;
                &lt;option value=&quot;1&quot;&gt;A&lt;/option&gt;
                &lt;option value=&quot;2&quot;&gt;B&lt;/option&gt;
            &lt;/select&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;txtReferenceNo_0&quot; type=&quot;text&quot; class=&quot;upload-TextBoxes&quot; /&gt;&lt;/td&gt;
        &lt;td&gt;
            &lt;input id=&quot;fileDocument_0&quot; class=&quot;file-upload&quot; type=&quot;file&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
    
&lt;div&gt;
    &lt;span id=&quot;addAnother&quot; class=&quot;add-another&quot;&gt;+ Add Another&lt;/span&gt;
&lt;/div&gt;


I want to make a copy of second row each time on add another button.So I  have used  &lt;br/&gt;

    $(document).ready(function () {
        $(&quot;#addAnother&quot;).click(function () {
            addAnotherRow();
        });
    });

    function addAnotherRow() {
        var row = $(&quot;#tblDoc tr:nth-child(2)&quot;).clone();
        $(&#39;#tblDoc&#39;).append(row);
    }

When I clone it give same id for second row.&lt;br/&gt;&lt;br/&gt;
I want second row with id:&lt;br/&gt;
1 - uploadrow_1&lt;br/&gt;
2 - ddlDocumentDescription_1 (Its a asp.net control so id will not look like this)&lt;br/&gt;
3 - txtCustomFileName_1&lt;br/&gt;
4 - ddlFileType_1&lt;br/&gt;
5 - txtReferenceNo_1&lt;br/&gt;
6 - fileDocument_1&lt;br/&gt;
and so on.&lt;br/&gt;&lt;br/&gt;
Thanks in advance for any help.


||||||||||||||http://jsfiddle.net/y7q6x4so/3/

Select the last row and add id incrementing by one all the time.    

        function addAnotherRow() {
            var row = $("#tblDoc tr").last().clone();
            var oldId = Number(row.attr('id').slice(-1));
            var id = 1 + oldId;
            
            
            row.attr('id', 'uploadrow_' + id );
            row.find('#txtCustomFileName_' + oldId).attr('id', 'txtCustomFileName_' + id);
            row.find('#ddlDocumentDescription_' + oldId).attr('id', 'ddlDocumentDescription_' + id);
            row.find('#ddlFileType_' + oldId).attr('id', 'ddlFileType_' + id);
            row.find('#txtReferenceNo_' + oldId).attr('id', 'txtReferenceNo_' + id);
            row.find('#fileDocument_' + oldId).attr('id', 'fileDocument_' + id);
            
            $('#tblDoc').append(row);
        }

![enter image description here][1]


  [1]: http://i.stack.imgur.com/zx5a2.png

--------------------------------------------------
Trying to create edge list (weighted) to create adjacency list
I am storing the open coords as two attributes in one list:
```
self.x, self.y = []
```
My attempt at an edge list (lifted from stack overflow lol):
```
edge = []
        for i in self.x, self.y:
            for j in i[1]:
                edge.append([i[0], j])
                for i in edge:
                    print(i)
```

Whenever I try this the error:
```
for j in i[1]:
TypeError: &#39;int&#39; object is not subscriptable
```
comes up.

I&#39;m guessing this is because it&#39;s a tuple? I am trying to create an adjacency list with weighted edges of the distance between the coords, but I haven&#39;t thought about adding the weights yet. 

The coords, when added to a big list, look like this:
```
[[[0, 0], [1, 0], [2, 0].....]]
```
But when I insert them into the class, I do it separately.

On another note, can I store the all coords into one attribute like this effectively? Or would the attribute overwrite each time and only do one coord??



I was expecting, or hoped, that it would create edges between nodes to then create an adjacency list (of which I have no clue how to code either). With the completed graph, I aim to create an a* algorithm..

Sorry, if this may be obvious but I haven&#39;t coded properly in a very long time. I am aware it is kinda messy.

Thank you.
||||||||||||||Something like this?

~~~python
import itertools
import math

class Graph:
    def __init__(self):
        self.coords = []  # ← array of coords tuples (x, y)
        self.adjacency_list = (
            {}
        )  # ^ dictionary (hash lookup structure of 'key': 'value' pairs):
        #      where each key is a tuple (x, y)
        #      and each value is a list of (neighbor, weight) tuples.

    def add_coord(self, x, y):
        self.coords.append((x, y))

    def calculate_distance(self, coord1, coord2):
        x1, y1 = coord1
        x2, y2 = coord2
        return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)

    def build_adjacency_list(self):
        for coord1 in self.coords:
            self.adjacency_list[coord1] = []
            for coord2 in self.coords:
                if coord1 != coord2:
                    distance = self.calculate_distance(coord1, coord2)
                    self.adjacency_list[coord1].append((coord2, distance))


# Demo adjacency list
g = Graph()
for (x, y) in list(itertools.product(range(3), range(3))):
    print(x, y)
    g.add_coord(x, y)
g.build_adjacency_list()

# Print out the adjacency list
for coord, neighbors in g.adjacency_list.items():
    print(f"\n{coord}: \n{neighbors}")
~~~

which prints:

~~~none
(0, 0): 
[((0, 1), 1.0), ((0, 2), 2.0), ((1, 0), 1.0), ((1, 1), 1.4142135623730951), ((1, 2), 2.23606797749979), ((2, 0), 2.0), ((2, 1), 2.23606797749979), ((2, 2), 2.8284271247461903)]

(0, 1): 
[((0, 0), 1.0), ((0, 2), 1.0), ((1, 0), 1.4142135623730951), ((1, 1), 1.0), ((1, 2), 1.4142135623730951), ((2, 0), 2.23606797749979), ((2, 1), 2.0), ((2, 2), 2.23606797749979)]

(0, 2): 
[((0, 0), 2.0), ((0, 1), 1.0), ((1, 0), 2.23606797749979), ((1, 1), 1.4142135623730951), ((1, 2), 1.0), ((2, 0), 2.8284271247461903), ((2, 1), 2.23606797749979), ((2, 2), 2.0)]

(1, 0): 
[((0, 0), 1.0), ((0, 1), 1.4142135623730951), ((0, 2), 2.23606797749979), ((1, 1), 1.0), ((1, 2), 2.0), ((2, 0), 1.0), ((2, 1), 1.4142135623730951), ((2, 2), 2.23606797749979)]

(1, 1): 
[((0, 0), 1.4142135623730951), ((0, 1), 1.0), ((0, 2), 1.4142135623730951), ((1, 0), 1.0), ((1, 2), 1.0), ((2, 0), 1.4142135623730951), ((2, 1), 1.0), ((2, 2), 1.4142135623730951)]

(1, 2): 
[((0, 0), 2.23606797749979), ((0, 1), 1.4142135623730951), ((0, 2), 1.0), ((1, 0), 2.0), ((1, 1), 1.0), ((2, 0), 2.23606797749979), ((2, 1), 1.4142135623730951), ((2, 2), 1.0)]

(2, 0): 
[((0, 0), 2.0), ((0, 1), 2.23606797749979), ((0, 2), 2.8284271247461903), ((1, 0), 1.0), ((1, 1), 1.4142135623730951), ((1, 2), 2.23606797749979), ((2, 1), 1.0), ((2, 2), 2.0)]

(2, 1): 
[((0, 0), 2.23606797749979), ((0, 1), 2.0), ((0, 2), 2.23606797749979), ((1, 0), 1.4142135623730951), ((1, 1), 1.0), ((1, 2), 1.4142135623730951), ((2, 0), 1.0), ((2, 2), 1.0)]

(2, 2): 
[((0, 0), 2.8284271247461903), ((0, 1), 2.23606797749979), ((0, 2), 2.0), ((1, 0), 2.23606797749979), ((1, 1), 1.4142135623730951), ((1, 2), 1.0), ((2, 0), 2.0), ((2, 1), 1.0)]
~~~

`itertools.product(range(3), range(3))` produces: 
~~~none
0 0
0 1
0 2
1 0
1 1
1 2
2 0
2 1
2 2
~~~

In calling the method to build the adjacency list, all coordinates-tuple combinatorial (cartesian product, specifically) pairwise (excluding self-pairs...) connections were added to the dictionary for each individual coordinates tuple. This is what is printed above as the output. Below this is visualized.

<hr>

## Visualize the adjacency list as a network graph

~~~python
import networkx as nx
from pyvis.network import Network

# Create a NetworkX graph
G_nx = nx.Graph()

# Add edges to the NetworkX graph
for coord, neighbors in g.adjacency_list.items():
    for neighbor, weight in neighbors:
        G_nx.add_edge(coord, neighbor, weight=weight)

# Create a Pyvis network
net = Network(
    notebook=True,
    cdn_resources="remote",
    width="100%",
    bgcolor="white",
    font_color="red",
)
net.repulsion()

# Add nodes to the Pyvis network
for coord in g.adjacency_list.keys():
    net.add_node(str(coord), size=5) # *

# Add edges to the Pyvis network
for coord, neighbors in g.adjacency_list.items():
    for neighbor, weight in neighbors:
        net.add_edge(str(coord), str(neighbor), weight=weight)


for edge in net.edges:
    source, target = edge["from"], edge["to"]
    weight = G_nx[eval(source)][eval(target)]["weight"]
    edge["label"] = str(round(weight, 2))


net.show("example.html")
~~~

><sup>* _**Note**_: The size of nodes is ok to explicitly set here because all nodes have the same number of edges anyways. By default, they come out in the visualized graph a bit too big IMO.</sup>

[![Networkx+pyvis graph of adjacency list][1]][1]

And also, visualizing a slightly different pyvis+networkx graph, where the calculated distances (used to represent 'weights' in the example here) have an influence (e.g., by instead setting `net.barnes_hut()`):

[![Same network graph different 'physics' setting][2]][2]


  [1]: https://i.stack.imgur.com/Bt9Sn.png
  [2]: https://i.stack.imgur.com/Fq17m.gif

--------------------------------------------------
ckeditor&#39;s popup input fields dont work when used with bootstrap 5 modal (ckeditor 4)
I have come across an error while using ckeditor in bootstrap 5 modal and it looks like it&#39;s a very known error and many have given solution for it for different bootstrap versions but i am not able to figure out one for bootstrap 5, please have a look.

Here is the problem with solution:- https://stackoverflow.com/a/31679096

Other similar problems:-

https://stackoverflow.com/questions/19570661/ckeditor-plugin-text-fields-not-editable

https://stackoverflow.com/questions/14420300/bootstrap-with-ckeditor-equals-problems/18554395#18554395

Mainly what would be the alternative of below line for bootstrap 5. $.fn.modal.Constructor.prototype.enforceFocus

If I search for it in bootstrap 4 js file I&#39;m able to find fn.modal.Constructor in there but not in bootstrap 5. Please if someone can recreate the verified solution in the above link according to bootstrap 5 it would be very appreciated. Thank you for your time.

[image describing problem][1]

Also few notes:- 
1. All the other input types like checkboxes and dropdown works but not just text field.

2. I have also tried removing tabindex=&quot;-1&quot; from bootstrap modal code but the problem remains.


  [1]: https://i.stack.imgur.com/X04EL.png


||||||||||||||Thanks for this. Saved me a lot of head scratching. As of Bootstrap 5.3, this requires a small tweak:

```
bootstrap.Modal.prototype._initializeFocusTrap = function () { return { activate: function () { }, deactivate: function () { } } };
```

--------------------------------------------------
Getting NameResolutionError: Failed to resolve &#39;oauth2.googleapis.com&#39; trying to upload file to Google Cloud Storage via app on local k8s cluster
I have Kubernetes deployment with the following config

```
resource &quot;kubernetes_deployment&quot; &quot;batch-producer&quot; {
  metadata {
    name = var.app-name
    namespace = var.k8s-namespace.metadata[0].name
    labels = {
      app = var.app-name
    }
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        app = var.app-name
      }
    }

    template {
      metadata {
        labels = {
          app = var.app-name
        }
      }

      spec {
        container {
          name  = var.app-name
          image = var.docker-image

          image_pull_policy = &quot;Never&quot;

          port { container_port = 80 }
          port { container_port = 443 }

          command = [
            &quot;sh&quot;,
            &quot;-exc&quot;,
            &lt;&lt;-EOT
            mkdir /secrets
            echo ${var.storage-sa-key} | base64 --decode &gt; ./secrets/gcp_creds.json
            python ./run.py
            
            EOT
            ,
            &quot;&quot;
          ]

          env_from {
            config_map_ref {
              name = &quot;batch-producer-config&quot;
            }
          }

          env {
            name = &quot;GOOGLE_APPLICATION_CREDENTIALS&quot;
            value = &quot;/secrets/gcp_creds.json&quot;
          }
        }
      }
    }
  }
}
```

And app should just write file to GCS
```
class GCSWriter(AbstractWriter):
    def __init__(self, properties: dict):
        self.storage_client = storage.Client() \
            .from_service_account_json(json_credentials_path=os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;])
        self.bucket_name = properties.get(&quot;bucket_name&quot;)
        self.logger = get_logger()

    def write(self, source_path):
        # time.sleep(100)
        bucket = self.storage_client.bucket(self.bucket_name)
        blob = bucket.blob(os.path.join(
            &quot;incomes_data_source&quot;,
            dt.today().strftime(&#39;%Y/%m/%d&#39;),
            os.path.split(source_path)[1]))
        blob.upload_from_filename(source_path)
        self.logger.info(&quot;File &#39;%s&#39; was uploaded to GCS successfully&quot;, source_path)
```
App deploys, but I&#39;m getting the following error after some time:
```
HTTPSConnectionPool(host=&#39;oauth2.googleapis.com&#39;, port=443): Max retries exceeded with url: /token (Caused by NameResolutionError(&quot;&lt;urllib3.connection.HTTPSConnection object at 0x7f993b606bd0&gt;: Failed to resolve &#39;oauth2.googleapis.com&#39; ([Errno -3] Temporary failure in name resolution)&quot;))
```


I tried to ping google.com or download random file via curl from pod - success.
I also tried to use existing access-key.json running same container just via docker - it also works and I can see uploaded file in GCS.
Looking for a clue how I can resolve this.
||||||||||||||Thanks to Vasilii Angapov comment I found this issue - https://github.com/docker/for-mac/issues/7110.

I didn't dive deep into root cause, I just followed recommendation for downgrading to CoreDNS-1.10.0.

```
kubectl edit deployment/coredns -n kube-system
```
Changed version from 1.11.1 to 1.10.0 and wait for deployment to restart.

Everything works after that in my setup (Docker Desktop v4.27.1, Kubernetes v1.29.1)

I also tried to launch same configuration via Rancher Desktop with Kubernetes v1.28.n, it also works well.

--------------------------------------------------
How to generate time based UUIDs?
I want to generate time-based universally unique identifier (UUID) in Java. 

The method [`java.util.UUID.randomUUID()`][1] generates a [UUID Version 4][2] where 122 of the 128 bits are from a [cryptographically-strong][3] random number generator. 

How to generate a [Version 1][4] (time based) UUID ? Is there a separate library for that or is it some how provided in the Java 7 API and I am missing it.


  [1]: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/UUID.html#randomUUID()
  [2]: https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_(random)
  [3]: https://en.wikipedia.org/wiki/Strong_cryptography#Cryptographically_strong_algorithms
  [4]: https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_1_(date-time_and_MAC_address)
||||||||||||||FasterXML Java Uuid Generator (JUG)

https://github.com/cowtowncoder/java-uuid-generator

    UUID uuid = Generators.timeBasedGenerator().generate();

--------------------------------------------------
Oracle with PHP on Docker
I&#39;m trying to install an Oracle database drive for my Laravel application. I&#39;m using Laravel Sail to provide Docker.

The problem is that the Oracle driver can&#39;t build. This message occurs:

```
/usr/bin/ld: skipping incompatible /opt/oracle/instantclient_19_14/libclntsh.so when searching for -lclntsh
/usr/bin/ld: cannot find -lclntsh
/usr/bin/ld: skipping incompatible /opt/oracle/instantclient_19_14/libclntsh.so when searching for -lclntsh
collect2: error: ld returned 1 exit status
make: *** [Makefile:227: oci8.la] Error 1
ERROR: `make&#39; failed
```

My dockerfile: https://pastebin.com/RTPWt1XK

I&#39;m using MacBook Pro (v. 12 with M1)
||||||||||||||Using the **Instant Client for Linux ARM** ([instantclient-basic-linux.arm64-19.10.0.0.0][1]) and **PHP 8.2**

This `dockerfile` works for me:

```
FROM ubuntu:22.04

LABEL maintainer="Taylor Otwell"

ARG WWWGROUP
ARG NODE_VERSION=18
ARG POSTGRES_VERSION=14

WORKDIR /var/www/html

ENV DEBIAN_FRONTEND noninteractive
ENV TZ=UTC

RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

RUN apt-get update \
    && apt-get install -y gnupg gosu curl wget ca-certificates zip unzip git supervisor sqlite3 libcap2-bin libpng-dev python2 dnsutils \
    && curl -sS 'https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x14aa40ec0831756756d7f66c4f4ea0aae5267a6c' | gpg --dearmor | tee /etc/apt/keyrings/ppa_ondrej_php.gpg > /dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/ppa_ondrej_php.gpg] https://ppa.launchpadcontent.net/ondrej/php/ubuntu jammy main" > /etc/apt/sources.list.d/ppa_ondrej_php.list \
    && apt-get update \
    && apt-get install -y php8.2-cli php8.2-dev \
       php8.2-pgsql php8.2-sqlite3 php8.2-gd \
       php8.2-curl \
       php8.2-imap php8.2-mysql php8.2-mbstring \
       php8.2-xml php8.2-zip php8.2-bcmath php8.2-soap \
       php8.2-intl php8.2-readline \
       php8.2-ldap \
       php8.2-msgpack php8.2-igbinary php8.2-redis php8.2-swoole \
       php8.2-memcached php8.2-pcov php8.2-xdebug \
    && php -r "readfile('https://getcomposer.org/installer');" | php -- --install-dir=/usr/bin/ --filename=composer \
    && curl -sLS https://deb.nodesource.com/setup_$NODE_VERSION.x | bash - \
    && apt-get install -y nodejs \
    && npm install -g npm \
    && curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | tee /etc/apt/keyrings/yarn.gpg >/dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/yarn.gpg] https://dl.yarnpkg.com/debian/ stable main" > /etc/apt/sources.list.d/yarn.list \
    && curl -sS https://www.postgresql.org/media/keys/ACCC4CF8.asc | gpg --dearmor | tee /etc/apt/keyrings/pgdg.gpg >/dev/null \
    && echo "deb [signed-by=/etc/apt/keyrings/pgdg.gpg] http://apt.postgresql.org/pub/repos/apt jammy-pgdg main" > /etc/apt/sources.list.d/pgdg.list \
    && apt-get update \
    && apt-get install -y yarn \
    && apt-get install -y mysql-client \
    && apt-get install -y postgresql-client-$POSTGRES_VERSION \
    && apt-get -y autoremove \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

ENV LD_LIBRARY_PATH="/opt/oracle/instantclient_19_10/"
ENV ORACLE_HOME="/opt/oracle/instantclient_19_10/"
ENV OCI_HOME="/opt/oracle/instantclient_19_10/"
ENV OCI_LIB_DIR="/opt/oracle/instantclient_19_10/"
ENV OCI_INCLUDE_DIR="/opt/oracle/instantclient_19_10/sdk/include"
ENV OCI_VERSION=19

# Download Oracle
RUN mkdir /opt/oracle \
    && cd /opt/oracle \
    && wget https://download.oracle.com/otn_software/linux/instantclient/191000/instantclient-basic-linux.arm64-19.10.0.0.0dbru.zip \
    && wget https://download.oracle.com/otn_software/linux/instantclient/191000/instantclient-sdk-linux.arm64-19.10.0.0.0dbru.zip \
    && unzip /opt/oracle/instantclient-basic-linux.arm64-19.10.0.0.0dbru.zip -d /opt/oracle \
    && unzip /opt/oracle/instantclient-sdk-linux.arm64-19.10.0.0.0dbru.zip -d /opt/oracle \
    && rm -rf /opt/oracle/*.zip \
    && echo /opt/oracle/instantclient_19_10 > /etc/ld.so.conf.d/oracle-instantclient.conf \
    && ldconfig

# Configure Oracle
RUN apt-get update \
    && apt-get install -y \
      php-dev \
      php-pear \
      build-essential \
      libaio1 \
      libaio-dev \
      freetds-dev
RUN pecl channel-update pecl.php.net \
    && echo 'instantclient,/opt/oracle/instantclient_19_10' | pecl install oci8 \
    && echo extension=oci8.so >> /etc/php/8.2/cli/php.ini \
    && echo "extension=oci8.so" >> /etc/php/8.2/mods-available/oci8.ini

RUN setcap "cap_net_bind_service=+ep" /usr/bin/php8.2

RUN groupadd --force -g $WWWGROUP sail
RUN useradd -ms /bin/bash --no-user-group -g $WWWGROUP -u 1337 sail

COPY start-container /usr/local/bin/start-container
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf
COPY php.ini /etc/php/8.2/cli/conf.d/99-sail.ini
RUN chmod +x /usr/local/bin/start-container

EXPOSE 8000

ENTRYPOINT ["start-container"]

```


  [1]: https://www.oracle.com/database/technologies/instant-client/linux-arm-aarch64-downloads.html

--------------------------------------------------
Combining filters between multiple columns
How to sum filter using conditions in multiple columns

        A      B          C        D
      Ben      1         Tom       1
      Joe      3         Ben       4
      Tom      2         Ben       1

I want to get the sum of B,D where A,C does not equal Joe...basically get everyone&#39;s hours except Joes.

update: f you on the negative score...the system does not let me preview the question before posting...so forced to edit on the fly.



This seems so simple but I have been racking my brain trying to get it to work...maybe an array?


UPDATE:  it was as simple as a SUMIF! I&#39;ve used sumif many times lol.  Thanks to googlesheetsguy
||||||||||||||Here's a possbile solution

```cpp
=INDEX(QUERY(WRAPROWS(TOCOL(A2:D4),2),"select sum(Col2) where Col1 <> 'Joe'"),2)
```

[![enter image description here][1]][1]

If you only have two ranges, you can also use:

```cpp
=SUM(FILTER({B2:B4;D2:D4},"Joe"<>{A2:A4;C2:C4}))
```

[![enter image description here][2]][2]

But if you have a lot it becomes unpractical.

  [1]: https://i.stack.imgur.com/iy9Ex.png
  [2]: https://i.stack.imgur.com/U7BRf.png

--------------------------------------------------
Laravel and ngrok: url domain is not correct for routes and assets
My setup:

- Homestead on Mac OSX with multiple sites configured
- I have one site setup using domfit.test as the local domain (auto mapped using hostsupdater)

My problem:

If I `vagrant ssh`, and then `share domfit.test` I get a random generated ngrok url as you&#39;d expect (http://whatever.ngrok.io), however when I access this URL all my resources / routes are being prefixed with `http://domfit.test/` (http://domfit.test/login for instance)

I&#39;ve tried the following:

- Setting APP_URL as the ngrok URL
- `php artisan config:clear`
- `php artisan cache:clear`
- `{{ url(&#39;login&#39;) }}`
- `{{ route(&#39;login&#39;) }}`

My understanding is that `url()` should return the actual URL that the browser requested (rather than using `APP_URL`) but it always returns `domfit.test`.

If I rename my site in `Homestead.yaml` (for example to `newdomfit.test`) and re-provision then this is the domain that `url()` and `route()` uses, regardless of my `APP_URL`. So the `Homestead.yaml` seems to be forcing that domain. Which begs the question - how are you meant to actually use the share functionality?

I&#39;m new to Laravel so I am not sure if all of this is expected behavior and I am misunderstanding something? 

I just want my links and resources in templates to work for local (`domfit.test`), shared (`ngrok`) and eventually production with the same piece of code. My worry is I will have to change all of my `route()` or `url()` references when I attempt to put this website live.

**EDIT BELOW**

OK I&#39;ve just tried again. Changed `APP_URL` for `ngrok`:

Searched my entire codebase for `domfit.test`, and only some random session files seem to have references:

code/domfit/storage/framework/sessions/

    APP_NAME=DomFit
    APP_VERSION=0.01
    APP_ENV=local
    APP_KEY=XXXX
    APP_DEBUG=true
    APP_URL=http://04b7beec.ngrok.io

Then in my Controller I have it doing this for some simple debugging:

    echo(url(&#39;/login&#39;));
    echo(route(&#39;login&#39;));
    echo($_SERVER[&#39;HTTP_HOST&#39;]);
    echo($_SERVER[&#39;HTTP_X_ORIGINAL_HOST&#39;]);

If I use the `ngrok` URL the output I get is:

    http://domfit.test/login
    http://domfit.test/login
    domfit.test
    04b7beec.ngrok.io

I don&#39;t understand how `$_SERVER[&#39;HTTP_HOST&#39;]` is returning the wrong url?

It looks like it could be related to this: https://github.com/laravel/valet/issues/342

**ANOTHER EDIT**

It looks like it has to do with Homestead&#39;s `share` command:

    function share() {
    if [[ &quot;$1&quot; ]]
    then
        ngrok http ${@:2} -host-header=&quot;$1&quot; 80
    else
        echo &quot;Error: missing required parameters.&quot;
        echo &quot;Usage: &quot;
        echo &quot;  share domain&quot;
        echo &quot;Invocation with extra params passed directly to ngrok&quot;
        echo &quot;  share domain -region=eu -subdomain=test1234&quot;
    fi
}

Which passes the option `-host-header` to `ngrok` which according to their documentation:

&gt; Some application servers like WAMP, MAMP and pow use the Host header for determining which development site to display. For this reason, ngrok can rewrite your requests with a modified Host header. Use the -host-header switch to rewrite incoming HTTP requests.

If I use `ngrok` without it, then the website that gets displayed is a different one (because I have multiple sites configured in Homestead) - so I&#39;m still not sure how to get around this. For the time being I could disable the other sites as I&#39;m not actively developing those.
||||||||||||||### Update for ngrok 3.0+

ngrok 3.0 stopped using the `X-Original-Host` header and started using the `X-Forwarded-Host` header.

Therefore, if using ngrok 3.0+ with TrustedProxies set to trust all proxies (`protected $proxies = '*';`), then there should be nothing else that needs to change.

However, if not using TrustedProxies, all the below information is still relevant, just replace any references of `HTTP_X_ORIGINAL_HOST` with `HTTP_X_FORWARDED_HOST`.

### For ngrok < 3.0

Even though you're going to the ngrok url, the host header in the request is still set as the name of your site. Laravel uses the host header to build the absolute url for links, assets, etc. ngrok includes the ngrok url in the `X-Original-Host` header, but Laravel doesn't know anything about that.

There are two basic solutions to the issue:

1. update the request with the proper server and header values, or
2. use the `forceRootUrl()` method to ignore the server and header values.

---

**TrustedProxies and Forwarded Host**

If you're using TrustedProxies (default in Laravel >= 5.5), and you have it configured to trust all proxies (`protected $proxies = '*';`), you can set the `X-Forwarded-Host` header to the `X-Original-Host` header. Laravel will then use the value in the `X-Forwarded-Host` header to build all absolute urls.

You can do this at the web server level. For example, if you're using apache, you can add this to your `public/.htaccess` file:

    # Handle ngrok X-Original-Host Header
    RewriteCond %{HTTP:X-Original-Host} \.ngrok\.io$ [NC]
    RewriteRule .* - [E=HTTP_X_FORWARDED_HOST:%{HTTP:X-Original-Host}]

If you prefer to handle this in your application instead of the web server, you will need to update the Laravel request. There are plenty of places you could choose to do this, but one example would be in your `AppServiceProvider::boot()` method:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $request->server->set('HTTP_X_FORWARDED_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
            $request->headers->set('X_FORWARDED_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

---

**Not Using TrustedProxies**

If you're not using TrustedProxies, you can't use the `.htaccess` method. However, you can still update the server and headers values in your application. In this case, you'd need to overwrite the Host header:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $request->server->set('HTTP_HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
            $request->headers->set('HOST', $request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

---

**Using `forceRootUrl()`**

If you don't want to modify any headers or the Laravel request, you can simply tell the URL generator what root url to use. The URL generator has a `forceRootUrl()` method that you can use to tell it to use a specific value instead of looking at the request. Again, in your `AppServiceProvider::boot()` method:

    public function boot(\Illuminate\Http\Request $request)
    {
        if ($request->server->has('HTTP_X_ORIGINAL_HOST')) {
            $this->app['url']->forceRootUrl($request->server->get('HTTP_X_FORWARDED_PROTO').'://'.$request->server->get('HTTP_X_ORIGINAL_HOST'));
        }
    }

--------------------------------------------------
Switching between GCC and Clang/LLVM using CMake
I have a number of projects built using CMake and I&#39;d like to be able to easily switch between using GCC or Clang/LLVM to compile them. I believe (please correct me if I&#39;m mistaken!) that to use Clang I need to set the following:

        SET (CMAKE_C_COMPILER             &quot;/usr/bin/clang&quot;)
        SET (CMAKE_C_FLAGS                &quot;-Wall -std=c99&quot;)
        SET (CMAKE_C_FLAGS_DEBUG          &quot;-g&quot;)
        SET (CMAKE_C_FLAGS_MINSIZEREL     &quot;-Os -DNDEBUG&quot;)
        SET (CMAKE_C_FLAGS_RELEASE        &quot;-O4 -DNDEBUG&quot;)
        SET (CMAKE_C_FLAGS_RELWITHDEBINFO &quot;-O2 -g&quot;)
        
        SET (CMAKE_CXX_COMPILER             &quot;/usr/bin/clang++&quot;)
        SET (CMAKE_CXX_FLAGS                &quot;-Wall&quot;)
        SET (CMAKE_CXX_FLAGS_DEBUG          &quot;-g&quot;)
        SET (CMAKE_CXX_FLAGS_MINSIZEREL     &quot;-Os -DNDEBUG&quot;)
        SET (CMAKE_CXX_FLAGS_RELEASE        &quot;-O4 -DNDEBUG&quot;)
        SET (CMAKE_CXX_FLAGS_RELWITHDEBINFO &quot;-O2 -g&quot;)
        
        SET (CMAKE_AR      &quot;/usr/bin/llvm-ar&quot;)
        SET (CMAKE_LINKER  &quot;/usr/bin/llvm-ld&quot;)
        SET (CMAKE_NM      &quot;/usr/bin/llvm-nm&quot;)
        SET (CMAKE_OBJDUMP &quot;/usr/bin/llvm-objdump&quot;)
        SET (CMAKE_RANLIB  &quot;/usr/bin/llvm-ranlib&quot;)

Is there an easy way of switching between these and the default GCC variables, preferably as a system-wide change rather than project specific (i.e. not just adding them into a project&#39;s CMakeLists.txt)?

Also, is it necessary to use the `llvm-*` programs rather than the system defaults when compiling using clang instead of gcc? What&#39;s the difference?
||||||||||||||CMake honors the environment variables `CC` and `CXX` upon detecting the C and C++ compiler to use:

    $ export CC=/usr/bin/clang
    $ export CXX=/usr/bin/clang++
    $ cmake ..
    -- The C compiler identification is Clang
    -- The CXX compiler identification is Clang

The compiler specific flags can be overridden by putting them into a make override file and pointing the [`CMAKE_USER_MAKE_RULES_OVERRIDE`][1] variable to it. Create a file `~/ClangOverrides.txt` with the following contents:

    SET (CMAKE_C_FLAGS_INIT                "-Wall -std=c11")
    SET (CMAKE_C_FLAGS_DEBUG_INIT          "-g")
    SET (CMAKE_C_FLAGS_MINSIZEREL_INIT     "-Os -DNDEBUG")
    SET (CMAKE_C_FLAGS_RELEASE_INIT        "-O3 -DNDEBUG")
    SET (CMAKE_C_FLAGS_RELWITHDEBINFO_INIT "-O2 -g")
    
    SET (CMAKE_CXX_FLAGS_INIT                "-Wall -std=c++17")
    SET (CMAKE_CXX_FLAGS_DEBUG_INIT          "-g")
    SET (CMAKE_CXX_FLAGS_MINSIZEREL_INIT     "-Os -DNDEBUG")
    SET (CMAKE_CXX_FLAGS_RELEASE_INIT        "-O3 -DNDEBUG")
    SET (CMAKE_CXX_FLAGS_RELWITHDEBINFO_INIT "-O2 -g")

The suffix `_INIT` will make CMake initialize the corresponding `*_FLAGS` variable with the given value. Then invoke `cmake` in the following way:

    $ cmake -DCMAKE_USER_MAKE_RULES_OVERRIDE=~/ClangOverrides.txt ..

Finally to force the use of the LLVM binutils, set the internal variable `_CMAKE_TOOLCHAIN_PREFIX`. This variable is honored by the `CMakeFindBinUtils` module:

    $ cmake -D_CMAKE_TOOLCHAIN_PREFIX=llvm- ..

Setting `_CMAKE_TOOLCHAIN_LOCATION` is no longer necessary for CMake version 3.9 or newer.

Putting this all together you can write a shell wrapper which sets up the environment variables `CC` and `CXX` and then invokes `cmake` with the mentioned variable overrides. 

Also see this [CMake FAQ][2] on make override files.


  [1]: https://cmake.org/cmake/help/latest/variable/CMAKE_USER_MAKE_RULES_OVERRIDE.html
  [2]: https://gitlab.kitware.com/cmake/community/-/wikis/FAQ#make-override-files

--------------------------------------------------
How to put each element of array into another array in same order
I have first array:

    Array
    (
        [0] =&gt; generala value 1
        [1] =&gt; specificatii value 1
    )

and second array is:

 

       Array
    (
        [0] =&gt; Array
            (
                [title] =&gt; generala title 1
                [atribute_cat_id] =&gt; 1
                [product_id] =&gt; 98
            )
    
        [1] =&gt; Array
            (
                [title] =&gt; specificatii title 1
                [atribute_cat_id] =&gt; 2
                [product_id] =&gt; 98
            )    
    )

I want to get this array form:

    Array
    (
        [0] =&gt; Array
            (
                [title] =&gt; generala title 1
                [atribute_cat_id] =&gt; 1
                [product_id] =&gt; 98
                [value] =&gt; generala value 1
            )
    
        [1] =&gt; Array
            (
                [title] =&gt; specificatii title 1
                [atribute_cat_id] =&gt; 2
                [product_id] =&gt; 98
                [value] =&gt; specificatii value 1
            )
    
    )

I tried with array_merge but this method put all elements from first array to each element from second array! 
Every time both arrays have same number of elements! 
Any ideea?
Thank you!
||||||||||||||Loop over one of the arrays, using the indexes to access the corresponding element in the other array.

```
foreach ($first_array AS $i => $value) {
    $second_array[$i]['value'] = $value;
}
```


--------------------------------------------------
A* algorithm only exploring a few nodes before stopping - without reaching goal node
I am attempting to implement the A* algorithm but it only goes to three nodes before just stopping completely.

This is the algorithm code:
```
def AStar(start_node, end_node):
    openSet = PriorityQueue()
    openSet.enequeue(0, start_node)

    infinity = float(&quot;inf&quot;)

    gCost = {}
    fCost = {}
    cameFrom = {}

    for node in graph:
        gCost[node] = infinity
        fCost[node] = infinity
    gCost[start_node] = 0
    fCost[start_node] = heuristic(start_node, end_node)

    while not openSet.isEmpty():
        current = openSet.dequeue()  # Doesn&#39;t work yet

        if current == end_node:
            RetracePath(cameFrom, end_node)

        for neighbour in find_neighbors(start_node, graph):
            tempGCost = gCost[current] + 1

            if tempGCost &lt; gCost[neighbour]:
                cameFrom[neighbour] = current
                gCost[neighbour] = tempGCost
                fCost[neighbour] = tempGCost + heuristic(neighbour, end_node)

                if not openSet.contains(neighbour):
                    openSet.enequeue(fCost[neighbour], neighbour)

        print(f&quot;Came from: {cameFrom}\nCurrent: {current}&quot;)
    return False
```

And this is the code that finds the adjacent nodes:
```

def find_neighbors(node, graph):
    x, y = node
    neighbors = []

    right_neighbor = (x + 1, y)
    left_neighbor = (x - 1, y)
    lower_neighbor = (x, y + 1)
    upper_neighbor = (x, y - 1)

    if right_neighbor in graph:
        neighbors.append(right_neighbor)
    if left_neighbor in graph:
        neighbors.append(left_neighbor)
    if lower_neighbor in graph:
        neighbors.append(lower_neighbor)
    if upper_neighbor in graph:
        neighbors.append(upper_neighbor)
```

And this is an example of what is being outputted:
```
Enemy coords: (6, 2)
Player coords: 10, 2
Enemy neighbours: [(7, 2), (6, 3)]
Priority Queue: [[0, (6, 2)]]
Priority Queue: [[4, (7, 2)]]
Priority Queue: [[4, (7, 2)], [6, (6, 3)]]
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (6, 2)
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (7, 2)
Came from: {(7, 2): (6, 2), (6, 3): (6, 2)}
Current: (6, 3)
```
Before it just stops the code without reaching the goal coordinates (player coords)

lmk if you have any questions about the code,
thank you.


||||||||||||||Your code check only the neighbors of the start node, when you call `find_neighbors` you always use `start_node` you should use `current` instead : 

```python
for neighbour in find_neighbors(current, graph):
    # your code
```

--------------------------------------------------
Why isn&#39;t my script printing all my results to the file?
I have the simple code below that loops through a dataframe and prints the results to the screen and also to a file.

My nag issue is however, it prints all the data to the screen just perfectly, but the file is only getting the last end of the data.

Here is my code:

    for star in Constellation_data(starDf.values.tolist()):
        print(star)
        sourceFile = open(&#39;stars.txt&#39;, &#39;w&#39;)
        print(star, file = sourceFile)
        sourceFile.close()

I open the file, then print to it, then close.  So I not sure why it doesn&#39;t contain all the data like the screen has.

Thanks!
||||||||||||||"w" deletes the existing file so for each iteration of the loop, you delete any previous content written. The normal way to handle this issue is to open the file once before the loop

    with open('stars.txt', 'w') as sourceFile:
        for star in Constellation_data(starDf.values.tolist()):
            print(star)
            print(star, file = sourceFile)

Note the `with` clause - it will automatically close the file when done.

If there is a reason why you want to close the file on each write (perhaps another file is reading it or you want to save state more often), then you can use append mode. I've added code to delete the old file and then append on each loop. The first append will create the file.

    if os.path.exists('stars.txt'):
        os.remove('stars.txt')
    for star in Constellation_data(starDf.values.tolist()):
        with open('stars.txt', 'a') as sourceFile:
            print(star)
            print(star, file = sourceFile)


--------------------------------------------------
DI resolves service by Interface of object and not by actual type
Let assume that we have following code

We have commands that all implement same interface
``` 
public interface ICommand {}
```
and that we have Command handlers that implement following interface 

``` 
public interface ICommandHandler&lt;T&gt; where T: ICommand {}
```

Actual implementations of command handlers are registered to DI. 

Now, if we have method that builds Commands based on some condition

```
public ICommand BuildCommand()
{
   if(someCondition) return new CommandA();
   else return new CommandB();
}
```

and we use it in code like this 
``` c#
public class SomeClass
{
  IServiceProvider _serviceProvder; 
  public void method_1()
  {
    ICommand command = BuildCommand();
    HandleCommand(command);
  }

  public void HandleCommand&lt;T&gt;(T command) 
  {
    var handler = _serviceProvider.GetRequiredService&lt;ICommandHandler&lt;T&gt;&gt;();
    handler.Handle();
  }
```
if will throw an error stating that it cannot resolve service 
``` ICommandHandler&lt;ICommand&gt; ```

I would expect DI to resolve by actual type (either CommandA or CommandB). How to get past this? 
||||||||||||||> I would expect DI to resolve by actual type (either CommandA or CommandB). How to get past this?

It can't resolve this, because of the following reasons:

* At compile time, you are supplying `ICommandHandler<ICommand>` to the `GetRequiredService<T>` method; it is given no runtime information that would allow it to spot anything different.
* When asked to resolve `ICommandHandler<ICommand>`, the container can't return anything else, because `ICommandHandler<ICommand>` is a different type to `ICommandHandler<CommandA>`. And even if it could, the request would even be ambiguous, because it could result in either an `ICommandHandler<CommandA>` *or* an `ICommandHandler<CommandB>`. Which one should it return?
* It's impossible to cast an `ICommandHandler<ICommand>` to `ICommandHandler<CommandA>` or vise versa in .NET, unless you make `ICommandHandler<T>` [variant][1] (i.e. you need to mark `T` with either `in` or `out`).

The solution here is to resort to using Reflection. For instance:

``` c#
public void HandleCommand(ICommand command) 
{
    Type handlerType = typeof(ICommandHandler<>).MakeGenericType(command.GetType());

    dynamic handler = _serviceProvider.GetRequiredService(handlerType);

    handler.Handle((dynamic)command);
}
```

In this example I'm using the `dynamic` keyword for simplicity. There are pros and cons to using this keyword. Obvious downside is of course loss of compile-time support. This is, IMO, not a big issue, because you would typically only have a single place in the application that calls command handlers using reflection, and that code can easily be tested. A more important downside, however, is that it will fail if a resolved handler implementation is internal - even when the `ICommandHandler<T>` is defined as `public`.

So instead of using dynamic, you can also invoke the method using the Reflection API:

```
public void HandleCommand(ICommand command) 
{
    Type handlerType = typeof(ICommandHandler<>).MakeGenericType(command.GetType());

    object handler = _serviceProvider.GetRequiredService(handlerType);

    MethodInfo method = handlerType.GetMethod("Handle");

    try
    {
        method.Invoke(handler, new object[] { command });
    }
    catch (TargetInvocationException ex)
    {
        ExceptionDispatchInfo.Capture(ex.InnerException).Throw();
    }
}
```


  [1]: https://learn.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/covariance-contravariance/variance-in-generic-interfaces

--------------------------------------------------
Generate P random N-dimensional points from list of ALL possible pairwise distances
I would like to generate random N-dimensional points with the constraint of having precise Euclidean distances (known) between each other.

Number of points `P = 100`
Number of dimensions of the hyperspace `N = 512`

Consequently, the possible number of pairwise distances is given by the formula `L = P*(P-1)/2`.
If `P = 100`, then `L = 4950`.

Let&#39;s say I have a list of 4950 distance values, where each value refers to a precise point-point combination.

Is it possible to implement this using numpy?

It is trivial to do it when considering pairs of points (`P = 2`) as `L = 1`, but I&#39;m trying to figure out if it can it be generalized to higher values of `P`?

This is my implementation for `P = 2`, considering `set_dist` as the desired distance value.

```
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances

N = 512

set_dist = 5.

point_0 = np.random.rand(N).reshape(1, -1)
point_1 = np.random.rand(N).reshape(1, -1)
rand_dist = euclidean_distances(point_0, point_1)
point_0 = point_0 * set_dist / rand_dist
point_1 = point_1 * set_dist / rand_dist
```
||||||||||||||With more spatial dimensions than points (i.e. *N* > *P*) this should be possible, if the distances are valid, which in particular means they have to satisfy the triangle inequality.

Let's take *N* = 3 for intuition. The first point you can pick anywhere. The distance between first and second defined a sphere around the first. The second has to lie somewhere on that sphere. The third has two distances to points you already placed. It has to lie on the intersection of the two corresponding spheres, which is a circle. A potential fourth point would lie on one of two points where three spheres intersect. For a fifth point you'd run out of dimensions, and rounding errors might make it difficult to satisfy all requirements simultaneously even if the distances originate from a real 3d configuration. That's why *N* > *P* is useful as you likely avoid this headache.

In terms of implementation, the above suggests that you would need to uniformly sample from hyperspheres of decreasing dimensions. You'd also have to inspect hyperspheres, and translate from the sample space to the actual positions. I don't know what rolls numpy has to offer to help with any of this.

Personally I'd also explore a different approach: generate the whole configuration in a simple well-defined position and orientation, then apply a random isometry (rotation, translation, perhaps reflection) to it. You would place the first point in the origin. The second point goes on the positive <i>x</i><sub>1</sub>-axis. The third on the <i>x</i><sub>1</sub>-<i>x</i><sub>2</sub>-plane with positive <i>x</i><sub>2</sub>-coordinate, and so on. So the number of zeros in the coordinate vector decreases by one for each point, and the newest coordinate is always positive. This should in general give you uniquely determined coordinates, shifting the whole randomisation to an operation on the complete configuration.

I haven't yet read the literature but I guess randomised sampling of isometries should have been discussed somewhere. But perhaps just applying a sequence of random operations, like some rotations around specific axes, will already make the result random enough? Depends on your requirements.

--------------------------------------------------
When I append items to a 2d list, it doesn&#39;t add to the same list
```
class PriorityQueue:
    def __init__(self):
        self.q = []

    def enqueue(self, priority, item):
        self.q.append([priority, item])
        self.q = sorted(self.q)
        return self.q


x = PriorityQueue()
print(x.enqueue(3, &quot;Potato&quot;))

y = PriorityQueue()
print(y.enqueue(1, &quot;Egg&quot;))
```

I&#39;m trying to do a priority list but it won&#39;t sort.

output;
```
[[3, &#39;Potato&#39;]]
[[1, &#39;Egg&#39;]]
```
How do I fix this?
||||||||||||||You are always creating a new `PriorityQueue`, I guess you want to have one queue:

    class PriorityQueue:
    
        def __init__(self):
            self.q = []
    
        def enqueue(self, priority, item):
            self.q.append([priority, item])
            self.q = sorted(self.q)
            return self.q
    
    
    y = PriorityQueue()
    print(y.enqueue(3, "Egg"))
    print(y.enqueue(4, "Potato"))
    print(y.enqueue(2, "Chesse"))
    print(y.enqueue(1, "Cake"))

Out:

    [[3, 'Egg']]
    [[3, 'Egg'], [4, 'Potato']]
    [[2, 'Cheese'], [3, 'Egg'], [4, 'Potato']]
    [[1, 'Cake'], [2, 'Cheese'], [3, 'Egg'], [4, 'Potato']]

--------------------------------------------------
Github workflow does not read variables from environments
Following is my simple github workflow. It is intended to print an environment variable. 
```
name: verify

on:
  workflow_dispatch:

jobs:
  read_env_variables:
    environment: build 
    runs-on: [ self-hosted, onprem_dae, docker ]
    steps:
      - name: cat on branch file
        run: |
          echo ${{ env.SOME_VARIABLE }}
```

I have created an environment named &quot;build&quot;. In this environment, I have an environment variable named `SOME_VARIABLE` set to *xyz*. 

When the workflow is triggered, I expected to echo value *xyz* but actual value is &quot;&quot;. Is there something missing? 

||||||||||||||Your issue here is related to the syntax.

To use the `${{ env.SOME_VARIABLE }}` syntax, you need to set an env variable at the workflow, job or step level.

**Here is an example:**

```yaml
name: Environment Workflow

on:
  workflow_dispatch:

env:
  WORKFLOW_VARIABLE: WORKFLOW

jobs:

  job1:
    runs-on: ubuntu-latest
    env:
      JOB_VARIABLE: JOB
    steps:
      - name: Run Commands with various variables
        if: ${{ env.WORKFLOW_VARIABLE == 'WORKFLOW' }}
        env:
          STEP_VARIABLE: STEP
        run: |
          echo "Hello World"
          echo "This is the $WORKFLOW_VARIABLE environment variable"
          echo "This is the $JOB_VARIABLE environment variable"
          echo "This is the $STEP_VARIABLE environment variable"
```

* * *

Now, if you want to use the **environment secrets for deployment**, [as explained here on the Github Documentation][1], the syntax would be different using the `job_id.environment` [as you are already using following this doc][2].

Here is an example:

```yaml
  job4:
    runs-on: ubuntu-latest
    environment: build
    steps:
      - name: Show repo env secret
        run: |
          echo ${{ secrets.REPO_ENV_SECRET }}
```

Note that **this variable is a secret**, therefore you won't be able to see it through an echo command on the step (it will show `***`)

* * *

Here is the workflow I used to validate all this implementation if you want to take a look:
- [workflow yaml file][3]
- [workflow run][4]


  [1]: https://docs.github.com/en/actions/deployment/targeting-different-environments/using-environments-for-deployment
  [2]: https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions#jobsjob_idenvironment
  [3]: https://github.com/GuillaumeFalourd/poc-github-actions/blob/main/.github/workflows/10-environment-workflow.yml
  [4]: https://github.com/GuillaumeFalourd/poc-github-actions/actions/runs/7297762218

--------------------------------------------------
How do I check the type of widget in GTK+3.0?
I saw [this][1] post but it was for Python so that doesn&#39;t help me too much. I&#39;m programming in C++, working on a code-base that I didn&#39;t write. I see some checks like `GTK_IS_ENTRY` and `GTK_IS_COMBO_BOX`, but I&#39;m not sure where this person found these or what other `GTK_IS_...` there are. Is there a reference to these somewhere? I searched online and also on the Gtk/GLib websites but I couldn&#39;t find anything. Thanks!


  [1]: https://stackoverflow.com/questions/60112777/find-type-of-gtk-widgets
||||||||||||||The type checks macros are typically part of the API contract for a GObject, and they are [conventionally provided by the library][1], so they don't end up in the documentation. All they do is call [`G_TYPE_CHECK_INSTANCE_TYPE`][2] with the given GType macro, like `GTK_TYPE_ENTRY` or `GTK_TYPE_COMBO_BOX`.


[1]: https://developer-old.gnome.org/gobject/stable/gtype-conventions.html
[2]: https://developer-old.gnome.org/gobject/stable/gobject-Type-Information.html#G-TYPE-CHECK-INSTANCE-TYPE:CAPS

--------------------------------------------------
Match all characters between two commas or between ,&quot; and &quot;, with regex powershell
Using powershell regex I would like it to find the first match between two commas or between ,&quot; and &quot;,

Example:

```
&quot;0x00000000&quot;,&quot;What do you want to eat? fish, meat or\n eggs?&quot;,&quot;&quot;
&quot;0x00030002&quot;,&quot;What do you want to eat?&quot;,&quot;&quot;
0x00030002,What do you want to eat?,
```

I want it to become:

```
What do you want to eat? fish, meat or eggs?
What do you want to eat?
What do you want to eat?
```




I tried this code but it doesn&#39;t behave correctly:

`(?&lt;=,&quot;|\?&lt;=,).*(?=&quot;,.*?|\?=,.*?)`

||||||||||||||Rather than using a regular expression for this (which has some pitfalls), I would use the native [`ConvertFrom-Csv` cmdlet](https://learn.microsoft.com/powershell/module/microsoft.powershell.utility/convertfrom-csv) for this:

    $List = @'
    "0x00000000","What do you want to eat? fish, meat or eggs?",""
    '"0x00030002","What do you want to eat?",""
    0x00000000,What do you want to eat? fish, meat or eggs?,
    0x00030002,What do you want to eat?,
    "0x00000000","What do you want to eat? ""fish"", ""meat"" or ""eggs?"""
    '@ -Split '\r?\n'

    $List | ConvertFrom-Csv -Header Hex, String, Rest | Select-Object -Expand String

    What do you want to eat? fish, meat or eggs?
    What do you want to eat?
    What do you want to eat? fish
    What do you want to eat?
    What do you want to eat? "fish", "meat" or "eggs?"

--------------------------------------------------
Why is Rails validator not using normalized value?
My model has a decimal amount attribute.

```
create_table :foos do |t|
  t.decimal :amount
end

class Foo &lt; ApplicationRecord
end
```

I always want the amount to be negative, so I add a normalisation:

```
class Foo &lt; ApplicationRecord
  normalizes :amount, with: -&gt; amount { - amount.abs }
end
```

This seems to work perfectly.

Now, to be safe, I add a validation:

```
class Foo &lt; ApplicationRecord
  normalizes :amount, with: -&gt; amount { - amount.abs }
  validates :amount, numericality: {less_than: 0}
end
```

Now when I set the amount to a positive value, although the normalisation converts it to a negative value, the validator seems to think the value is still positive and adds a validation error.

```
foo = Foo.new amount: 4
foo.amount  # =&gt; -4
foo.valid?  # =&gt; false
foo.errors  # =&gt; #&lt;ActiveModel::Error attribute=amount, type=less_than, options={:value=&gt;4, :count=&gt;0}&gt;
```

According to the tests for `normalizes`, [normalisation happens before validation](https://github.com/rails/rails/blob/0add5dba834f2f1b84fcf1bd1b758545b325fb73/activerecord/test/cases/normalized_attribute_test.rb#L35).

How can I get this to work?
||||||||||||||Numericality validator seems to be specifically using raw value for validation:  
*https://github.com/rails/rails/blob/v7.1.3/activemodel/lib/active_model/validations/numericality.rb#L129*

```rb
if record.respond_to?(came_from_user)
  if record.public_send(came_from_user)
    raw_value = record.public_send(:"#{attr_name}_before_type_cast")
```

Don't know if that is another bug or intentional. You could write your own validation to bypass this problem:

```rb
validate do
  errors.add(:amount, :less_than, value: amount, count: 0) unless amount.negative?
end
```

--------------------------------------------------
add recyclerview and cardview dependencies to gradle module
i want to add recyclerview and cardview dependencies to gradle module but it keeps giving error : the library should not use different version(25) then compile sdk version(26) ... i have latest updated android studio sdk version 26... here is code:apply plugin: &#39;com.android.application&#39;

    android {
        compileSdkVersion 26
        buildToolsVersion &quot;26.0.0&quot;
        defaultConfig {
            applicationId &quot;com.dpl_it.m.hamzam.widgets&quot;
            minSdkVersion 15
            targetSdkVersion 26
            versionCode 1
            versionName &quot;1.0&quot;
            testInstrumentationRunner &quot;android.support.test.runner.AndroidJUnitRunner&quot;
        }
        buildTypes {
            release {
                minifyEnabled false
                proguardFiles getDefaultProguardFile(&#39;proguard-android.txt&#39;), &#39;proguard-rules.pro&#39;
            }
        }
    }
    
    dependencies {
        compile fileTree(include: [&#39;*.jar&#39;], dir: &#39;libs&#39;)
        androidTestCompile(&#39;com.android.support.test.espresso:espresso-core:2.2.2&#39;, {
            exclude group: &#39;com.android.support&#39;, module: &#39;support-annotations&#39;
        })
        compile &#39;com.android.support:appcompat-v7:26.0.0&#39;
        compile &#39;com.android.support.constraint:constraint-layout:1.0.2&#39;
        testCompile &#39;junit:junit:4.12&#39;
        compile &#39;com.android.support:cardview-v7:25.4.0&#39;
        compile &#39;com.android.support:recyclerview-v7:25.4.0&#39;
    
    }


||||||||||||||Your dependency should be

    compile 'com.android.support:cardview-v7:26.0.0-beta2'
    compile 'com.android.support:recyclerview-v7:26.0.0-beta2'

but the support library for SDK 26 is in beta. See here for the recent notes

https://developer.android.com/topic/libraries/support-library/revisions.html

--------------------------------------------------
IntelliJ System.out.println() - Cannot resolve method println(java.lang.String)
I am using IntelliJ IDEA, learning Java. All went well until yesterday, when the mentioned error occurred.

I didn&#39;t make any changes. I was looking for the solution the following ways:

1. reboot the pc
2. restart IntelliJ.
3. delete the project directory and use another one (both on desktop) 

nothing helps. buy running simple hello world method. It keeps showing this error:

[![screenshot][1]][1]

Is there someone able to help me?


  [1]: http://i.stack.imgur.com/yyvkp.jpg
||||||||||||||ok, is solved.

file -> invalidated caches / Restart

--------------------------------------------------
CSS: Control space between bullet and &lt;li&gt;
I&#39;d like to control how much horizontal space a bullet pushes its `&lt;li&gt;` to the right in an `&lt;ol&gt;` or `&lt;ul&gt;`.

That is, instead of always having

    *  Some list text goes
       here.

I&#39;d like to be able to change that to be

    *         Some list text goes
              here.

or
 
    *Some list text goes
     here.

I looked around but could only find instructions for shifting the entire block left or right, for example, http://www.alistapart.com/articles/taminglists/
||||||||||||||Put its content in a `span` which is relatively positioned, then you can control the space by the `left` property of the `span`.

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-css -->

    li span {
      position: relative;
      left: -10px;
    }

<!-- language: lang-html -->

    <ul>
      <li><span>item 1</span></li>
      <li><span>item 2</span></li>
      <li><span>item 3</span></li>
    </ul>

<!-- end snippet -->



--------------------------------------------------
Is it possible for a client to receive an http response but the server not be certain that it did?
Is there every a case, with HTTP or HTTPS, where a server sends an HTTP response to a client, the client gets the response in full, but the server cannot be certain that the client got the response in full, for example if the final ACK or FIN message from the client wasn&#39;t received by the server?

And, if so, what are the conditions under which this might occur?

I looked through several RFCs and googled around, but couldn&#39;t find any relevant answers.
||||||||||||||Let's ask slightly different questions:

* Q1: Is it possible for the server to determine its response was successfully sent?
  
  A: Yes.

* Q2: Is it possible for the server to detect an error occurred sending its response?

  A: Yes.

* Q3: What happens if the TCP/IP connection abnormally terminates before a message is completely received by the client?

  A: Both the client and the server get a RST.

The answer to Q1 and Q2 is "Yes" on either/both of two different levels:

  * [TCP/IP connection level](https://accedian.com/blog/close-tcp-sessions-diagnose-disconnections): the connection is closed gracefully ... or not.

  * [HTTP "Persistent Connection" (RFC 2616)](https://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html): provides additional Application Layer error reporting capabilities.


--------------------------------------------------
Shadcn UI installation breaks Tailwind CSS
Shadcn UI (https://ui.shadcn.com/) was working fine until I just for a couple weeks until yesterday, when I ran my NextJS app in my local host and none of the tailwind was working. To debug the issue, I created a blank NextJS 13 app in a completely new file location, and everything worked fine; tailwind was working on the default nextJS 13 page. I then ran

```
npx shadcn-ui init
```

without installing any of the components. Which did not spit out any errors, but then none of the tailwind styling worked anymore.

my tailwind.config.js after instillation:

```
/** @type {import(&#39;tailwindcss&#39;).Config} */
module.exports = {
  darkMode: [&quot;class&quot;],
  content: [
    &#39;./pages/**/*.{ts,tsx}&#39;,
    &#39;./components/**/*.{ts,tsx}&#39;,
    &#39;./app/**/*.{ts,tsx}&#39;,
	],
  theme: {
    container: {
      center: true,
      padding: &quot;2rem&quot;,
      screens: {
        &quot;2xl&quot;: &quot;1400px&quot;,
      },
    },
    extend: {
      colors: {
        border: &quot;hsl(var(--border))&quot;,
        input: &quot;hsl(var(--input))&quot;,
        ring: &quot;hsl(var(--ring))&quot;,
        background: &quot;hsl(var(--background))&quot;,
        foreground: &quot;hsl(var(--foreground))&quot;,
        primary: {
          DEFAULT: &quot;hsl(var(--primary))&quot;,
          foreground: &quot;hsl(var(--primary-foreground))&quot;,
        },
        secondary: {
          DEFAULT: &quot;hsl(var(--secondary))&quot;,
          foreground: &quot;hsl(var(--secondary-foreground))&quot;,
        },
        destructive: {
          DEFAULT: &quot;hsl(var(--destructive))&quot;,
          foreground: &quot;hsl(var(--destructive-foreground))&quot;,
        },
        muted: {
          DEFAULT: &quot;hsl(var(--muted))&quot;,
          foreground: &quot;hsl(var(--muted-foreground))&quot;,
        },
        accent: {
          DEFAULT: &quot;hsl(var(--accent))&quot;,
          foreground: &quot;hsl(var(--accent-foreground))&quot;,
        },
        popover: {
          DEFAULT: &quot;hsl(var(--popover))&quot;,
          foreground: &quot;hsl(var(--popover-foreground))&quot;,
        },
        card: {
          DEFAULT: &quot;hsl(var(--card))&quot;,
          foreground: &quot;hsl(var(--card-foreground))&quot;,
        },
      },
      borderRadius: {
        lg: &quot;var(--radius)&quot;,
        md: &quot;calc(var(--radius) - 2px)&quot;,
        sm: &quot;calc(var(--radius) - 4px)&quot;,
      },
      keyframes: {
        &quot;accordion-down&quot;: {
          from: { height: 0 },
          to: { height: &quot;var(--radix-accordion-content-height)&quot; },
        },
        &quot;accordion-up&quot;: {
          from: { height: &quot;var(--radix-accordion-content-height)&quot; },
          to: { height: 0 },
        },
      },
      animation: {
        &quot;accordion-down&quot;: &quot;accordion-down 0.2s ease-out&quot;,
        &quot;accordion-up&quot;: &quot;accordion-up 0.2s ease-out&quot;,
      },
    },
  },
  plugins: [require(&quot;tailwindcss-animate&quot;)],
}
```

my utils.ts after instillation

```
import { ClassValue, clsx } from &quot;clsx&quot;
import { twMerge } from &quot;tailwind-merge&quot;
 
export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
```

[the default page after instillation](https://i.stack.imgur.com/Ta8jG.png)

EDIT: after some testing, the issue seems to be coming from the globals.css and tailwind.config.js, still not sure what about them though.
||||||||||||||In my case the Shadcn components did not find the styles exported by tailwind.
As answered by @moyindavid the problem is in tailwind.config.

Solution:
Add the '@' folder to the exports of the tailwind attributes.

```
module.exports = {
   darkMode: ["class"],
   content: [
     './pages/**/*.{ts,tsx}',
     './components/**/*.{ts,tsx}',
     './app/**/*.{ts,tsx}',
     './@/**/*.{ts,tsx}', // <- HERE
     ],
```

--------------------------------------------------
How to disable/enable the row in table with the same button?
In my table I have one buttons . In the table I want to disable/enable the entire row with the help of the same button. the button default is enable.
I want to each row all can enable/disable when click the button of the row .
When I click on &#39;enable&#39; button the entire row color will change to red and the button value change to &#39;disable&#39;. 
 click again to the &#39;disable&#39; button , thenthe entire row color recovery and the button value change to &#39;enable&#39;. 
How to do it . Help needed.

  part of  my code : 
[jsfiddle][1]


  [1]: https://jsfiddle.net/rZqLX/1/
    
    &lt;table &gt;
    &lt;tr&gt;
        &lt;th&gt;Value1&lt;/td&gt;
        &lt;th&gt;Value2&lt;/td&gt;
        &lt;th&gt;
            &lt;input type=&quot;button&quot; value=&quot;enable&quot; /&gt;
        &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Value3&lt;/td&gt;
        &lt;th&gt;Value4&lt;/td&gt;
        &lt;th&gt;
            &lt;input type=&quot;button&quot; value=&quot;enable&quot; /&gt;
        &lt;/th&gt;
    &lt;/tr&gt;
    &lt;/table&gt;


||||||||||||||You could change the value of the button and then check if it's 'enable' or 'disable'

Here's the code:

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    $('td input[type="button"]').on('click', function() {
        $(this).val((_, val) => val == "enable" ? "disable" : "enable");
        $(this).closest('tr').toggleClass('selected');
    });

<!-- language: lang-css -->

    .selected {
        background-color:red;
    }
    table {
        padding:0px;
        border-collapse: collapse;
    }

<!-- language: lang-html -->

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
    <table>
        <tr>
            <td>Value1</td>
            <td>Value2</td>
            <td>
                <input type="button" value="enable" />
            </td>
        </tr>
        <tr>
            <td>Value3</td>
            <td>Value4</td>
            <td>
                <input type="button" value="enable" />
            </td>
        </tr>
    </table>

<!-- end snippet -->



--------------------------------------------------
MySql error: incompatible with sql_mode=only_full_group_by
I&#39;ve inherited a CodeIgniter query that generates this SQL

EXAMPLE:

    SELECT `users`.`id`, `users`.`username`, `users`.`email`, `users`.`photo`, `users`.`rating`
    FROM `pool_details`
    JOIN `users` ON `users`.`id` = `pool_details`.`captain_id`
    WHERE `pool_details`.`pool_type` =0
    AND `pool_details`.`pool_close` &gt; &#39;2020-01-02 18:39:42&#39;
    GROUP BY `pool_details`.`captain_id`
    ORDER BY `pool_details`.`members_count` DESC
    LIMIT 20

&gt; ERROR - 2020-01-02 18:39:42 --&gt; Query error: Expression #1 of ORDER BY
&gt; clause is not in GROUP BY clause and contains nonaggregated column
&gt; &#39;pool_details.members_count&#39; which is not functionally
&gt; dependent on columns in GROUP BY clause; this is incompatible with
&gt; sql_mode=only_full_group_by - Invalid query:

Here is the same data with some &quot;extra columns&quot; and the offending clauses removed:

    SELECT users.id, users.username, users.email,users.rating,pool_details.members_count,pool_details.pool_type,pool_details.pool_close
    FROM  pool_details
    JOIN  users ON users.id = pool_details.captain_id
    WHERE pool_details.pool_type = 0 AND pool_details.pool_close &gt; &#39;2020-01-02 18:39:42&#39;
    ORDER BY pool_details.members_count DESC;
    //GROUP BY `pool_details`.`captain_id`
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    | id | username | email                    | rating | members_count | pool_type | pool_close          |
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         | 2020-01-04 03:00:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         | 2020-01-03 23:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |           100 | 0         
    ...
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 00:00:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 00:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-04 21:30:00 |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |             0 | 0         | 2020-01-03 03:00:00 |
    +----+----------+--------------------------+--------+---------------+-----------+---------------------+
    28 rows in set (0.00 sec)

What I want is to:

  1. Show User&#39;s username, email and rating
  2. Order by &quot;members_count&quot;
  3. Show the user only *once*

For example:

    SELECT DISTINCT users.id, users.username, users.email,users.rating
    FROM  pool_details
    JOIN  users ON users.id = pool_details.captain_id
    WHERE pool_details.pool_type = 0 AND pool_details.pool_close &gt; &#39;2020-01-02 18:39:42&#39;;
    +----+----------+--------------------------+--------+
    | id | username | email                    | rating |
    +----+----------+--------------------------+--------+
    |  5 | wheel    | wheel@boxpik.com         | NULL   |
    | 13 | Ronaldo  | pulisicblues07@gmail.com | 4.6    |
    +----+----------+--------------------------+--------+
    2 rows in set (0.00 sec)
    &lt;= This shows the users individually ... but it&#39;s *NOT* ordered by &quot;members_count&quot;.

Q: Is there any combination of &quot;GROUP BY&quot; and/or &quot;DISTINCT&quot; that I can use with mySql 5.7 that will give me the result set I need?


||||||||||||||Other than the `ORDER BY`, you seem to just want `EXISTS`.  That said, you can use another subquery in the `ORDER BY`:

    SELECT u.*
    FROM users u
    WHERE EXISTS (SELECT 1
                  FROM pool_details pd
                  WHERE pd.captain_id = u.id AND
                        pd.pool_type = 0 AND
                        pd.pool_close > '2020-01-02 18:39:42'
                 )
    ORDER BY (SELECT MAX(pd2.members_count)
              FROM pool_details pd2
              WHERE pd2.captain_id = u.id AND
                    pd2.pool_type = 0 AND
                    pd2.pool_close > '2020-01-02 18:39:42'
             ) DESC
    LIMIT 20

EDIT:

You can also write this as:

    SELECT u.*,
           (SELECT MAX(pd2.members_count)
            FROM pool_details pd2
            WHERE pd2.captain_id = u.id AND
                  pd2.pool_type = 0 AND
                  pd2.pool_close > '2020-01-02 18:39:42'
          ) as max_members_count
    FROM users u
    HAVING max_member_count IS NOT NULL
    ORDER BY max_member_count DESC
    LIMIT 20

Or:

    SELECT u.*
    FROM users u JOIN
         (SELECT pd2.captain_id, MAX(pd2.members_count) as max_member_count
          FROM pool_details pd2
          WHERE pd2.pool_type = 0 AND
                pd2.pool_close > '2020-01-02 18:39:42'
         ) pd
         ON pd.captain_id = u.id
    ORDER BY max_member_count DESC
    LIMIT 20l


--------------------------------------------------
Using a java class to create a database
I&#39;m working in an application that uses servlets and mysql.

I&#39;d like to create a .jar file able to create the database that the application will be using. This will only be done once, in order to create the db.

I&#39;ve no problem in getting to access to a database, doing something like this:

    Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance();
    Connection conexion = (Connection)DriverManager.getConnection(&quot;jdbc:mysql://localhost/test&quot;,&quot;admin&quot;,&quot;admin&quot;);
    if (!conexion.isClosed())
    {	
       Statement st = (Statement) conexion.createStatement();
       ResultSet rs = st.executeQuery(&quot;select * from table_name&quot; );
    }
    conexion.close();

This is ok, but what I need to do is to create a new database (and its tables) from a java class, is that possible?

I&#39;m trying this:		

    Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance();
    Connection conexion = (Connection)DriverManager.getConnection(&quot;jdbc:mysql://localhost/mysql&quot;,&quot;admin&quot;,&quot;admin&quot;);
		
    Statement st = (Statement) conexion.createStatement();       
    st.executeUpdate(&quot;CREATE DATABASE hrapp&quot;);

but I&#39;m getting the following error:

    Exception in thread &quot;main&quot; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Access denied for user &#39;admin&#39;@&#39;localhost&#39; to database &#39;hrapp&#39;
    	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    	at java.lang.reflect.Constructor.newInstance(Unknown Source)
    	at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    	at com.mysql.jdbc.Util.getInstance(Util.java:381)
    	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1030)
    	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
    	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3491)
    	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3423)
    	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1936)
    	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2060)
    	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2536)
    	at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1564)
    	at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1485)
    	at BaseDatosSetup.BaseDatosSetup.main(BaseDatosSetup.java:18)

I solved it by granting the create action to the user. I don&#39;t know why, I was doing it as an administrator.
||||||||||||||W3CSchools.com -- [SQL CREATE DATABASE Statement][1]. You wouldn't use `executeQuery` though. Instead use `executeUpdate`.

[Here][2] is a simple example.

As mentioned by other users, you probably don't want to be creating databases from your code. It just isn't good practice.


  [1]: http://www.w3schools.com/SQl/sql_create_db.asp
  [2]: http://www.java2s.com/Code/Java/Database-SQL-JDBC/CreateDatabaseforMySQL.htm

--------------------------------------------------
Server Error in &#39;/&#39; Application. Object reference not set to an instance of an object
I&#39;ve been trying to figure it out but not getting it.

**Description:** An unhandled exception occurred during the execution of the current web request. Please review the stack trace for more information about the error and where it originated in the code.

**Exception Details:**

    System.NullReferenceException: Object reference not set to an instance of an object.

**Source Error:**

An unhandled exception was generated during the execution of the current web request. Information regarding the origin and location of the exception can be identified using the exception stack trace below.

**Stack Trace:**


    [NullReferenceException: Object reference not set to an instance of an object.]
       Microsoft.WebTools.BrowserLink.Runtime.Tracing.PageInspectorHttpModule.OnPreRequestHandlerExecute(Object sender, EventArgs e) +662
       System.Web.SyncEventExecutionStep.System.Web.HttpApplication.IExecutionStep.Execute() +141
       System.Web.HttpApplication.ExecuteStepImpl(IExecutionStep step) +74
       System.Web.HttpApplication.ExecuteStep(IExecutionStep step, Boolean&amp; completedSynchronously) +92
||||||||||||||If you are using VS2019, make sure to uncheck Enable Browser Link
[VSCode EnableBroswerLink][1]


  [1]: https://i.stack.imgur.com/xlU6n.png

--------------------------------------------------
How to create a PDF/A from command line with Libre Office Draw in headless mode?
LibreOffice Draw allows you to open a non PDF/A file and export this a PDF/A-1b or PDF/A-2b file.

[![export as PDF][1]][1]

The same is possible from the command line by calling on macOS

```bash
/Applications/LibreOffice.app/Contents/MacOS/soffice --headless \
        --convert-to pdf:draw_pdf_Export \
        --outdir ./pdfout \
        ./input-non-pdfa.pdf
```

or an a Linux simply

```bash
libreoffice --headless \
        --convert-to pdf:draw_pdf_Export \
        --outdir ./pdfout \
        ./input-non-pdfa.pdf
```

On the command line it is possible to tell the `convert-to` to create a pdf and use LibreOffice Draw to do this by telling `--convert-to pdf:draw_pdf_Export`. 

Is there also a way to tell LibreOffice to produce a PDF/A document in **headless** mode?

  [1]: https://i.stack.imgur.com/mpSA6.png
||||||||||||||For PDF/A-1(means `PDF/A-1b`?):
```
soffice --headless --convert-to pdf:"writer_pdf_Export:SelectPdfVersion=1" --outdir outdir input.pdf
```
Change the value from `1` to `2` for PDF/A-2, here is the Libreoffice source code [Common.xcs](https://github.com/LibreOffice/core/blob/d4f5299fd2806d8f5dcd467742effeaa0dee8863/officecfg/registry/schema/org/openoffice/Office/Common.xcs#L5417-L5449), [pdfexport.cxx](https://github.com/LibreOffice/core/blob/e83b5f6a015269ed7e5407a8440c0fc99fcfa397/filter/source/pdf/pdfexport.cxx#L590-L623) and [pdffilter.cxx](https://github.com/LibreOffice/core/blob/bdbb5d0389642c0d445b5779fe2a18fda3e4a4d4/filter/source/pdf/pdffilter.cxx#L85).

- (Maybe outdated) [API/Tutorials/PDF export - Apache OpenOffice Wiki](https://wiki.openoffice.org/wiki/API/Tutorials/PDF_export)
- [Python Guide - PDF export filter data - The Document Foundation Wiki](https://wiki.documentfoundation.org/Macros/Python_Guide/PDF_export_filter_data)
- [excel->pdf変換 command のdpi設定 - Ask LibreOffice](https://ask.libreoffice.org/ja/question/229354/excel-pdfbian-huan-command-nodpishe-ding/)
- [Change default resolution in batch PNG conversion [closed] - Ask LibreOffice](https://ask.libreoffice.org/en/question/68775/change-default-resolution-in-batch-png-conversion/)

--------------------------------------------------
how extract &#39;cancellationDate&#39; this:
	
    [&quot;{\&quot;RequestedByUser\&quot;:false,\&quot;RequestedBySystem\&quot;:null,\&quot;RequestedBySellerNotification\&quot;:null,\&quot;RequestedByPaymentNotification\&quot;:true,\&quot;Reason\&quot;:null,\&quot;CancellationDate\&quot;:\&quot;2024-01-16T00:40:59.0928615+00:00\&quot;}&quot;]

tryingjson_extract, jsonpath and nothing
||||||||||||||We have a JSON string inside a list. If you want to extract values from this JSON string, you can use the json module in Python. 

Try like this 

```python
import json

# Your input data
data = ["{\"RequestedByUser\":false,\"RequestedBySystem\":null,\"RequestedBySellerNotification\":null,\"RequestedByPaymentNotification\":true,\"Reason\":null,\"CancellationDate\":\"2024-01-16T00:40:59.0928615+00:00\"}"]

# Assuming there is only one element in the list, you can access it using data[0]
json_data = json.loads(data[0])
cancellation_date = json_data["CancellationDate"]
print("CancellationDate:", cancellation_date)
```

Output:

```
CancellationDate: 2024-01-16T00:40:59.0928615+00:00
```


--------------------------------------------------
Is it in line with the DCO that a github sign-off needs and publishes full name + an email &quot;that matches the commit author&quot;?
Coming from Stack Overflow where a pseudo name is normal and enough, a github beginner like me does not expect to have to sign-off a git pull request with the full name and kind of full-name-email being published. Going over to github, I simply do not expect more than what Stack Overflow is asking for. I thought the other contributors on github just chose willingly to sign with their full names and respective e-mails, and I was astonished to see my personal mail being published.

The tasks you follow to do the pull request on github (not from the DCO, this was just a helping comment):

&gt; You need sign-off your PR with your email address. Below are steps to
&gt; sign-off a commit. At first, you need configure your git with user
&gt; name and email: `git config --global user.name &quot;FIRST_NAME LAST_NAME&quot;`
&gt; `git config --global user.email &quot;MY_NAME@example.com&quot;`
&gt; Next run `git push --force-with-lease origin YOURBRANCHNAME`



I have read the DCO Developer Certificate of Origin now in the github version https://github.com/apps/dco and in the original version https://developercertificate.org/.

The github version asks for more than the original DCO, in my opinion.
&gt; It requires all commit messages to contain the Signed-off-by line with an email address that matches the commit author.


further below...


&gt; Contributors sign-off that they adhere to these requirements by adding a Signed-off-by line to commit messages.
&gt; This is my commit message
&gt; 
&gt; Signed-off-by: Random J Developer &lt;random@developer.example.org&gt;

Here you could already discuss if &quot;Random J Developer&quot; has to be the full name or just a pseudo name, and also whether the name (or pseudo name respectively) should be part of the mail. The original DCO speaks just generally of the personal information in the sign-off:


&gt; I understand and agree that this project and the contribution are
&gt; public and that a record of the contribution (including all personal
&gt; information I submit with it, including my sign-off) is maintained
&gt; indefinitely and may be redistributed consistent with this project or
&gt; the open source license(s) involved.

In its intro, the github DCO mentions the email that &quot;matches the commit author&quot; as the core of the personal information, and later adds the name in the example. This &quot;matches the commit author&quot; is already a stricter requirement than the original DCO is asking for, thus this requirement could already be questioned. From the original DCO I read the option to put your full name and full name email, but not the need to do so, as the github user name and a mail that includes the github author name would be personal information enough to identify you as well, which is the main requirement. From the github DCO I read the wish that you put your full name, but it is only in the example, not in the text, and I could also go around that now by putting my github username and an email that does not show my full name but includes my github name, and still following the DCO, as I read it.

My final question after this long explanation:
Is the github DCO requirement of full name and an &quot;email address that matches the commit author&quot; in line with the official DCO? Or does it ask too much, and a pseudo name + email using that pseudo name would be already enough? Or as a third option, would a pseudo name + email not using any pseudo or full name already be enough?


p.s.:

To anyone of github reading this. If publishing the full name and respective email is really needed, I simply would like to be informed about this when doing my first pull request, because few people will read the DCO before starting.
||||||||||||||There are two items here which are separate and different.  One is the commit metadata which is stored in your commits, which is set with `user.name` and `user.email`.  This information is embedded by Git (not GitHub) in all commits you make so that people know who the author and committer are.

It is not required that `user.name` reflect your personal name, but it is customary.  Some projects, such as Git, strongly prefer that people use their personal name unless they are primarily known by a pseudonym, such as chromatic, the Perl contributor.  Other projects do not care.

GitHub itself does not impose any restrictions on either one (other, possibly, than that it be in UTF-8) but it does use the email address embedded in your commits (the `user.email` value) to attribute the commit to your account.  If the commit email address doesn't match any account, then it displays it as associated with the name (`user.name`) value you've specified.

In addition, some projects, such as Git, require that authors use the sign-off functionality (`git commit -s`) to intentionally state that they grant the rights to the project stated in the Developer's Certificate of Origin (or another project-specific document).  This is essentially a legal statement that the contributor has the right to contribute that code to the project.  The fact that this is a legal statement is why many projects prefer a personal name rather than a pseudonym.

When you use the `-s` (`--signoff`) option to `git commit`, Git embeds both the `user.name` and `user.email` value into the commit message as a sign-off in the format normally used inside the commit object.  A sign-off usually cannot be parsed without both.

A contributor who submits someone else's code to a project using the DCO needs to sign-off that commit.  So, for example, the Git for Windows maintainer provided me a patch which he signed off, and when I contributed it to Git, I added my sign-off, certifying that I had received it under those terms and not modified it. The Git project maintainer will sign-off the commit as well, asserting the same thing.

Thus, if you choose to add the DCO add-on to your project, it is reasonable that one of the sign-offs on the commit match an email address on your account.  Note that, as far as I'm aware, the DCO add-on doesn't require your name to match, which is reasonable: some people use a nickname or shortened form of their name, and a bit-for-bit match would be overly burdensome.

As I mentioned, GitHub doesn't require much of anything about your name and email, and technically the DCO add-on (which is a matter of project policy) does not require anything beyond a matching email.  Git will, however, embed whatever name you give it inside your commits; the only change with the DCO is that you agree that the project can maintain that indefinitely.

--------------------------------------------------
using Django crispy forms, how do you shrink the text box for a TextField?
The text boxes that crispy forms lay out for TextField columns are too tall for my app, they are taking up too much screen space.  The user can make them taller if desired by dragging the bottom frame line, but the minimum size is too tall.  I would like the default height to be 4 rows or so.

I tried some ideas I got off related posts, but nothing  worked -- the text boxes were still the same height and too tall.

Here is the model I am working on at the moment:

    class Brand(models.Model):
        cTitle          = models.CharField(
                            &#39;brand name&#39;, max_length = 48, db_index = True)
        bWanted         = models.BooleanField(
                            &#39;want anything from this brand?&#39;, default = True )
        bAllOfInterest  = models.BooleanField(
                            &#39;want everything from this brand?&#39;, default = True )
        cLookFor        = models.TextField(
                            &#39;Considered a hit if this text is found &#39;
                            &#39;(each line evaluated separately, &#39;
                            &#39;put different look for tests on different lines)&#39;,
                            null=True, blank = True )
        iStars          = IntegerRangeField(
                            &#39;desireability, 10 star brand is most desireable&#39;,
                            min_value = 0, max_value = 10, default = 5 )
        cComment        = models.TextField( &#39;comments&#39;, null = True, blank = True )
        cNationality    = CountryField( &quot;nationality&quot;, null = True )
        cExcludeIf      = models.TextField(
                            &#39;Not a hit if this text is found &#39;
                            &#39;(each line evaluated separately, &#39;
                            &#39;put different exclude tests on different lines)&#39;,
                            null=True, blank = True )
        iLegacyKey      = models.PositiveIntegerField(&#39;legacy key&#39;, null = True )
        tLegacyCreate   = models.DateTimeField( &#39;legacy row created on&#39;,
                            null=True, blank = True )
        tLegacyModify   = models.DateTimeField( &#39;legacy row updated on&#39;,
                            null=True, blank = True )
        iUser           = models.ForeignKey( User, verbose_name = &#39;Owner&#39;,
                            on_delete=models.CASCADE )
        tCreate         = models.DateTimeField( &#39;created on&#39;, auto_now_add= True )
        tModify         = models.DateTimeField( &#39;updated on&#39;, auto_now    = True )
        #
    
        def __str__(self):
            return self.cTitle
        
        class Meta():
            verbose_name_plural = &#39;brands&#39;
            ordering            = (&#39;cTitle&#39;,)
            db_table            = verbose_name_plural

But only these fields are on the form:

    tModelFields = (
        &#39;cTitle&#39;,
        &#39;bWanted&#39;,
        &#39;bAllOfInterest&#39;,
        &#39;cLookFor&#39;,
        &#39;iStars&#39;,
        &#39;cComment&#39;,
        &#39;cNationality&#39;,
        &#39;cExcludeIf&#39; )

Guidance would be appreciated.

||||||||||||||Since you are using `crispy_forms` according to their documentation you could use `Layouts` to define some attributes of the field's "element" that will be added to the template, [as it is explained here in the documentation][1].

In this case it would be something like this: `Field('cExcludeIf', rows='4')`


  [1]: https://django-crispy-forms.readthedocs.io/en/latest/layouts.html#layout-objects-attributes

--------------------------------------------------
PotentialStubbingProblem solved by moving a class. How does it work?
Here&#39;s an MRE:
```java
package com.example.mockitomre;

public interface DocumentedEndpoint {
    EndpointDetails getDetails();
}
```
```java
package com.example.mockitomre;

public interface EndpointDetails {
    String getPath();
}
```
```java
package com.example.mockitomre;

public interface EndpointSieve {
    boolean isAllowed(DocumentedEndpoint endpoint);
}
```
```java
package com.example.mockitomre;

import org.springframework.util.AntPathMatcher;

public class EndpointSieveConfig {
    public EndpointSieve errorPathEndpointSieve(GatewayMeta gatewayMeta, AntPathMatcher antPathMatcher) {
        return endpoint -&gt; gatewayMeta.getIgnoredPatterns().stream()
                .noneMatch(ignoredPattern -&gt; antPathMatcher.match(ignoredPattern, endpoint.getDetails().getPath()));
    }
}
```
```java
package com.example.mockitomre;

import lombok.Getter;

import java.util.List;

@Getter
public final class GatewayMeta {
    private List&lt;String&gt; ignoredPatterns;
}
```
```java
package com.example.mockitomre;

import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.util.AntPathMatcher;

import java.util.List;

import static org.assertj.core.api.AssertionsForClassTypes.assertThat;
import static org.mockito.Mockito.RETURNS_DEEP_STUBS;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@ExtendWith(MockitoExtension.class)
class ErrorPathEndpointSieveTest {
    private final EndpointSieveConfig endpointSieveConfig = new EndpointSieveConfig();
    @Mock
    private GatewayMeta gatewayMetaMock;
    @Mock
    private AntPathMatcher antPathMatcherMock;
    private EndpointSieve errorPathEndpointSieve;

    @Test
    void doesntAllowIgnoredPatterns() {
        String ignoredPattern = &quot;/ignored-path/**&quot;;
        String anotherIgnoredPattern = &quot;/*/another-ignored-path&quot;;
        when(gatewayMetaMock.getIgnoredPatterns()).thenReturn(List.of(
                ignoredPattern, anotherIgnoredPattern
        ));

        String pathToExclude = &quot;/ignored-path&quot;;
        String anotherPathToExclude = &quot;/it-is/another-ignored-path&quot;;

        when(antPathMatcherMock.match(ignoredPattern, pathToExclude)).thenReturn(true);
        when(antPathMatcherMock.match(anotherIgnoredPattern, anotherPathToExclude)).thenReturn(true);

        DocumentedEndpoint endpointToExclude = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(endpointToExclude.getDetails().getPath()).thenReturn(pathToExclude);

        DocumentedEndpoint anotherEndpointToExclude = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(anotherEndpointToExclude.getDetails().getPath()).thenReturn(anotherPathToExclude);

        String okPath = &quot;/another-ignored-path/on-second-thought-it-is-not&quot;;
        DocumentedEndpoint endpointToKeep = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(endpointToKeep.getDetails().getPath()).thenReturn(okPath);

        errorPathEndpointSieve = endpointSieveConfig.errorPathEndpointSieve(gatewayMetaMock, antPathMatcherMock);

        assertThat(errorPathEndpointSieve.isAllowed(endpointToExclude)).isFalse();
        assertThat(errorPathEndpointSieve.isAllowed(anotherEndpointToExclude)).isFalse();

        assertThat(errorPathEndpointSieve.isAllowed(endpointToKeep)).isTrue();
    }
}
```
```xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;3.2.2&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;mockito-mre&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;mockito-mre&lt;/name&gt;
    &lt;description&gt;mockito-mre&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;17&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
```
So here&#39;s the problem: once I run the test, I get
```
org.mockito.exceptions.misusing.PotentialStubbingProblem: 
Strict stubbing argument mismatch. Please check:
 - this invocation of &#39;match&#39; method:
    antPathMatcherMock.match(
    &quot;/ignored-path/**&quot;,
    &quot;/it-is/another-ignored-path&quot;
);
    -&gt; at com.example.mockitomre.EndpointSieveConfig.lambda$errorPathEndpointSieve$0(EndpointSieveConfig.java:8)
 - has following stubbing(s) with different arguments:
    1. antPathMatcherMock.match(
    &quot;/*/another-ignored-path&quot;,
    &quot;/it-is/another-ignored-path&quot;
);
      -&gt; at com.example.mockitomre.ErrorPathEndpointSieveTest.doesntAllowIgnoredPatterns(ErrorPathEndpointSieveTest.java:37)
Typically, stubbing argument mismatch indicates user mistake when writing tests.
Mockito fails early so that you can debug potential problem easily.
However, there are legit scenarios when this exception generates false negative signal:
  - stubbing the same method multiple times using &#39;given().will()&#39; or &#39;when().then()&#39; API
    Please use &#39;will().given()&#39; or &#39;doReturn().when()&#39; API for stubbing.
  - stubbed method is intentionally invoked with different arguments by code under test
    Please use default or &#39;silent&#39; JUnit Rule (equivalent of Strictness.LENIENT).
For more information see javadoc for PotentialStubbingProblem class.

	at org.springframework.util.AntPathMatcher.match(AntPathMatcher.java:195)
	at com.example.mockitomre.EndpointSieveConfig.lambda$errorPathEndpointSieve$0(EndpointSieveConfig.java:8)
```
I don&#39;t know what exactly Mockito wants from me, but I made some experiments and it&#39;s not about

1) mocking the same method with different arguments;
2) the fact that the method is also called with set of arguments not involved in any stubbing.
```java
// here, Mockito is fine with calling match(&quot;/ignored-path/**&quot;, &quot;/it-is/another-ignored-path&quot;)

    @Test
    void doesntAllowIgnoredPatterns() {
        when(antPathMatcherMock.match(&quot;/ignored-path/**&quot;, &quot;/ignored-path&quot;)).thenReturn(true);
        when(antPathMatcherMock.match(&quot;/*/another-ignored-path&quot;, &quot;/it-is/another-ignored-path&quot;)).thenReturn(true);

        antPathMatcherMock.match(&quot;/ignored-path/**&quot;, &quot;/ignored-path&quot;);
        antPathMatcherMock.match(&quot;/*/another-ignored-path&quot;, &quot;/it-is/another-ignored-path&quot;);

        antPathMatcherMock.match(&quot;/ignored-path/**&quot;, &quot;/it-is/another-ignored-path&quot;);
    }
```

You know what helps (beside ditching Mockito&#39;s extension, I hate it, it brings more problems that it solves)? Making `EndpointSieveConfig` a nested class of `ErrorPathEndpointSieveTest` like so:
```java
package com.example.mockitomre;

import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.util.AntPathMatcher;

import java.util.List;

import static org.assertj.core.api.AssertionsForClassTypes.assertThat;
import static org.mockito.Mockito.RETURNS_DEEP_STUBS;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@ExtendWith(MockitoExtension.class)
class ErrorPathEndpointSieveTest {
    private final EndpointSieveConfig endpointSieveConfig = new EndpointSieveConfig();
    @Mock
    private GatewayMeta gatewayMetaMock;
    @Mock
    private AntPathMatcher antPathMatcherMock;
    private EndpointSieve errorPathEndpointSieve;

    @Test
    void doesntAllowIgnoredPatterns() {
        String ignoredPattern = &quot;/ignored-path/**&quot;;
        String anotherIgnoredPattern = &quot;/*/another-ignored-path&quot;;
        when(gatewayMetaMock.getIgnoredPatterns()).thenReturn(List.of(
                ignoredPattern, anotherIgnoredPattern
        ));

        String pathToExclude = &quot;/ignored-path&quot;;
        String anotherPathToExclude = &quot;/it-is/another-ignored-path&quot;;

        when(antPathMatcherMock.match(ignoredPattern, pathToExclude)).thenReturn(true);
        when(antPathMatcherMock.match(anotherIgnoredPattern, anotherPathToExclude)).thenReturn(true);

        DocumentedEndpoint endpointToExclude = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(endpointToExclude.getDetails().getPath()).thenReturn(pathToExclude);

        DocumentedEndpoint anotherEndpointToExclude = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(anotherEndpointToExclude.getDetails().getPath()).thenReturn(anotherPathToExclude);

        String okPath = &quot;/another-ignored-path/on-second-thought-it-is-not&quot;;
        DocumentedEndpoint endpointToKeep = mock(DocumentedEndpoint.class, RETURNS_DEEP_STUBS);
        when(endpointToKeep.getDetails().getPath()).thenReturn(okPath);

        errorPathEndpointSieve = endpointSieveConfig.errorPathEndpointSieve(gatewayMetaMock, antPathMatcherMock);

        assertThat(errorPathEndpointSieve.isAllowed(endpointToExclude)).isFalse();
        assertThat(errorPathEndpointSieve.isAllowed(anotherEndpointToExclude)).isFalse();

        assertThat(errorPathEndpointSieve.isAllowed(endpointToKeep)).isTrue();
    }

    public class EndpointSieveConfig {
        public EndpointSieve errorPathEndpointSieve(GatewayMeta gatewayMeta, AntPathMatcher antPathMatcher) {
            return endpoint -&gt; gatewayMeta.getIgnoredPatterns().stream()
                    .noneMatch(ignoredPattern -&gt; antPathMatcher.match(ignoredPattern, endpoint.getDetails().getPath()));
        }
    }
}
```
Now it passes! The questions:

1. What does Mockito want from me?
2. Why does making that weird change make Mockito happy?

No, [this question][1] doesn&#39;t help

## Anything that doesn&#39;t address moving the class is **not** an answer to this question. Stop the &quot;duplicate&quot; nonsense please and actually read the question

To those suggesting [Javadoc][2]: it&#39;s not a reliable source of information. I literally pasted the code from there, and it *doesn&#39;t* trigger `PotentialStubbingProblem`, it triggers `UnnecessaryStubbingException`
```java
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;

import static org.mockito.BDDMockito.given;

@ExtendWith(MockitoExtension.class)
public class MockitoTest {
    @Mock
    SomeClass mock;
    @Test
    void test() {
        //test method:
        Something something = new Something();
        given(mock.getSomething(100)).willReturn(something);

        //code under test:
        Something something2 = mock.getSomething(50); // &lt;-- stubbing argument mismatch
    }

    abstract class SomeClass {
        abstract Something getSomething(int arg);
    }

    class Something {}
}
```


  [1]: https://stackoverflow.com/questions/52139619/simulation-of-service-using-mockito-2-leads-to-stubbing-error
  [2]: https://www.javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/exceptions/misusing/PotentialStubbingProblem.html
||||||||||||||Firstly, let's agree on terminology [Strictness in Mockito
#769](https://github.com/mockito/mockito/issues/769)

> Strictness in Mockito can have 2 forms:
>
> - Strict stubbing that requires that all declared stubs are actually used
> - Strict mocks, that require all non-void methods to be stubbed before they are called
>
>Future direction:
>
> - strict stubbing on by default, opt-out available
> - strict mocking off by default, opt-in available

In your case, we are asking talking strict stubbing behaviour.

strict stubbing is turned on implicitly in DefaultMockitoSessionBuilder:
```lang-java
Strictness effectiveStrictness = this.strictness == null ? Strictness.STRICT_STUBS : this.strictness;
```

Let's turn on STRICT_STUBS explicitely for 2 mocks:
- one created via `@Mock` and MockitoExtension
- one created via Mockito.mock
```lang-java
@Mock(strictness = Mock.Strictness.STRICT_STUBS)
private PathMatcher pathMatcherMock;

private PathMatcher pathMatcherMock = Mockito.mock(PathMatcher.class, withSettings().strictness(org.mockito.quality.Strictness.STRICT_STUBS));
```

The goal is to have the mocks as similar as possible - without implicit defaults.

**Problem 1: Mockito.mock version does not throw PotentialStubbingProblem**

Inspection of the mock's `CreationSettings.stubbingLookupListeners` shows that they are empty. [DefaultStubbingLookupListener](https://github.com/mockito/mockito/blob/219350728d0f1bad9739bfa054074d6c43f1bdb1/src/main/java/org/mockito/internal/junit/DefaultStubbingLookupListener.java) is a listener responsible for throwing `PotentialStubbingProblem`, and its code is never executed.

IMHO this is confusing and poorly documented (maybe a bug?).

**Problem 2: @Mock version throws PotentialStubbingProblem, but moving of code to test file resolves PotentialStubbingProblem**

Inspection of the mock's `CreationSettings.stubbingLookupListeners` shows that they contain a `DefaultStubbingLookupListener`

The code responsible for this behaviour is again in [DefaultStubbingLookupListener](https://github.com/mockito/mockito/blob/219350728d0f1bad9739bfa054074d6c43f1bdb1/src/main/java/org/mockito/internal/junit/DefaultStubbingLookupListener.java#L75)

```lang-java
private static List<Invocation> potentialArgMismatches(
        Invocation invocation, Collection<Stubbing> stubbings) {
    List<Invocation> matchingStubbings = new LinkedList<>();
    for (Stubbing s : stubbings) {
        if (UnusedStubbingReporting.shouldBeReported(s)
                && Objects.equals(
                        s.getInvocation().getMethod().getName(),
                        invocation.getMethod().getName())
                // If stubbing and invocation are in the same source file we assume they are in
                // the test code,
                // and we don't flag it as mismatch:
                && !Objects.equals(
                        s.getInvocation().getLocation().getSourceFile(),
                        invocation.getLocation().getSourceFile())) {
            matchingStubbings.add(s.getInvocation());
        }
    }
    return matchingStubbings;
}
```

IMHO this is confusing at best.


--------------------------------------------------
How to enter full screen mode in Flutter web app
Is there a way of making a flutter web app enter fullscreen mode (hide addressbar, tabsbar and taskbar)? Or is there a way of programmatically pressing F11?

I&#39;ve tried...

``` dart
@override
void dispose() {
  SystemChrome.setEnabledSystemUIOverlays(SystemUiOverlay.values);
  super.dispose();
}

@override
initState() {
  SystemChrome.setEnabledSystemUIOverlays([]);
  super.initState();
}
```
but it didn&#39;t work in the web app (I wasn&#39;t expecting it to)
||||||||||||||You might try this:

```dart
import 'dart:html';

void goFullScreen() {
  document.documentElement.requestFullscreen();
}

--------------------------------------------------
spring boot starter graphql not working
I recently started working with `graphql` and found it very intriguing. Since most of my `rest` apps were in `java`, I decided to do a quick setup using the provided [spring boot starter](https://github.com/graphql-java/graphql-spring-boot) project by the `graphql-java` team. It comes with `graph-iql` autoconf spring setup, which makes it easier to query `/graphql` endpoint. 

After spending a few good hours on the project setup in IDEA, I was able to run the [graphql-sample-app](https://github.com/graphql-java/graphql-spring-boot/tree/master/graphql-sample-app). But I think my servlet is still not enabled, and only the `graphiql` endpoint is running, as the default query is returning `404`. 

This is `application.yml`:

    spring:
          application:
                   name: graphql-todo-app
    server:
          port: 9000
    
    graphql:
          spring-graphql-common:
                   clientMutationIdName: clientMutationId
                   injectClientMutationId: true
                   allowEmptyClientMutationId: false
                   mutationInputArgumentName: input
                   outputObjectNamePrefix: Payload
                   inputObjectNamePrefix: Input
                   schemaMutationObjectName: Mutation
          servlet:
                 mapping: /graphql
                 enabled: true
                 corsEnabled: true
    
    graphiql:
        mapping: /graphiql
        enabled: true

This is what my `build.gradle` file looks like:

    buildscript {
        repositories {
            maven { url &quot;https://plugins.gradle.org/m2/&quot; }
            maven { url &#39;http://repo.spring.io/plugins-release&#39; }
        }
        dependencies {
            classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:1.5.2.RELEASE&quot;)
            classpath &quot;com.jfrog.bintray.gradle:gradle-bintray-plugin:1.6&quot;
        }
    }
    
    apply plugin: &#39;java&#39;
    apply plugin: &#39;org.springframework.boot&#39;
    
    repositories {
        jcenter()
        mavenCentral()
    }
    
    dependencies{
    //    compile(project(&quot;:graphql-spring-boot-starter&quot;))
    //    compile(project(&quot;:graphiql-spring-boot-starter&quot;))
        compile &#39;com.graphql-java:graphql-spring-boot-starter:3.6.0&#39;
    
        // to embed GraphiQL tool
        compile &#39;com.graphql-java:graphiql-spring-boot-starter:3.6.0&#39;
    
        compile &quot;com.embedler.moon.graphql:spring-graphql-common:$LIB_SPRING_GRAPHQL_COMMON_VER&quot;
    
        compile(&quot;org.springframework.boot:spring-boot-starter-web&quot;)
        compile(&quot;org.springframework.boot:spring-boot-starter-actuator&quot;)
    
        testCompile(&quot;org.springframework.boot:spring-boot-starter-test&quot;)
    }
    
    jar.enabled = true
    uploadArchives.enabled = false
    bintrayUpload.enabled = false

After running `gradle build`, I run the generated `jar` file from the terminal. This is what I get on localhost:

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/2qGRf.png
||||||||||||||I had the same issue using Spring boot 2.0.0 (M6). Switching back to 1.5.8.RELEASE solved the problem. They're working on the issue, it will be released as soon as there is a non milestone release for Spring boot 2.x

https://github.com/graphql-java/graphql-spring-boot/issues/40

https://github.com/graphql-java/graphql-spring-boot/pull/36

--------------------------------------------------
&#39;raise&#39; inside &#39;try&#39; , when and how do I use &#39;raise&#39;?
So here&#39;s the code:
    
    def fancy_divide(list_of_numbers, index):
        try:
            try:
                raise Exception(&quot;0&quot;)
            finally:
                denom = list_of_numbers[index]
                for i in range(len(list_of_numbers)):
                    list_of_numbers[i] /= denom
        except Exception as ex:
            print(ex)

If I call :

    fancy_divide([0, 2, 4], 0)

why does it not print out &#39;0&#39; ?

and if I edit the code like this :


    def fancy_divide(list_of_numbers, index):
        try:
            try:
                raise Exception(&quot;0&quot;)
            finally:
                denom = list_of_numbers[index]
                for i in range(len(list_of_numbers)):
                    list_of_numbers[i] /= denom
        except Exception as ex:
            raise Exception(&quot;0&quot;)
            print(ex)

and then call the same thing, it prints:

    Traceback (most recent call last):

      File &quot;&lt;ipython-input-16-c1b0ac98281c&gt;&quot;, line 1, in &lt;module&gt;
        fancy_divide([0, 2, 4], 0)

      File &quot;/Users/dsn/.spyder-py3/temp.py&quot;, line 10, in fancy_divide
        raise Exception(&quot;0&quot;)

    Exception: 0

Why is that ? And what is the right way to / when should I use raise?


||||||||||||||Your `finally` block is raising an exception itself, a divide-by-zero error (because your denominator is 0). If a `finally` block executes as an exception is bubbling, and raises an exception of its own, it either:

 1. On Python 2, replaces the existing exception
 2. On Python 3, it wraps the existing exception in the new exception (creating a chain of exceptions, where the outermost one is the one that is checked, but the inner exceptions exist for context)

Your other code prints the traceback because you don't catch the second exception you raise at all (and it bypasses your `print`).

I'd suggest [reading the exception tutorial](https://docs.python.org/3/tutorial/errors.html) to understand more; your example code is so contrived/pointless it's impossible to say what misunderstandings you really have, and what is just to illustrate a specific behavior to support your question.

--------------------------------------------------
getting the total count of search results in wordpress
How to get the total count of results, in the search results page in wordpress ... i think my question is clear .i need the total number of search results that displayed in  the search results page .And also need to find the count of results from page and post separately  
what i have tried is

        &lt;?php echo count($posts); ?&gt;



by using this i got the total number of search results . but i also need the count of pages and posts in the search results
||||||||||||||Try this code,

    $allsearch = new WP_Query("s=$s&showposts=0"); 
    echo $allsearch ->found_posts.' results found.';

Hope this will helps you.

For more please visit,

[Result Count in WordPress][1]

[Display Search Result Count][2]


  [1]: http://www.wpbeginner.com/wp-tutorials/display-search-term-and-result-count-in-wordpress/
  [2]: https://wordpress.stackexchange.com/questions/108865/display-search-result-count

--------------------------------------------------
Talking to a HID
I have a sensor developed by PNI Corp called the spacepoint-fusion. I need to interface with this device in C++ and continuously read new data from the device. 

When I plug the device into my computer, I see /dev/hidraw1 and /dev/hidraw2 show up. Also /dev/usb/hiddev0 shows up. 

My problem is that I have no idea how to read these devices. I can&#39;t find any examples or documentation online. I don&#39;t even know where to start with this. I have been looking at libhid and hiddev as possible solutions, but as of yet, I can&#39;t figure out how to use either of these libraries. 

So how do I read from this human interface device in c++ on a linux machine? Examples would be greatly appreciated. 
Thanks.
||||||||||||||You've got a complicated road ahead of you.  You will first need information about the "spacepoint-fusion" (hopefully it came with documentation).

Some initial values such as Product ID/Vendor ID can be gained with the terminal command:

    lsusb

Next, you will probably need to know the Endpoints, which could again be found using:

    lsusb -v

From this, you can find what addresses on the device can be written to, and what addresses can be read from (and possibly the size of the read/write buffers).  But this is as far as you can get without proper documents.  You will need to know what values to write to the device, and what values to expect back from the device.

Assuming you DO know what values to read/write from/to the device try and follow this example:

<https://web.archive.org/web/20130902234909/http://www.lvr.com/code/generic_hid.c>

I am also making the assumption that your device is HID compliant, which does not have to be the case at all.  Anyway, I wish you well on your USB journey.

--------------------------------------------------
Form pattern validation with react-hook-form
I have been working on a react form and I need to restrict users to put special characters and allow only these ones: [A-Za-z].

I have tried the below code but I am still able to insert inside special characters such as: &#39;♥&#39;, &#39;&gt;&#39;, &#39;+&#39;, etc. 

    export default Component (props {
      ...
      return (
       &lt;input 
        pattern={props.pattern}
       /&gt; 
      )
    }

And I am sending it as a prop to my form:

    &lt;Component 
    pattern=&quot;[A-Za-z]+&quot;
    /&gt;

Can you let me know what I am missing and point out what could be the issue? Many thanks.






||||||||||||||The `pattern` attribute on `input` only works on `submit` in vanilla HTML forms.

If you're using `react-hook-form`, it should be in the ref, like this:

    <input
        name="email"
        ref={register({
          required: "Required",
          pattern: {
            value: /^[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}$/i,
            message: "invalid email address"
          }
        })}
      />

please have a check on react-hook-form doc. 

--------------------------------------------------
Bitbucket Pipeline schedule trigger
I can&#39;t see anyone talking about what I&#39;m looking to do. I&#39;m currently running a pipeline on a branch merge within the bitbucket area.

      branches:
        staging:
          - step:
              name: Clone
              script:
                - echo &quot;Clone all the things!&quot; 

What I want to do is when a branch gets merged into master, trigger an event that will enable the schedule to run for the next day. 

If there are no changes I don&#39;t want anything to run, however, if there are I want the schedule to kick in and work.

I&#39;ve read through the Pipeline triggers:

https://support.atlassian.com/bitbucket-cloud/docs/pipeline-triggers/

But I can&#39;t see anywhere that would allow me to do it. Has anyone done this sort of thing? Is it possible, or am I limited by bitbucket itself?
||||||||||||||Never done this, but there's an API for creating schedules. I think you would need to determine the date and specify the single cron task, e.g. March 30, 2022 at midnight:
0 0 30 3 * 2022

However year is an extension, not a standard CRON field; "at" is an alternative that may be accessible (but also not standard). It all depends on what Bitbucket allows for CRON schedule, so I think this is not a conclusive answer (still needs info on how to setup the schedule).

Here is the docs 
https://developer.atlassian.com/bitbucket/api/2/reference/resource/repositories/%7Bworkspace%7D/%7Brepo_slug%7D/pipelines_config/schedules/

--------------------------------------------------
Problem with spring boot graphql. Request /graphql results with 404
I&#39;m trying to run simplest graphql example. I created application with spring initializer and only added graphql dependencies. My `build.gradle`

    buildscript {
    	ext {
    		springBootVersion = &#39;2.1.1.RELEASE&#39;
    	}
    	repositories {
    		mavenCentral()
    	}
    	dependencies {
    		classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}&quot;)
    	}
    }
    
    apply plugin: &#39;java&#39;
    apply plugin: &#39;eclipse&#39;
    apply plugin: &#39;org.springframework.boot&#39;
    apply plugin: &#39;io.spring.dependency-management&#39;
    
    group = &#39;com.example&#39;
    version = &#39;0.0.1-SNAPSHOT&#39;
    sourceCompatibility = 1.8
    
    repositories {
    	mavenCentral()
    }
    
    
    dependencies {
    	implementation(&#39;org.springframework.boot:spring-boot-starter-web&#39;)
    	testImplementation(&#39;org.springframework.boot:spring-boot-starter-test&#39;)
    
        compile &#39;com.graphql-java-kickstart:graphql-spring-boot-starter:5.3.1&#39;
        compile &#39;com.graphql-java-kickstart:graphiql-spring-boot-starter:5.3.1&#39;
        compile &#39;com.graphql-java-kickstart:voyager-spring-boot-starter:5.3.1&#39;
    }

DemoApplication.java

    package com.example.demo;
    
    import org.springframework.boot.SpringApplication;
    import org.springframework.boot.autoconfigure.SpringBootApplication;
    
    @SpringBootApplication
    public class DemoApplication {
    
    	public static void main(String[] args) {
    		SpringApplication.run(DemoApplication.class, args);
    	}
    }

When I run the project and hit the endpoint `/graphql` it returns `404`. What is missing in my configuration?
||||||||||||||The docs (https://github.com/graphql-java-kickstart/graphql-spring-boot#enable-graphql-servlet) say:

> The servlet becomes accessible at /graphql if graphql-spring-boot-starter added as a dependency to a boot application and a GraphQLSchema bean is present in the application.

...and the minimum example it links to looks like this:

    @SpringBootApplication
    public class ApplicationBootConfiguration {
    
        public static void main(String[] args) {
            SpringApplication.run(ApplicationBootConfiguration.class, args);
        }

        @Bean
        GraphQLSchema schema() {
            return GraphQLSchema.newSchema()
                .query(GraphQLObjectType.newObject()
                    .name("query")
                    .field(field -> field
                        .name("test")
                        .type(Scalars.GraphQLString)
                        .dataFetcher(environment -> "response")
                    )
                    .build())
                .build();
        }
    }

So you're missing a Graphql schema to be used. It says if there is one, the API endpoint will be exposed automatically.  
Good luck!

--------------------------------------------------
How do I convert the WebVTT format to plain text?
Here is a sample of WebVTT

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-html --&gt;

    WEBVTT
    Kind: captions
    Language: en
    Style:
    ::cue(c.colorCCCCCC) { color: rgb(204,204,204);
     }
    ::cue(c.colorE5E5E5) { color: rgb(229,229,229);
     }
    ##

    00:00:00.060 --&gt; 00:00:03.080 align:start position:0%
     
    &lt;c.colorE5E5E5&gt;okay&lt;00:00:00.690&gt;&lt;c&gt; so&lt;/c&gt;&lt;00:00:00.750&gt;&lt;c&gt; this&lt;/c&gt;&lt;00:00:01.319&gt;&lt;c&gt; is&lt;/c&gt;&lt;00:00:01.469&gt;&lt;c&gt; a&lt;/c&gt;&lt;/c&gt;&lt;c.colorCCCCCC&gt;&lt;00:00:01.500&gt;&lt;c&gt; newsflash&lt;/c&gt;&lt;00:00:02.040&gt;&lt;c&gt; page&lt;/c&gt;&lt;00:00:02.460&gt;&lt;c&gt; for&lt;/c&gt;&lt;/c&gt;

    00:00:03.080 --&gt; 00:00:03.090 align:start position:0%
    &lt;c.colorE5E5E5&gt;okay so this is a&lt;/c&gt;&lt;c.colorCCCCCC&gt; newsflash page for
     &lt;/c&gt;

    00:00:03.090 --&gt; 00:00:08.360 align:start position:0%
    &lt;c.colorE5E5E5&gt;okay so this is a&lt;/c&gt;&lt;c.colorCCCCCC&gt; newsflash page for&lt;/c&gt;
    &lt;c.colorE5E5E5&gt;Meraki&lt;00:00:03.659&gt;&lt;c&gt; printing&lt;/c&gt;&lt;00:00:05.120&gt;&lt;c&gt; so&lt;/c&gt;&lt;00:00:06.529&gt;&lt;c&gt; all&lt;/c&gt;&lt;00:00:07.529&gt;&lt;c&gt; we&lt;/c&gt;&lt;00:00:08.040&gt;&lt;c&gt; need&lt;/c&gt;&lt;00:00:08.130&gt;&lt;c&gt; to&lt;/c&gt;&lt;00:00:08.189&gt;&lt;c&gt; do&lt;/c&gt;&lt;/c&gt;

    00:00:08.360 --&gt; 00:00:08.370 align:start position:0%
    &lt;c.colorE5E5E5&gt;Meraki printing so all we need to do
     &lt;/c&gt;

    00:00:08.370 --&gt; 00:00:11.749 align:start position:0%
    &lt;c.colorE5E5E5&gt;Meraki printing so all we need to do
    here&lt;00:00:08.700&gt;&lt;c&gt; is&lt;/c&gt;&lt;00:00:08.820&gt;&lt;c&gt; to&lt;/c&gt;&lt;00:00:09.000&gt;&lt;c&gt; swap&lt;/c&gt;&lt;00:00:09.330&gt;&lt;c&gt; out&lt;/c&gt;&lt;00:00:09.480&gt;&lt;c&gt; the&lt;/c&gt;&lt;00:00:09.660&gt;&lt;c&gt; logo&lt;/c&gt;&lt;00:00:09.929&gt;&lt;c&gt; here&lt;/c&gt;&lt;00:00:10.650&gt;&lt;c&gt; and&lt;/c&gt;&lt;00:00:10.830&gt;&lt;c&gt; I&lt;/c&gt;&lt;/c&gt;

    00:00:11.749 --&gt; 00:00:11.759 align:start position:0%
    here is to swap out the logo here&lt;c.colorE5E5E5&gt; and I
     &lt;/c&gt;

    00:00:11.759 --&gt; 00:00:16.400 align:start position:0%
    here is to swap out the logo here&lt;c.colorE5E5E5&gt; and I
    should&lt;00:00:11.969&gt;&lt;c&gt; also&lt;/c&gt;&lt;00:00:12.120&gt;&lt;c&gt; work&lt;/c&gt;&lt;00:00:12.420&gt;&lt;c&gt; on&lt;/c&gt;&lt;00:00:12.630&gt;&lt;c&gt; move&lt;/c&gt;&lt;00:00:12.840&gt;&lt;c&gt; out&lt;/c&gt;&lt;00:00:13.049&gt;&lt;c&gt; as&lt;/c&gt;&lt;00:00:13.230&gt;&lt;c&gt; well&lt;/c&gt;&lt;00:00:15.410&gt;&lt;c&gt; and&lt;/c&gt;&lt;/c&gt;

    00:00:16.400 --&gt; 00:00:16.410 align:start position:0%
    &lt;c.colorE5E5E5&gt;should also work on move out as well and
     &lt;/c&gt;

&lt;!-- end snippet --&gt;

I used &lt;a href=&quot;https://github.com/rg3/youtube-dl&quot;&gt;youtube-dl&lt;/a&gt; to grab it from YouTube.

I want to convert this to plain text. I can&#39;t just strip out the times and colour tags as the text repeats itself .

So I&#39;m wondering if something exists to convert this to plain text or if there is some pseudo code someone could offer so I could code that up?

I have also posted an issue about this with &lt;a href=&quot;https://github.com/rg3/youtube-dl/issues/17178&quot;&gt;youtube-dl&lt;/a&gt;.
||||||||||||||I've used [WebVTT-py](https://webvtt-py.readthedocs.io/en/latest/) to extract the plain text transcription.

<!-- language: py -->
    import webvtt
    vtt = webvtt.read('subtitles.vtt')
    transcript = ""
    
    lines = []
    for line in vtt:
        # Strip the newlines from the end of the text.
        # Split the string if it has a newline in the middle
        # Add the lines to an array
        lines.extend(line.text.strip().splitlines())
    
    # Remove repeated lines
    previous = None
    for line in lines:
        if line == previous:
           continue
        transcript += " " + line
        previous = line
    
    print(transcript)

--------------------------------------------------
Why I&#39;m not able to unwrap and serialize a Java map using the Jackson Java library?
My bean looks like this:

    class MyBean {
    	
    	private @JsonUnwrapped HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();
    	
    	private String name;
    	
    	public HashMap&lt;String, String&gt; getMap() {
    		return map;
    	}
    
    	public void setMap(HashMap&lt;String, String&gt; map) {
    		this.map = map;
    	}
    
    	public String getName() {
    		return name;
    	}
    
    	public void setName(String name) {
    		this.name = name;
    	}
    }

While I&#39;m serializing the bean using the following code: 

    MyBean bean = new MyBean();
    HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();;
    map.put(&quot;key1&quot;, &quot;value1&quot;);
    map.put(&quot;key2&quot;, &quot;value2&quot;);
    bean.setMap(map);
    bean.setName(&quot;suren&quot;);
    ObjectMapper mapper = new ObjectMapper();
    System.out.println(&quot;\n&quot;+mapper.writeValueAsString(bean));

I&#39;m getting result like this:

    {&quot;map&quot;:{&quot;key2&quot;:&quot;value2&quot;,&quot;key1&quot;:&quot;value1&quot;},&quot;name&quot;:&quot;suren&quot;}

but

    {&quot;key2&quot;:&quot;value2&quot;,&quot;key1&quot;:&quot;value1&quot;,&quot;name&quot;:&quot;suren&quot;}

is expected per the [JacksonFeatureUnwrapping documentation](http://wiki.fasterxml.com/JacksonFeatureUnwrapping). Why am I not getting the unwrapped result?
||||||||||||||`@JsonUnwrapped` doesn't work for maps, only for proper POJOs with getters and setters. For maps, You should use [`@JsonAnyGetter`][1] and [`@JsonAnySetter`][2] (available in jackson version >= 1.6).

In your case, try this:

    @JsonAnySetter 
    public void add(String key, String value) {
        map.put(key, value);
    }
    
    @JsonAnyGetter
    public Map<String,String> getMap() {
        return map;
    }

That way, you can also directly add properties to the map, like `add('abc','xyz')` will add a new key `abc` to the map with value `xyz`. 

  [1]: http://jackson.codehaus.org/1.6.0/javadoc/org/codehaus/jackson/annotate/JsonAnyGetter.html
  [2]: http://jackson.codehaus.org/1.6.0/javadoc/org/codehaus/jackson/annotate/JsonAnySetter.html

--------------------------------------------------
Call methods from Swift initializer
Let&#39;s say I have the following class in Swift (which has obvious problems)

    class MyClass {
        let myProperty: String

        init() {
            super.init()
            self.setupMyProperty()
        }

        func setupMyProperty() {
            myProperty = &quot;x&quot;
        }
    }

This is overly simplified but I&#39;m basically trying to delegate the initialization of `myProperty` into the `setupMyProperty()` method. It&#39;s a pattern I use often to break down the different parts of the setup of a class.

But of course, I can&#39;t call `self` until the super initializer has run, and I can&#39;t run the super initializer until all the properties have been set, so I&#39;m in a catch 22. On top of it since `setupMyProperty()` isn&#39;t considered an initializer, it won&#39;t be able to assign `myProperty` anyway.

Can anyone tell me how to implement this pattern in Swift?

||||||||||||||declare it as an implicitly unwrapped optional

    class MyClass : NSObject {
        var myProperty: String!
        
        init() {
            super.init()
            self.setupMyProperty()
        }
        
        func setupMyProperty() {
            self.myProperty = "x"
        }
    }

Source: [Swift documentation, Unowned References and Implicitly Unwrapped Optional Properties](https://docs.swift.org/swift-book/documentation/the-swift-programming-language/automaticreferencecounting/#Unowned-References-and-Implicitly-Unwrapped-Optional-Properties)

--------------------------------------------------
How to get full range of onEdit(e)?
How to get the whole range of the onEdit(e) by Google Sheets script? I have tried the following script:

    var sheet = spreadsheet.getActiveSheet();
    function onEdit(e){
       var row = e.range.getRow();
       var column = e.range.getColumn();
       sheet.getRange(&#39;A1&#39;).setNote(range.getRow() + &#39;,&#39; + range.getColumn());
    }
Well, it works if I edit only one cell. For example, if I edit cell B2, then it will set the note of cell A1 to be &quot;2,2&quot;. But what if I edit multiple cells simultaneously? For example, suppose the values of range(&#39;A2:B3&#39;) are all non-empty (say, all &#39;abc&#39;) and I copy range(&#39;A2:B3&#39;) and paste it to range(&#39;A10:B11&#39;). Then, the note of cell A1 will only show &quot;10,1&quot; (i.e., row and column of cell A10). How can I show the WHOLE RANGE (instead of merely the top left cell), for example, &quot;10,1,2,2&quot; or the rows and columns of &#39;A10&#39; and &#39;B11&#39;? Thank you!
||||||||||||||    var sheet = SpreadsheetApp.getActiveSheet();
    function onEdit(e){
       var row = e.range.rowStart;
       var column = e.range.columnStart;
       var endRow = e.range.rowEnd;
       var colEnd = e.range.columnEnd;
       sheet.getRange('A1').setNote(row + ',' + column+','+endRow+','+colEnd);
    }

You can use these variables to get the whole edited range

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/kaVWX.png

--------------------------------------------------
How many number of comparisons are required to find an element in an unordered list that is neither maximum nor minimum?
If we have an unordered list has n distinct elements so Now I am confused between Time complexity to be ϴ(n) or ϴ(1) , since in worst case we may end up making n comparisons ,but then if I take 3 elements at a time then I can find the 2nd largest element in ϴ(1) time, so I am confused with these two approaches ,please guide .
||||||||||||||It's pretty simple:

The first approach has a lot of completely useless overhead. Since the list only contains distinct items one of the first three items **must** fulfil the constraint of being neither the largest nor smallest element of the list. All other comparisons are completely useless for the purpose of finding an **arbitrary** element matching the constraints.

--------------------------------------------------
AttributeError: &#39;Adam&#39; object has no attribute &#39;get_updates&#39;
I&#39;m training a VAE with TensorFlow Keras backend and I&#39;m using Adam as the optimizer. the code I used is attached below.

        def compile(self, learning_rate=0.0001):
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        self.model.compile(optimizer=optimizer,
                           loss=self._calculate_combined_loss,
                           metrics=[_calculate_reconstruction_loss,
                                    calculate_kl_loss(self)])

The TensorFlow version I&#39;m using is 2.11.0. The error I&#39;m getting is

    AttributeError: &#39;Adam&#39; object has no attribute &#39;get_updates&#39;

I&#39;m suspecting the issues arise because of the version mismatch. Can someone please help me to sort out the issue? Thanks in advance.
||||||||||||||Try replacing your 2nd line 
"optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
by 
"optimizer = tf.keras.optimizers.**legacy**.Adam(learning_rate=learning_rate)"

For further information, check tf 2.11.0 Release 11/28/2022 in https://github.com/tensorflow/tensorflow/releases 
It indicates in particular that: 
"The tf.keras.optimizers.Optimizer base class now points to the new Keras optimizer, while the old optimizers have been moved to the tf.keras.optimizers.legacy namespace."


--------------------------------------------------
Git clone / pull continually freezing at &quot;Store key in cache?&quot;
I&#39;m attempting to clone a repo from my BitBucket account to my Windows 10 laptop (running GitBash). I&#39;ve completed all of the steps necessary to connect (set up my SSH key, verified by successfully SSHing git@bitbucket.org, etc). However, whenever I attempt to clone a repo, the prompt continually hangs up after confirming that I want to cache Bitbucket&#39;s key.  

    User@Laptop MINGW64 /C/Repos
    $ git clone git@bitbucket.org:mygbid/test.git
    Cloning into &#39;test&#39;...
    The server&#39;s host key is not cached in the registry. You
    have no guarantee that the server is the computer you
    think it is.
    The server&#39;s rsa2 key fingerprint is:
    ssh-rsa 2048 97:8c:1b:f2:6f:14:6b:5c:3b:ec:aa:46:46:74:7c:40
    If you trust this host, enter &quot;y&quot; to add the key to
    PuTTY&#39;s cache and carry on connecting.
    If you want to carry on connecting just once, without
    adding the key to the cache, enter &quot;n&quot;.
    If you do not trust this host, press Return to abandon the
    connection.
    Store key in cache? (y/n) y

No files are cloned, and the result is an empty repo. Trying to initiate a git pull origin master from this repo also asks to cache the key, then hangs with no feedback. Despite not asking for the key to be cached when I do a test SSH, git operations always ask for the key every time before failing.

With no error messages to work with, I&#39;m really at a loss as to what is wrong. I&#39;ve tried multiple repos, including very small ones, with no success at all.
||||||||||||||I had this problem when cloning a repo on Windows 10 too. 

I got around it by using the Putty GUI to SSH to the server in question (in your case: bitbucket.org) then clicked 'Yes' when the prompt asks if you want to save the server key to the cache. Running the clone command again then worked for me!

--------------------------------------------------
Can I type an object in such a way to allow only its keys as its value?
Simplified problem: let&#39;s say we have a graph and that each node has its unique name. If we were to have a type that describes graph edges as a map from name to list of node names you could come to from that node, how would we do that?

Logically, that would be something like:

    type EdgesT = { [name: string]: Exclude&lt;keyof EdgesT, name&gt;[]; };

But I just can&#39;t get that description working, even without exclude part, as it always sees keyof EdgesT as string.

Alternatively, if the same problem was for a function argument, we could do something like:

    type BuildEdges&lt;T&gt; = { [Name in keyof T]: Exclude&lt;keyof T, Name&gt;[]; };
    
    const fn = &lt;T&gt;(edges: BuildEdges&lt;T&gt;) =&gt; null;

And that works, for that explicit case. But if we were to get more info out of type argument (for something else), we couldn&#39;t. For type argument, `&lt;T&gt;` works, `&lt;T extends Record&lt;string, any&gt;&gt;` works, `&lt;T extends Record&lt;string, unknown&gt;&gt;` works, but anything else does not. For example, `&lt;T extends Record&lt;string, number&gt;&gt;` does not work - does anyone has any idea why?

Any idea about working around this situation, achieving a bit more, or describing this limitation?

||||||||||||||There is no specific type in TypeScript that works this way. It is fundamentally [generic](https://www.typescriptlang.org/docs/handbook/2/generics.html). You can't represent it as a large [union](https://www.typescriptlang.org/docs/handbook/2/everyday-types.html#union-types) of `BuildEdges<T>` for every possible `T`.  Conceptually it would be a special kind of generic type called an *[existentially quantified](https://en.wikipedia.org/wiki/Type_system#Existential_types) generic*... you want to say "a value is a `BuildEges` if there *exists* some `T` that works". But TypeScript doesn't directly support existential types (although there is an open feature request for them as [microsoft/TypeScript#14466](https://github.com/microsoft/TypeScript/issues/14466#)). In an alternate universe where that was implemented, maybe you could write

    // THIS IS NOT VALID TYPESCRIPT, DO NOT TRY:
    type BuildEdges = <exists T>{ [K in keyof T]: Exclude<keyof T, K> []; }

But for now you need to do something else.  

---

There are ways to encode existential types in TypeScript, but they end up being more complicated than I think you need here.  Instead, you could just leave it as a regular generic type like

    type BuildEdges<T> = { [K in keyof T]: Exclude<keyof T, K>[]; };

And then write a helper function to infer `T` given the value.  This was your approach anyway, but it seems you didn't find a good [constraint](https://www.typescriptlang.org/docs/handbook/2/generics.html#generic-constraints).  Here's how I'd do it:

    const fn = <T extends BuildEdges<T>>(edges: T) => null;
    
That's a recursive constraint. It essentially validates that `T` is a proper `BuildEdges<T>`, leading to the following desirable behavior:


    fn({ a: ["c"], b: ["a"], c: ["a", "b"] }); // okay
    fn({ a: ["c"], b: ["a"], c: ["a", "c"] }); // error!

[Playground link to code](https://www.typescriptlang.org/play?ts=5.3.3#code/PQKgUALgngDgpgAgEIFcCWAbAJgUSwczgGcEBeBAHjgA80iISAVAPgG8EBtAaQTQDsEAazhQA9gDMEjALoAuBDmoBjDCixwKwsZMYAaBF2adpAbgQBfMCGCRYiVJlwFiFFmQTtuvAVolS5Csqq6poifnoGzBymFiZgYEqifPQI4gLkrgg0EHB8WCQO2HiERK7MzAAUcM5E8owAlGRGfCgYGHFgaRXsAIbyHABESgPS+gBG-QM9I-pKk9P6A2MjFvVmwMAIooI9UJ183Qh9nEMzCBMn06MIc5cDi8PSq+ubcABOb6JvAIRgQA)

--------------------------------------------------
Select first row of each group by group in SQL Server
I would like to get the first row of each group with the smallest &quot;differenz&quot; value of the group like I tried here. How can I achieve that?

This is my query


```
SELECT 
    ae.vpid AS aeVpid,
    ae.EreignisDat AS aeEreignisDat, 
    be.StichtagDat AS beStichtagDat,
    ABS(DATEDIFF(day, ae.EreignisDat, be.StichtagDat)) AS differenz
FROM 
    AEGE AS ae 
LEFT OUTER JOIN
    BEST AS be ON ae.VPID = be.VPID
WHERE 
    ae.EreignisDat &lt;&gt; be.StichtagDat
ORDER BY
    aevpid, differenz ASC, beStichtagDat DESC
```

I get this result:

![this](https://i.stack.imgur.com/6xr7q.png)

where &quot;beStichtagDat&quot; is variable and depending on the first two attributes &quot;aeVpid&quot; and &quot;aeEreignisDat&quot;

I tried some group by&#39;s on some attributes and subqueries with a `min()` on &quot;differenz&quot; and I also tried the `row_number()` over the groups but I could not manage to get it done
||||||||||||||`row_number()` + `partition by` is probably the way to go here, but this is still incomplete because the question doesn't show anything about how to split up the groups:

```
SELECT  aeVpid, aeEreignisDat, beStichtagDat, differenz
FROM (
    select
        ae.vpid as aeVpid,
        ae.EreignisDat as aeEreignisDat, 
        be.StichtagDat as beStichtagDat,
        abs(datediff(day, ae.EreignisDat, be.StichtagDat)) as differenz,
        row_number() over (partition by ???? order by abs(datediff(day, ae.EreignisDat, be.StichtagDat))) rn

    from AEGE as ae 
    left join BEST as be on ae.VPID = be.VPID
    where ae.EreignisDat <> be.StichtagDat
) t
WHERE rn = 1
ORDER BY aevpid, differenz, beStichtagDat DESC
```

--------------------------------------------------
Quarkus Mutiny Multi and Uni memory leaks and io.vertx.core.VertxException: Thread blocked
I have this problem and have been stuck with it for a week now

I&#39;m querying a huge amount of data (millions) from a DB, and then breaks them into partition using `Multi` and for each of them, process using `Uni`

My main function code looks roughly something like this:
```java
public Uni&lt;Void&gt;processData(Request request) {
    AtomicReference&lt;List&lt;Item&gt;&gt; listItemRef = new AtomicReference&lt;&gt;();
    return getAllMatchingItem(request)
            .onItem().transformToMulti(result -&gt; Multi.createFrom().iterable(ListUtils.partition(result, 10000)))
            .onItem().transformToUni(resultBatched -&gt; Uni.createFrom().item(() -&gt; resultBatched)
                    .chain(resultBatched -&gt; {
                        listItemRef.setRelease(resultBatched);
                        return getOtherItemBasedOnItem(resultBatched);
                    })
                    .chain(otherItems -&gt; {
                        List&lt;Item&gt; items = listItemRef.getPlain();
                        return createProcessedItems(items, otherItems);
                    })
                    .chain(processedItems -&gt; {
                        return insertProcessedItems(processedItems);
                    })
                    .onTermination().invoke(() -&gt; {
                        LOGGER.info(&quot;Data process success&quot;);
                    })
                    .replaceWithVoid() // to make sure no remaining reference to upstream Unis
            )
            .merge(1)
            .collect().asList()
            .replaceWithVoid()
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}

```

and the supporting functions are roughly like this:

```java
public Uni&lt;List&lt;Item&gt;&gt; getAllMatchingItem(Request request) {
    // get data to DB, basically PanacheRepository.find(...).list
    return Uni.createFrom().item(() -&gt; repo.find(&quot; ... &quot;).list)
            .runSubscriptionOn(Infrastructure.getDefaultWorkerPool())
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}

public Uni&lt;List&lt;OtherItem&gt;&gt; getOtherItemBasedOnItem(List&lt;Item&gt; items) {
    // get data to DB, basically PanacheRepository.find(id in items.getIdSet).list()
    return Uni.createFrom().item(() -&gt; repo.find(&quot;id in ?1&quot;, items.getIDSet()).list)
            .runSubscriptionOn(Infrastructure.getDefaultWorkerPool())
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}

public Uni&lt;List&lt;ProcessedItem&gt;&gt; createProcessedItems(List&lt;Item&gt; items, List&lt;OtherItem&gt; otherItems) {
    return Uni.createFrom().item(() -&gt; doCreateProcessedItems(item, otherItems))
            .runSubscriptionOn(Infrastructure.getDefaultWorkerPool())
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}

public List&lt;ProcessedItem&gt; doCreateProcessedItems(List&lt;Item&gt; items, List&lt;OtherItem&gt; otherItems) {
    // process those items
    // basically create Map, create list, etc
    // no external function call, just plain code
}

public Uni&lt;Void&gt; insertProcessedItems(List&lt;ProcessedItem&gt; processedItems) {
    // basically just insert/update
    return Uni.createFrom().voidItem()
            .runSubscriptionOn(Infrastructure.getDefaultWorkerPool())
            .invoke(() -&gt; repo.persist(processedItems))
            .emitOn(MutinyHelper.executor(Vertx.currentContext()));
}
```

I suspect there are memory leaks because the log `Data process success` appears a few times before getting this exception
```
java.lang.OutOfMemoryError: Java heap space
```

I also getting this warning:
```
(vertx-blocked-thread-checker) Thread Thread[vert.x-eventloop-thread-0,5,main] has been blocked for 3203 ms, time limit is 2000 ms: io.vertx.core.VertxException: Thread blocked
2024-02-03 21:20:10,152 WARN [io.ver.cor.imp.BlockedThreadChecker] (vertx-blocked-thread-checker) Thread Thread[vert.x-eventloop-thread-0,5,main] has been blocked for 3203 ms, time limit is 2000 ms: io.vertx.core.VertxException: Thread blocked
```

I appreciate any clue I could get on how to solve this complex issue

Note: I&#39;m using Quarkus version 2.16 if that matters
||||||||||||||A Vert.x thread block checker warning means that you are doing some blocking I/O and/or long-running work on an event-loop thread.

From what I see in your code I suspect that you are doing calls to Hibernate (not Reactive) and then put data from memory into reactive pipelines, eventually collecting everything as a list (`.collect().asList()`) does just that.

Reactive is great if you do it properly end-to-end with non-blocking I/O. If that's not the case I'd recommend rewriting the code as plain imperative code. 

--------------------------------------------------
how do I refresh a HierarchicalDataSource for a Kendo TreeView?
TreeView creation:

    function CreateNotificationTree(userId)
    {
        debugger;
        var data = new kendo.data.HierarchicalDataSource({
            transport: {
                read: {
                    url: &quot;../api/notifications/byuserid/&quot; + userId,
                    contentType: &quot;application/json&quot;
                }
            },
            schema: {
                model: {
                    children: &quot;notifications&quot;
                }
            }
        });
    
        $(&quot;#treeview&quot;).kendoTreeView({
            dataSource: data,
            loadOnDemand: true,
            dataUrlField: &quot;LinksTo&quot;,
            checkboxes: {
                checkChildren: true
            },
            dataTextField: [&quot;notificationType&quot;, &quot;NotificationDesc&quot;],
            select: treeviewSelect
        });
    
        function treeviewSelect(e)
        {
            var node = this.dataItem(e.node);
            window.open(node.NotificationLink, &quot;_self&quot;);
        }
    }

Where things get updated and I need to refresh the dataSet:

    $(&#39;#btnDelete&#39;).on(&#39;click&#39;, function()
    {
        var treeView = $(&quot;#treeview&quot;).data(&quot;kendoTreeView&quot;);
        var userId = $(&#39;#user_id&#39;).val();
    
        $(&#39;#treeview&#39;).find(&#39;input:checkbox:checked&#39;).each(function()
        {
            debugger;
            var li = $(this).closest(&quot;.k-item&quot;)[0];
            var notificationId = treeView.dataSource.getByUid(li.getAttribute(&#39;data-uid&#39;)).ID;
    
            if (notificationId == &quot;undefined&quot;)
            {
                alert(&#39;No ID was found for one or more notifications selected. These notifications will not be deleted. Please contact IT about this issue.&#39;);
            }
            else
            {
                $.ajax(
                    {
                        url: &#39;../api/notifications/deleteNotification?userId=&#39; + userId + &#39;&amp;notificationId=&#39; + notificationId,
                        type: &#39;DELETE&#39;,
                        success: function()
                        {
                            alert(&#39;Delete successful.&#39;);
                            //Here is where I try to refresh the data source.
                            CreateNotificationTree(userId);
                        },
                        failure: function()
                        {
                            alert(&#39;Delete failed.&#39;);
                        }
                    });
                treeView.remove($(this).closest(&#39;.k-item&#39;));
            }
        });
    });

The problem here is that it does refresh the tree view.... BUT NOT the CHILDREN nodes...

anyone know how to get this working?
||||||||||||||It looks like you are completely rebuilding the tree view. Any reason why you don't just refresh the tree view's data source?  

Given your code above, I would recommend this:

    treeView.dataSource.read();

Also, depending on what type of server you are getting the JSON from, it may be allowing the browser to cache the results, as Kendo data sources default to using GET statements.  This could be fixed on the server side, or you could switch to using a POST to retrieve the data:

    read: {
        url: "../api/notifications/byuserid/" + userId,
        contentType: "application/json",
        type: "POST" // Fixes issue if browser was caching GET requests
    }

--------------------------------------------------
Is it possible to overlay a marker on top of a plotly.js box plot?
Suppose I&#39;m using the [simple box plot example](https://plot.ly/javascript/box-plots/#box-plot-that-displays-the-underlying-data) in plotly&#39;s documentation:

[![simple box plot example][1]][1]

&lt;!-- language: lang-js --&gt;

    var data = [
      {
        y: [0, 1, 1, 2, 3, 5, 8, 13, 21],
        boxpoints: &#39;all&#39;,
        jitter: 0.3,
        pointpos: -1.8,
        type: &#39;box&#39;
      }
    ];
    
    Plotly.newPlot(&#39;myDiv&#39;, data);

I want to overlay a marker on top of the underlying data scatter plot that&#39;s to the left of the box plot. This marker would have its own hover text and everything. This is how I envision this looking:

[![sample with marker][2]][2]

Is there a way to do this in plotly? I&#39;ve looked all over for an example of this, and I can&#39;t find anything that looks relevant. Thanks!

  [1]: https://i.stack.imgur.com/8On0u.png
  [2]: https://i.stack.imgur.com/3buNI.png
||||||||||||||If you are plotting your points on top of the box plot (`pointpos = 0`) you can add another trace with an x value which is identical to your boxplot name, `trace 0` in this case. 

If you are plotting your points next to your boxplot, it becomes a lot more tricky because the scatter points do not have defined x-values on the axis.

You could your new point manually but then the hover info is still in the old position.

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    var data = [{
        y: [0, 1, 1, 2, 3, 5, 8, 13, 21],
        boxpoints: 'all',
        jitter: 0.3,
        pointpos: 0,
        type: 'box'
      },
      {
        y: [0, 1, 1, 2, 3, 5, 8, 13, 21],
        boxpoints: 'all',
        jitter: 0.3,
        pointpos: 1.8,
        type: 'box'
      },
      {
        x: ['trace 0'],
        y: [18],
        name: 'My special marker',
        text: 'Some really interesting hover info',
        marker: {
          size: 20
        }
      },
      {
        x: ['trace 1'],
        y: [18],
        name: 'Another special marker',
        text: 'Some really interesting hover info',
        marker: {
          size: 20
        }
      }
    ];

    Plotly.newPlot('myDiv', data);
    var boxPoint = document.getElementsByClassName('trace boxes')[1].getElementsByClassName('point')[0];
    var point = document.getElementsByClassName('scatterlayer')[0].getElementsByClassName('point')[1];
    var y = point.attributes['transform'].value.split(',')[1];
    var x = boxPoint.attributes['transform'].value.split(',')[0];
    point.setAttribute('transform', x + ', ' + y);

<!-- language: lang-html -->

    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <div id="myDiv"></div>

<!-- end snippet -->



--------------------------------------------------
React native navigation to second page
I want to create a basic `ScrollView` in which the first page will be visible and it will take the full height of the screen.
When user clicks my button I want to scroll to the second page which will also occupy the full height of the `ScrollView`
What is the best way to achieve this?
||||||||||||||To achieve a full-screen, scrollable interface where each page takes the full height of the screen, and you can navigate between pages by pressing a button, you can utilize a combination of FlatList and ScrollView. Here's a step-by-step guide:

**1. Define Your Pages:** First, outline the structure of each page you want to display. This can be done using an array of objects, each representing a page:

```
const STEPS = [
  {
    id: 1,
    title: 'Screen1 Title',
    component: <Screen1 />,
  },
  {
    id: 2,
    title: 'Screen2 Title',
    component: <Screen2 />,
  },
  // Add more pages as needed
];

```

**2. Setup FlatList:** Use a FlatList to handle the horizontal paging between each full-screen view. Ensure the FlatList is configured to snap to each page and disable scroll indicators for a cleaner look:
```
import React, { useRef, useState } from 'react';
import { Dimensions, FlatList, ScrollView, StyleSheet } from 'react-native';

const WIDTH = Dimensions.get('window').width;

const YourComponent = () => {
  const ref = useRef();
  const [currentSlideIndex, setCurrentSlideIndex] = useState(0);

  // Function to navigate to the next page
  const goToNextSlide = () => {
    const nextSlideIndex = currentSlideIndex + 1;
    if (nextSlideIndex < STEPS.length) { // Ensure we don't exceed our page array
      const offset = nextSlideIndex * WIDTH;
      ref?.current?.scrollToOffset({ offset, animated: true });
      setCurrentSlideIndex(nextSlideIndex);
    }
  };

  return (
    <FlatList
      ref={ref}
      data={STEPS}
      keyExtractor={item => item.id.toString()}
      snapToAlignment="start"
      decelerationRate="fast"
      snapToInterval={WIDTH}
      showsHorizontalScrollIndicator={false}
      horizontal
      bounces={false}
      renderItem={({ item }) => (
        <ScrollView
          style={styles.container}
          showsVerticalScrollIndicator={false}>
          {/* Conditional rendering based on current slide index */}
          {currentSlideIndex === item.id - 1 && item.component}
        </ScrollView>
      )}
      scrollEnabled={false} // Disable FlatList scroll to control it via button
    />
  );
};

const styles = StyleSheet.create({
  container: {
    width: WIDTH,
    height: '100%', // Ensure full height
  },
});

```
The **goToNextSlide** function is triggered by a button (not shown in the snippet) to navigate to the next page. Remember to replace `<Screen1 />` and `<Screen2 />` with your actual component content.

This approach provides a smooth, app-like paging experience that can be easily controlled programmatically and is highly customizable to fit your specific needs.


--------------------------------------------------
Ansible | Add line &amp; update /etc/hosts when outdated
I want to insert line or update my `/etc/hosts` if there is new data. 
My following playbook is using `lineinfile`, however, I experienced issue that when one of my servers got new IP, the module would basically add new line containing the new IP, which is good, but the old one would still be present.

I&#39;d like to do it as neat as possible, hopefully just use 1 module, not the combination of `lineinfile` &amp; `replace`. 



Example of existing playbook
```
- ansible.builtin.lineinfile:
    path: /etc/hosts
    line: &quot;{{ hostvars[item].ansible_all_ipv4_addresses.0 }} {{ hostvars[item].ansible_hostname }} {{ hostvars[item].ansible_hostname }}&quot;
  loop: &quot;{{ groups[&#39;web&#39;] }}&quot;
```

So... I basiaclly end up with:

*/etc/hosts*
```
192.168.1.1 web1.example.com web1
192.168.1.2 web2.example.com web2
192.168.1.3 web3.example.com web3
```

**Provisioned new web2:**
```
192.168.1.1 web1.example.com web1
192.168.1.2 web2.example.com web2
192.168.1.3 web3.example.com web3
192.168.1.4 web2.example.com web2
```
||||||||||||||You need to use the [`regexp` option](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/lineinfile_module.html#parameter-regexp) to tell `lineinfile` which lines to replace. That might look something like this:

```yaml
- hosts: web
  gather_facts: true

- hosts: localhost
  gather_facts: false
  tasks:
    - ansible.builtin.lineinfile:
        path: hosts
        line: "{{ hostvars[item].ansible_all_ipv4_addresses.0 }} {{ hostvars[item].ansible_fqdn }} {{ hostvars[item].ansible_hostname }}"
        regexp: "{{ hostvars[item].ansible_fqdn }}"
      loop: "{{ groups['web'] }}"
```

In my test environment, running it the first time produces:

```
172.17.0.4 web1.example.com web1
172.17.0.3 web2.example.com web2
172.17.0.2 web3.example.com web3
```

And if I update the ip address of `web2`, I get:

```
172.17.0.4 web1.example.com web1
172.17.0.5 web2.example.com web2
172.17.0.2 web3.example.com web3
```

Ansible has replaced the line for `web2.example.com`, rather than appending a new line.

--------------------------------------------------
Populating a dynamic table based on drop down selections?
I have raw data in columns A:E, I want to bring it over to columns H2:K dynamically based on whatever values are f2 and g2. If f2 = column D name then bring over the data for that row highlighted in columns H:K

[![enter image description here][1]][1]

If I were to do this in excel, I&#39;d do:

    =let(
    _lastrow,match(2/1(A:A&lt;&gt;&quot;&quot;),
    _Product,A2:INDEX(A:A,),_lastrow),
    _Comments,B2:INDEX(B:B),_lastrow),
    _Criteria1,D2:INDEX(D:D),_lastrow),
    _Criteria2,E2:INDEX(E:E),_lastrow),
    _Hstack,CHOOSE({1,2,3,4},_Product,_Comments,_Criteria1,_Criteria2),
    FILTER(_Hstack,(_Criteria1=F2)*(_Criteria2=G2)))

but it&#39;s not working in google sheets. I know function syntax is a little different, but has anyone figured this out? 

Link to sheet: 

  [1]: https://i.stack.imgur.com/A2krb.png
||||||||||||||You can use the [`FILTER`](https://support.google.com/docs/answer/3093197?hl=en) function 

```cpp
=FILTER({A:A,C:E},D:D=F2,E:E=G2)
```

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/QoM7X.png

--------------------------------------------------
Convert numbers to English strings
Websites like http://www.easysurf.cc/cnvert18.htm and http://www.calculatorsoup.com/calculators/conversions/numberstowords.php tries to convert a numerical string into an english strings, but they are giving natural sounding output.

For example, on http://www.easysurf.cc/cnvert18.htm:

    [in]: 100456
    [out]:  one hundred  thousand four hundred fifty-six

this website is a little better, http://www.calculator.org/calculate-online/mathematics/text-number.aspx:

    [in]: 100456
    [out]: one hundred thousand, four hundred and fifty-six
    
    [in]: 10123124001
    [out]: ten billion, one hundred and twenty-three million, one hundred and twenty-four thousand, one 


but it breaks at some point:

    [in]: 10000000001
    [out]: ten billion, , , one 

I&#39;ve wrote my own version but it involves lots of rules and it caps at one billion, from http://pastebin.com/WwFCjYtt:

    import codecs
     
    def num2word (num):
      ones = {1:&quot;one&quot;,2:&quot;two&quot;,3:&quot;three&quot;,4:&quot;four&quot;,
              5:&quot;five&quot;,6:&quot;six&quot;,7:&quot;seven&quot;,8:&quot;eight&quot;,
              9:&quot;nine&quot;,0:&quot;zero&quot;,10:&quot;ten&quot;}
      teens = {11:&quot;eleven&quot;,12:&quot;twelve&quot;,13:&quot;thirteen&quot;,
               14:&quot;fourteen&quot;,15:&quot;fifteen&quot;}
      tens = {2:&quot;twenty&quot;,3:&quot;thirty&quot;,4:&quot;forty&quot;,
              5:&quot;fifty&quot;,6:&quot;sixty&quot;,7:&quot;seventy&quot;,
              8:&quot;eighty&quot;,9:&quot;ninety&quot;}
      lens = {3:&quot;hundred&quot;,4:&quot;thousand&quot;,6:&quot;hundred&quot;,7:&quot;million&quot;,
              8:&quot;million&quot;, 9:&quot;million&quot;,10:&quot;billion&quot;#,13:&quot;trillion&quot;,11:&quot;googol&quot;,
              }
     
      if num &gt; 999999999:
        return &quot;Number more than 1 billion&quot;
     
      # Ones
      if num &lt; 11:
        return ones[num]
      # Teens
      if num &lt; 20:
        word = ones[num%10] + &quot;teen&quot; if num &gt; 15 else teens[num]
        return word
      # Tens
      if num &gt; 19 and num &lt; 100:
        word = tens[int(str(num)[0])]
        if str(num)[1] == &quot;0&quot;:
          return word
        else:
          word = word + &quot; &quot; + ones[num%10]
          return word
     
      # First digit for thousands,hundred-thousands.
      if len(str(num)) in lens and len(str(num)) != 3:
        word = ones[int(str(num)[0])] + &quot; &quot; + lens[len(str(num))]
      else:
        word = &quot;&quot;
       
      # Hundred to Million  
      if num &lt; 1000000:
        # First and Second digit for ten thousands.  
        if len(str(num)) == 5:
          word = num2word(int(str(num)[0:2])) + &quot; thousand&quot;
        # How many hundred-thousand(s).
        if len(str(num)) == 6:
          word = word + &quot; &quot; + num2word(int(str(num)[1:3])) + \
                &quot; &quot; + lens[len(str(num))-2]
        # How many hundred(s)?
        thousand_pt = len(str(num)) - 3
        word = word + &quot; &quot; + ones[int(str(num)[thousand_pt])] + \
                &quot; &quot; + lens[len(str(num))-thousand_pt]
        # Last 2 digits.
        last2 = num2word(int(str(num)[-2:]))
        if last2 != &quot;zero&quot;:
          word = word + &quot; and &quot; + last2
        word = word.replace(&quot; zero hundred&quot;,&quot;&quot;)
        return word.strip()
     
      left, right = &#39;&#39;,&#39;&#39;  
      # Less than 1 million.
      if num &lt; 100000000:
        left = num2word(int(str(num)[:-6])) + &quot; &quot; + lens[len(str(num))]
        right = num2word(int(str(num)[-6:]))
      # From 1 million to 1 billion.
      if num &gt; 100000000 and num &lt; 1000000000:
        left = num2word(int(str(num)[:3])) +  &quot; &quot; + lens[len(str(num))]
        right = num2word(int(str(num)[-6:]))
      if int(str(num)[-6:]) &lt; 100:
        word = left + &quot; and &quot; + right
      else:  
        word = left + &quot; &quot; + right
      word = word.replace(&quot; zero hundred&quot;,&quot;&quot;).replace(&quot; zero thousand&quot;,&quot; thousand&quot;)
      return word
     
    print num2word(int(raw_input(&quot;Give me a number:\n&quot;)))

**How can I make the script i&#39;ve wrote accept `&gt; billion`?**

**Is there any other way to get the same output?**

**Can my code be written in a less verbose way?**
||||||||||||||A more general approach to this problem uses repeated division (i.e. `divmod`) and only hardcodes the special/edge cases necessary.

For example, `divmod(1034393, 1000000) -> (1, 34393)`, so you've effectively found the number of millions and are left with a remainder for further calculations.

Possibly more illustrative example: `divmod(1034393, 1000) -> (1034, 393)` which allows you to take off groups of 3 decimal digits at a time from the right.

In English we tend to group digits in threes, and similar rules apply. This should be parameterized and not hard coded. For example, "303" could be three hundred and three million, three hundred and three thousand, or three hundred and three. The logic should be the same except for the suffix, depending on what place you're in. Edit: looks like this is sort of there due to recursion.

Here is a partial example of the kind of approach I mean, using a generator and operating on integers rather than doing lots of `int(str(i)[..])` everywhere.

    say_base = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven',
        'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen',
        'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen']
    
    say_tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy',
        'eighty', 'ninety']
    
    def hundreds_i(num):
        hundreds, rest = divmod(num, 100)
        if hundreds:
            yield say_base[hundreds]
            yield ' hundred'
        if 0 < rest < len(say_base):
            yield ' and '
            yield say_base[rest]
        elif rest != 0:
            tens, ones = divmod(rest, 10)
            yield ' and '
            yield say_tens[tens]
            if ones > 0:
                yield '-'
                yield say_base[ones]
    
    assert "".join(hundreds_i(245)) == "two hundred and forty-five"
    assert "".join(hundreds_i(999)) == 'nine hundred and ninety-nine'
    assert "".join(hundreds_i(200)) == 'two hundred'



--------------------------------------------------
Is it indeed possible to manage roles and member accounts using MS Graph?
I&#39;ve been tasked to look into using MS Graph to manage Azure AD (aka Entra) roles and members by code. Currently I am on C# NET6 for that.

Before I dive in too deep and promise my manager something I can&#39;t deliver I want to be sure this is actually possible, because I do not see anything immediately directory related in the [Graph Explorer][1].

**What I think I need**

Using this [link][2] you can find the available Directory related scopes:

[![enter image description here][3]][3]

I think I would need the highlighted scope, so Directory.ReadWrite.All.

**What I don&#39;t see**

Using Graph Explorer I can find Groups, but I think that&#39;s AD groups, likely that&#39;s teams and Office 365 related?

If my above assumption is correct it&#39;s strange that I do not see anything Directory 
related.

**What I have**

I have used the Visual Studio Blazor Server template to generate that starter app, now with MS Graph and authentication enabled so that I also get this view:

[![enter image description here][4]][4]

(That line is the actual name)

It means I can confirm it&#39;s possible to at least get basic information from MS Graph, but not much else yet.

As mentioned I don&#39;t want to dive in yet, also because I would need privileged access and that needs to be discussed with the sys admins first.

**My questions**

1. Is it indeed possible to use MS Graph for Entra Role / Members management?

2. Is what I described above the correct first steps to take?

3. I remember reading that granting privileged access for MS Graph is still overruled by Entra itself. Is that true and does that mean what you can see / update is still limited by what you&#39;re allowed at Entra level?


  [1]: https://aka.ms/ge
  [2]: https://graphpermissions.merill.net/permission/Directory.ReadWrite.All
  [3]: https://i.stack.imgur.com/1Ydg0.png
  [4]: https://i.stack.imgur.com/t2T4X.png
||||||||||||||If I understand correctly you want to manage Azure Entra Id role assignments like the Global Administrator, User Administrator, Application Developer etc. roles. In that case the answer is yes this is possible.

For example, you can get [the role definitions](https://developer.microsoft.com/en-us/graph/graph-explorer?request=roleManagement%2Fdirectory%2FroleDefinitions&method=GET&version=v1.0&GraphUrl=https://graph.microsoft.com) and [assignments](https://developer.microsoft.com/en-us/graph/graph-explorer?request=roleManagement%2Fdirectory%2FroleAssignments&method=GET&version=v1.0&GraphUrl=https://graph.microsoft.com).

For your C# code, take a look at the `graphClient.RoleManagement.Directory` object

```csharp
// Code snippets are only available for the latest version. Current version is 5.x

// To initialize your graphClient, see https://learn.microsoft.com/en-us/graph/sdks/create-client?from=snippets&tabs=csharp
var result = await graphClient.RoleManagement.Directory.RoleAssignments.GetAsync((requestConfiguration) =>
{
	requestConfiguration.QueryParameters.Filter = "roleDefinitionId eq '62e90394-69f5-4237-9190-012177145e10'";
	requestConfiguration.QueryParameters.Expand = new string []{ "principal" };
});
```

Required privilege for role assignments is `RoleManagement.ReadWrite.Directory`

--------------------------------------------------
What are reference counted variables in Delphi?
I know what normal global variables and local variables are, but what are &quot;local reference-counted&quot; variables and &quot;local non reference-counted&quot; variables?

What is it? What&#39;s the difference? How do they work?

Can someone show code examples and explain please?
||||||||||||||A reference-counted object maintains an internal counter of how many variables refer to it at runtime.  Delphi native types that have reference counting include the string types `AnsiString` and `UnicodeString`, as well as `interface`s and dynamic arrays.

Such types are automatically managed by the Delphi compiler for you. Multiple variables of these types can refer to the same object in memory.  When a managed variable refers to an existing object, the compiler increases <sup>1</sup> the object's counter.  When that variable no longer refers to the object, because it was reassigned or went out of scope, the compiler decreases <sup>1</sup> the object's counter.

<sup>1: unless the variable is `const`, then it doesn't.</sup>

When an object's counter falls to zero, meaning no more variables refer to it, then the compiler frees the object from memory.

--------------------------------------------------
TypeScript array.sort() of typed values
Let&#39;s assume we have a type and an array:
```javascript
type Fruit = &#39;apple&#39; | &#39;orange&#39; | &#39;pineapple&#39;

const initialFruits: Fruit[] = [&#39;pineapple&#39;, &#39;apple&#39;, &#39;orange&#39;]
```
If I try to sort the array before assign the value I get an error:
```javascript
const initialFruits: Fruit[] = [&#39;pineapple&#39;, &#39;apple&#39;, &#39;orange&#39;].sort()

// Type &#39;string[]&#39; is not assignable to type &#39;Fruit[]&#39;.
```

I found one way to save types after sorting, but it looks weird:
```javascript
const initialFruits: Fruit[] = [
  &#39;pineapple&#39; as const,
  &#39;apple&#39; as const,
  &#39;orange&#39; as const
].sort()
```
Is there more convenient way to fix that?

||||||||||||||Array is sorted according to `Fruit` type.

    type Fruit = 'apple' | 'orange' | 'pineapple'

    const initialFruits: Fruit[] = ['pineapple', 'apple', 'orange'].sort() as Fruit[];

    const sortedFruits: Fruit[] = ['pineapple', 'apple', 'orange'].sort((a, b) => {
      return initialFruits.indexOf(a) - initialFruits.indexOf(b);
    }) as Fruit[];

--------------------------------------------------
WSL vscode command returning error &quot;not found&quot;
I have this message whenever I try to type `code .`

[![Screen capture containing an output obtained probably at a Unix-based WSL 2 terminal running on Windows. The output text is quoted exactly in the rest of the question for accessibility.][1]][1]

```bash
/mnt/c/Users/jerom/.vscode/extensions/ms-vscode-remote.remote-wsl-0.63.13/scripts/wslCode.sh: 69: /home/jerome/.vscode-server-server/bin/f80445acd5a3dadef24aa209168452a3d97cc326/bin/code: not found
```

  [1]: https://i.stack.imgur.com/6cQOm.png

Can someone help me please?

Thanks in advance
||||||||||||||I just now had the same issue on WSL2 Kali. I tried everything given here: https://github.com/microsoft/vscode-remote-release/issues/2962.

My problem was VS Code was not on Kali's path. Here's how I fixed it :)

```bash
echo $PATH
# Confirm PATH is missing this: 
/mnt/c/Users/wcd/AppData/Local/Programs/Microsoft VS Code/bin/

# Confirm VS Code's location then export:
export PATH=$PATH:"/mnt/c/Users/wcd/AppData/Local/Programs/Microsoft VS Code/bin/" 
# It's temporarily added to path...
echo $PATH
# This should now trigger VS Code Server install,
# then open ~/ in VS Code. 
code .

# If above works, make it permanent:
echo 'export PATH=$PATH:"/mnt/c/Users/wcd/AppData/Local/Programs/Microsoft VS Code/bin/"' >> ~/.bashrc

# Restart shell + test
exec "$SHELL"
code .
```

--------------------------------------------------
Concurrent Consumers in Quarkus
Is there a way to manage concurrency for concurrent consumers while trying  to consume messages from Quarkus?

I am looking for Batch Processing with concurrency to process several messages in seconds. I tried the same in SpringBoot earlier and it works ok. Trying to find a way if Quarkus is better.
||||||||||||||Check the [SmallRye Reactive Messaging][1] tutorial and the [Kafka Guide][2]. 

Basically, you can either do batch processing in an imperative or reactive way. 

Imperative way:

```java
@Incoming("your-topic")
public void handle(List<Message> messages) {
    // process
}
```
Reactive way:

```java
@Incoming("your-topic")
public Uni<Void> handle(Multi<Message> messages) {
    // process
}
```



  [1]: https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/3.3/model/model.html#processing-streams
  [2]: https://quarkus.io/guides/kafka#receiving-kafka-records-in-batches

--------------------------------------------------
Connecting to confluent kafka cloud from flink sql
I get the below error when I try to connect to [Confluent][1] cloud.

```
Caused by: org.apache.kafka.common.KafkaException: java.security.NoSuchAlgorithmException: TLSv1.3 SSLContext not available
    at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory.createSSLContext(DefaultSslEngineFactory.java:268)
    at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory.configure(DefaultSslEngineFactory.java:173)
    at org.apache.kafka.common.security.ssl.SslFactory.instantiateSslEngineFactory(SslFactory.java:140)
    at org.apache.kafka.common.security.ssl.SslFactory.configure(SslFactory.java:97)
    at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:180)
    ... 18 more
Caused by: java.security.NoSuchAlgorithmException: TLSv1.3 SSLContext not available
    at java.base/sun.security.jca.GetInstance.getInstance(GetInstance.java:159)
    at java.base/javax.net.ssl.SSLContext.getInstance(SSLContext.java:168)
    at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory.createSSLContext(DefaultSslEngineFactory.java:243)
```

The flink SQL I am using:

```
CREATE TABLE IF NOT EXISTS some_source_table
(
    headers     VARCHAR NOT NULL,
    id          VARCHAR NOT NULL,
    `timestamp` TIMESTAMP_LTZ(3) NULL,
    type        VARCHAR NOT NULL,
    contentJson VARCHAR NOT NULL
) WITH (
    &#39;connector&#39; = &#39;kafka&#39;,
    &#39;topic-pattern&#39; = &#39;kafka_topic__.+?&#39;,
    &#39;properties.bootstrap.servers&#39; = &#39;some.aws.confluent.cloud:9092&#39;,
    &#39;properties.group.id&#39; = &#39;some-id-1&#39;,
    &#39;scan.startup.mode&#39; = &#39;latest-offset&#39;,
    &#39;format&#39; = &#39;json&#39;,
    &#39;json.timestamp-format.standard&#39; = &#39;ISO-8601&#39;,
    &#39;scan.topic-partition-discovery.interval&#39;= &#39;60000&#39;,
    &#39;json.fail-on-missing-field&#39; = &#39;false&#39;,
    &#39;json.ignore-parse-errors&#39; = &#39;true&#39;,
    &#39;properties.security.protocol&#39; = &#39;SASL_SSL&#39;,
    &#39;properties.sasl.mechanism&#39; = &#39;PLAIN&#39;,
    &#39;properties.sasl.jaas.config&#39; = &#39;org.apache.kafka.common.security.plain.PlainLoginModule required username=*** password=***;&#39;,
    &#39;properties.ssl.endpoint.identification.algorithm&#39; = &#39;https&#39;
);
```

When I check the Java version in the [Kubernetes][2] pod that run the task manager I see `11.0.18`. I get this by running `System.getProperty(&quot;java.version&quot;)`.


UPDATE
my gradle file looks like this:
    implementation &quot;org.apache.flink:flink-streaming-java:${flinkVersion}&quot;
    implementation &quot;org.apache.flink:flink-table-api-java-bridge:${flinkVersion}&quot;
    implementation &quot;org.apache.flink:flink-table-planner_${scalaVersion}:${flinkVersion}&quot;
    implementation &quot;org.apache.flink:flink-json:${flinkVersion}&quot;
    implementation &quot;org.apache.flink:flink-clients:${flinkVersion}&quot;

    //this seems to be needed : removing this is causing error in guice JsonProperty.Naming
    implementation &quot;com.fasterxml.jackson.core:jackson-databind:${jacksonVersion}&quot;
    implementation &quot;com.fasterxml.jackson.datatype:jackson-datatype-jsr310:${jacksonVersion}&quot;

    implementation &quot;org.apache.flink:flink-statebackend-rocksdb:${flinkVersion}&quot;

    //needed for a local flink ui to show when running environment = local
    implementation &quot;org.apache.flink:flink-runtime-web:${flinkVersion}&quot;

    implementation &quot;org.apache.logging.log4j:log4j-core:${log4jVersion}&quot;
    implementation &quot;org.apache.logging.log4j:log4j-api:${log4jVersion}&quot;
    implementation &quot;org.apache.logging.log4j:log4j-slf4j-impl:${log4jVersion}&quot;

    implementation &#39;io.jsonwebtoken:jjwt:0.2&#39;
    implementation &#39;com.mashape.unirest:unirest-java:1.4.9&#39;
    implementation &#39;org.rocksdb:rocksdbjni:7.9.2&#39;

    // --------------------------------------------------------------
    // Dependencies that should be part of the shadow jar, e.g.
    // connectors. These must be in the flinkShadowJar configuration!
    // --------------------------------------------------------------
    flinkShadowJar files(&#39;libs/flink-connector-kafka-1.16.0.jar&#39;)
    flinkShadowJar &quot;org.apache.flink:flink-connector-base:${flinkVersion}&quot;

    flinkShadowJar &quot;org.apache.commons:commons-text:1.10.0&quot;
    flinkShadowJar &quot;org.projectlombok:lombok:1.18.26&quot;
    flinkShadowJar &quot;com.fasterxml.jackson.core:jackson-databind:${jacksonVersion}&quot;
    flinkShadowJar &quot;com.fasterxml.jackson.datatype:jackson-datatype-jsr310:${jacksonVersion}&quot;
    flinkShadowJar &quot;com.sailpoint:atlas:${ATLAS_VERSION}&quot;
    flinkShadowJar &quot;com.sailpoint:atlas-event:${ATLAS_VERSION}&quot;
    flinkShadowJar &quot;com.google.inject:guice:5.1.0&quot;
    flinkShadowJar &quot;org.apache.flink:flink-s3-fs-hadoop:${flinkVersion}&quot;
    flinkShadowJar &#39;io.jsonwebtoken:jjwt:0.2&#39;
    flinkShadowJar &#39;com.mashape.unirest:unirest-java:1.4.9&#39;

What could I be missing here?

  [1]: https://en.wikipedia.org/wiki/Confluent,_Inc.
  [2]: https://en.wikipedia.org/wiki/Kubernetes


||||||||||||||I'm assuming you're using [Flink's SQL connector][1], which has shaded the Kafka Clients JAR in there. Because of the shading, that means that the JAAS Config needs to be pointed to the shaded PlainLoginModule, e.g.

```
'properties.sasl.jaas.config' = 'org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="key" password="value";'
```


  [1]: https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/table/kafka/

--------------------------------------------------
Match regex except if it ends like a specified string?
I&#39;m trying to match everything before the top property except if it has style.top. This is for old browsers so negative lookbehind is not allowed. Any ideas?

Tried a lot of different strategies already. Not great at regex.
||||||||||||||You can use a negative lookahead:

    ^(?!(?:.*[^$\s])?\bstyle\W*\.top\b).*\.top\b.*

The pattern matches:

- `^` Start of the string
- `(?!` Negative lookahead
  - `(?:` Non capture group
    - `.*[^$\s]` Match optional characters followed by a non whitespace char other than `$`
  - `)?` Close the non capture group and make it optional
  - `\bstyle\W*\.top\b` Match the word `style` followed by optional non word characters and then `.top`
- `)` Close the lookahead
- `.*\btop\b.*` Match the string containing  `.top`

See the [regex tests](https://regexr.com/7ql9g).

<hr>

If it should be `style` or `(style)` then:

    ^(?!(?:.*[^$\s])?(?:\bstyle|\(style\))\.top\b).*\.top\b.*

--------------------------------------------------
How to create a Github action for labeling issues created by specific users?
I&#39;d like to create a Github action that automatically labels issues of my Patreon supporters.

I created a file with the supporters&#39; Github usernames:

#### `./github/supporters.json`

```js
{
  usernames: [
    &#39;test1&#39;, 
    &#39;test2&#39;
  ]
}
```
(I can change the file to any other format, if that would make it easier)

How do I check if the issue was created by a user on the list?

I&#39;ve been thinking about using this marketplace action:
https://github.com/marketplace/actions/super-labeler

But it seems like it only accepts a Regex pattern. Should I just list usernames in the Regex directly?
||||||||||||||I figured out a solution, using this action https://github.com/actions-ecosystem/action-add-labels:

I'm specifying usernames in the github action .yaml file directly:

```
[
  "some-username-1",
  "some-username-2"
]
```

#### `./github/workflows/label-issue.yaml`

```
name: Label supporter

on:
  issues:
    types: [opened, edited]

jobs:
  add_label_tier_1:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions-ecosystem/action-add-labels@v1.1.0
        if: 
          ${{ 
            contains(
              fromJson('[
                "some-username-1",
                "some-username-2"
              ]'), 
              github.actor
            ) 
          }}
        with:
          github_token: ${{ github.token }}
          labels: |
            supporter 💖
            priority +1
            
  add_label_tier_2:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions-ecosystem/action-add-labels@v1.1.0
        if: 
          ${{ 
            contains(
              fromJson('[
                "some-username-3"
              ]'), 
              github.actor
            ) 
          }}
        with:
          github_token: ${{ github.token }}
          labels: |
            supporter 💖
            priority +2
```


--------------------------------------------------
How to apply piecewise linear fit in Python?
I am trying to fit piecewise linear fit as shown in fig.1 for a data set

![enter image description here][1]

This figure was obtained by setting on the lines. I attempted to apply a piecewise linear fit using the code:

    from scipy import optimize
	import matplotlib.pyplot as plt
	import numpy as np


	x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15])
	y = np.array([5, 7, 9, 11, 13, 15, 28.92, 42.81, 56.7, 70.59, 84.47, 98.36, 112.25, 126.14, 140.03])


	def linear_fit(x, a, b):
	    return a * x + b
	fit_a, fit_b = optimize.curve_fit(linear_fit, x[0:5], y[0:5])[0]
	y_fit = fit_a * x[0:7] + fit_b
	fit_a, fit_b = optimize.curve_fit(linear_fit, x[6:14], y[6:14])[0]
	y_fit = np.append(y_fit, fit_a * x[6:14] + fit_b)


	figure = plt.figure(figsize=(5.15, 5.15))
	figure.clf()
	plot = plt.subplot(111)
	ax1 = plt.gca()
	plot.plot(x, y, linestyle = &#39;&#39;, linewidth = 0.25, markeredgecolor=&#39;none&#39;, marker = &#39;o&#39;, label = r&#39;\textit{y_a}&#39;)
	plot.plot(x, y_fit, linestyle = &#39;:&#39;, linewidth = 0.25, markeredgecolor=&#39;none&#39;, marker = &#39;&#39;, label = r&#39;\textit{y_b}&#39;)
	plot.set_ylabel(&#39;Y&#39;, labelpad = 6)
	plot.set_xlabel(&#39;X&#39;, labelpad = 6)
	figure.savefig(&#39;test.pdf&#39;, box_inches=&#39;tight&#39;)
	plt.close()    

But this gave me fitting of the form in fig. 2, I tried playing with the values but no change I can&#39;t get the fit of the upper line proper. The most important requirement for me is how can I get Python to get the gradient change point. 

I want the code to recognize and fit two linear fits in the appropriate range. How can this be done in Python?

![enter image description here][2]


  [1]: http://i.stack.imgur.com/Thrit.png
  [2]: http://i.stack.imgur.com/UjrF6.png
||||||||||||||You can use `numpy.piecewise()` to create the piecewise function and then use `curve_fit()`, Here is the code

    from scipy import optimize
    import matplotlib.pyplot as plt
    import numpy as np
    %matplotlib inline
    
    x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15], dtype=float)
    y = np.array([5, 7, 9, 11, 13, 15, 28.92, 42.81, 56.7, 70.59, 84.47, 98.36, 112.25, 126.14, 140.03])
    
    def piecewise_linear(x, x0, y0, k1, k2):
        return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])
    
    p , e = optimize.curve_fit(piecewise_linear, x, y)
    xd = np.linspace(0, 15, 100)
    plt.plot(x, y, "o")
    plt.plot(xd, piecewise_linear(xd, *p))

the output:

![enter image description here][1]


For an N parts fitting, please reference [segments_fit.ipynb][2]


  [1]: http://i.stack.imgur.com/xw0QH.png
  [2]: https://gist.github.com/ruoyu0088/70effade57483355bbd18b31dc370f2a

--------------------------------------------------
Can&#39;t install xdebug on Mac with Homebrew
I&#39;m kind of new to using Homebrew, but I love it.  It&#39;s so easy.  I&#39;m trying install Xdebug.  Some of the posts on the web say to do this:

    brew install xdebug

But it doesn&#39;t work.  I get: `Error, no available formula.`  

I did `brew search xdebug` and it returned:

    josegonzalez/php/php53-xdebug	 josegonzalez/php/php54-xdebug

I tried several different iterations of `brew install` with this including `brew install php53-xdebug`, but still no luck.  Can someone help me?  I can&#39;t find anything on Xdebug&#39;s site about using Homebrew, but yet posts on the web seem to indicate it&#39;s possible.
||||||||||||||Add this repository: https://github.com/josegonzalez/homebrew-php#readme

Then use `brew install php54-xdebug` for PHP 5.4

Or `brew install php53-xdebug` for PHP 5.3

Or `brew install php55-xdebug` for PHP 5.5

--------------------------------------------------
React router 6 - Avoid re-render the parent components of a nested child-route
Here is one my Routes:

    &lt;Routes&gt;
      &lt;Route path=&quot;A&quot;&gt;
        &lt;Route path=&quot;:date&quot; element={
    	  &lt;Wrapper&gt;
    	    &lt;ProviderB&gt;
    		  &lt;Summary /&gt;
      	  &lt;/ProviderB&gt;
      	&lt;/Wrapper&gt;
        }
        /&gt;
      &lt;/Route&gt;
    &lt;/Routes&gt;

This is **Wrapper**:

    export const Wrapper: React.FC&lt;Props&gt; = ({children}) =&gt; {
       return (
           &lt;ProviderA&gt;
             {children}
           &lt;/ProviderA&gt;
       );
    };

This is component **ProviderA**: 

    const ContextAction = createContext({
      ...
    });
    
    const ContextValue = createContext({
      ...
    });
    
    export const ProviderA: React.FC&lt;Props&gt; = ({children}) =&gt; {
       const [state, setState] = useState(&#39;&#39;);
    
      return (
        &lt;ContextAction.Provider value={setState}&gt;
          &lt;ContextValue.Provider value={state}&gt;
            {children}
          &lt;/ContextValue.Provider&gt;
        &lt;/ContextAction.Provider&gt;
      );
    };

And this is component **ProviderB**: 

    const ContextAction = createContext({
      ...
    });
    
    const ContextValue = createContext({
      ...
    });
    
    export const ProviderB: React.FC&lt;Props&gt; = ({children}) =&gt; {
       const [state, setState] = useState(&#39;&#39;);
    
      return (
        &lt;ContextAction.Provider value={setState}&gt;
          &lt;ContextValue.Provider value={state}&gt;
            {children}
          &lt;/ContextValue.Provider&gt;
        &lt;/ContextAction.Provider&gt;
      );
    };


What I want to achieve is to render only **Summary** when the date in the URL does change. For example, a URL change from A/26-01-2024 to A/25-01-2014 should not trigger the re-rendering of **Wrapper**; instead it should re-render only the child-nested route **Summary**. How may I achieve that by using `&lt;Outlet /&gt;`?
Hopefully, the above makes sense even if it is tricky.
||||||||||||||> What I want to achieve is to render only Summary when the date in the
> URL does change.

This is just how React works. If you don't want `ProviderA` and `ProviderB` to rerender when the route rerenders then move them higher up the ReactTree.

I suggest creating a layout route that renders the providers and an `Outlet` for the nested routes.

Example:

```jsx
import { Outlet } from 'react-router-dom';

export const ProviderLayout = () => (
  <ProviderA>
    <ProviderB>
      <Outlet />
    </ProviderB>
  </ProviderA>
);
```

```jsx
<Routes>
  <Route path="A">
    <Route element={<ProviderLayout />}>
      <Route path=":date" element={<Summary />} />
    </Route>
  </Route>
</Routes>
```

If rerendering is still an issue then continue pushing the providers higher and higher up the ReactTree until they are being rendered from a stable component that doesn't rerender when the route(s) rerender(s).

--------------------------------------------------
Flutter change color when button gets clicked
I am working on an app where the user is able to expand and collapse text.
I want to text at the bottom to be blurred. As soon as the user expands the text the text should be displayed normal. When the text gets collapsed again, I want the text at the bottom again to be blurred.
I solved it the following way. My problem is that when the user collapse the text, the text at the bottom is not blurred again.


Here is my code:

    class ExpandableText extends StatefulWidget {
      final String text;
      const ExpandableText({Key? key, required this.text}) : super(key: key);
    
      @override
      State&lt;ExpandableText&gt; createState() =&gt; _ExpandableTextState();
    }
    
    class _ExpandableTextState extends State&lt;ExpandableText&gt; {
      late String firstHalf;
      late String secondHalf;
    
      bool hiddenText = true;
    
      Color fadeStrong = Colors.black.withOpacity(0.8);
      Color fadeWeak = Colors.black.withOpacity(0.6);
    
      // double textHeight = Dimensions.screenHight / 5.63;
      double textHeight = 180;
    
      @override
      void initState() {
        super.initState();
        if (widget.text.length &gt; textHeight) {
          firstHalf = widget.text.substring(0, textHeight.toInt());
          secondHalf =
              widget.text.substring(textHeight.toInt() + 1, widget.text.length);
        } else {
          firstHalf = widget.text;
          secondHalf = &quot;&quot;;
        }
      }
    
      @override
      Widget build(BuildContext context) {
        return Stack(children: [
          Container(
            child: secondHalf.isEmpty
                ? Text(
                    firstHalf,
                    style: const TextStyle(color: Colors.white),
                  )
                : Column(
                    children: [
                      Text(
                        hiddenText ? (&quot;$firstHalf...&quot;) : (firstHalf + secondHalf),
                        style: const TextStyle(color: Colors.white),
                      ),
                      InkWell(
                        onTap: () {
                          setState(() {
                            hiddenText = !hiddenText;
                            hiddenText
                                ? fadeStrong = fadeStrong
                                : fadeStrong = Colors.transparent;
                        hiddenText
                            ? fadeWeak = fadeWeak
                            : fadeWeak = Colors.transparent;
                          });
                        },
                        child: Row(
                          children: [
                            hiddenText
                                ? const Text(
                                    &#39;Read more&#39;,
                                    style: TextStyle(color: Colors.blue),
                                  )
                                : const Text(&#39;Read less&#39;,
                                    style: TextStyle(color: Colors.blue)),
                            Icon(
                              hiddenText
                                  ? Icons.arrow_drop_down
                                  : Icons.arrow_drop_up_outlined,
                              color: Colors.blue,
                            )
                          ],
                        ),
                      )
                    ],
                  ),
          ),
          Container(
            margin: const EdgeInsets.only(top: 52),
            height: 20,
            color: fadeStrong,
          ),
          Container(
            margin: const EdgeInsets.only(top: 32),
            height: 20,
            color: fadeWeak,
          ),
        ]);
      }
    }

[![Text collapsed][1]][1]

[![Text expanded][2]][2]

[![Text collapsed again][3]][3]


  [1]: https://i.stack.imgur.com/fBj8y.png
  [2]: https://i.stack.imgur.com/P6hqX.png
  [3]: https://i.stack.imgur.com/gsWjd.png
||||||||||||||When you overrides the color on tap event, that lose the initial reference

                   onTap: () {
                      setState(() {
                        hiddenText = !hiddenText;
                        hiddenText
                            ? fadeStrong = fadeStrong
                            : fadeStrong = Colors.transparent;
                    hiddenText
                        ? fadeWeak = fadeWeak
                        : fadeWeak = Colors.transparent;
                      });
                    },


maybe you should try something like that will help you

                   onTap: () {
                      setState(() {
                        hiddenText = !hiddenText;
                        hiddenText
                            ? fadeStrong = Colors.black.withOpacity(0.8)
                            : fadeStrong = Colors.transparent;
                    hiddenText
                        ? fadeWeak = Colors.black.withOpacity(0.6)
                        : fadeWeak = Colors.transparent;
                      });
                    },

--------------------------------------------------
Sending outlook email using win32com on python from a secondary mailbox account

I&#39;m having an issue [like this one][1]

```
        outlook = win32com.client.Dispatch(&#39;outlook.application&#39;)
        accounts = win32com.client.Dispatch(&quot;outlook.Application&quot;).Session.Accounts
        print(accounts[1])
        mail = outlook.CreateItem(0)
        mail.SentOnBehalfOfName  =  accounts[1]
        mail.SendUsingAccount  =  accounts[1]
        mail.To = to
        mail.Subject = &#39;Subject TEST&#39;
        mail.HTMLBody = emailBody
        mail.display()
        mail.Send()
```
if I comment ```mail.Send()``` the window that shows up will show everything correctly but if I send it, i&#39;ll get reply ```This message could not be sent. You do not have the permission to send the message on behalf of the specified user.``` which is obviously not true since if instead of sending directly and choose the exact same email from the dropdown menu in From, and then click SEND, it will send the email with no issues.

  [1]: https://social.msdn.microsoft.com/Forums/en-US/cc25d70a-eb6d-4bca-bcae-33c05380e61e/outlook-macro-sentonbehalfofname?forum=outlookdev
||||||||||||||So with the help of [@Eugene Astafiev][1] and [this post][2] I fixed the issue like this:

```
        outlook = win32com.client.Dispatch('outlook.application')
        for account in outlook.Session.Accounts:
            if account.DisplayName == "email@email.om":
                print(account)
                mail = outlook.CreateItem(0)
                mail._oleobj_.Invoke(*(64209, 0, 8, 0, account))
                mail.To = to
                mail.Subject = 'Vodafone Support'
                mail.HTMLBody = emailBody
                #mail.display()
                mail.Send()
```


  [1]: https://stackoverflow.com/a/74195997/10747598
  [2]: https://stackoverflow.com/questions/57826082/python-win32-client-sent-email-from-a-secondary-outlook-account-from-its-own-out

--------------------------------------------------
JTextArea setText() &amp; UndoManager
I&#39;m using an `UndoManager` to capture changes in my `JTextArea`.

The method `setText()` however deletes everything and then pastes the text. When I undo I firstly see an empty area and then it would show which text it had before.

How to reproduce:

1. Run the following code
2. Click the `setText()` button
3. Press &lt;kbd&gt;CTRL+Z&lt;/kbd&gt; to undo (you&#39;ll see an empty textarea!)
4. Press &lt;kbd&gt;CTRL+Z&lt;/kbd&gt; to undo (you&#39;ll see the actual previous text)

I want to skip 3).

    import javax.swing.AbstractAction;
    import javax.swing.JFrame;
    import javax.swing.JTextArea;
    import javax.swing.KeyStroke;
    import javax.swing.event.UndoableEditEvent;
    import javax.swing.event.UndoableEditListener;
    import javax.swing.text.Document;
    import javax.swing.undo.CannotRedoException;
    import javax.swing.undo.CannotUndoException;
    import javax.swing.undo.UndoManager;
    
    import java.awt.event.ActionEvent;
    import javax.swing.JButton;
    import java.awt.event.ActionListener;
    
    @SuppressWarnings(&quot;serial&quot;)
    public class JTextComponentSetTextUndoEvent extends JFrame
    {
    	JTextArea area = new JTextArea();
    
    	public JTextComponentSetTextUndoEvent()
    	{
    		setSize(300, 300);
    		setDefaultCloseOperation(EXIT_ON_CLOSE);
    		getContentPane().setLayout(null);
    
    		area.setText(&quot;Test&quot;);
    		area.setBounds(0, 96, 146, 165);
    		getContentPane().add(area);
    
    		JButton btnSettext = new JButton(&quot;setText()&quot;);
    		btnSettext.addActionListener(new ActionListener()
    		{
    			public void actionPerformed(ActionEvent arg0)
    			{
    				area.setText(&quot;stackoverflow.com&quot;);
    			}
    		});
    		btnSettext.setBounds(0, 28, 200, 50);
    		getContentPane().add(btnSettext);
    
    		final UndoManager undoManager = new UndoManager();
    		Document doc = area.getDocument();
    
    		doc.addUndoableEditListener(new UndoableEditListener()
    		{
    			public void undoableEditHappened(UndoableEditEvent evt)
    			{
    				undoManager.addEdit(evt.getEdit());
    			}
    		});
    
    		area.getActionMap().put(&quot;Undo&quot;, new AbstractAction(&quot;Undo&quot;)
    		{
    			public void actionPerformed(ActionEvent evt)
    			{
    				try
    				{
    					if (undoManager.canUndo())
    					{
    						undoManager.undo();
    					}
    				} catch (CannotUndoException e)
    				{
    				}
    			}
    		});
    
    		area.getInputMap().put(KeyStroke.getKeyStroke(&quot;control Z&quot;), &quot;Undo&quot;);
    
    		area.getActionMap().put(&quot;Redo&quot;, new AbstractAction(&quot;Redo&quot;)
    		{
    			public void actionPerformed(ActionEvent evt)
    			{
    				try
    				{
    					if (undoManager.canRedo())
    					{
    						undoManager.redo();
    					}
    				} catch (CannotRedoException e)
    				{
    				}
    			}
    		});
    
    		area.getInputMap().put(KeyStroke.getKeyStroke(&quot;control Y&quot;), &quot;Redo&quot;);
    	}
    
    	public static void main(String[] args)
    	{
    		new JTextComponentSetTextUndoEvent().setVisible(true);
    	}
    }
||||||||||||||You can try something like this:

	//Works fine for me on Windows 7 x64 using JDK 1.7.0_60:
	import java.awt.*;
	import java.awt.event.*;
	import java.util.*;
	import javax.swing.*;
	import javax.swing.event.*;
	import javax.swing.text.*;
	import javax.swing.undo.*;
	
	public final class UndoManagerTest {
	  private final JTextField textField0 = new JTextField("default");
	  private final JTextField textField1 = new JTextField();
	  private final UndoManager undoManager0 = new UndoManager();
	  private final UndoManager undoManager1 = new UndoManager();
	
	  public JComponent makeUI() {
	    textField1.setDocument(new CustomUndoPlainDocument());
	    textField1.setText("aaaaaaaaaaaaaaaaaaaaa");
	
	    textField0.getDocument().addUndoableEditListener(undoManager0);
	    textField1.getDocument().addUndoableEditListener(undoManager1);
	
	    JPanel p = new JPanel();
	    p.add(new JButton(new AbstractAction("undo") {
	      @Override public void actionPerformed(ActionEvent e) {
	        if (undoManager0.canUndo()) {
	          undoManager0.undo();
	        }
	        if (undoManager1.canUndo()) {
	          undoManager1.undo();
	        }
	      }
	    }));
	    p.add(new JButton(new AbstractAction("redo") {
	      @Override public void actionPerformed(ActionEvent e) {
	        if (undoManager0.canRedo()) {
	          undoManager0.redo();
	        }
	        if (undoManager1.canRedo()) {
	          undoManager1.redo();
	        }
	      }
	    }));
	    p.add(new JButton(new AbstractAction("setText(new Date())") {
	      @Override public void actionPerformed(ActionEvent e) {
	        String str = new Date().toString();
	        textField0.setText(str);
	        textField1.setText(str);
	      }
	    }));
	
	    Box box = Box.createVerticalBox();
	    box.setBorder(BorderFactory.createEmptyBorder(5, 5, 5, 5));
	    box.add(makePanel("Default", textField0));
	    box.add(Box.createVerticalStrut(5));
	    box.add(makePanel("replace ignoring undo", textField1));
	
	    JPanel pp = new JPanel(new BorderLayout());
	    pp.add(box, BorderLayout.NORTH);
	    pp.add(p, BorderLayout.SOUTH);
	    return pp;
	  }
	  private static JPanel makePanel(String title, JComponent c) {
	    JPanel p = new JPanel(new BorderLayout());
	    p.setBorder(BorderFactory.createTitledBorder(title));
	    p.add(c);
	    return p;
	  }
	  public static void main(String[] args) {
	    EventQueue.invokeLater(new Runnable() {
	      @Override public void run() {
	        createAndShowGUI();
	      }
	    });
	  }
	  public static void createAndShowGUI() {
	    JFrame f = new JFrame();
	    f.setDefaultCloseOperation(WindowConstants.EXIT_ON_CLOSE);
	    f.getContentPane().add(new UndoManagerTest().makeUI());
	    f.setSize(320, 240);
	    f.setLocationRelativeTo(null);
	    f.setVisible(true);
	  }
	}
	
	class CustomUndoPlainDocument extends PlainDocument {
	  private CompoundEdit compoundEdit;
	  @Override protected void fireUndoableEditUpdate(UndoableEditEvent e) {
	    if (compoundEdit == null) {
	      super.fireUndoableEditUpdate(e);
	    } else {
	      compoundEdit.addEdit(e.getEdit());
	    }
	  }
	  @Override public void replace(
	      int offset, int length,
	      String text, AttributeSet attrs) throws BadLocationException {
	    if (length == 0) {
	      System.out.println("insert");
	      super.replace(offset, length, text, attrs);
	    } else {
	      System.out.println("replace");
	      compoundEdit = new CompoundEdit();
	      super.fireUndoableEditUpdate(new UndoableEditEvent(this, compoundEdit));
	      super.replace(offset, length, text, attrs);
	      compoundEdit.end();
	      compoundEdit = null;
	    }
	  }
	}

--------------------------------------------------
Can I have persistent sessions for VS Code Remote Development?
Visual Studio Code offers a really nice way to do remote development over SSH with its Remote Development extension. Does this extension and its terminal has support for permanent sessions that would survive disconnects?

- You can achieve this by running UNIX `screen` within Visual Studio Code terminal

- However, Visual Studio Code itself installs its remote agent, so maybe this remote agent offers similar functionality built-in, leading to easier user experience (no need to hassle with `screen`)
||||||||||||||# Introduction
> Does this extension and its terminal has support for permanent sessions that would survive disconnects?

No, it seems that, currently, the [«Remote - SSH» Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh) does not have such a feature.

# Straightforward solution
It seems to be a known «Remote - SSH» Visual Studio Code extension issue:

* GitHub issue: [Persistent SSH session · Issue #3096 · microsoft/vscode-remote-release · GitHub](https://github.com/microsoft/vscode-remote-release/issues/3096).

Roughly speaking, the solution is:

1. To communicate with the maintainers (for example, by posting comments on the GitHub issue).
2. If applicable, to wait for a next «Remote - SSH» Visual Studio Code extension release with the resolved issue.

--------------------------------------------------
FastAPI: Performance results differ between run_in_threadpool() and run_in_executor() with ThreadPoolExecutor

Here&#39;s a minimal reproducible example of my FastAPI app. I have a strange behavior and I&#39;m not sure I understand the reason. 

I&#39;m using ApacheBench (`ab`) to send multiple requests as follows:
```
ab -n 1000 -c 50 -H &#39;accept: application/json&#39; -H &#39;x-data-origin: source&#39; &#39;http://localhost:8001/test/async&#39;
```

**FastAPI app**

    import time
    import asyncio
    import enum
    from typing import Any

    from fastapi import FastAPI, Path, Body
    from starlette.concurrency import run_in_threadpool

    app = FastAPI()
    loop = asyncio.get_running_loop()
    def sync_func() -&gt; None:
        time.sleep(3)
        print(&quot;sync func&quot;)
    
    async def sync_async_with_fastapi_thread() -&gt; None:
        await run_in_threadpool( time.sleep, 3)
        print(&quot;sync async with fastapi thread&quot;)
    
    async def sync_async_func() -&gt; None:
        await loop.run_in_executor(None, time.sleep, 3)
    
    async def async_func() -&gt; Any:
        await asyncio.sleep(3)
        print(&quot;async func&quot;)
    
    @app.get(&quot;/test/sync&quot;)
    def test_sync() -&gt; None:
        sync_func()
        print(&quot;sync&quot;)
    
    @app.get(&quot;/test/async&quot;)
    async def test_async() -&gt; None:
        await async_func()
        print(&quot;async&quot;)
    
    @app.get(&quot;/test/sync_async&quot;)
    async def test_sync_async() -&gt; None:
        await sync_async_func()
        print(&quot;sync async&quot;)
    
    @app.get(&quot;/test/sync_async_fastapi&quot;)
    async def test_sync_async_with_fastapi_thread() -&gt; None:
        await sync_async_with_fastapi_thread()
        print(&quot;sync async with fastapi thread&quot;)

Here&#39;s the ApacheBench results:

**async with (asyncio.sleep)** : 
*Concurrency Level:      50
- Time taken for tests:   63.528 seconds
- Complete requests:      1000
- Failed requests:        0
- Total transferred:      128000 bytes
- HTML transferred:       4000 bytes
- Requests per second:    15.74 [#/sec] (mean)
- **Time per request:       3176.407 [ms] (mean)**
- Time per request:       63.528 [ms] (mean, across all concurrent requests)
Transfer rate:          1.97 [Kbytes/sec] received*




**sync (with time.sleep):**
Concurrency Level:      50
- *Time taken for tests:   78.615 seconds
- Complete requests:      1000
- Failed requests:        0
- Total transferred:      128000 bytes
- HTML transferred:       4000 bytes
- Requests per second:    12.72 [#/sec] (mean)
- **Time per request:       3930.751 [ms] (mean)**
- Time per request:       78.615 [ms] (mean, across all concurrent requests)
Transfer rate:          1.59 [Kbytes/sec] received*


**sync_async (time sleep with run_in_executor) :** *Concurrency Level:      50
- Time taken for tests:   256.201 seconds
- Complete requests:      1000
- Failed requests:        0
- Total transferred:      128000 bytes
- HTML transferred:       4000 bytes
- Requests per second:    3.90 [#/sec] (mean)
- **Time per request:       12810.038 [ms] (mean)**
- Time per request:       256.201 [ms] (mean, across all concurrent requests)
Transfer rate:          0.49 [Kbytes/sec] received*


**sync_async_fastapi (time sleep with run_in threadpool):**
*Concurrency Level:      50
- Time taken for tests:   78.877 seconds
- Complete requests:      1000
- Failed requests:        0
- Total transferred:      128000 bytes
- HTML transferred:       4000 bytes
- Requests per second:    12.68 [#/sec] (mean)
- **Time per request:       3943.841 [ms] (mean)**
- Time per request:       78.877 [ms] (mean, across all concurrent requests)
Transfer rate:          1.58 [Kbytes/sec] received*


In conclusion, I&#39;m experiencing a surprising disparity in results, especially when using run_in_executor, where I&#39;m encountering significantly higher average times (12 seconds). I don&#39;t understand this outcome.

--- EDIT ---
**After AKX answer.**

    Here the code working as expected: 
    import time
    import asyncio
    from anyio import to_thread
    
    to_thread.current_default_thread_limiter().total_tokens = 200
    loop = asyncio.get_running_loop()
    executor = ThreadPoolExecutor(max_workers=100)
    def sync_func() -&gt; None:
        time.sleep(3)
        print(&quot;sync func&quot;)
    
    async def sync_async_with_fastapi_thread() -&gt; None:
        await run_in_threadpool( time.sleep, 3)
        print(&quot;sync async with fastapi thread&quot;)
    
    async def sync_async_func() -&gt; None:
        await loop.run_in_executor(executor, time.sleep, 3)
    
    async def async_func() -&gt; Any:
        await asyncio.sleep(3)
        print(&quot;async func&quot;)
    
    @app.get(&quot;/test/sync&quot;)
    def test_sync() -&gt; None:
        sync_func()
        print(&quot;sync&quot;)
    
    @app.get(&quot;/test/async&quot;)
    async def test_async() -&gt; None:
        await async_func()
        print(&quot;async&quot;)
    
    @app.get(&quot;/test/sync_async&quot;)
    async def test_sync_async() -&gt; None:
        await sync_async_func()
        print(&quot;sync async&quot;)
    
    @app.get(&quot;/test/sync_async_fastapi&quot;)
    async def test_sync_async_with_fastapi_thread() -&gt; None:
        await sync_async_with_fastapi_thread()
        print(&quot;sync async with fastapi thread&quot;)
||||||||||||||## Using [`run_in_threadpool()`][1]
Starlette's [`run_in_threadpool()`][1] uses [`anyio.to_thread.run_sync()`][2], behind the scenes, which "will run the *sync* blocking function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked"&mdash;see [this answer][3] and AnyIO's [Working with threads][4] documentation for more details. Calling `anyio.to_thread.run_sync()`&mdash;which internally calls [`AsyncIOBackend.run_sync_in_worker_thread()`][5]&mdash;will return a coroutine that can be `await`ed to get the eventual result of the *sync* function (e.g., `result = await run_in_threadpool(...)`), and hence, FastAPI will still work *asynchronously*. As can be seen in Starlette's source code (link is given above), `run_in_threadpool()` simply looks like this (supporting both *sequence* and *keyword* arguments):
```python
async def run_in_threadpool(
    func: typing.Callable[P, T], *args: P.args, **kwargs: P.kwargs
) -> T:
    if kwargs:  # pragma: no cover
        # run_sync doesn't accept 'kwargs', so bind them in here
        func = functools.partial(func, **kwargs)
    return await anyio.to_thread.run_sync(func, *args)
```

As described in [AnyIO's documentation][6]:

> #### Adjusting the default maximum worker thread count
> 
> The **default** AnyIO worker thread limiter has a value of `40`, meaning
> that any calls to `to_thread.run_sync()` without an explicit `limiter`
> argument will cause a **maximum of `40` threads** to be spawned. You can
> adjust this limit like this: 
> ``` 
> from anyio import to_thread
>
> async def foo():
>     # Set the maximum number of worker threads to 60
>     to_thread.current_default_thread_limiter().total_tokens = 60
> ```
> **Note**
>
> AnyIO’s default thread pool limiter does not affect the default thread pool executor on `asyncio`.

Since [FastAPI uses Startlette's `concurrency` module][7] to run requests/blocking functions in an external threadpool, the default value of the thread limiter is also applied, i.e., `40` threads maximum&mdash;see the relevant [`AsyncIOBackend.current_default_thread_limiter()`][8] method that returns the [`CapacityLimiter`][9] with the default number of threads. As described above, one can adjust that value, thus **increasing the number of threads**, which might lead to an improvement in performance results&mdash;always **depending** on the number of requests your API is expected to serve concurrently. For instance, if you expect the API to serve no more than 50 requests at a time, then set the maximum number of threads to 50&mdash;if you have *synchronous*/*blocking* background tasks/`StreamingResponse`'s generators (i.e., functions defined with normal `def` instead of `async def`), or use `UploadFile`'s operations as well, you could add more threads as required, as FastAPI actually runs all those in an external threadpool, using `run_in_threadpool`&mdash;it is all explained in [this answer][3] in details. 

Note that using the approach below, which was described [here][10], would have the same effect on adjusting the number of worker threads:
```python
from anyio.lowlevel import RunVar
from anyio import CapacityLimiter

RunVar("_default_thread_limiter").set(CapacityLimiter(60))
```

But, it would be best to follow the approach provided by AnyIO's official documentation (as shown earlier). It is also a good idea to have this done when the application starts up, using a `lifespan` event handler, as demonstrated [here][11]. 

#### Working Example 1
```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from anyio import to_thread
import time


@asynccontextmanager
async def lifespan(app: FastAPI):    
    to_thread.current_default_thread_limiter().total_tokens = 60
    yield


app = FastAPI(lifespan=lifespan)


@app.get("/sync")
def test_sync() -> None:
    time.sleep(3)
    print("sync")


@app.get('/get_available_threads')
async def get_available_threads():
    return to_thread.current_default_thread_limiter().available_tokens
```

Using ApacheBench, you could test the example above as follows, which will send `1000` requests in total with `50` being sent simultaneously at a time (`-n`: Number of requests, `-c` : Number of concurrent requests):
```
ab -n 1000 -c 50 "http://localhost:8000/sync"
```

Since the `/sync` endpoint above is defined with normal `def` instead of `async def`, FastAPI will use `run_in_threadpool()`, behind the scenes, to run it in a separate thread and `await` it, thus ensuring that event loop (and hence, the main thread) does not get blocked due to the blocking operations (either blocking IO-bound or CPU-bound) that will be performed inside that endpoint. 

While running a performance test on the example above, if you call the `/get_available_threads` endpoint from your browser, e.g., `http://localhost:8000/get_available_threads`, you would see that the amount of threads **available** is always 10 or above (since only 50 threads are used at a time in this test, but the thread limiter was set to `60`), meaning that setting the maximum number of threads on AnyIO's thread limiter to a number that is well above your needs, like `200` as shown in some other answer and in your recent example, wouldn't bring about any improvements in the performance; on the contrary, you would end up with a number of threads "sitting" there without being used. As explained earlier, the number of maximum threads should depend on the number of requests your API is expected to serve concurrently, as well as any other blocking tasks/functions that would run in the threadpool by FastAPI itself, under the hood (and of course, on the server machine's resources available).

The example below is the **same** as the one above, but instead of letting FastAPI itself to handle the blocking operation(s) inside the `def` endpoint (by running the `def` endpoint in the external threadpool and `await`ing it), the endpoint is now defined with `async def` (meaning that FastAPI will run it directly in the event loop), but inside the endpoint, `run_in_threadpool()` is used (which returns an `await`able) to run the blocking operation. Performing a benchmark test on the example below would yield similar results to the previous example.

#### Working Example 2
```python
from fastapi import FastAPI
from fastapi.concurrency import run_in_threadpool
from contextlib import asynccontextmanager
from anyio import to_thread
import time


@asynccontextmanager
async def lifespan(app: FastAPI):    
    to_thread.current_default_thread_limiter().total_tokens = 60
    yield


app = FastAPI(lifespan=lifespan)


@app.get("/sync_async_run_in_tp")
async def test_sync_async_with_run_in_threadpool() -> None:
    await run_in_threadpool(time.sleep, 3)
    print("sync_async using FastAPI's run_in_threadpool")


@app.get('/get_available_threads')
async def get_available_threads():
    return to_thread.current_default_thread_limiter().available_tokens
```

Using ApacheBench, you could test the example above as follows:
```
ab -n 1000 -c 50 "http://localhost:8000/sync_async_run_in_tp"
```

## Using [`loop.run_in_executor()`][12] with [`ThreadPoolExecutor`][13]
When using `asyncio`'s [`loop.run_in_executor()`][12]&mdash;after obtaining the running event loop using [`asyncio.get_running_loop()`][14]&mdash;one could pass `None` to the `executor` argument, which would lead to the *default* executor being used; that is, a [`ThreadPoolExecutor`][13]. **Note** that when calling `loop.run_in_executor()` and passing `None` to the `executor` argument, this **does not** create a new instance of a `ThreadPoolExecutor` every time you do that; instead, a `ThreadPoolExecutor` is only initialised once the first time you do that, but for subsequent calls to `loop.run_in_executor()` with passing `None` to the `executor` argument, Python **reuses** that very same instance of `ThreadPoolExecutor` (hence, the *default* executor). This can been seen in the [source code of `loop.run_in_executor()`][15]. That means, the number of threads that can be created, when calling `await loop.run_in_executor(None, ...)`, is **limited** to the default number of thread workers in the `ThreadPoolExecutor` class.

As described in the documentation of `ThreadPoolExecutor`&mdash;and as shown in its implementation [here][16]&mdash;by default, the `max_workers` argument is set to `None`, in which case, the number of worker threads is set based on the following equation: `min(32, os.cpu_count() + 4)`. The [`os.cpu_count()`][17] function reutrns the number of *logical* CPUs in the current system. As explained in [this article][18], *physical* cores refers to the number of CPU cores provided in the hardware (e.g., the chips), while *logical* cores is the number of CPU cores **after** hyperthreading is taken into account. If, for instance, your machine has 4 physical cores, each with hyperthreading (most modern CPUs have this), then Python will see 8 CPUs and will allocate 12 threads (8 CPUs + 4) to the pool by default (Python limits the number of threads to 32 to "avoid consuming surprisingly large resources on multi-core machines"; however, one could always adjust the `max_workers` argument on their own when using a custom `ThreadPoolExecutor`, instead of using the *default* one). You could check the default number of worker threads on your system as follows:
```python
import concurrent.futures

# create a thread pool with the default number of worker threads
pool = concurrent.futures.ThreadPoolExecutor()

# report the number of worker threads chosen by default
# Note: `_max_workers` is a protected variable and may change in the future
print(pool._max_workers)
```

Now, as shown in your original example, you are not using a custom `ThreadPoolExecutor`, but instead using the *default* `ThreadPoolExecutor` every time a request arrives, by calling `await loop.run_in_executor(None, time.sleep, 3)` (inside the `sync_async_func()` function, which is triggered by the `/test/sync_async` endpoint). Assuming your machine has 4 physical cores with hyperthreading enabled (as explained in the example earlier), then the default number of worker threads for the *default* `ThreadPoolExecutor` would be 12. That means, based on your original example and the `/test/sync_async` endpoint that triggers the `await loop.run_in_executor(None, time.sleep, 3)` function, your application could only handle 12 concurrent requests at a time. That is the **main reason** for the difference observed in the performance results when compared to using `run_in_threadpool()`, which comes with `40` allocated threads by default.

One way to solve this is to create a new instance of `ThreadPoolExecutor` (on your own, instead of using the *default* executor) every time a request arrives and have it terminated once the task is completed (using the `with` statement), as shown below:
```python
import concurrent.futures
import asyncio

loop = asyncio.get_running_loop()
with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
    await loop.run_in_executor(pool, time.sleep, 3)
```

While this should wok just fine, it would be best to instantiate a `ThreadPoolExecutor` once at the application startup, adjust the number of worker threads as needed, and re-use that executor when required. Having said that, depending on the blocking task and/or external libraries you might be using for that task, if you ever encounter a memory leak after tasks are completed when re-using a `ThreadPoolExecutor`&mdash;i.e., memory that is no longer needed, but is not released&mdash;you might find creating a new instance of `ThreadPoolExecutor` each time, as shown above, more suitable (Note, however, that if this was a [`ProcessPoolExecutor`][19] instead, creating and destroying many processes over and over could become **computationally expensive**).

Below is a complete working example, demonstrating how to create a re-usable custom `ThreadPoolExecutor`. Calling  the `/get_active_threads` endpoint from your browser, e.g., `http://localhost:8000/get_active_threads`, while running a performance test with ApacheBench (using `50` concurrent requests, as described in your question and as shown below), you would see that the number of **active** threads never goes above `51` (50 concurrent threads + 1, which is the main thread), despite setting the `max_workers` argument to `60` in the example below. This is simply because, in this performance test, the application is never required to serve more than `50` requests at the same time. Also, `ThreadPoolExecutor` won't spin new threads, if idle threads are available (thus saving resources)&mdash;see the [relevant implementation part][20]. Hence, again, initialising the `ThreadPoolExecutor` with `max_workers=100`, as shown in your recent update, would be unecessary, if you never expect your FastAPI application to serve more than 50 requests at a time.

#### Working Example
```python
from fastapi import FastAPI, Request
from contextlib import asynccontextmanager
import concurrent.futures
import threading
import asyncio
import time


@asynccontextmanager
async def lifespan(app: FastAPI):    
    pool = concurrent.futures.ThreadPoolExecutor(max_workers=60)
    yield {'pool': pool}
    pool.shutdown()


app = FastAPI(lifespan=lifespan)


@app.get("/sync_async")
async def test_sync_async(request: Request) -> None:
    loop = asyncio.get_running_loop()
    await loop.run_in_executor(request.state.pool, time.sleep, 3)  
    print("sync_async")


@app.get('/get_active_threads')
async def get_active_threads():
    return threading.active_count()
```

Using ApacheBench, you could test the example above as follows:
```
ab -n 1000 -c 50 "http://localhost:8000/sync_async"
```

## Final Notes
In general, you should always aim for using *asynchronous* code (i.e., using `async`/`await`), wherever is possible, as `async` code, also known as coroutines, run in the event loop, which runs in the main thread and executes all tasks in that thread. That means there is only **one** thread that can take a lock on the interpreter. When dealing with *sync* blocking IO-bound tasks though, you could either (1) define your endpoint with `def` and let FastAPI handle it behind the scenes as described earlier and in [this answer][3], or (2) define your endpoint with `async def` and use `run_in_threadpool()` on your own to run that blocking task in a separate thread and `await` it, or (3) use `asyncio`'s `loop.run_in_executor()` with a custom (preferably re-usable) `ThreadPoolExecutor`, adjusting the number of workers as required. When required to perform blocking CPU-bound tasks, while running such tasks in an external thread and `await`ing them would successfully prevent the event loop from getting blocked, it wouldn't, however, provide the performance improvement you would expect from running code in parallel. Thus, for CPU-bound tasks, one may choose to use a `ProcessPoolExecutor` instead (**Note:** when using processes in general, you need to explicitly protect the entry point with `if __name__ == '__main__'`)&mdash;example on using a `ProcessPoolExecutor` can be found in [this answer][21]. 

To run tasks in the background, without waiting for them to complete in order to proceed with executing the rest of the code in an endpoint, you could use FastAPI's [`BackgroundTasks`][22], as shown [here][23] and [here][24]. If the background task function is defined with `async def`, FastAPI will run it directly in the event loop, whereas if it is defined with normal `def`, FastAPI will use `run_in_threadpool()` and `await` the returned coroutine (same concept as API endpoints). Another option when you need to run an `async def` function in the background, but not necessarily having it trigerred after returning a FastAPI response (which is the case in `BackgroundTasks`), is to use [`asyncio.create_task()`][25], as shown in [this answer][26] and [this answer][27]. If you need to [perform heavy background computation][28] and you don't necessarily need it to be run by the same process, you may benefit from using other bigger tools such as Celery.

Finally, regarding the **optimal/maximum number of worker threads**, I would suggest reading [this article][29] (have a look at [this article][30] as well for more details on `ThreadPoolExecutor` in general). As explained in the article:

> It is important to **limit the number** of worker threads in the thread
> pools to the number of asynchronous tasks you wish to complete, **based
> on** the resources in your system, or on the number of resources you
> intend to use within your tasks.
> 
> Alternately, you may wish to **increase the number** of worker threads
> dramatically, **given the greater capacity** in the resources you intend
> to use.
>
> [...]
>
> It is common to have **more threads than CPUs** (physical or logical) in
> your system. The reason for this is that threads are used for IO-bound tasks, not
> CPU-bound tasks. This means that threads are used for tasks that wait
> for relatively slow resources to respond, like hard drives, DVD
> drives, printers, network connections, and much more.
> 
> Therefore, **it is not uncommon** to have tens, hundreds and even
> thousands of threads in your application, **depending on your specific
> needs**. It is unusual to have more than one or a few thousand threads.
> If you require this many threads, then alternative solutions may be
> preferred, such as `AsyncIO`.

Also, in the same article:

> #### Does the Number of Threads in the `ThreadPoolExecutor` Match the Number of CPUs or Cores?
> 
> The number of worker threads in the `ThreadPoolExecutor` is **not
> related** to the number of CPUs or CPU cores in your system.
> 
> You can configure the number of threads **based on** the number of
> tasks you need to execute, the amount of local system resources you
> have available (e.g., memory), and the limitations of resources you
> intend to access within your tasks (e.g., connections to remote
> servers).
> 
> #### How Many Threads Should I Use?
> 
> If you have hundreds of tasks, you should probably set the number of
> threads to be equal to the number of tasks.
> 
> If you have thousands of tasks, you should probably cap the number of
> threads at hundreds or 1,000.
> 
> If your application is intended to be executed multiple times in the
> future, you can test different numbers of threads and compare overall
> execution time, then choose a number of threads that gives
> approximately the best performance. You may want to mock the task in
> these tests with a random sleep operation.
> 
> #### What Is the Maximum Number of Worker Threads in the `ThreadPoolExecutor`?
> 
> There is no maximum number of worker threads in the
> `ThreadPoolExecutor`.
> 
> Nevertheless, your system will have an upper limit of the number of
> threads you can create based on **how much main memory (RAM) you have
> available**.
> 
> Before you exceed main memory, you will reach a point of diminishing
> returns in terms of adding new threads and executing more tasks. This
> is because your operating system must switch between the threads,
> called *context switching*. With too many threads active at once, your
> program may spend more time context switching than actually executing
> tasks.
> 
> A sensible upper limit for many applications is hundreds of threads to
> perhaps a few thousand threads. More than a few thousand threads on a
> modern system may result in too much context switching, depending on
> your system and on the types of tasks that are being executed.


  [1]: https://github.com/encode/starlette/blob/ec417f7f84f3533f928a4bc2b8dd0c6c51cdbbad/starlette/concurrency.py#L36
  [2]: https://anyio.readthedocs.io/en/stable/api.html#anyio.to_thread.run_sync
  [3]: https://stackoverflow.com/a/71517830/17865804
  [4]: https://anyio.readthedocs.io/en/stable/threads.html#working-with-threads
  [5]: https://github.com/agronholm/anyio/blob/137de708f40262e959d9d43178dda600432da56b/src/anyio/_backends/_asyncio.py#L2088
  [6]: https://anyio.readthedocs.io/en/stable/threads.html#adjusting-the-default-maximum-worker-thread-count
  [7]: https://github.com/tiangolo/fastapi/blob/3f3ee240dd8656962e94e89eceb3838508982068/fastapi/concurrency.py#L7
  [8]: https://github.com/agronholm/anyio/blob/137de708f40262e959d9d43178dda600432da56b/src/anyio/_backends/_asyncio.py#L2435
  [9]: https://github.com/agronholm/anyio/blob/137de708f40262e959d9d43178dda600432da56b/src/anyio/_backends/_asyncio.py#L1657
  [10]: https://github.com/tiangolo/fastapi/issues/4221#issuecomment-982260467
  [11]: https://stackoverflow.com/a/76322910/17865804
  [12]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor
  [13]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
  [14]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop
  [15]: https://github.com/python/cpython/blob/39ec7fbba84663ab760853da2ac422c2e988d189/Lib/asyncio/base_events.py#L872
  [16]: https://github.com/python/cpython/blob/d466052ad48091a00a50c5298f33238aff591028/Lib/concurrent/futures/thread.py#L145
  [17]: https://docs.python.org/3/library/os.html#os.cpu_count
  [18]: https://superfastpython.com/number-of-cpus-python/
  [19]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
  [20]: https://github.com/python/cpython/blob/9afc6d102d16080535325f645849cd84eb04d57d/Lib/concurrent/futures/thread.py#L181
  [21]: https://stackoverflow.com/a/77862153/17865804
  [22]: https://fastapi.tiangolo.com/tutorial/background-tasks/
  [23]: https://stackoverflow.com/a/76280152
  [24]: https://stackoverflow.com/a/73283272
  [25]: https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task
  [26]: https://stackoverflow.com/a/70873984/17865804
  [27]: https://stackoverflow.com/a/76148361/17865804
  [28]: https://stackoverflow.com/a/74508996/17865804
  [29]: https://superfastpython.com/threadpoolexecutor-number-of-threads/
  [30]: https://superfastpython.com/threadpoolexecutor-in-python/

--------------------------------------------------
How to make child div scrollable when it exceeds parent height?
I have 2 child divs nested in a parent div in row-column pattern: the parent is a column, and the children are rows.![enter image description here][1]

The upper child div is of variable height, but is guaranteed to be less than the height of the parent div.

The lower child div is also of variable height.  In some cases, the heights of the child divs will make the lower child div exceed the parent.  In this case, I need to make the lower div scrollable. Note that I want only the lower div to be scrollable, not the whole parent div.

How do I handle this?

See attached jsfiddle for case example: http://jsfiddle.net/0yxnaywu/5/

**HTML:**

  

     &lt;div class=&quot;parent&quot;&gt;
        &lt;div class=&quot;child1&quot;&gt;
            hello world filler
        &lt;/div&gt;
        &lt;div class=&quot;child2&quot;&gt;
            this div should overflow and scroll down
        &lt;/div&gt;
    &lt;/div&gt;

**CSS:**

&lt;!-- language: css --&gt;

    .parent {
        width: 50px;
        height: 100px;
        border: 1px solid black;
    }
    
    .child1 {
        background-color: red;
    }
    
    .child2 {
        background-color: blue;
    }

  [1]: http://i.stack.imgur.com/9YxIb.png
||||||||||||||Overflow only works when you give it a value to overflow when greater than. Your value is relative to how big the top is, so using jQuery, grab that value then subtract from the parent.

<!-- language: js -->

    $(document).ready(function() {
      $(".child2").css("max-height", ($(".parent").height()-$(".child1").height()));
    });

and add `overflow`'s to the children

<!-- language: css -->

    .child1 {
        background-color: red;
        overflow: hidden;
    }

    .child2 {
        background-color: blue;
        overflow: auto;
    }

http://jsfiddle.net/m9goxrbk/

--------------------------------------------------
Create a column based on multiple columns for certain rows Power BI
I have the next table in Power BI

    | Ubicaci&#243;n.Name     | Fecha_entrega__c | Sector_entrante__c |
    | ------------------ | ---------------- | -------------------| 
    | PAD FP.c-1050      | 5/31/2021        | Perforaci&#243;n        |
    | PAD LAnch.x-2(h)   | 4/30/2022        | Terminaci&#243;n        | 
    | PAD LAnch.x-2(h)   | 2/28/2022        | Perforaci&#243;n        |
    | PAD LAnch.x-2(h)   | 7/13/2022        | Well Testing       | 
    | PAD de Pozos 1003  | 4/23/2022        | Terminaci&#243;n        |  
    | PAD de Pozos 1003  | 8/11/2022        | Perforaci&#243;n        | 

I would like to create a column based on the next logic

For a certain group in &quot;Ubicaci&#243;n.Name&quot;, for example, &quot;PAD LAnch.x-2(h)&quot;, there are three rows of this kind, I want to check whether the newest of these rows, based on &quot;Fecha_entrega__c&quot; column, has in &quot;Sector_entrante__c&quot; column the string &quot;Well Testing&quot;. In this case we can see in row 4 which has the newest date of rows 2,3 and 4 (all of them part of &quot;PAD LAnch.x-2(h)&quot;) that in column &quot;Sector_entrante__c&quot; it says &quot;Well Testing&quot;, so I want a column that gives the number 1 for rows 2,3 and 4. If it didn&#39;t have &quot;Well Testing&quot; I would like to give the value 0 for rows 2,3 and 4. 

    | Ubicaci&#243;n.Name     | Fecha_entrega__c | Sector_entrante__c | Column |
    | ------------------ | ---------------- | -------------------| -------|
    | PAD FP.c-1050      | 5/31/2021        | Perforaci&#243;n        | 0      |
    | PAD LAnch.x-2(h)   | 4/30/2022        | Terminaci&#243;n        | 1      |
    | PAD LAnch.x-2(h)   | 2/28/2022        | Perforaci&#243;n        | 1      |
    | PAD LAnch.x-2(h)   | 7/13/2022        | Well Testing       | 1      |
    | PAD de Pozos 1003  | 4/23/2022        | Terminaci&#243;n        | 0      |  
    | PAD de Pozos 1003  | 8/11/2022        | Perforaci&#243;n        | 0      |

Thanks. 

The only thing I have been able to do is to use Rank and Filter to identify with numbers which are the newest and oldest of these group

`RANK Column = RANKX(FILTER(&#39;Form  NQN PAD Handover (3)&#39;,&#39;Form  NQN PAD Handover (3)&#39;[Ubicaci&#243;n.Name]=EARLIER(&#39;Form  NQN PAD Handover (3)&#39;[Ubicaci&#243;n.Name])),&#39;Form  NQN PAD Handover (3)&#39;[Fecha_entrega__c],,DESC)`

But I really have no idea how to apply logic to certain groups within columns.
||||||||||||||[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/edZMW.png


    Column = 
    VAR x = CALCULATE(MAX('Table'[Fecha_entrega__c]), ALLEXCEPT('Table', 'Table'[Ubicación.Name]))
    VAR y = CALCULATE(MAX('Table'[Sector_entrante__c]), 'Table'[Fecha_entrega__c] = x, ALLEXCEPT('Table', 'Table'[Ubicación.Name]))
    
    RETURN IF(y = "Well Testing", 1,0)

--------------------------------------------------
$ flutter pub run build_runner build in project with hive not responding
When run $ flutter pub run build_runner build in project with hive, it just stops here(i have even waited 2 hours and its not going any further),
i have tried creating a new project specifically for hive implementation. but its the same issue
```
[INFO] Generating build script...
[INFO] Generating build script completed, took 528ms
[WARNING] Deleted previous snapshot due to missing asset graph.
[INFO] Creating build script snapshot......
[INFO] Creating build script snapshot... completed, took 21.3s
[INFO] Initializing inputs
[INFO] Building new asset graph...
[INFO] Building new asset graph completed, took 1.2s
[INFO] Checking for unexpected pre-existing outputs....
[INFO] Checking for unexpected pre-existing outputs. completed, took 2ms
[INFO] Running build...
[INFO] Generating SDK summary...
```

this is my class:

```
import &#39;package:hive/hive.dart&#39;;

part &#39;person.g.dart&#39;;

@HiveType(typeId: 0)
class Person extends HiveObject {
  @HiveField(0)
  int id;
  @HiveField(1)
  String name;
  @HiveField(2)
  DateTime birthDate;
  Person(this.id, this.name, this.birthDate);
}
```
and my pubspec.yaml file:

```
environment:
  sdk: &quot;&gt;=2.7.0 &lt;3.0.0&quot;

dependencies:
  flutter:
    sdk: flutter
  hive:
  hive_flutter:
  path_provider:

  # The following adds the Cupertino Icons font to your application.
  # Use with the CupertinoIcons class for iOS style icons.
  cupertino_icons: ^0.1.3

dev_dependencies:
  flutter_test:
    sdk: flutter
  build_runner:
  hive_generator:


flutter:

  
```
||||||||||||||*I Was Also facing the same issue and solved it with,*

    flutter pub upgrade

*If that doesn't help you, then try these steps too*

    flutter clean
    
    flutter pub get
    
    flutter packages pub run build_runner build --delete-conflicting-outputs  

--------------------------------------------------
How to access localStorage in node.js?
I tried searching the web for a node module that can access the client&#39;s localStorage but wasn&#39;t able to find anything. Anyone know of one?
||||||||||||||You can use :

`node-localstorage` npm module to use `localStorage` at the [tag:NodeJS] server side.

    var LocalStorage = require('node-localstorage').LocalStorage,
    localStorage = new LocalStorage('./scratch');

--------------------------------------------------
Get all runes with id and name from League of legends API
When performing requests to the riot API endpoint [/lol/match/v4/matches/{matchId}](https://developer.riotgames.com/apis#match-v4) the response contains rune data for each player in the match .e.g

```
&quot;perk0&quot;: 8005,
&quot;perk0Var1&quot;: 2107,
&quot;perk0Var2&quot;: 1319,
&quot;perk0Var3&quot;: 788,
```

These are only the Id values for the runes. Where can I get the corresponding name for the rune ?

I&#39;ve tried the following request : https://euw1.api.riotgames.com/lol/static-data/v1/runes , but returns the following response :

```
{
    &quot;status&quot;: {
        &quot;message&quot;: &quot;Forbidden&quot;,
        &quot;status_code&quot;: 403
    }
}
```
||||||||||||||I found the resource that stores all the needed data here : http://ddragon.leagueoflegends.com/cdn/10.16.1/data/en_US/runesReforged.json

--------------------------------------------------
How to efficiently make a recursive table in graphql?
I am writing a graphql schema using amplify. I have a categories table in which I want to relate each category with other categories. Every Category can have multiple parents or children.
This is what I have right now.

```
type Category @model {
  id: ID! @primaryKey
  name: String!
  parents: [Category] @hasMany
  subCategories:[Category] @hasMany
  categoryParentsId: [String] @index(name: &quot;byParent&quot;)
  categorySubCategoriesId: [String] @index(name: &quot;byChild&quot;)
}

type Query {
  getAllCategories: [Category!]!
}

input CreateCategoryInput {
  name: String!
  categoryParentsId: [String]
  categorySubCategoriesId: [String]
}
```
I am using the mutation generated by amplify: 
```
export const createCategory = /* GraphQL */
  mutation CreateCategory( $input: CreateCategoryInput! $condition: ModelCategoryConditionInput ) {
    createCategory(input: $input, condition: $condition) {
      id
      name
      parents {
        nextToken
        __typename
      }
      items {
        nextToken
        __typename
      }
      subCategories {
        nextToken
        __typename
      }
      categoryParentsId
      categorySubCategoriesId
      createdAt
      updatedAt
      __typename
    }
  }
```
I am using a react app in the frontend. Whenever I try to input this error shows up from Amplify.
&gt; &quot;One or more parameter values were invalid: Type mismatch for Index Key categoryParentsId Expected: S Actual: L IndexName: gsi-Category.parents (Service: DynamoDb, Status Code: 400&quot;

I understand that it is generating a GSI which are categoryParentsId and categorySubCategoriesId which can&#39;t be other than string, number or boolean. So it kind of conflicts with what I stated as an array of strings.

Should I just add another field that stores that array of strings? Is there a better approach?
||||||||||||||You don't need to refer to both the parent and child types **and** the corresponding ids in your GraphQL type:
```
type Category @model {
  id: ID! @primaryKey
  name: String!
  parents: [Category] @hasMany
  subCategories:[Category] @hasMany
 }
```
should suffice.

> Normally references to related objects are done by referring to the object *type* and not the id (you can always get the id(s) of the related objects by including them in your query). In your case you have both.

Also please include the definition of the mutation you are attempting. You showed the `input type` but not the mutation.

--------------------------------------------------
Exception has occurred: ImportError cannot import name &#39;string_int_label_map_pb2&#39; from &#39;object_detection.protos&#39;
I am facing import error in this code please help me out it is in label map utility function i dint need code to fix it i need how to solve this import error.


```
&quot;&quot;&quot;Label map utility functions.&quot;&quot;&quot;

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import logging
import sys
sys.path.append(&#39;models/research/object_detection&#39;)
import numpy as np
from six import string_types
from six.moves import range
import tensorflow.compat.v1 as tf
from google.protobuf import text_format
import object_detection
from object_detection.protos import string_int_label_map_pb2
```
||||||||||||||You just need to put that files in the same folder and just write 

    import string_int_label_map_pb2

instead of 

    from object_detection.protos import string_int_label_map_pb2

Also make sure you have converted protos files to .py file.


--------------------------------------------------
Strings in c and printing the string by printf
```c
#include &lt;stdio.g&gt;
#include &lt;string.h&gt;

int main()
{
    char s1[]=&quot;harry &quot;;
    char s2[]=&quot;is a friend of &quot;;
    char s3[]=&quot;ravi&quot;;
    char s4[20];
    char s5[20];
    strcpy(s4,strcat(s1,s2));
    strcpy(s5,strcat(s4,s3));
    printf(&quot;%s&quot;, s5);
    return 0;
}
```

Why I m getting segmentation fault here why it&#39;s not printing by use of printf..??

I tried using printf it wasn&#39;t printing but with the use of puts it was printing why so ??
Please anyone do tell me what&#39;s the fault in this
||||||||||||||`strcat()` puts the concatenated strings in its first argument. Since `s1` is only long enough to hold the string it was initialized with, you can't use it as the destination when concatenating to itself.

Instead of using `strcat()` as the source of `strcpy()`, you should copy the first string and then concatenate the second string in the destination.

You also need to make the destination strings larger, so they have room for the final string (including the terminating null).

```
#include <stdio.g>
#include <string.h>

int main()
{
    char s1[]="harry ";
    char s2[]="is a friend of ";
    char s3[]="ravi";
    char s4[30];
    char s5[30];
    strcpy(s4, s1);
    strcat(s4, s2);
    strcpy(s5, s4);
    strcat(s5, s3);
    printf("%s\n", s5);
    return 0;
}
```

You can ensure that the destination strings are long enough by declaring the sizes dynamically:

```
char s4[sizeof s1 + sizeof s2];
char s5[sizeof s4 + sizeof s3];
```


--------------------------------------------------
Jenkins Maven Build Step fails but runs from command line
I&#39;ve been searching the web and StackOverflow for a solution. Problem is that Jenkins will not invoke `mvn` from the the build step. In my build step, I&#39;m using _Invoke top-level Maven targets_. From the command line `mvn clean install` runs just fine from the directory where the POM file is located. So I believe Maven is installed correctly.

From the console I see:

    [XXXX] $ cmd.exe /C &quot;mvn -f C:\Users\XXX\XXX\XXX\pom.xml clean install &amp;&amp; exit %%ERRORLEVEL%%&quot;

Error message is:

    &#39;mvn&#39; is not recognized as an internal or external command, operable program or batch file.

What I have tried:

1. change the settings file option to point too the `settings.xml` in the Maven install folder
2. change the global setting file option to point too the `settings.xml` in the Maven install folder
3. I&#39;ve set the `MAVEN_HOME` environment variable in Jenkins Configure System

What I expect to happen:

1. Maven to build my project from the build step.
||||||||||||||I just removed pom.xml file from the path.. it's worked .. 

So try the below option

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/4vqXo.png

--------------------------------------------------
Would this prevent the row from being read during the transaction?
I remember an example where reads in a transaction then writing back the data is not safe because another transaction may read/write to it in the time between. So i would like to check the date and prevent the row from being modified or read until my transaction is finish. Would this do the trick? and are there any sql variants that this will not work on?

    update tbl set id=id where date&gt;expire_date and id=@id

Note: date&gt;expire_date happens to be my condition. It could be anything. Would this prevent other transaction from reading the row until i commit or rollback?
||||||||||||||In a lot of cases, your UPDATE statement will not prevent other transactions from reading the row.

Depending on the isolation level, databases use different types of locking. At the highest level, locking can be divided into two categories:  
- pessimistic,  
- optimistic

MS SQL 2008, for example, has 6 isolation levels, 4 of them are pessimistic, 2 are optimistic. By default , it uses READ COMMITTED isolation level, which falls into the pessimistic category.

Oracle, on another note, uses optimistic locking by default.

The statement that will lock your record for writing is

    SELECT * FROM TBL WITH UPDLOCK WHERE id=@id

From that point on, no other transaction will be able to update your record with id=@id
And only transactions running in isolation level READ UNCOMMITTED will be able to read it.

With the default transaction level, READ COMMITTED, no other thansaction will be able to read or write into this record until you either commit or roll back your entire transaction.

--------------------------------------------------
angular 6 warning for using formControlName and ngModel
I recently upgraded the angular version to 6-rc. I got following warning 

&gt; It looks like you&#39;re using ngModel on the same form field as
&gt; formControlName. Support for using the ngModel input property and
&gt; ngModelChange event with reactive form directives has been deprecated
&gt; in Angular v6 and will be removed in Angular v7
&gt; 
&gt; For more information on this, see our API docs here:
&gt;     https://angular.io/api/forms/FormControlName#use-with-ngmodel

What does it say exactly? the link does not have any fragment for `#use-with-ngmodel`

I guess I need to remove `ngModel` and use formGroup as my data binding object. 
||||||||||||||Now you can find the documentation here: 

https://angular.io/api/forms/FormControlName#use-with-ngmodel-is-deprecated

So you have 3 options:

 1. use Reactive forms

 2. use Template driven forms

 3.  silence warning (not recommended)

    <!-- language: lang-ts -->

        imports: [
          ReactiveFormsModule.withConfig({warnOnNgModelWithFormControl: 'never'});
        ]


  [1]: https://next.angular.io

--------------------------------------------------
How to check if session exists or not?
I am creating the session using

    HttpSession session = request.getSession();

Before creating session I want to check if it exists or not. How would I do this?
||||||||||||||If you want to check this *before* creating, then do so:

    HttpSession session = request.getSession(false);
    if (session == null) {
        // Not created yet. Now do so yourself.
        session = request.getSession();
    } else {
        // Already created.
    }

If you don't care about checking this *after* creating, then you can also do so:

    HttpSession session = request.getSession();
    if (session.isNew()) {
        // Freshly created.
    } else {
        // Already created.
    }

That saves a line and a `boolean`. The [`request.getSession()`][1] does the same as [`request.getSession(true)`][2].


  [1]: http://java.sun.com/javaee/5/docs/api/javax/servlet/http/HttpServletRequest.html#getSession%28%29
  [2]: http://java.sun.com/javaee/5/docs/api/javax/servlet/http/HttpServletRequest.html#getSession%28boolean%29

--------------------------------------------------
PdfSharp System.NotImplementedException: Cannot create value for key: /Info
I have written an app in C# with Visual Studio using PdfSharp &amp; MigraDoc. This works fine. 
When I build the app as standalone exe then the app will crash when I will calculate the text with with the method MeasureString. 
The Exception which I will become is System.NotImplementedException: Cannot create value for key: /Info
I will only get this error when the app is a standalone exe. When I compile it as not a standalone exe then it works. 
 

I have googled it but only find this link here on GitHub: https://github.com/ststeiger/PdfSharpCore/issues/120
They are using Unity and not Visual Studio. And I don&#39;t know where to place this files which they have added. 
Does somebody know how to solve my problem? 
If nothing works, then I have to use the complete folder with the dll&#39;s inside instead of a standalone exe.

The traceback:
```
System.NotImplementedException: Cannot create value for key: /Info
   at PdfSharp.Pdf.PdfDictionary.DictionaryElements.GetValue(String, VCF)
   at PdfSharp.Pdf.Advanced.PdfTrailer.get_Info()
   at PdfSharp.Pdf.PdfDocument.get_Info()
   at PdfSharp.Pdf.PdfDocument..ctor()
   at PDFVerificationProtocol.PDFCreator.CalculateRequiredColumnWidth(String) in Z:\PDF-Pr&#252;fprotokoll_Test\PDFVerificationProtocol\PDFVerificationProtocol\PDFCreator.cs:line 381
   at PDFVerificationProtocol.PDFCreator.AddPSetTable(Section) in Z:\PDF-Pr&#252;fprotokoll_Test\PDFVerificationProtocol\PDFVerificationProtocol\PDFCreator.cs:line 229
   at PDFVerificationProtocol.PDFCreator.Run() in Z:\PDF-Pr&#252;fprotokoll_Test\PDFVerificationProtocol\PDFVerificationProtocol\PDFCreator.cs:line 54
   at PDFVerificationProtocol.Program.RunProgram() in Z:\PDF-Pr&#252;fprotokoll_Test\PDFVerificationProtocol\PDFVerificationProtocol\Program.cs:line 34
   at PDFVerificationProtocol.Program.Main(String[]) in Z:\PDF-Pr&#252;fprotokoll_Test\PDFVerificationProtocol\PDFVerificationProtocol\Program.cs:line 11
```

In this method occurs the problem:
```
private double CalculateRequiredColumnWidth(string text)
        {
            // Use the global style font settings
            XFont font = new XFont(globalStyle.Font.Name, globalStyle.Font.Size);

            // Dummy
            PdfDocument document = new PdfDocument();
            document.AddPage();
            XGraphics gfx = XGraphics.FromPdfPage(document.Pages[0]);

            // Measure the width of the text using XGraphics
            XSize textSize = gfx.MeasureString(text, font);

            // Calculate the required width including padding
            double requiredWidth = textSize.Width + (2 * cellPadding);

            return requiredWidth;
        }
```


||||||||||||||I can confirm that, as the author of the question already mentioned, `<TrimMode>partial</TrimMode>` in the `.csproj` file under `<PropertyGroup>` indeed is sufficient to resolve this issue. In my specific scenario, the relevant portion of the `.csproj` file appears as follows:

    <Project Sdk="Microsoft.NET.Sdk">
    
      <PropertyGroup>
        <OutputType>Exe</OutputType>
        <TargetFramework>net8.0</TargetFramework>
        ...
        <TrimMode>partial</TrimMode>
      </PropertyGroup>
    ...
    
    </Project>

Alternatively, you can uncheck the "Trim unsued code" option in the Publish dialog. However, in this case, the .exe file will be relatively large.

<img src="https://i.stack.imgur.com/CK2Ix.png" width="400" />

--------------------------------------------------
Range &quot;view&quot; that modifies the assigned value
I&#39;m implementing a number of audio effects on an embedded platform. From the hardware I get an input buffer of samples in a format that needs some massaging &lt;sup&gt;1&lt;/sup&gt;, then there&#39;s the process itself, followed by further massaging. There will be several different types of effects, but for now you can think that the actual `process` -function is given as a template parameter &lt;sup&gt;2&lt;/sup&gt;.

I&#39;d like to extract to massaging out so that there&#39;s no (source) code duplication, and also without having to copy to a separate buffer. Furthermore, while most effects will operate on float-in float-out, I&#39;d like to have the option for a process to work on integers directly. So, using an as-yet-mythical syntax, it would ideally look something like:
```
class Effect1
{
    void process(AudioBuffers&lt;float, float&gt; buffers)
    {
        // A simple case, in and out buffers are processed in lockstep
        for (auto &amp;&amp; [in, out] : buffers)
        {
            // Note that &quot;in&quot; is massaged lazily, i.e. on this line
            // to a float, and out is clamped and converted to int
            // on assign
            out = doSomething(in);
        }
    }
};

class Effect2
{
    void process(AudioBuffers&lt;uint32_t, int32_t&gt; buffers)
    {
        // A completely silly effect, just to demonstrate random access
        // and no conversion
        std::size_t k = buffers.out.size() / 2;
        for (std::size_t i = 0; i &lt; buffers.in.size(); ++i)
        {
            buffers.out[k] = buffers.in[i]; // No need for conversion, we&#39;re just shuffling samples
            k = k + 1 &lt; buffers.out.size() ? k + 1 : 0;
        }
    }
};

...

template&lt;typename InputType, typename OutputType&gt;
AudioBuffers
{
     // Note that the constructor arguments are always the types as provided
     // by hardware, so they don&#39;t depend on the template arguments
     AudioBuffers(std::array&lt;const uint32_t, kBufferLength&gt; &amp;in, std::array&lt;int32_t, kBufferLength&gt; &amp;out)
     {
       ...
     }
     // Implementation. Here we know the requested InputType and OutputType,
     // so we can insert the necessary massaging when constructing iterators...
     ...
}

...
template&lt;class Effect&gt;
void process(std::array&lt;const uint32_t, kBufferLength&gt; &amp;in, std::array&lt;int32_t, kBufferLength&gt; &amp;out, Effect &amp;effect)
{   
    // Use CTAD to determine the correct instantiation/specialization of AudioBuffers which does the necessary massaging, as determined by the class Effect.
    effect.process({in, out});
}

```

I can certainly implement the AudioBuffer template in &quot;brute force&quot; -style:
 1. basically implement iterators for the input and output which do the massaging on dereference and assign on input and output, respectively
 2. wrap the input and output buffers in a class that provides `begin` and `end` returning these iterators, and an `operator[]` for random access
 3. provide a `begin` and `end` in `AudioBuffers` returning a zip (which I&#39;ll have to implement / import an implementation of, since `std::zip` is C++23) of these iterators.
 4. Possibly some more boilerplate to get all the deductions to go right

However, I get the feeling that what I want is _almost_ a reimplementation of parts of std::ranges and views. I *think* I can do the wrapping of the input buffer by using `std::views::transform`. However, I can&#39;t figure out how to wrap the output such that an extra processing step (and type conversion) is done on assign using elements from `std::ranges`.

Finally, **my question**: can I build the functionality described above somehow by combining elements that are already there in `std` or `std::ranges`? If not, anything to simplify implementing the iterators, which would be entirely boilerplate except for derefence / assign? Any other suggestions or maybe frame challenges?

**Footnotes:**
1) specifically, input is 24 bit ints except that the sign bit is not extended above the  24th bit, and output needs to be clamped to 24-bit int range and converted from float to int. Regarding the &quot;just shuffling samples&quot; example above, the output&#39;s bits above the 24th have no effect, anything goes.
2) The actual implementation will be an `std::variant&lt;Effect1, Effect2, ...&gt;`  which is `visit`ed.

||||||||||||||I'm not sure exactly what you are asking but if the question is "Can you modify the values in a zip view?", the answer is no. The following for-loop will not compile because a non-const reference can only be bound to an l-value.
<!-- language: C++ -->

    namespace rv = std::ranges::views;
    std::vector<int> in_vals = { 1,2,3,4 };
    std::vector<int> out_vals(in_vals.size());
    for (auto& [in, out] : rv::zip(in_vals, out_vals)) {  // << no
    	out = do_something(in);
    }
You could make a view of reference wrappers by doing something like

    namespace rv = std::ranges::views;
    
    auto to_references(auto rng) {
    	return rng | rv::transform(
    		[](auto& v) {
    			return std::ref(v);
    		}
    	);
    }
    
    int do_something(int v) {
    	return 2 * v;
    }
    
    int main() {
    	std::vector<int> in_vals = { 1,2,3,4 };
    	std::vector<int> out_vals(in_vals.size());
    	for (auto [in, out] : rv::zip(in_vals, to_references(rv::all(out_vals)))){
    		out.get() = do_something(in);
    	}
    
    	for (auto v : out_vals) {
    		std::println("{}", v);
    	}
    }

which, I think, would work, but basically understand that this is not idiomatic. 

The intent of ranges is not to modify containers in place. You want to use range functions in the functional style in which an input range comes in and an output range goes out. For example,

    namespace r = std::ranges;
    namespace rv = std::ranges::views;
    
    int do_something(int v) {
    	return 2 * v;
    }
    
    int main() {
    	std::vector<int> in_vals = { 1,2,3,4 };
    	auto out_vals = in_vals |
            rv::transform(do_something) | 
            r::to<std::vector<int>>();
    
    	for (auto v : out_vals) {
    		std::println("{}", v);
    	}
    }

--------------------------------------------------
Converting MathJax into pdf with wkhtmltopdf yields too small maths
By using `pdfkit-python`based on `wkhtmltopdf`, I have managed to convert `MathJax`into pdf. `wkhtmltopdf` configuration options are the following:

    options = {
        &#39;quiet&#39;: &#39;&#39;,
        &#39;javascript-delay&#39; : &#39;5000&#39;,
        &#39;page-size&#39;: &#39;A4&#39;,
        &#39;margin-top&#39;: &#39;0.75in&#39;,
        &#39;margin-right&#39;: &#39;0.75in&#39;,
        &#39;margin-bottom&#39;: &#39;0.75in&#39;,
        &#39;margin-left&#39;: &#39;0.75in&#39;,
        &#39;disable-smart-shrinking&#39;: &#39;&#39;,
        &#39;dpi&#39;: &#39;400&#39;,
    }

This allows to obtain the `markdown` text that is large as expected, however maths do not scale accordingly. 

Here a snapshot of the pdf obtained :

[![mathJax to pdf yield tiny maths][1]][1]

where maths appears definitely too small.

And here how it is rendered on the browser:

[![enter image description here][2]][2]

Any idea on how to tackle the problem, in other words obtain maths scaling with the `markdown` text in the pdf output, will be greatly appreciated.

Here below the MathJax config:

    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
    MathJax.Hub.Config({
        TeX: {extensions: [&quot;mhchem.js&quot;]},
        tex2jax: {
        inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],
        displayMath: [ [&#39;$$&#39;,&#39;$$&#39;], [&quot;\\[&quot;,&quot;\\]&quot;] ],
        processEscapes: true
        }
    });
    &lt;/script&gt;

    &lt;script type=&quot;text/javascript&quot; async
        src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
    &lt;/script&gt;
    

  [1]: https://i.stack.imgur.com/KG0CN.jpg
  [2]: https://i.stack.imgur.com/OGGcp.jpg
||||||||||||||Found a solution, by adding the following to `MathJax`configuration:

    MathJax.Hub.Config({
        CommonHTML: {
            minScaleAdjust: 100,
        }
    });

thus increasing to 100% while default value is only 50 %. Reference is [here][1].


  [1]: https://docs.mathjax.org/en/v2.7-latest/options/output-processors/CommonHTML.html

--------------------------------------------------
Error Creating a GEV distribution in ggplot
I was trying to create GEV distributions with ggplot but received a persistent error. I was, however, successful creating these curves using the &quot;plot&quot; function, just like this:

    library(evir)

    Gpdf &lt;- plot(dgev(seq(1,500),mu=170.19, sigma=53.59, xi=0.09),type = &#39;l&#39;,col=&#39;black&#39;, lty = 
    1, ylim = c(0,0.010), lwd = 5, ylab = &quot;Density&quot;,xlab = &quot;mm/day&quot;, cex.lab = 2, cex.axis = 2, 
    main = &quot;Global&quot;, cex.main = 4) + lines(dgev(seq(1,500),mu = 195.38, sigma = 63.13, xi = 
    -0.05),type=&#39;l&#39;, col = &#39;blue&#39;, lty = 1, lwd = 5) 

[![enter image description here][1]][1]

ggplot attempt:

    library(ggplot2)
    library(evir)

    Gpdf &lt;- ggplot(geom_density(dgev(seq(1,500),mu = 170.19, sigma = 53.59, xi = 0.09), color = 
    &quot;black&quot;, size = 3, linetype = &quot;solid&quot;, ylim = c(0,0.010), ylab = &quot;Density&quot;, xlab= &quot;mm/day&quot;, 
    main = &quot;Global&quot;, cex.main=4) + geom_density(dgev(seq(1,500),mu = 195.38, sigma = 63.13, xi = 
    -0.05), col=&quot;blue&quot;, linetype = &quot;solid&quot;) + theme(plot.title = element_text(size = 54))+ 
    theme(axis.title = element_text(size = 54)) + theme(axis.text = element_text(size = 
    54))+theme(panel.background = element_blank()) + theme(plot.title = element_text(size = 54, 
    face = &quot;bold&quot;)))
 
Which yields this error:

    Error in dgev(seq(1, 500), mu = 170.19, sigma = 53.59, xi = 0.09) : 
    unused arguments (mu = 170.19, sigma = 53.59, xi = 0.09)

Any help or feedback with this would be greatly appreciated!

Thanks!


  [1]: https://i.stack.imgur.com/1LesO.jpg
||||||||||||||You are using completely the wrong syntax for ggplot. In addition, you should pass your data in as a long format data frame. One column should be the x values (two repetitions of 1:500), one column should be the y values (your two `dgev` vectors concatenated together), and a third column labeling which of the two lines each x, y pair belongs to:
``` r
library(evir)
library(ggplot2)

df <- data.frame(x = rep(1:500, 2),
                 y = c(dgev(1:500, mu = 170.19, sigma = 53.59, xi = 0.09),
                       dgev(1:500, mu = 195.38, sigma = 63.13, xi = -0.05)),
                 var = rep(c('1', '2'), each = 500))

ggplot(df, aes(x, y, color = var)) +
  geom_line(linewidth = 2) +
  theme_classic(base_size = 20) +
  scale_color_manual(values = c('black', 'blue')) +
  labs(title = 'Global', x = 'mm/day', y = 'Density')
```
[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/p1vB5.jpg

--------------------------------------------------
AWS CLI - Get error in create-state-machine --definition
I have a file that I exported from a Step Function in JSON format, and I made some modifications. I&#39;m trying to create another Step Function using the AWS CLI, but I always receive an error:
```
Command:
aws stepfunctions create-state-machine --name &#39;firstStep&#39; --definition .\CriarContaFaturamento-PrimeiraEtapa.asl.json --role-arn arn:aws:iam::xxxxxxxxxxxxx:role/aws-stepfunctions-xxxxxxx-dev-default --profile xxx --region us-east-1
```
```
Error:
An error occurred (InvalidDefinition) when calling the CreateStateMachine operation: Invalid State Machine Definition: &#39;INVALID_JSON_DESCRIPTION: Unexpected character (&#39;.&#39; (code 46)): expected a valid value (JSON String, Number, Array, Object or token &#39;null&#39;, &#39;true&#39; or &#39;false&#39;)
 at [Source: (String)&quot;.\CriarContaFaturamento-PrimeiraEtapa.asl.json&quot;; line: 1, column: 2] at [Source: (String)&quot;.\CriarContaFaturamento-PrimeiraEtapa.asl.json&quot;; line: 1, column: 2]&#39;
```

Do you know what might be wrong with the definition and what I could do to resolve it?
Im using powershell, but i tried in CMD and i got the same error.
||||||||||||||Unexpected character '.' suggests me the command line didn't parse your `.\CriarContaFaturamento-PrimeiraEtapa.asl.json` instruction. Have your tried something like:

```
$ aws stepfunctions create-state-machine --name 'state-machine-name' --definition file://CriarContaFaturamento-PrimeiraEtapa.asl.json --role-arn arn:aws:iam::xxxxxxxxxxxxx:role/aws-stepfunctions-xxxxxxx-dev-default --profile xxx --region us-east-1
```

--------------------------------------------------
How to resolve errors around uploadthing and NextJS. The upload functionality worked before the version 14 and now doesn&#39;t
I am getting errors with uploadthing and NextJS. The upload functionality worked before the version 14 and now doesn&#39;t. I tried reverting to the previous version of NextJS 13 and now I&#39;m getting console errors (Windows VSCode Terminal).

```
PS C:\Users\HP\Desktop\Development\Kind Courses\kind-courses&gt; npm i
npm WARN ERESOLVE overriding peer dependency
npm WARN While resolving: kind-courses@0.1.0
npm WARN Found: next@14.0.1
npm WARN node_modules/next
npm WARN   next@&quot;^13.4.12&quot; from the root project
npm WARN   2 more (@clerk/nextjs, @uploadthing/react)
npm WARN
npm WARN Could not resolve dependency:
npm WARN peerOptional next@&quot;14.0.1&quot; from @uploadthing/react@6.0.1
npm WARN node_modules/@uploadthing/react
npm WARN   @uploadthing/react@&quot;^6.0.1&quot; from the root project
npm ERR! code ECONNRESET
npm ERR! errno ECONNRESET
npm ERR! network Invalid response body while trying to fetch https://registry.npmjs.org/@next%2fenv: aborted
npm ERR! network This is a problem related to network connectivity.        
npm ERR! network In most cases you are behind a proxy or have bad network settings.
npm ERR! network
npm ERR! network If you are behind a proxy, please make sure that the
npm ERR! network &#39;proxy&#39; config is set properly.  See: &#39;npm help config&#39;   

npm ERR! A complete log of this run can be found in: C:\Users\HP\AppData\Local\npm-cache\_logs\2023-11-25T23_37_06_402Z-debug-0.log
```

At this point I wanted to try something else, like the native file upload but as a complete beginner on Nextjs and React and TypeScript, I have no clue where to start. I followed the whole documentation and did things right (I know this because the functionality worked). I also noticed that uploadthing gets UPDATED almost everyday and I think that&#39;s why the functionality is working.

In my app/api/uploadthing/core.ts

```
import { createUploadthing, type FileRouter } from &quot;uploadthing/next&quot;;
 
const f = createUploadthing();
 
const auth = (req: Request) =&gt; ({ id: &quot;fakeId&quot; }); // Fake auth function
 
// FileRouter for your app, can contain multiple FileRoutes
export const ourFileRouter = {
  // Define as many FileRoutes as you like, each with a unique routeSlug
  imageUploader: f({ image: { maxFileSize: &quot;4MB&quot; } })
    // Set permissions and file types for this FileRoute
    .middleware(async ({ req }) =&gt; {
      // This code runs on your server before upload
      const user = await auth(req);
 
      // If you throw, the user will not be able to upload
      if (!user) throw new Error(&quot;Unauthorized&quot;);
 
      // Whatever is returned here is accessible in onUploadComplete as `metadata`
      return { userId: user.id };
    })
    .onUploadComplete(async ({ metadata, file }) =&gt; {
      // This code RUNS ON YOUR SERVER after upload
      console.log(&quot;Upload complete for userId:&quot;, metadata.userId);
 
      console.log(&quot;file url&quot;, file.url);
 
      // !!! Whatever is returned here is sent to the clientside `onClientUploadComplete` callback
      return { uploadedBy: metadata.userId };
    }),
} satisfies FileRouter;
 
export type OurFileRouter = typeof ourFileRouter;
```

app/api/uploadthing/route.ts
```
import { createNextRouteHandler } from &quot;uploadthing/next&quot;;
 
import { ourFileRouter } from &quot;./core&quot;;
 
// Export routes for Next App Router
export const { GET, POST } = createNextRouteHandler({
  router: ourFileRouter,
});
```

tailwind.config.ts

```
import { withUt } from &quot;uploadthing/tw&quot;;
 
export default withUt({
  // Your existing Tailwind config
  content: [&quot;./src/**/*.{ts,tsx,mdx}&quot;],
  ...
});
```

app/(dashboard)/(routes)/teacher/courses/[courseId]/_components/image-form.tsx

```
&quot;use client&quot;

import * as z from &quot;zod&quot;;
import axios from &quot;axios&quot;;
import { ImageIcon, Pencil, PlusCircle } from &quot;lucide-react&quot;;
import { useState } from &quot;react&quot;;
import toast from &quot;react-hot-toast&quot;;
import { useRouter } from &quot;next/navigation&quot;;
import Image from &#39;next/image&#39;

import { Button } from &quot;@/components/ui/button&quot;;
import { Class } from &quot;@prisma/client&quot;;
import { FileUpload } from &quot;@/components/file-upload&quot;;


interface ImageFormProps {
    initialData: Class;
    courseId: string;//or class
};

const formShema = z.object({
    imageUrl: z.string().min(1, ({
        message: &quot;Image is required&quot;,
    }))
})

export const ImageForm = ({
    initialData,
    courseId,
}: ImageFormProps) =&gt; {
    const [isEditing, setIsEditing] = useState(false);


    const toggleEdit = () =&gt; {
        setIsEditing((current) =&gt; !current);
    }

    const router = useRouter();

    const onSubmit = async (values: z.infer&lt;typeof formShema&gt;) =&gt; {
        console.log(&quot;Form Values:&quot;, values);
        try {
            await axios.patch(`/api/courses/${courseId}`, values);
            toast.success(&quot;Class Updated&quot;);
            toggleEdit();
            router.refresh();
        } catch {
            toast.error(&quot;Something went wrong&quot;);
        }
    }

    return(
        &lt;div className=&quot;mt-6 border bg-slate-100 rounded-md p-4&quot;&gt;
            &lt;div className=&quot;font-medium flex items-center justify-between&quot;&gt;
                Class Image
                &lt;Button onClick={toggleEdit} variant=&quot;ghost&quot;&gt;
                    { isEditing &amp;&amp; (
                        &lt;&gt;Cancel&lt;/&gt;
                    )}
                    {!isEditing &amp;&amp; !initialData.imageUrl &amp;&amp; (
                        &lt;&gt;
                            &lt;PlusCircle className=&quot;h-4 w-4 mr-2&quot; /&gt;
                            Add Image
                        &lt;/&gt;
                    )}
                    {
                        !isEditing &amp;&amp; initialData.imageUrl &amp;&amp; (
                            &lt;&gt;
                                &lt;Pencil className=&quot;h-4 w-4 mr-2 &quot;/&gt;
                                Edit Image
                            &lt;/&gt;
                        )
                    }
                &lt;/Button&gt;
            &lt;/div&gt;
            {!isEditing &amp;&amp; (
                !initialData.imageUrl ? (
                    &lt;div className=&quot;flex items-center justify-center h-60 bg-slate-200 rounded-md&quot;&gt;
                        &lt;ImageIcon className=&quot;h-10 w-10 text-slate-500&quot;/&gt;
                    &lt;/div&gt;
                ) : (
                    &lt;div className=&quot;relative aspect-video mt-2&quot;&gt;
                        &lt;Image alt=&quot;Upload&quot; fill className=&quot;object-cover rounded-md&quot; src={initialData.imageUrl }/&gt;
                    &lt;/div&gt;
                )
            )}
            {isEditing &amp;&amp; (
                &lt;div&gt;
                    &lt;FileUpload 
                        endpoint=&quot;courseImage&quot;//or classImage
                        onChange={(url) =&gt; {
                            if(url){
                                onSubmit({ imageUrl: url})
                            }
                        }}
                    /&gt;
                    &lt;div className=&quot;text-xs text-muted-foreground mt-4&quot;&gt;
                        16:9 aspect ratio recomended

                    &lt;/div&gt;
                &lt;/div&gt;
            )}
        &lt;/div&gt;
    )
}
```


And I&#39;m also encountering this when I run the app and try to upload a file.

```
✓ Compiled /api/uploadthing/route in 1218ms (899 modules)
 ○ Compiling /not-found ...
 ✓ Compiled /not-found in 3.8s (1468 modules)
[UT] UploadThing dev server is now running!
[UT] UploadThing dev server is now running!
[UT] SIMULATING FILE UPLOAD WEBHOOK CALLBACK http://localhost:3000/api/uploadthing?slug=courseImage
INFO: Clerk: The request to /api/uploadthing is being protected (401) because there is no signed-in user, and the path is included in `apiRoutes`. To 
prevent this behavior, choose one of:

1. To prevent Clerk authentication from protecting (401) the api route, remove the rule matching &quot;/api/uploadthing&quot; from the `apiRoutes` array passed 
to authMiddleware
2. To make the route accessible to both signed in and signed out users, pass `publicRoutes: [&quot;/api/uploadthing&quot;]` to authMiddleware
3. To prevent Clerk authentication from running at all, pass `ignoredRoutes: [&quot;/((?!api|trpc))(_next.*|.+\.[\w]+$)&quot;, &quot;/api/uploadthing&quot;]` to authMiddleware
4. Pass a custom `afterAuth` to authMiddleware, and replace Clerk&#39;s default behavior of redirecting unless a route is included in publicRoutes        

For additional information about middleware, please visit https://clerk.com/docs/nextjs/middleware
(This log only appears in development mode, or if `debug: true` is passed to authMiddleware)
[UT] Failed to simulate callback for file. Is your webhook configured correctly? 93f6b985-5279-4803-992c-416dcf7614f9-6s0ujg.png
```
||||||||||||||You can bypass this by making your route public in `authMiddleware()`: Go to your root directory if you are using app, then search for "middleware.ts". 

    import { authMiddleware } from "@clerk/nextjs";
     
    // This example protects all routes including api/trpc routes
    // Please edit this to allow other routes to be public as needed.
    // See https://clerk.com/docs/references/nextjs/auth-middleware for more information about configuring your Middleware
    export default authMiddleware({ 
      publicRoutes:["/api/uploadthing"]
    });
     
    export const config = {
      matcher: ['/((?!.+\\.[\\w]+$|_next).*)', '/', '/(api|trpc)(.*)'],

    };

--------------------------------------------------
searchview.setonquerytextlistener() type mismatch in Navigation Fragment in Kotlin
I&#39;m getting a &quot;type mismatch required OnQueryTextListener!&quot; error when trying to implement searchview in Kotlin in a Navigation Fragment.  I&#39;ve searched as many samples and stackoverflow questions as I could and everything says my code should be correct.  Note that my searchview is PERSISTANT (not part of the menu), so I cannot do a menu.finditem. 

[![enter image description here][1]][1]

Here is my HomeFragment code :


    class HomeFragment : Fragment() {
    private lateinit var homeViewModel: HomeViewModel

    override fun onCreateView(inflater: LayoutInflater, container: ViewGroup?, savedInstanceState: Bundle?): View? {
        return inflater.inflate(R.layout.fragment_home, container, false)
    }

    override fun onActivityCreated(savedInstanceState: Bundle?) {
        super.onActivityCreated(savedInstanceState)
        println(&quot;***************** Home Fragment *******************&quot;)

        searchView.setOnQueryTextListener(object : SearchView.OnQueryTextListener {
            override fun onQueryTextSubmit(query: String): Boolean {
                // task HERE
                return false
            }
            override fun onQueryTextChange(newText: String): Boolean {
                return false
            }
        })
    }

and the layout code:


    &lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;
    xmlns:app=&quot;http://schemas.android.com/apk/res-auto&quot;
    xmlns:tools=&quot;http://schemas.android.com/tools&quot;
    android:layout_width=&quot;match_parent&quot;
    android:layout_height=&quot;match_parent&quot;
    android:background=&quot;@color/materialBackgroundGrey&quot;&gt;

    &lt;androidx.cardview.widget.CardView
        android:id=&quot;@+id/cardView&quot;
        android:layout_width=&quot;match_parent&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:layout_margin=&quot;10dp&quot;
        android:iconifiedByDefault=&quot;false&quot;
        app:layout_constraintStart_toStartOf=&quot;parent&quot;
        app:layout_constraintTop_toTopOf=&quot;parent&quot;
        app:queryBackground=&quot;@null&quot;&gt;

        &lt;SearchView
            android:id=&quot;@+id/searchView&quot;
            android:layout_width=&quot;match_parent&quot;
            android:layout_height=&quot;wrap_content&quot;
            android:queryHint=&quot;Search&quot;
            app:iconifiedByDefault=&quot;false&quot;
            app:layout_constraintStart_toStartOf=&quot;parent&quot;
            app:layout_constraintTop_toTopOf=&quot;parent&quot; /&gt;

    &lt;/androidx.cardview.widget.CardView&gt;

    &lt;androidx.recyclerview.widget.RecyclerView
        android:id=&quot;@+id/recyclerView&quot;
        android:layout_width=&quot;match_parent&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:layout_margin=&quot;10dp&quot;
        android:background=&quot;@color/colorWhite&quot;
        app:layout_constraintStart_toStartOf=&quot;parent&quot;
        app:layout_constraintTop_toBottomOf=&quot;@id/cardView&quot;
        app:layout_constraintBottom_toBottomOf=&quot;parent&quot;
        /&gt;

&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;


  [1]: https://i.stack.imgur.com/7aSpL.jpg
||||||||||||||There are two `SearchView` widgets:

- `android.widget.SearchView`
- `androidx.appcompat.widget.SearchView`

These fill the same role but are not compatible, and their nested interfaces, like `OnQueryTextListener`, are not compatible.

Make sure that you use the same one both in resources (such as your layout) and in any `import` statements. If you are using AppCompat, you probably want `androidx.appcompat.widget.SearchView`.



--------------------------------------------------
Find index of next word after the given word
I am trying to write a function in which if I pass a string, word, nextWord it should return me the index of nextWord if matches else -1.

Suppose I have String 

    String s = &quot;Hatred was spreading everywhere, blood was being spilled everywhere, wars were breaking out everywhere      and dinosaurs are everywhere, but please note I want the first and index.&quot;

In the function I will pass String s, word=&quot;everywhere&quot;, nextWord=&quot;and&quot;. If &quot;and&quot; is just after &quot;everyWhere&quot; as in the above string return me the index of &quot;and&quot; else return -1.

Don&#39;t want to use Regex.

Sample tried code

    public static int indexOfPreceded(String str, String word, String nextWord) {
    		int i = StringUtils.indexOfIgnoreCase(str, nextWord);
    		if (i &lt; 0) {
    			return -1;
    		}
    		return StringUtils.indexOfIgnoreCase(str, word, i + nextWord.length());
    	}

In above code if I pass below sample string where the word &quot;and&quot; is not after &quot;everywhere&quot;

    String s =  &quot;Hatred was spreading everywhere, blood was being spilled everywhere, wars were breaking out everywhere dinosaurs are everywhere, but please note I want the first and index.&quot;

word = &quot;and&quot;
nextWord=&quot;everywhere&quot;

The code is returning index as 158 while it should return -1 because in the string there is no &quot;and&quot; just after &quot;everywhere&quot;
||||||||||||||You need to modify your function to properly check if the nextWord appears just after the word. The approach should be to find the index of the word first, and then check if the nextWord immediately follows it.

    public static int indexOfPreceded(String str, String word, String nextWord) {
        int currentIndex = 0;
        int wordIndex = -1;
        
        while ((wordIndex = str.indexOf(word, currentIndex)) != -1) {
            int nextWordIndex = wordIndex + word.length();
            int endIndex = nextWordIndex + nextWord.length();
    
            if (endIndex <= str.length() && str.substring(nextWordIndex, endIndex).equals(nextWord)) {
                return nextWordIndex;
            }
    
            currentIndex = nextWordIndex;
        }
    
        return -1;
    }



--------------------------------------------------
How to Sign-in Users with OAuth Tokens
I am building an ASP.NET Core MVC web application that has two ways of authenticating; one way uses `.AddOpenIdConnect()` and is already working.  

The second way is to retrieve OAuth tokens from a web service. 

```
// MVC
public async Task&lt;IActionResult&gt; GetToken()
{
	var tokenResponse = await _tokenService.GetToken();

	// This header did not appear in the JwtBearerEvents
	string headerValue = new AuthenticationHeaderValue(&quot;Bearer&quot;, tokenResponse.AccessToken).ToString();
	Request.Headers.Authorization = headerValue;

	return RedirectToAction(&quot;ShowToken&quot;, &quot;Account&quot;);
}

// This AuthorizeAttribute causes JwtBearerEvents to fire
[Authorize(AuthenticationSchemes = JwtBearerDefaults.AuthenticationScheme)]
public IActionResult ShowToken()
{    
    return View();
}
```


After retrieving tokens from a web service, 
 1. How do I pass the the OAuth tokens to the authentication middleware?  (I already tried setting the authorization header, but that didn&#39;t work.)
 2. How do I configure the authentication middleware to handle them? 

```
builder.Services.AddAuthentication(options =&gt;
{
	options.DefaultScheme = CookieAuthenticationDefaults.AuthenticationScheme;

	options.DefaultAuthenticateScheme = CookieAuthenticationDefaults.AuthenticationScheme;
	options.DefaultSignInScheme = CookieAuthenticationDefaults.AuthenticationScheme;
	options.DefaultSignOutScheme = CookieAuthenticationDefaults.AuthenticationScheme;

	options.DefaultChallengeScheme = CookieAuthenticationDefaults.AuthenticationScheme; 
})
.AddJwtBearer(JwtBearerDefaults.AuthenticationScheme, options =&gt;
{
	options.TokenHandlers.Add(new StrictTokenHandler());
	options.TokenValidationParameters = tokenValidationParameters;
	options.SaveToken = true;
	options.Events = new JwtBearerEvents
	{
		OnMessageReceived = context =&gt;
		{
			var p = context.Properties;
			// context.Token = &lt;context does not contain token&gt;                        
			return Task.CompletedTask;
		},
		OnChallenge = context =&gt;
		{
			var p = context.Properties;
			// have token, but can&#39;t set Token
			return Task.CompletedTask;
		},
		OnTokenValidated = context =&gt;
		{
			return Task.CompletedTask;
		},
	};
})
.AddCookie(options =&gt;
{
	options.LoginPath = new PathString(&quot;/Account/Login&quot;);
})
.AddOpenIdConnect(options =&gt;
{
	options.Authority = issuer;
	// the rest omitted for brevity
	// this is working
});
```

In the `.AddJwtBearer()` method above, the context `MessageReceivedContext` has a settable `Token` property, but I don&#39;t have it yet.  The `OnChallenge` event has the token if I set it in `AuthenticationProperties`, but `JwtBearerChallengeContext` does not have a settable `Token` property.  


This is the token information returned from the token service.  How do I get this into the Authentication middleware?

[![Token Information Screenshot][1]][1]

  [1]: https://i.stack.imgur.com/heecS.png
||||||||||||||You can customize behavior by subclassing the [JwtBearerHandler](https://github.com/dotnet/aspnetcore/blob/main/src/Security/Authentication/JwtBearer/src/JwtBearerHandler.cs) and overriding `HandleAuthenticateAsync`. This might involve doing your work to get the token, updating the request's HTTP Authorization header, then calling the base class. Or you can write a custom handler as in [this code of mine](https://github.com/gary-archer/oauth.apisample.netcore/blob/master/src/plumbing/middleware/CustomAuthenticationHandler.cs).

**SECURITY**

It is not clear from your question how the incoming call is secured though. Eg. the caller should provide the token. How is security being managed if the backend code itself gets tokens from a web service?

**SEPARATION**

Usually `AddJwtBearer` is used by a web API (aka resource server). Whereas `AddCookie` and `AddOpenIdConnect` are used by backend websites. A preferred way to separate these concerns is to split them into multiple components. The API can then be called by both web and mobile clients and uses a single authentication scheme in both cases:

- API
- Web client
- Mobile client

This will also keep code simpler. Otherwise you may struggle with areas like error handling:

- A cookie based website redirects unauthorized requests
- An API returns a 401 response 

--------------------------------------------------
Can we promote RDS read replica to primary using cloud formation template?
We can easily promote RDS read replica using console and CLI, but is there a way to give master access means promote to primary using cloud formation?
||||||||||||||CloudFormation uses the property `"SourceDBInstanceIdentifier"` to determine whether a `DBInstance` is a read replica or not. If this property has any non-empty value, the `DBInstance` will be created as a read replica.

If you remove this property once a read replica is created and update your CloudFormation stack, a new `DBInstance` will be created that won't be a read replica, it will be it's own standalone database. However, the new standalone database will not have the contents of the read replica.

The solution to this will be to regularly back up your read replica. When you want to promote your read replica, remove `"SourceDBInstanceIdentifier"` and set `"DBSnapshotIdentifier"` to the most recent snapshot of the read replica. This should effectively promote your read replica to master with most the data still intact



--------------------------------------------------
What are some FS2 Error Hanlding Practices?
I am new to FS2, Cats Effect, etc., but I have been using Scala since 2005 and Akka since 2010...

I tuned up the FS2 example code to play with error-handling ideas, but I wondered if there are better idiomatic ways to deal with this.

I tried the various Stream mechanisms, but, it was very troublesome, and the best I could do was collapse the whole stream into a single output value. I had a lot of trouble finding good documentation on error handling in FS2.

```
import cats.effect.{IO, IOApp}
import fs2.{Stream, text}
import fs2.io.file.{Files, Path}

import scala.util.{Failure, Success}

object Converter extends IOApp.Simple {

  private val converter: Stream[IO, Unit] = {
    def fahrenheitToCelsius(f: Double) =
      (f - 32.0) * (5.0/9.0)

    def convert(string: String) =
      try
        Success(fahrenheitToCelsius(string.toDouble).toString)
      catch
        case cause: Exception =&gt; Failure(cause)

    Files[IO].readUtf8Lines(Path(&quot;testdata/fahrenheit.txt&quot;))
      .filter(s =&gt; s.trim.nonEmpty &amp;&amp; !s.startsWith(&quot;//&quot;))
      .map(line =&gt; convert(line) match 
        case Success(value) =&gt; value
        case Failure(cause) =&gt; s&quot;Failed to convert $line: ${cause.getMessage}&quot;
      )
      .intersperse(&quot;\n&quot;)
      .through(text.utf8.encode)
      .through(Files[IO].writeAll(Path(&quot;testdata/celsius.txt&quot;)))
  }

  def run: IO[Unit] =
    converter.compile.drain
}
```
||||||||||||||Thanks to Luis Miguel Mejia Suarez, I tweaked this to become

```
import cats.effect.{IO, IOApp}
import fs2.{Stream, text}
import fs2.io.file.{Files, Path}

object Converter extends IOApp.Simple {

  private val converter: Stream[IO, Unit] = {
    def fahrenheitToCelsius(f: Double) =
      (f - 32.0) * (5.0/9.0)

    Files[IO].readUtf8Lines(Path("testdata/fahrenheit.txt"))
      .filter(s => s.trim.nonEmpty && !s.startsWith("//"))
      .map(line => line.toDoubleOption match
        case Some(value) => fahrenheitToCelsius(value).toString
        case None => s"Failed to convert $line to a number."
      )
      .intersperse("\n")
      .through(text.utf8.encode)
      .through(Files[IO].writeAll(Path("testdata/celsius.txt")))
  }

  def run: IO[Unit] =
    converter.compile.drain
}
```

If `toDoubleOption` did not exist, the right thing to do would have been to implement it. It might be worth implementing `toDoubleEither` or `toDoubleTry` as those do not seem to exist.

I had tried playing with `Stream` methods like `handleError`, `HandleErrorWith`, `evalTap`, `evalFilter`, etc., but I could not achieve what I wanted. I still have a lot to learn. 🙄

--------------------------------------------------
If you create a visual with custom measures, does clicking on this visual still filter related visuals?
I created a new visual with some custom measures.

Now, when I click on any other visual, it still filters the rest of the visuals same as before (including the new visual). But clicking on the new visual itself doesn&#39;t filter the others.

Essentially, the new visual (with custom measures) can be filtered by all other visuals, but not vice versa. What is creating this one way flow?
Is this always going to be the case when you use custom measures?

The new visual is a funnel using pipeline stages from a sales pipeline. The pipeline stage is one column in a wider table of sales data which I have used for other visuals.

I created a separate measure for each one of the bars (the pipeline stage) in the funnel visual. For example:

   `CALCULATE(COUNTROWS(&#39;Combined Leads&#39;),FILTER(&#39;Combined Leads&#39;,&#39;Combined Leads&#39;[Pipeline stage] &lt;&gt; &quot;Lead Qualification&quot; &amp;&amp;&#39;Combined Leads&#39;[Pipeline stage] &lt;&gt; &quot;Meeting / Presentation&quot;`

||||||||||||||You can control the interaction between visuals, by default it will apply cross-highlight or cross-filter between the chosen visual and all the others in the page.

[Visual Interactions][1]

Your measures can ignore these filters using DAX, by adding/removing filters you can ignore or overwrite the filter context imposed by the interaction from other visuals.

Check the query being generated by the visual with Performance Analyzer and you should see some TREATAS variables being issued on the query, those are the filters.

  [1]: https://learn.microsoft.com/en-us/power-bi/create-reports/service-reports-visual-interactions?tabs=powerbi-desktop

--------------------------------------------------
Cloning node along with dropdown
I need to clone a dropdown and text box along with its values and append to next line on clicking of a button.

Here is my HTML code:

    &lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;mynode&quot;&gt;
    &lt;span class=&quot;label&quot;&gt;If value: &lt;/span&gt; &lt;select class=&quot;options&quot;&gt;
    &lt;option&gt;more than&lt;/option&gt; &lt;option&gt;below than&lt;/option&gt; &lt;option&gt;same&lt;/option&gt;
    &lt;/select&gt;
    &lt;span class=&quot;label&quot;&gt;Temperature: &lt;/span&gt; &lt;input text=&quot;number&quot; value=&quot;55&quot; /&gt;
    &lt;/div&gt;
    &lt;/div&gt;

    &lt;button type=&quot;button&quot; id=&quot;myBtn&quot;&gt;Add Row&lt;/button&gt;

Here is my JS code:

    const btn = document.getElementById(&quot;myBtn&quot;);
    btn.addEventListenet(&#39;click&#39;, function() {
    	const container = document.querySelectorAll(&quot;.mynode&quot;);
    	const mynode = container[container.length-1];
    	const latest = mynode.cloneNode(true);
    	document.querySelector(&#39;.container&#39;).appendChild(latest);
    });

In JS, I am copying the last row of the container and appending it correctly. But if I modify values in dropdown, then the new row is not having the updated selected value. How to solve this?	I need to modify only JS code not the HTML. Also I don&#39;t want to use jquery or other libraries.
||||||||||||||You're copying the node itself, but not its value. Also, there's a typo: you need to use addEventListener instead of addEventListenet. This is the corrected code (HTML is not changed, as requested):

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-js -->

    const btn = document.getElementById("myBtn");
    btn.addEventListener('click', function() {
        const container = document.querySelectorAll(".mynode");
        const mynode = container[container.length - 1];
        const latest = mynode.cloneNode(true);

        const originalDropdown = mynode.querySelector('.options');
        const clonedDropdown = latest.querySelector('.options');
        clonedDropdown.value = originalDropdown.value;

        document.querySelector('.container').appendChild(latest);
    });

<!-- language: lang-html -->

    <div class="container">
    <div class="mynode">
    <span class="label">If value: </span> <select class="options">
    <option>more than</option> <option>below than</option> <option>same</option>
    </select>
    <span class="label">Temperature: </span> <input text="number" value="55" />
    </div>
    </div>

    <button type="button" id="myBtn">Add Row</button>


<!-- end snippet -->



--------------------------------------------------
ADD time 23:59:59.999 to end date for between
I have been having an issue with using the following:

    Column_Name BETWEEN @StartDate AND @EndDate. 

This is because the @EndDate = 00:00:00.000 for the time, which doesn&#39;t pick up all the values for that day.

How would I convert the @EndDate (Always 00:00:00.000) to always be Date + 23:59:59.999?




||||||||||||||One option that avoids needing to add EndDate + 23:59:59.999 is to not use the `between` comparison and instead use `column_name >= @StartDate and column_name < @EndDate +1`

--------------------------------------------------
TeamsBot SendActivityAsync with ActivityTypes.Typing no longer works
We&#39;ve been using a Teams Bot to send back a typing reply in `OnMessageActivityAsync`. It worked until recently, doing it in this manner

    protected override async Task OnMessageActivityAsync(ITurnContext&lt;IMessageActivity&gt; turnContext, CancellationToken cancellationToken)
    {
        await turnContext.SendActivityAsync(new Activity { Type = ActivityTypes.Typing }, cancellationToken);
    }

As of recently, in new Teams, this no longer works. Does anyone have any idea why? 
||||||||||||||There seems to be an issue at the moment - see also this question from 2 weeks ago: https://stackoverflow.com/questions/77878531/bot-framework-typing-indicator-is-not-working-in-new-ms-team-work-or-school . I'm seeing it my side as well.

--------------------------------------------------
Unable to push test user to SQLite DB
I am trying to add a test user to the user table within my sqlite db that is a part of a website. When trying to add the test user, I get a TypeError shown here:

```
TypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a TypeError.
```

Full code is shown here:

```
from flask import Flask
from flask import render_template
from flask import redirect
from flask import url_for
from flask_sqlalchemy import SQLAlchemy

app = Flask(__name__)
app.config[&#39;SQLALCHEMY_DATABASE_URI&#39;] = &#39;sqlite:///users_db.db&#39;
app.secret_key = &quot;secret&quot;
db = SQLAlchemy(app)

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(20), unique=False, nullable=False)
    email = db.Column(db.String(), unique=True, nullable=False)
    password = db.Column(db.String(), unique=False, nullable=False)

    def __repr__(self):
        return f&quot;User(&#39;{self.username}&#39;, &#39;{self.email}&#39;, &#39;{self.id}&#39;)&quot;


@app.route(&#39;/&#39;)
def home():
    return render_template(&#39;home.html&#39;)

@app.route(&#39;/test_db&#39;)
def test_db():
    try:
        test_user = User(username=&#39;TestUser&#39;, email=&#39;testuser@example.com&#39;, password=&#39;testpassword&#39;)
        db.session.add(User(test_user))
        db.session.commit()
        return &quot;Test Passed&quot;
    except Exception as e:
        return e

if __name__ == &quot;__main__&quot;:
    with app.app_context():
        db.create_all()
    app.run(debug=False)
```
||||||||||||||The problem is

```
    except Exception as e:
        return e
```
This is returning the exception object, but this is not a valid type for a route function to return. 

The simple fix is to convert the exception to a string:

```
    except Exception as e:
        return str(e)
```

However, in a production application you generally shouldn't expose internal error messages to the user. It would be better to use the `logging` module to log messages, and return a generic error message string

```
    except Exception as e:
        logging.error(str(e))
        return 'An internal error occurred'
```


--------------------------------------------------
SonarQube rule Classes from &quot;com.sun.*&quot; and &quot;sun.*&quot; packages should not be used
I have a J2EE project with the following characteristics:

    CDI 1.0
    Dynamic Web Module 3.0
    Java 1.7 (it&#39;s being changed to 1.8)
    JSF 2.0
    JPA 2.0

I&#39;m running SonarQube 5.6.6 rules against it and it felt into the rule

&lt;i&gt;Classes from &quot;com.sun.*&quot; and &quot;sun.*&quot; packages should not be used
&lt;br&gt;squid : S1191
&lt;br&gt;&lt;br&gt;Classes in com.sun.* and sun.* packages are considered implementation details, and are not part of the Java API. They can cause problems when moving to new versions of Java because there is no backwards compatibility guarantee. Such classes are almost always wrapped by Java API classes that should be used instead.&lt;/i&gt;

because I&#39;m using classes &lt;b&gt;com.sun.faces.application.ApplicationAssociate&lt;/b&gt; and &lt;b&gt;com.sun.faces.application.ApplicationResourceBundle&lt;/b&gt;.

I&#39;ve seached another threads about this and most of them say I should change the rule to exclude the specific package or class.

I think there is no point in simply circumvent the rule, so I would like to know if there are actualy java API (1.7 or 1.8) classes for these sun classes.

If not, I believe it&#39;s better to keep the alert until java API classes become available for these sun classes.

Any tip/advice on this?
||||||||||||||That's a bug in SonarQube. It's overgeneralizing the `sun.*` package as mentioned in [Why Developers Should Not Write Programs That Call 'sun' Packages][1] to `com.sun.*` package. This is incorrect. Oracle didn't mean to say that in abovelinked article. SonarQube should really only penalize usage of `sun.*` package or whatever is internally used by an arbitrary JRE/JDK implementation. The `com.sun.*` package is not JRE/JDK API/impl related at all.

Either turn off the S1191 rule, or mark all hits on `com.sun.*` as false positive.

### See also:

- https://stackoverflow.com/questions/8565708/what-is-inside-com-sun-package/
- [SonarJava issue 437][2]


  [1]: https://www.oracle.com/java/technologies/faq-sun-packages.html
  [2]: https://jira.sonarsource.com/browse/SONARJAVA-437

--------------------------------------------------
AWS Amplify: How to switch aws profile that amplify env is pointing to
I started an Amplify project using:

```
amplify init
```

I created an environment named `env` and chose AWS profile `default`.

It seems like `env` is pointing to the `default` profile. Is there any way for me to make `env` pointing to different AWS profile?


For example, if I have two profiles: 

1. default
2. personal


Can I make `env` point to the `personal` AWS profile? 


||||||||||||||I know this question is a couple of years old, but I had a similar issue and came across it. 

The relevant configuration is in `<your-project>/amplify/.config/local-aws-info.json`, which will look  something like this:

```json
// <your-project>/amplify/.config/local-aws-info.json
{
  "develop": {
    "configLevel": "project",
    "useProfile": true,
    "profileName": "SomeNamedProfile"
  },
  "main": {
    "configLevel": "project",
    "useProfile": true,
    "profileName": "SomeOtherNamedProfile"
  }
}
```

Changing the value of `profileName` to specify a different AWS CLI profile worked without any issues. 

--------------------------------------------------
How to use Grafana template variable to select datasource
I have multiple Prometheus instances and I have a Custom multi-value variable `PrometheusInstance` with values A,B,C.

I have 3 different data sources whose URLs look like this:  
`http://A.foo.com:9090 etc`.

I would like the data source to change based on the value of the variable and my 19 panels to display the metrics from the corresponding instance of Prometheus.

How to achieve this? Using `$PrometheusInstance` in the data source URL or name did not work.

Grafana version 9.1.6
||||||||||||||First of all, you cannot dynamically specify address for your data source. Main reason: variables exist in the scope of dashboard, but data source is in global scope.

If all you need is to allow to switch between data sources on dashboard, you can create variable with variable type ["Data source"][1], set type to "Prometheus". After that you'll need to go through your panels, and change data source to this variable (in drop-down for data source).

Selecting the value for this variable will look exactly the same as other variables, with value populated by names of existing Prometheus data sources.

Note, your data sources have to be configured manually as usual.

If you need to also use it in ways, other then selecting data source for panels, you can still use it as a usual variable `${variable_name}`, and data source name will be used as a value.


  [1]: https://grafana.com/docs/grafana/latest/dashboards/variables/add-template-variables/#add-a-data-source-variable

--------------------------------------------------
When is `deinit` exactly called? (in Swift)
When is `deinit` exactly called?

Is it like `C++` guaranteed to be called when last reference gets out of scope (by return, throw or exit)?

Or is `Swift` using Garbage-Collector?
||||||||||||||
The `deinit` is intended to release resources (such as freeing memory that is not under ARC).

(Thanks to [Martin](https://stackoverflow.com/users/1187415/martin-r)
and [Rob](https://stackoverflow.com/users/1271826/rob)'s input, we can conclude below)

## When is `deinit` called?
Usually, when last strong-reference gets out of scope,
`deinit` is called instantly (then deallocation happens).

**But:**
1. **If affected by `autorelease` feature** (which has conditions), `deinit` is called significantly later,
  long after last reference gets out of scope (when `autorelease` pool is drained).
2. **Or** when App is terminating, `deinit` is guaranteed to never get called **!!**
  (if `deinit` was not already called).
3. Also in extremely common cases, `deinit` is called before strong-ref-variable's scope ends:
   - In Swift unlike other languages, when we set a weak-reference equal to a strong-reference,
     it could result to `nil` (which is absolutely allowed by Swift).

   - This happens if compiler detects that the remaining lines of scope,
     have NOT any strong-reference.

   - Possible workaround is using `withExtendedLifetime(_:_:)` global-method / function, like:
   ```
   withExtendedLifetime(myStrongRefVariable) {
       // Do something that only needs a non-nil weak reference.
   }
   ```

**Note** that `deinit` may be skipped if related `init` throws at the right/wrong moment, see:
https://forums.swift.org/t/deinit-and-throwing-initializers/38871

## Is it like `C++` destructor?
There is no equivalent of a `C++` destructor in `ObjC` or `Swift`.

(`Objective-C++` object's destructor (`dealloc`) are called during program termination,
because that is required by `C++` spec,
but that's all and else Obj-C++'s `dealloc` behavior is almost same as `deinit`.)

## Is Swift using Garbage-Collector?
No, but whenever `autorelease` feature affects objects,
the `deinit` can be postponed (till autorelease-pool is drained, as mentioned above).


--------------------------------------------------
Data binding the TextBlock Runs in Avalonia
I want to do this

[![Text with different colors][1]][1]

from this view model

    public class MainWindowViewModel : ViewModelBase
    {
        public ObservableCollection&lt;TextRun&gt; RunList { get; set; }
    
        public MainWindowViewModel()
        {
            RunList = new ObservableCollection&lt;TextRun&gt;(new List&lt;TextRun&gt;
                {
                    new TextRun(&quot;Th&quot;, &quot;Red&quot;),
                    new TextRun(&quot;is&quot;, &quot;Green&quot;),
                    new TextRun(&quot; i&quot;, &quot;Maroon&quot;),
                    new TextRun(&quot;s &quot;, &quot;Blue&quot;),
                    new TextRun(&quot;a &quot;, &quot;Yellow&quot;),
                    new TextRun(&quot;te&quot;, &quot;Black&quot;),
                    new TextRun(&quot;st&quot;, &quot;Orange&quot;),
                    new TextRun(&quot;!!&quot;, &quot;Violet&quot;),
                    new TextRun(&quot;!!&quot;, &quot;Cyan&quot;)
                });
        }
    }
    
    public class TextRun
    {
        public string Text { get; set; }
        public string Color { get; set; }
    
        public TextRun(string title, string color)
        {
            Text = title;
            Color = color;
        }
    }



exactly the same way this extract of the [accepted answer to this similar question][2] suggest:

&gt; You may alternatively want to consider exposing an abstract model of
&gt; text, font, etc. (i.e. a view model) and creating the actual Inline
&gt; objects via a DataTemplate.

i.e. **no code, only axaml**. Sadly, the linked answer doesn&#39;t expand on this way of doing things and I need the details. I&#39;ve spent the last two days trying out different combinations and I&#39;m almost starting to hit my head against the wall. But in principle it should be easy. For instance, if the text were to be on different lines, this would be the axaml:

    &lt;ItemsControl ItemsSource=&quot;{Binding RunList}&quot; &gt;
      &lt;ItemsControl.ItemTemplate&gt;
        &lt;DataTemplate&gt;
          &lt;TextBlock Text=&quot;{Binding Text}&quot; Foreground=&quot;{Binding Color}&quot;/&gt;
        &lt;/DataTemplate&gt;
      &lt;/ItemsControl.ItemTemplate&gt;
    &lt;/ItemsControl&gt;

but I need the text with different colors to be on the same line.

I know I must be using runs. This static axaml manages to do exactly that:

    &lt;TextBlock&gt;
      &lt;Run Text=&quot;Th&quot; Foreground=&quot;Red&quot;/&gt;&lt;Run Text=&quot;is&quot; Foreground=&quot;Green&quot;/&gt;&lt;Run Text=&quot; a&quot; Foreground=&quot;Blue&quot;/&gt;&lt;Run Text=&quot; t&quot; Foreground=&quot;Yellow&quot;/&gt;&lt;Run Text=&quot;es&quot; Foreground=&quot;Black&quot;/&gt;&lt;Run Text=&quot;t!&quot; Foreground=&quot;Orange&quot;/&gt;&lt;Run Text=&quot;!!&quot; Foreground=&quot;Violet&quot;/&gt;
    &lt;/TextBlock&gt;

But I need it to be &quot;Modelview binded&quot; (Dynamic) as the extract suggest.

I just started playing with Avalonia the day before yesterday, so I&#39;m fumbling in the dark here, please bear with me. So far it feels intuitive. This is the first speed bump I hit.

Included WPF tag because the WPF&#39;s xaml solution to this problem should be very similar to Avalonia&#39;s axaml. Sadly, I have no WPF experience either.

Update:

As [Danial][3] suggested (I&#39;m very very grateful for your suggestion!), I tried his variant:

    &lt;TextBlock&gt;
      &lt;Run Text=&quot;Th&quot; Foreground=&quot;Red&quot;/&gt;&lt;Run Text=&quot;is&quot; Foreground=&quot;Green&quot;/&gt;&lt;Run Text=&quot; i&quot; Foreground=&quot;Maroon&quot;/&gt;&lt;Run Text=&quot;s &quot; Foreground=&quot;Blue&quot;/&gt;&lt;Run Text=&quot;a &quot; Foreground=&quot;Yellow&quot;/&gt;&lt;Run Text=&quot;te&quot; Foreground=&quot;Black&quot;/&gt;&lt;Run Text=&quot;st&quot; Foreground=&quot;Orange&quot;/&gt;&lt;Run Text=&quot;!!&quot; Foreground=&quot;Violet&quot;/&gt;&lt;Run Text=&quot;!!&quot; Foreground=&quot;Cyan&quot;/&gt;
    &lt;/TextBlock&gt;
    &lt;ItemsControl ItemsSource=&quot;{Binding RunList}&quot; &gt;
        &lt;ItemsControl.ItemsPanel&gt;
            &lt;ItemsPanelTemplate&gt;
                &lt;StackPanel Orientation=&quot;Horizontal&quot;/&gt;
            &lt;/ItemsPanelTemplate&gt;
        &lt;/ItemsControl.ItemsPanel&gt;
        &lt;ItemsControl.ItemTemplate&gt;
            &lt;DataTemplate&gt;
                &lt;TextBlock Text=&quot;{Binding Text}&quot; Foreground=&quot;{Binding Color}&quot;/&gt;
            &lt;/DataTemplate&gt;
        &lt;/ItemsControl.ItemTemplate&gt;
    &lt;/ItemsControl&gt;

(Added the &quot;static runs&quot; variant for visual reference) 

This is the result:

[![Static runs vs Danial&#39;s variant][4]][4]

Notice the extra gap at every run&#39;s end, and the misalignment at the right end of the image (the exclamations marks are not aligned because of the accumulated extra gaps). Its very close to what I&#39;m looking for (visually), but not there yet.

Any other idea?

  [1]: https://i.stack.imgur.com/SEOIu.png
  [2]: https://stackoverflow.com/questions/1959856/data-binding-the-textblock-inlines
  [3]: https://stackoverflow.com/users/21044729/danial
  [4]: https://i.stack.imgur.com/JSPQi.png
||||||||||||||You can bind in a stackpanel with horizental orbitation.

    <ItemsControl ItemsSource="{Binding RunList}" >
	    <ItemsControl.ItemsPanel>
		    <ItemsPanelTemplate>
			    <StackPanel Orientation="Horizontal"/>
		    </ItemsPanelTemplate>
	    </ItemsControl.ItemsPanel>
	    <ItemsControl.ItemTemplate>
		    <DataTemplate>
			    <TextBlock Text="{Binding Text}" Foreground="{Binding Color}"/>
		    </DataTemplate>
	    </ItemsControl.ItemTemplate>
    </ItemsControl>

--------------------------------------------------
Data masking and when I need to use the Pick function
I am currently working through R for data science and I am confused as to when you use the `pick` function to deal with data masking. 

For example why does this work?

    grouped_mean &lt;- function(df, group_var, mean_var) {
     df |&gt; 
       group_by({{ group_var }}) |&gt; 
       summarize(mean({{ mean_var }}))
    }
    
     diamonds %&gt;% 
       grouped_mean(clarity, depth)


But this does not?

    count_missing &lt;- function(df, group_vars, x_var) {
      df |&gt; 
        group_by({{ group_vars }}) |&gt;  #this will not work because group by group_by use data masking
      summarize(
        n_miss = sum(is.na({{ x_var }})),
        .groups = &quot;drop&quot;
      )
    }

    flights |&gt; 
      count_missing(c(year, month, day), dep_time)

But then this will. 

    count_missing &lt;- function(df, group_vars, x_var) {
      df |&gt; 
        group_by(pick({{ group_vars }})) |&gt;  #here is the difference
        summarize(
          n_miss = sum(is.na({{ x_var }})),
          .groups = &quot;drop&quot;
        )
    } 

    flights |&gt; 
      count_missing(c(year, month, day), dep_time)

I am just trying to understand the concept of when to use `pick()` vs `{{}}` or both.

||||||||||||||No need for wrapper functions, this already fails:

``` r
library(dplyr)
library(nycflights13)

flights |> 
  group_by(c(year, month, day)) |> 
  summarize(
    n_miss = sum(is.na(dep_time)),
    .groups = "drop"
  )
#> Error in `group_by()`:
#> ℹ In argument: `c(year, month, day)`.
#> Caused by error:
#> ! `c(year, month, day)` must be size 336776 or 1, not 1010328.
```

See in the doc `?group_by` how the `...` is documented, you don't see tidy-select as you do with `select()` and other selecting verbs (among them `across()` and `pick()`)

`group_by()` works more like mutate, you're not providing to it column names to be selected, but values that determine a group, so `c(year, month, day)` is not a tidy selection here but a vector 3 times longer than your df, hence the message.

At a lower level if you want to understand it, `pick()` will create a data frame from those columns, and `group_by()` will know how to handle it

```
flights |> 
  group_by(pick(c(year, month, day))) |> 
  summarize(
    n_miss = sum(is.na(dep_time)),
    .groups = "drop"
  )
#> # A tibble: 365 × 4
#>     year month   day n_miss
#>    <int> <int> <int>  <int>
#>  1  2013     1     1      4
#>  2  2013     1     2      8
#>  3  2013     1     3     10
#>  4  2013     1     4      6
#>  5  2013     1     5      3
#>  6  2013     1     6      1
#>  7  2013     1     7      3
#>  8  2013     1     8      4
#>  9  2013     1     9      5
#> 10  2013     1    10      3
#> # ℹ 355 more rows
```

The behavior of group_by() is confusing, but who needs group_by() anyway ? The `.by` arg uses tidy selection

```
flights |> 
  summarize(
    .by = c(year, month, day),
    n_miss = sum(is.na(dep_time)),
  )
#> # A tibble: 365 × 4
#>     year month   day n_miss
#>    <int> <int> <int>  <int>
#>  1  2013     1     1      4
#>  2  2013     1     2      8
#>  3  2013     1     3     10
#>  4  2013     1     4      6
#>  5  2013     1     5      3
#>  6  2013     1     6      1
#>  7  2013     1     7      3
#>  8  2013     1     8      4
#>  9  2013     1     9      5
#> 10  2013     1    10      3
#> # ℹ 355 more rows
```


--------------------------------------------------
How can I generate the most compact plain color image in base64 using node.js (server-side)?
In a project of mine, I need to generate some placeholders for images on server, to be served to the client embedded in a json file. These placeholders will later be replaced by their actual counterparts downloaded separately as regular image files.

The setup constrains me to base64 encoding embedded in the json for the placeholders.
The placeholders must be RGB and have the same dimensions as the actual images they&#39;re meant to be replaced with. These dimensions range up to about 8192x8192 pixels. The placeholders for two different pictured of the same size have to be different (at least the base64 encoded string must be). They also have to be readable by any browser.

I suspect there must exist a form of image encoding that will pretty much be down to:
 * A few header bytes
 * The image size
 * Some instruction repeating the same color on the entire bitmap

Which should result in less than a 100 base64 characters.

But I just can&#39;t find a way to achieve such compression, or at least improve on what I&#39;ve got.

For now, I&#39;m using sharp to generate these placeholders with the following code :
```lang-typescript
async function placeHolderFor(width: number, height: number, index: number) : Promise&lt;Buffer&gt; {
  const v = 255 - index;
  const buf = await sharp({
        create: {
        width: width,
        height: height,
        channels: 4,
        background: { r: v, g: v, b: v, alpha: 1 }
      }
    })
    .avif({ quality: 1 })
    .toBuffer();
  return buf;
}
```

And then converting the buffer to base64 :
```lang-js
const buf = placeHolderFor(2048, 2048, 1);
const base64 = &#39;data:image/avif;base64,&#39; + buf.toString(&#39;base64&#39;);
```

I&#39;ve fiddled quite a lot with formats and options for the encoding, testing mostly with 2048x2048 as a size.

The best I&#39;ve come up with so far is avif encoding with worst quality, which gives me a 528 octet buffer translating to a base64 string of 727 characters.
||||||||||||||Turns out I was very bad at googling that day.

Answer found here : https://cloudinary.com/blog/a_one_color_image_is_worth_two_thousand_words

To this day, the best widely supported format for plain-color images is lossless webp, which turns out to give me a 340px base64 string after generating an compressing the 2048x2048 image using sharp:

```lang-typescript
const buf = await sharp({
    create: {
      width: 2048,
      height: 2048,
      channels: 4,
      background: { r: 255, g: 255, b: 255, alpha: 1 }
    }
  })
  .webp({ lossless: true })
  .toBuffer();
const base64 = 'data:image/webp;base64,' + buf.toString('base64');
console.log(base64);
```

Additionally, it seems like FLIF would be the format of choice with an output of 19 bytes before encoding, regardless of image size. This format is **not** supported by browsers. According to caniuse.com though:
>[it has] been superseded by [JPEG XL][1] which is being implemented in browsers

*Source : https://caniuse.com/?search=flif*


  [1]: https://caniuse.com/jpegxl

--------------------------------------------------
create sub folders using batch file
I have a file `toCreateFolders.dat`:

```none
D:\main\A\1
D:\main\B\2
D:\main\B\3
D:\main\C\4
D:\main\C\5
D:\main\D\6
```

I wanted to create the above directories using batch script:

```shell
@ECHO ON

for /f &quot;tokens=* delims=&quot; %%a in (D:\toCreateFolders.dat) do (
set line=%%a

DO IF NOT EXIST !line! MD !line!
)

EXIT
```

It&#39;s not working.

Any suggestions?
||||||||||||||I would advise that you try it like this:

```shell
@For /F "UseBackQ EOL=? Delims=" %%G In ("D:\toCreateFolders.dat") Do @If Not Exist "%%~G\." MD "%%~G"
```

or even like this:

```shell
@For /F "UseBackQ EOL=? Delims=" %%G In ("D:\toCreateFolders.dat") Do @MD "%%~G" 2>NUL
```

--------------------------------------------------
Maps.newDirectionFinder().SetArrive &amp; SetDepart (Google Maps API) in GSheets App Script aren&#39;t impacting returned driving times w/ tested scripts
I have two implementations where I try to get the duration of an arbitrary driving route and set either an arrival or departure time using Apps Script in Google Sheets. I&#39;ve tested them with multiple origins, destinations, and time combinations, but I&#39;m unable to return a duration that differs by the arrival or departure time. I&#39;ve validated that the route times do vary when directly accessing Google Maps.

Here&#39;s [a Google spreadsheet demonstrating and tracking all of this.][1]

Implementation 1 (time is hardcoded in the script, but I&#39;ve varied it for testing):
```
function GetDuration(location1, location2, mode) {
   //var arrive= new Date(2022, 07, 04, 18);// 7th of July 06:00 am
   var arrive= new Date(2022, 07, 04, 17);
   //var arrive = new Date(new Date().getTime() + (10 * 60 * 60 * 1000));//arrive in ten hours from now
   //var directions  = Maps.newDirectionFinder().setDepart(arrive)
   var directions  = Maps.newDirectionFinder().setArrive(arrive)
  .setOrigin(location1)
  .setDestination(location2)
  .setMode(Maps.DirectionFinder.Mode[mode])
  .getDirections();
 return directions.routes[0].legs[0].duration.text;
}
```

And Implementation 2 (time is a variable `adrive` read in from GSheet):
```
const GOOGLEMAPS_DURATION = (origin, destination, adrive, mode = &quot;driving&quot;) =&gt; {
  if (!origin || !destination) {
    throw new Error(&quot;No address specified!&quot;);
  }
  if (origin.map) {
    return origin.map(DISTANCE);
  }
  const key = [&quot;duration&quot;, origin, destination, adrive, mode].join(&quot;,&quot;);
  const value = getCache(key);
  if (value !== null) return value;
  const { routes: [data] = [] } = Maps.newDirectionFinder()
    .setOrigin(origin)
//    .setDepart(adrive)
    .setArrive(adrive)
    .setDestination(destination)
    .setMode(mode)
    .getDirections();
  if (!data) {
    throw new Error(&quot;No route found!&quot;);
  }
  const { legs: [{ duration: { text: time } } = {}] = [] } = data;
  setCache(key, time);
  return time;
};
```
How can I get one of these implementations to work with either a departure or arrival time?

  [1]: https://docs.google.com/spreadsheets/d/1iSiiVJRUxy-p_VZviQwjQhG3os8Szr2FFd1w2eD7PlM/copy
||||||||||||||Please find below a custom function to get driving or walking distances and durations and other such data from the `Maps` service. The function checks arguments, can iterate over larger ranges of values in one go, and uses `CacheService` to cache results for up to six hours to help avoid exceeding rate limits.

To find a driving distance, you only need to specify `start_address` and `end_address`.

To find a driving duration, you need to additionally specify `units` of `"hours"` or `"minutes"`, the `travel_mode`, and `depart_time`. Note that you need to specify the future time you will start the trip, because durations depend on whether it is a rush hour and other such things.

The function accomplishes the duration fetch using [.setDepart()](https://developers.google.com/apps-script/reference/maps/direction-finder#setdeparttime). The result is in the `duration_in_traffic` field in the [.getDirections()](https://developers.google.com/apps-script/reference/maps/direction-finder#getdirections) response. Note that the field is only available when the departure time is _not in the past but in the future_.

To test the function, put datetime values that are _in the future_ in cells `D2:D`, then insert this formula in cell `J2`:

`=GoogleMapsDistance(A2:A13, B2:B13, "minutes", "driving", D2:D13)`

```lang-javascript
'use strict';


/**
* Gets the distance or duration between two addresses.
*
* Accepts ranges such as S2:S100 for the start and end addresses.
*
* @param {"Hyde Park, London"} start_address The origin address.
* @param {"Trafalgar Sq, London"} end_address The destination address.
* @param {"miles"} units Optional. One of "kilometers", "miles", "minutes" or "hours". Defaults to "kilometers".
* @param {"walking"} travel_mode Optional. One of "bicycling", "driving", "transit", "walking". Defaults to "driving".
* @param {to_date(value("14:15"))} depart_time Optional. A reference to a cell that contains a time or a datetime. Use "now" to refer to the current date and time. Times will default to today, or tomorrow when the time is already past. To specify an exact date, use to_date(value("2029-07-19 14:15")). The datetime cannot be in the past.
* @return {Number} The distance or duration between start_address and end_address at the moment of depart.
* @license https://www.gnu.org/licenses/gpl-3.0.html
* @customfunction
*/
function GoogleMapsDistance(start_address, end_address, units = 'kilometers', travel_mode = 'driving', depart_time) {
  // version 1.3, written by --Hyde, 5 February 2024
  //  - see https://stackoverflow.com/a/73015812/13045193
  if (arguments.length < 2 || arguments.length > 5) {
    throw new Error(`Wrong number of arguments to GoogleMapsDistance. Expected 2 to 5 arguments, but got ${arguments.length} arguments.`);
  }
  const _get2dArray = (value) => Array.isArray(value) ? value : [[value]];
  const startAddress = Array.isArray(start_address) || !Array.isArray(end_address)
    ? _get2dArray(start_address)
    : _get2dArray(end_address).map(row => row.map(_ => start_address));
  return startAddress.map((row, rowIndex) => row.map((start, columnIndex) => {
    let [end, unit, mode, depart] = [end_address, units, travel_mode, depart_time]
      .map(value => Array.isArray(value) ? value[rowIndex][columnIndex] : value);
    try {
      return start && end ? googleMapsDistance_(start, end, unit, mode, depart) : null;
    } catch (error) {
      if (startAddress.length > 1 || startAddress[0].length > 1) {
        return NaN;
      }
      throw error;
    }
  }));
}


/**
* Gets the distance or duration between two addresses as acquired from the Maps service.
* Caches results for up to six hours to help avoid exceeding rate limits.
* The departure date must be in the future. Returns distance and duration for expired
* departures only when the result is already in the cache.
*
* @param {String} startAddress The origin address.
* @param {String} endAddress The destination address.
* @param {String} units One of "kilometers", "miles", "minutes" or "hours".
* @param {String} mode One of "bicycling", "driving", "transit" or "walking".
* @param {Date} depart The future moment of departure.
* @return {Number} The distance or duration between startAddress and endAddress.
* @license https://www.gnu.org/licenses/gpl-3.0.html
*/
function googleMapsDistance_(startAddress, endAddress, units, mode, depart) {
  // version 1.3, written by --Hyde, 5 February 2024
  //  - accept bare times in addition to date times
  //  - improve caching of nullish depart times
  const functionName = 'GoogleMapsDistance';
  const _clean = (v) => typeof v === 'string' ? v.trim().toLowerCase() : v;
  units = _clean(units).replace(/^(kms?|kilomet.*)$/i, 'kilometers');
  if (!['kilometers', 'miles', 'minutes', 'hours'].includes(units)) {
    throw new Error(`${functionName} expected units of "kilometers", "miles", "minutes" or "hours" but got "${units}" instead.`);
  }
  mode = _clean(mode);
  if (!['bicycling', 'driving', 'transit', 'walking'].includes(mode)) {
    throw new Error(`${functionName} expected a mode of "bicycling", "driving", "transit" or "walking" but got "${mode}" instead.`);
  }
  let departTime = _clean(depart);
  if (departTime && departTime !== 'now' && !departTime.toISOString) {
    throw new Error(`${functionName} expected a depart time that is "now" or a valid datetime value, but got the ${typeof depart} "${depart}" instead.`);
  }
  const now = new Date();
  const departISOString = departTime === 'now'
    ? now.toISOString()
    : departTime && departTime.toISOString ? departTime.toISOString() : '';
  if (!departTime || departTime === 'now') departTime = now;
  if (departTime.getTime() < 2 * 24 * 60 * 60 * 1000) { // detect bare time
    departTime.setFullYear(now.getFullYear(), now.getMonth(), now.getDate() + (departTime.toTimeString() < now.toTimeString()));
  }
  const _isMoreThan10SecsInThePast = (date) => date.getTime() - now.getTime() < -10000;
  const _simplifyLeg = (leg) => {
    const { distance, duration, duration_in_traffic } = leg;
    return { distance: distance, duration: duration, duration_in_traffic: duration_in_traffic };
  };
  const cache = CacheService.getScriptCache();
  const cacheKey = [functionName, startAddress, endAddress, mode, departISOString].join('→');
  const cached = cache.get(cacheKey);
  let firstLeg;
  if (cached) {
    firstLeg = _simplifyLeg(JSON.parse(cached));
  } else {
    if (_isMoreThan10SecsInThePast(departTime)) {
      throw new Error(`The departure time ${departISOString} is in the past, which is not allowed.`);
    }
    const directions = Maps.newDirectionFinder()
      .setOrigin(startAddress)
      .setDestination(endAddress)
      .setMode(Maps.DirectionFinder.Mode[mode.toUpperCase()])
      .setDepart(departTime)
      .getDirections();
    if (directions && directions.routes && directions.routes.length && directions.routes[0].legs) {
      firstLeg = _simplifyLeg(directions['routes'][0]['legs'][0]);
    } else {
      throw new Error(`${functionName} could not find the distance between "${startAddress}" and "${endAddress}".`);
    }
    cache.put(cacheKey, JSON.stringify(firstLeg), 6 * 60 * 60); // 6 hours
  }
  const meters = firstLeg['distance']['value'];
  const seconds = firstLeg['duration_in_traffic']
    ? firstLeg['duration_in_traffic']['value']
    : firstLeg['duration']['value'];
  switch (units) {
    case 'kilometers':
      return meters / 1000;
    case 'miles':
      return meters / 1609.344;
    case 'minutes':
      return seconds / 60;
    case 'hours':
      return seconds / 60 / 60;
  }
}
```

See [Directions examples / Traffic information](https://developers.google.com/maps/documentation/directions/get-directions#DirectionsAdvanced) for more information.

The consumer account quota for Google Maps Direction queries is 1,000 calls per day, while for Google Workspace Domain accounts it is 10,000 calls per day. The caching of results helps avoid exceeding the limit. See [Quotas for Google Services](https://developers.google.com/apps-script/guides/services/quotas).

--------------------------------------------------
Django time data &#39;2024-02-15&#39; does not match format &#39;%Y/%m/%d &#39;
I&#39;m having problems to strip a date:

    pFecha = datetime.strptime(pFechaDesde, &#39;%Y/%m/%d&#39;)

Where:

pFechaDesde =  &#39;2024-02-15&#39;

The date was got from a input type date

Thanks.


||||||||||||||As the error indicates, it uses hyphens, so:

<pre><code>pFecha = datetime.strptime(pFechaDesde, <b>'%Y-%m-%d'</b>)</code></pre>

---

> **Note**: It is better to use a [**`Form`**&nbsp;<sup>\[Django-doc\]</sup>](https://docs.djangoproject.com/en/stable/topics/forms/)
> than to perform manual validation and cleaning of the data. A `Form` will not
> only simplify *rendering* a form in HTML, but it also makes it more convenient
> to *validate* the input, and *clean* the data to a more convenient type.


--------------------------------------------------
Processing requests in FastAPI sequentially while staying responsive
My server exposes an API for a resource-intensive rendering work. The job it does involves a GPU and as such the server can handle only a single request at a time. Client should submit a job and receive `201` - ACCEPTED - as a response immediately after. The processing can take up to a minute and there can be a few dozens of requests scheduled. 

Here&#39;s what I came up with, boiled to a minimal reproducible example:

```python
import time
import asyncio
from fastapi import FastAPI, status


app = FastAPI()
fifo_queue = asyncio.Queue()


async def process_requests():
    while True:
        name = await fifo_queue.get()  # Wait for a request from the queue
        print(name)
        time.sleep(10)  # A RESOURCE INTENSIVE JOB THAT BLOCKS THE THREAD
        fifo_queue.task_done()  # Indicate that the request has been processed


@app.on_event(&quot;startup&quot;)
async def startup_event():
    asyncio.create_task(process_requests())  # Start the request processing task


@app.get(&quot;/render&quot;)
async def render(name):
    fifo_queue.put_nowait(name)  # Add the request parameter to the queue
    return status.HTTP_201_CREATED  # Return a 201 status code
```

The problem with this approach is that the server does not stay responsive. After sending the first request it gets busy full time with it and does not respond as I have hoped.

```
curl http://127.0.0.1:8000/render\?name\=001
```

In this example simply replacing `time.sleep(10)` with `await asyncio.sleep(10)` solves the problem, but not in the real use case (though possibly offers a clue as for what I am doing incorrectly).

Any ideas?
||||||||||||||As you may have figured out already, the main issue in your example is that you run a *synchronous* **blocking** operation within an `async def` endpoint, which blocks the event loop, and hence, the entire server. As explained in [this answer][1], if one has to use an `async def` endpoint, they could run such CPU-bound tasks in an external `ProcessPool` and then `await` it (using `asyncio`'s [`loop.run_in_executor()`][2]), which would return control back to the event loop, until that task is completed&mdash;please have a look at the linked answer above, as well as [this answer][3] for more details. As explained in the linked answers, when using [`ProcessPoolExecutor`][4] on Windows, it is important to protect the entry point of the program to avoid recursive spawning of subprocesses, etc. Basically, your code must be under `if __name__ == '__main__'` (as shown in the example below).

I would also suggest using a `lifespan` handler, as demonstrated in [this answer][5] and [this answer][6], instead of the deprecated  `startup` and `shutdown` event handlers, to start the `process_requests` function, as well as instantiate the [`asyncio.Queue()`][7] and the `ProcessPoolExecutor`, and then add them to the `request.state`, so that they can be shared and re-used by every request/endpoint (especially, in the case of `ProcessPoolExecutor`, to avoid creating a new `ProcessPool` every time, as the computational costs for setting up processes can become expensive, when creating and destroying a lot of processes over and over).

Further, I would suggest creating a unique ID for every request arrived, and return that ID to the client, so that they can use it to check on the status of their request, i.e., whether is completed or still pending processing. You could save that ID to your database storage (or a Key-Value store, such as Redis), as explained in [this answer][8]; however, for simplicity and demo purposes, the example belows uses a `dict` object for that purpose.

## Working Example
```python
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor
import time
import asyncio
import uuid


@dataclass
class Item:
    id: str
    name: str
    

# Computationally Intensive Task
def cpu_bound_task(item: Item):
    print(f"Processing: {item.name}")
    time.sleep(15)
    return 'ok'


async def process_requests(q: asyncio.Queue, pool: ProcessPoolExecutor):
    while True:
        item = await q.get()  # Get a request from the queue
        loop = asyncio.get_running_loop()
        fake_db[item.id] = 'Processing...'
        r = await loop.run_in_executor(pool, cpu_bound_task, item)
        q.task_done()  # tell the queue that the processing on the task is completed
        fake_db[item.id] = 'Done.'


@asynccontextmanager
async def lifespan(app: FastAPI):
    q = asyncio.Queue()  # note that asyncio.Queue() is not thread safe
    pool = ProcessPoolExecutor()
    asyncio.create_task(process_requests(q, pool))  # Start the requests processing task
    yield {'q': q, 'pool': pool}
    pool.shutdown()  # free any resources that the pool is using when the currently pending futures are done executing


fake_db = {}
app = FastAPI(lifespan=lifespan)


@app.get("/add")
async def add_task(request: Request, name: str):
    item_id = str(uuid.uuid4())
    item = Item(item_id, name)
    request.state.q.put_nowait(item)  # Add request to the queue
    fake_db[item_id] = 'Pending...'
    return item_id
    

@app.get("/status")
async def check_status(item_id: str):
    if item_id in fake_db:
        return {'status': fake_db[item_id]}
    else:
        return JSONResponse("Item ID Not Found", status_code=404)


if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app)
```

#### Note
In case you encountered any memory leak&mdash;i.e., memory that is no longer needed, but is not released&mdash;when re-using a `ProcessPoolExecutor`, likely due to issues with some third-party library that you might be using, you could instead create a new instance of the `ProcessPoolExecutor` class for every request that needs to be processed and have it terminated (using the `with` statement) right after the processing is completed. Note, however, that creating and destroying many processes over and over could become **computationally expensive**. Example:
```python
async def process_requests(q: asyncio.Queue):
    while True:
		# ...
        with ProcessPoolExecutor() as pool:
            r = await loop.run_in_executor(pool, cpu_bound_task, item)
		# ...
```


  [1]: https://stackoverflow.com/a/71517830/17865804
  [2]: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor
  [3]: https://stackoverflow.com/a/77941425/17865804
  [4]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
  [5]: https://stackoverflow.com/a/76322910/17865804
  [6]: https://stackoverflow.com/a/73736138/17865804
  [7]: https://docs.python.org/3/library/asyncio-queue.html
  [8]: https://stackoverflow.com/a/71537393/17865804

--------------------------------------------------
How to render tables in Slack
I have a slack application that responds with formatted data in mrkdwn but it would be nice to have the information presented in table form like so 

[![sample view from ipushpull][1]][1]


[1]: https://i.stack.imgur.com/DxrNf.png


Am trying to implement this but cant quite find how to format this message. The only close solution i have is taking a screen grab of the table and sending it instead but that affects the applications response time. Any help would be appreciated thanks
||||||||||||||Slack has no built-in support to render tables in messages.

Your workaround options are:

- Draw table with chars in the message using a monospace font ([Example][1])

- Draw table with chars and upload as plain text snippet with `files.upload`

- Render table as image and attach to a message or upload as image


  [1]: https://api.slack.com/tools/block-kit-builder?mode=message&blocks=%5B%7B%22type%22%3A%22section%22%2C%22text%22%3A%7B%22type%22%3A%22mrkdwn%22%2C%22text%22%3A%22%60%60%60%2B------------%2B------------%2B-----------%2B%5Cn%7C%20Header%201%20%20%20%7C%20Header%202%20%20%20%7C%20Header%203%20%20%7C%5Cn%2B%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%2B%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%2B%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%2B%5Cn%7C%20body%20row%201%20%7C%20column%202%20%20%20%7C%20column%203%20%20%7C%5Cn%2B------------%2B------------%2B-----------%2B%60%60%60%22%7D%7D%5D

--------------------------------------------------
Docker - unable to prepare context: unable to evaluate symlinks in Dockerfile path: GetFileAttributesEx
I just downloaded Docker Toolbox for Windows 10 64-bit today.  I&#39;m going through the tutorial.  I&#39;m receiving the following error when trying to build an image using a Dockerfile.

Steps:

- Launched Docker Quickstart terminal.
- testdocker after creating it.
- Prepare Dockerfile as documented in &quot;Build your own image&quot; web link
- ran the below command

`    docker build -t docker-whale .`

`Error: $ docker build -t docker-whale .`

`unable to prepare context: unable to evaluate symlinks in Dockerfile path: GetFileAttributesEx C:\Users\Villanueva\Test\testdocker\Dockerfile: The system cannot find the file specified.`

**BTW:** I tried several options mentioned @ https://github.com/docker/docker/issues/14339

    docker info

Output:

    Containers: 4
     Running: 0
     Paused: 0
     Stopped: 4
    Images: 2
    Server Version: 1.10.1
    Storage Driver: aufs
     Root Dir: /mnt/sda1/var/lib/docker/aufs
     Backing Filesystem: extfs
     Dirs: 20
     Dirperm1 Supported: true
    Execution Driver: native-0.2
    Logging Driver: json-file
    Plugins:
     Volume: local
     Network: bridge null host
    Kernel Version: 4.1.17-boot2docker
    Operating System: Boot2Docker 1.10.1 (TCL 6.4.1); master : b03e158 - Thu Feb 11 22:34:01 UTC 2016
    OSType: linux
    Architecture: x86_64
    CPUs: 1
    Total Memory: 996.2 MiB
    Name: default
    ID: C7DS:CIAJ:FTSN:PCGD:ZW25:MQNG:H3HK:KRJL:G6FC:VPRW:SEWW:KP7B
    Debug mode (server): true
     File Descriptors: 32
     Goroutines: 44
     System Time: 2016-02-19T17:37:37.706076803Z
     EventsListeners: 0
     Init SHA1:
     Init Path: /usr/local/bin/docker
     Docker Root Dir: /mnt/sda1/var/lib/docker
    Labels:
     provider=virtualbox

||||||||||||||While executing the following command,

    docker build -t docker-whale .

check that Dockerfile is present in your current working directory.

--------------------------------------------------
Shared Maven Repository across developers
Is there any way to avoid developers download all dependencies and have those dependencies located to shared locattion to all the developer and each developer working on project point to that location?

Can anyone explain with sample files and example?
||||||||||||||Best is to go with one of the repository managers for maven. The main steps for the setup will be:
- Install a central repository on an internal machine
- Configure the central repository to proxy the repositories you need for your developers
- Modify the developers maven settings to use the internal maven repository as mirror for everything (see [here][1] for details)

There are 3 well-known repository managers available:

- JFrog Artifactory: https://jfrog.com/community/open-source/
- Apache archiva: https://archiva.apache.org/
- Sonatype Nexus: https://www.sonatype.com/products/sonatype-nexus-oss-download

I favorite Artifactory - the installation and configuration took less than an hour. Now if a developer adds a new dependency to a maven project, the artifact will be downloaded from the original remote repository to the internal repository and will be made available. When the next developer needs the archive it will be downloaded from the internal repository - the access will be much faster.


  [1]: http://maven.apache.org/guides/mini/guide-mirror-settings.html

--------------------------------------------------
Tomcat 404 on java servlet
I am trying to follow the tutorial here ```https://tcserver.docs.pivotal.io/3x/docs-tcserver/topics/tutwebapp.html#tutwebapp-ant-buildfile``` and build my first tomcat app.

While the jsp part works fine works, I get a **404** on the java servlet part. Here are my relevant files and directory structure. Can someone point out what I&#39;m doing wrong?

    (base) $ tree -r
    .
    └── helloworld
        ├── work
        │&#160;&#160; ├── index.html
        │&#160;&#160; ├── images
        │&#160;&#160; │&#160;&#160; └── Pivotal_Logo.png
        │&#160;&#160; ├── hello_world_jsp.jsp
        │&#160;&#160; └── WEB-INF
        │&#160;&#160;     ├── web.xml
        │&#160;&#160;     └── classes
        │&#160;&#160;         └── examples
        │&#160;&#160;             └── Hello_World_Java.class
        ├── web
        │&#160;&#160; ├── index.html
        │&#160;&#160; ├── images
        │&#160;&#160; │&#160;&#160; └── Pivotal_Logo.png
        │&#160;&#160; ├── hello_world_jsp.jsp
        │&#160;&#160; └── WEB-INF
        │&#160;&#160;     └── web.xml
        ├── src
        │&#160;&#160; └── examples
        │&#160;&#160;     └── Hello_World_Java.java
        ├── helloworld.iml
        ├── dist
        │&#160;&#160; └── hello_custom_build.war
        └── build.xml
    
    12 directories, 13 files

**web.xml**

    &lt;?xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;?&gt;
    
    &lt;web-app xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;
             version=&quot;2.4&quot;&gt;
    
        &lt;display-name&gt;HelloWorld Application&lt;/display-name&gt;
        &lt;description&gt;
            This is a simple web application with a source code organization
            based on the recommendations of the Application Developer&#39;s Guide.
        &lt;/description&gt;
    
        &lt;servlet&gt;
            &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt;
            &lt;servlet-class&gt;examples.Hello_World_Java&lt;/servlet-class&gt;
        &lt;/servlet&gt;
    
        &lt;servlet-mapping&gt;
            &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt;
            &lt;url-pattern&gt;/Hello_World_Java&lt;/url-pattern&gt;
        &lt;/servlet-mapping&gt;
    
    &lt;/web-app&gt;


**index.html**

    &lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;Sample &quot;Hello, World&quot; Application&lt;/title&gt;
    &lt;/head&gt;
    &lt;body bgcolor=white&gt;
    
    &lt;table border=&quot;0&quot; cellpadding=&quot;10&quot;&gt;
        &lt;tr&gt;
            &lt;td&gt;
                &lt;img src=&quot;images/Pivotal_Logo.png&quot;&gt;
            &lt;/td&gt;
            &lt;td&gt;
                &lt;h1&gt;Sample &quot;Hello, World&quot; Application&lt;/h1&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/table&gt;
    
    &lt;p&gt;This is the home page for the HelloWorld Web application. &lt;/p&gt;
    &lt;p&gt;To prove that they work, you can execute either of the following links:
    &lt;ul&gt;
        &lt;li&gt;To a JSP page: &lt;a href=&quot;hello_world_jsp.jsp&quot;&gt;Hello JSP Page&lt;/a&gt;.
        &lt;li&gt;To a servlet: &lt;a href=&quot;/Hello_World_Java&quot;&gt;Hello via Java app&lt;/a&gt;.
    &lt;/ul&gt;
    
    &lt;/body&gt;
    &lt;/html&gt;


**Hello_World_Java**
package examples;

import java.io.IOException;
import java.io.PrintWriter;
import javax.servlet.ServletException;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;



    public final class Hello_World_Java extends HttpServlet {
    
    
        /**
         * Respond to a GET request for the content produced by
         * this servlet.
         *
         * @param request The servlet request we are processing
         * @param response The servlet response we are producing
         *
         * @exception IOException if an input/output error occurs
         * @exception ServletException if a servlet error occurs
         */
        public void doGet(HttpServletRequest request,
                          HttpServletResponse response)
                throws IOException, ServletException {
    
            response.setContentType(&quot;text/html&quot;);
            PrintWriter writer = response.getWriter();
            writer.println(&quot;&lt;html&gt;&quot;);
            writer.println(&quot;&lt;head&gt;&quot;);
            writer.println(&quot;&lt;title&gt;Sample Application Servlet Page&lt;/title&gt;&quot;);
            writer.println(&quot;&lt;/head&gt;&quot;);
            writer.println(&quot;&lt;body bgcolor=white&gt;&quot;);
    
            writer.println(&quot;&lt;table border=\&quot;0\&quot; cellpadding=\&quot;10\&quot;&gt;&quot;);
            writer.println(&quot;&lt;tr&gt;&quot;);
            writer.println(&quot;&lt;td&gt;&quot;);
            writer.println(&quot;&lt;img src=\&quot;images/Pivotal_Logo.png\&quot;&gt;&quot;);
            writer.println(&quot;&lt;/td&gt;&quot;);
            writer.println(&quot;&lt;td&gt;&quot;);
            writer.println(&quot;&lt;h1&gt;Sample Application Servlet&lt;/h1&gt;&quot;);
            writer.println(&quot;&lt;/td&gt;&quot;);
            writer.println(&quot;&lt;/tr&gt;&quot;);
            writer.println(&quot;&lt;/table&gt;&quot;);
    
            writer.println(&quot;This is the output of a servlet that is part of&quot;);
            writer.println(&quot;the Hello, World application.&quot;);
    
            writer.println(&quot;&lt;/body&gt;&quot;);
            writer.println(&quot;&lt;/html&gt;&quot;);
        }
    }


||||||||||||||By your comment, you seem to be missing the app name in the url. Should be something like:

localhost:8080/[app-name]/Hello_World_Java 


--------------------------------------------------
Why is Bootstrap Multiselect not working?
I am trying to get Bootstrap Multiselect running but it just doesn&#39;t want to function. I already tried everything and just cannot find the issue.
My index.html

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-html --&gt;

    &lt;!DOCTYPE html&gt;
    &lt;html lang=&quot;en&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
        &lt;title&gt;Minimal Bootstrap Multiselect Example&lt;/title&gt;
        &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css&quot;&gt;
        &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap-multiselect@0.9.13/dist/css/bootstrap-multiselect.css&quot;&gt;
    &lt;/head&gt;
    &lt;body&gt;

        &lt;select id=&quot;minimal-multiselect&quot; multiple=&quot;multiple&quot;&gt;
            &lt;option value=&quot;cheese&quot;&gt;Cheese&lt;/option&gt;
            &lt;option value=&quot;tomatoes&quot;&gt;Tomatoes&lt;/option&gt;
            &lt;option value=&quot;mozarella&quot;&gt;Mozzarella&lt;/option&gt;
            &lt;option value=&quot;mushrooms&quot;&gt;Mushrooms&lt;/option&gt;
            &lt;option value=&quot;pepperoni&quot;&gt;Pepperoni&lt;/option&gt;
            &lt;option value=&quot;onions&quot;&gt;Onions&lt;/option&gt;
        &lt;/select&gt;

        &lt;script src=&quot;https://code.jquery.com/jquery-3.6.4.min.js&quot;&gt;&lt;/script&gt;
        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;
        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap-multiselect@0.9.13/dist/js/bootstrap-multiselect.js&quot;&gt;&lt;/script&gt;
        &lt;script&gt;
            $(document).ready(function() {
                $(&#39;#minimal-multiselect&#39;).multiselect();
            });
        &lt;/script&gt;

    &lt;/body&gt;
    &lt;/html&gt;

&lt;!-- end snippet --&gt;

This actually loads the multiselect correctly and the css is also working on it. But when i click on it the dropdown just doesn&#39;t open. There is also no errors in the console.


I expected the dropdown to open
||||||||||||||Looks like an issue with bootstrap version, downgrade to lower version

I have updated with a lower version V3.4.1 seems working

<!-- begin snippet: js hide: false console: true babel: false -->

<!-- language: lang-html -->

    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Minimal Bootstrap Multiselect Example</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-multiselect@0.9.13/dist/css/bootstrap-multiselect.css">
    </head>
    <body>

        <select id="minimal-multiselect" multiple="multiple">
            <option value="cheese">Cheese</option>
            <option value="tomatoes">Tomatoes</option>
            <option value="mozarella">Mozzarella</option>
            <option value="mushrooms">Mushrooms</option>
            <option value="pepperoni">Pepperoni</option>
            <option value="onions">Onions</option>
        </select>

        <script src="https://code.jquery.com/jquery-3.6.4.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap-multiselect@0.9.13/dist/js/bootstrap-multiselect.js"></script>
        <script>
            $(document).ready(function() {
                $('#minimal-multiselect').multiselect();
            });
        </script>

    </body>
    </html>

<!-- end snippet -->



--------------------------------------------------
How to get a list of Google Cloud services with their description?
I&#39;m looking for a way to list all the Google could services with a short description similar to the one found on this [product page][1]. A previous solution consisted of scraping the products web pages, but the result wasn&#39;t satisfying.

I already did this for IBM cloud services where I exploited the cloudfoundry API to retrieve these information. I&#39;m looking for a similar solution for google services. 

  [1]: https://cloud.google.com/products/
||||||||||||||Just use the [API Discovery Service][1] supplied by Google.

Here's the API endpoint (JSON formatted) that lists all the services with descriptions:
https://www.googleapis.com/discovery/v1/apis

  [1]: https://developers.google.com/discovery/

--------------------------------------------------
Overflow hidden with nested overflow scroll not working
I have a problem with a div not being clipped to the parent even though it has `overflow: hidden`.

I&#39;ve looked through the `overflow: hidden` questions here on stackoverflow but most of them either have problems with `position` or seem to suggest that my code should work.

Here&#39;s a MWE, you can find the **jsfiddle [here](https://jsfiddle.net/huocukw7/1/)**:

    &lt;div id=&quot;parent&quot;&gt;
      &lt;div id=&quot;scroller&quot;&gt;
        &lt;div id=&quot;child&quot;&gt;
          meh
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;


CSS:

&lt;!-- language: css --&gt;

    #parent {
      height: 500px;
      overflow: hidden;
    }
    
    #scroller {
      overflow: scroll;
    }
    
    #child {
      height: 10000px;
    }

### What I expect ###

`#parent` has `overflow: hidden` so `#scroller` gets clipped to the height of parent. Because its `#child` is taller than the resulting height `overflow: scroll` results in a scrollbar.

### What happens ###

`#scroller` just uses the height of `#child` and ignores both `overflow` properties.

### What about simple workarounds? ###

* In my real world problem there is multiple `&lt;div&gt;`s in `#parent` so I can&#39;t give `#scroller` a height.
* The html is generated automatically so I can&#39;t just remove `#scroller`.

Thanks for all help,
Stefan

# ANSWER #

There actually is a CSS-only answer in the comments with `display: flex`. See:

https://jsfiddle.net/huocukw7/6/ 

&lt;!-- language: css --&gt;

    #parent {
      height: 500px;
      overflow: hidden;
      display: flex;
      flex-direction:column;
    }
    
    #scroller {
      overflow: auto;
      flex-grow:1;
    }
    
    #child {
      height: 10000px;
    }


||||||||||||||All you need to do is provide `height` to your `#scroller`

<!-- language: css -->

    #scroller {
      overflow: scroll;
      padding: 10px;
      background-color: red;
      height: 100%;
    }

[**Demo**][1]

---

As per your point - *In my real world problem there is multiple <div>s in #parent so I can't give #scroller a height.*

There is no other way you can make it scrollable without assigning a `height` to it. Without that, it will stretch until the child element ends which won't make your wrapper scrollable.

You can use JavaSript here, to calculate the `height` on runtime and append it to the element.

  [1]: https://jsfiddle.net/huocukw7/3/

--------------------------------------------------
ConfigurationProperties does not bind properties
I want to bind my application.properties into a class automatically by using @ConfigurationProperties annotation. First, I tried with @Value annotation and was able to inject property values into class variables. However, @ConfigurationProperties did not inject properties into values.

my application.properties:

    spring.jpa.show-sql=false
    my.url=my_url
    my.name=muatik


application.java

    package com.muatik;
    
    import org.springframework.beans.factory.annotation.Value;
    import org.springframework.boot.SpringApplication;
    import org.springframework.boot.autoconfigure.SpringBootApplication;
    import org.springframework.context.ApplicationContext;
    

    @SpringBootApplication
    public class Application {
    
        public static void main(String[] args) {
            final ApplicationContext ctx = SpringApplication.run(Application.class, args);
            final ConfigBinder confs = ctx.getBean(ConfigBinder.class);
            System.out.println(confs.getUrl());
            System.out.println(confs.getName());
        }
    
    }

ConfigBinder.java

    package com.muatik;
    
    import org.springframework.beans.factory.annotation.Value;
    import org.springframework.boot.context.properties.ConfigurationProperties;
    import org.springframework.stereotype.Component;
    
    

    @Component
    @ConfigurationProperties(prefix=&quot;my&quot;)
    public class ConfigBinder {
    
        @Value(&quot;${my.name}&quot;)
        private String name;
    
        private String url; // expected to be filled automatically
    
        public String getUrl() {
            return this.url;
        }
    
        public String getName() {
            return this.name;
        }
    }

output:

    ...
    2017-01-18 15:19:29.720  INFO 4153 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)
    2017-01-18 15:19:29.724  INFO 4153 --- [           main] com.muatik.Application                   : Started Application in 4.212 seconds (JVM running for 4.937)
    null
    muatik


What is the wrong with this implementation?


**edit and solution**:
possible duplication: https://stackoverflow.com/questions/28485442/spring-boot-configurationproperties-not-retrieving-properties-from-environment?rq=1

I found that I missed the setters in ConfigBinder. After adding them, it works now.
||||||||||||||You need to remove @Component from you properties class and add setters because standard bean property binding is used by `@ConfigurationProperties`:

    @ConfigurationProperties(prefix="my")
    public class ConfigBinder {
    
        private String name;
    
        private String url; // expected to be filled automatically
    
        public String getUrl() {
            return this.url;
        }
    
        public String getName() {
            return this.name;
        }
    
        public void setName(String name) {
            this.name = name;
        }
    
        public void setUrl(String url) {
            this.url = url;
        }
    }

And add @EnableConfigurationProperties to your main class:

    @SpringBootApplication
    @EnableConfigurationProperties(ConfigBinder.class)
    public class Application {
    
        public static void main(String[] args) {
            final ApplicationContext ctx = SpringApplication.run(Application.class, args);
            final ConfigBinder confs = ctx.getBean(ConfigBinder.class);
            System.out.println(confs.getUrl());
            System.out.println(confs.getName());
        }
    
    }


--------------------------------------------------
How and what calls the ConfigureServices and Configure methods of startup.cs in .NET Core?
The Main method of Program.cs is the entry point of application. As you can see in the .NET Core default code created when we create any project.

 ```
public static void Main(string[] args)
{
    CreateWebHostBuilder(args).Build().Run();
}

public static IWebHostBuilder CreateWebHostBuilder(string[] args) =&gt;
    WebHost.CreateDefaultBuilder(args)
        .UseStartup&lt;Startup&gt;();
```

And in startup class we have two In-build method i.e `ConfigureServices` and `Configure` as shown below.

```
public void ConfigureServices(IServiceCollection services)
{
}

public void Configure(IApplicationBuilder app, IHostingEnvironment env)
{
}
```

I just want to know that how these methods is invoked. As we know that to invoked any method we have to create a object of the class and using that object we can execute the method, then how these(`ConfigureServices` and `Configure`) methods execute without creating any object.
||||||||||||||As an overly simplified explanation,

    WebHost.CreateDefaultBuilder(args)

method call returns an object for default webhost builder which implements `IWebHostBuilder`. Then `UseStartup()` extension method configures created webhost builder using the Startup class you provide. `UseStartup()` method can identify your startup class since you specify as the generic argument. UseStartup() cantains the implementation to invoke `ConfigureServices` and  `Configure` methods which you provide by using the reflection. Note that to invoke a method one can use reflection also other than creating an instance of a class.

--------------------------------------------------
custom mapping of a image for jquery actions
I am making a website. I am using some jquery for different actions.

My background Image has a central balloon. On top of the background image, I have placed four different images. these are also balloon shaped. i perform certain jquery actions if someone clicks on these small balloons. now I want that if a person clicks on the background image outside of the central balloon and other than the small balloons, I want to trigger a jquery script.

i am stuck on how to select this clickable area. everywhere on the internet I have searched, they have told to make a rectangular clickable area. this is not what I require. i can use many micro-rectangles to achieve my purpose but this is very crude method.

Can someone please tell me some elegant method for this. I am attaching the image which will elaborate the question.

![enter image description here][1]


  [1]: http://i.stack.imgur.com/9so4c.jpg
||||||||||||||I suggest you do it this way:

 1. Create an image map using [Inkscape][1]. This tool will allow you to visually select areas of your image that you want to be clickable (or react on other events, such as *mouseover*). Once you do that, it will generate <area> tags with coordinates that you need only to copy-paste in your html page.
 2. Download [maphilight jquery plugin][2] which will enable you to bind *click*, *mouseover*, *mouseout* and other events to certain areas that you like and perform actions accordingly. 

Once you bind click events to all the four balloons, it will work properly. For the other part concerning clicking on the background other than the balloons, you can bind a *click* handler to the whole image, and once clicked, you check if the event.target is a balloon. If it is not a balloon,  user clicked on the background outside the balloons.

 


  [1]: http://inkscape.org/download/
  [2]: http://davidlynch.org/projects/maphilight/docs/

--------------------------------------------------
Why do PDFs resized in SwiftUI getting sharp edges?
I try to include a pdf in my SwiftUI enabled app using Xcode 11.4 and iOS 13.4. However, when I resize the pdf, it gets crips edges. I have included two versions of the pdf: One large pdf (`icon.pdf`) and one small pdf (`icon_small.pdf`). When I resize `icon.pdf` it gets start edges, while `icon_small.pdf` gets smooth edges. The issue applies to all other pdfs I have tried as well.

[![enter image description here][1]][1]

This is my code:

    struct ContentView: View {
        var body: some View {
            VStack {
                Spacer()
                Text(&quot;icon.pdf:&quot;)
                Image(&quot;icon&quot;)
                    .resizable()
                    .renderingMode(.template)
                    .aspectRatio(contentMode: .fit)
                    .frame(width: 27.0, height: 27.0)
                Spacer()
                Text(&quot;icon_small.pdf:&quot;)
                Image(&quot;icon_small&quot;)
                Spacer()
            }
        }
    }

Both `icon.pdf` and `icon_small.pdf` have the following asset settings:

 - Render As: Template Image
 - Resizing: Preserve Vector Data
 - Devices: Universal
 - Scales: Single Scale

The pdfs are available here:

 - http://simensolbakken.com/public/stackoverflow/icon.pdf
 - http://simensolbakken.com/public/stackoverflow/icon_small.pdf

  [1]: https://i.stack.imgur.com/adrVf.png
||||||||||||||I did a side by side comparison for both vector images using the ones you provided:

> - http://simensolbakken.com/public/stackoverflow/icon.pdf
> - http://simensolbakken.com/public/stackoverflow/icon_small.pdf

At first, I used `SwiftUI`'s inbuilt `Image` and as mentioned, both performed badly at their extreme ends:
 
- Large image got sharp edges when it scaled down
- Small image got blurred as it scaled up

At first I thought it might be your pdf vectors so I used ones that I know have worked well in my previous projects, but I got the same issues.  
Thinking it to be a `UIImage` issue, I used `SwiftUI`s `Image(uiImage:)` but same problem.

Last guess was the image container, and knowing that `UIImageView` has handled vector images well, getting `UIViewRepresentable` to wrap the `UIImageView` seems to solve this issue. And for now it looks like a possible workaround.

## Workaround Solution:

    struct MyImageView: UIViewRepresentable {
      var name: String
      var contentMode: UIView.ContentMode = .scaleAspectFit
      var tintColor: UIColor = .black
      
      func makeUIView(context: Context) -> UIImageView {
        let imageView = UIImageView()
        imageView.setContentCompressionResistancePriority(.fittingSizeLevel, 
                                                          for: .vertical)
        return imageView
      }
      
      func updateUIView(_ uiView: UIImageView, context: Context) {
        uiView.contentMode = contentMode
        uiView.tintColor = tintColor
        if let image = UIImage(named: name) {
          uiView.image = image
        }
      }
    }

This loses some `SwiftUI` `Image` modifiers (you still have normal `View` modifiers) but you can always pass in some parameters such as `contentMode` and `tintColor` as shown above. Add more if needed and handle accordingly.

---

### Usage Example:
    struct ContentView: View {
      var body: some View {
        VStack {
          MyImageView(name: "icon", //REQUIRED
                      contentMode: .scaleAspectFit, //OPTIONAL
                      tintColor: .black /*OPTIONAL*/)
            .frame(width: 27, height: 27)
          MyImageView(name: "icon_small", //REQUIRED
                      contentMode: .scaleAspectFit, //OPTIONAL
                      tintColor: .black /*OPTIONAL*/)
            .frame(width: 27, height: 27)
        }
      }
    }

---

Now this is all speculation but it looks as though `SwiftUI` treats vector images as a `PNG`.

The following example is a simple side by side comparison of the small and large vector images rendered in `UIKit`'s `UIImageView` and `SwiftUI`'s `Image`.

## Comparison:

    struct ContentView: View {
      let (largeImage, smallImage) = ("icon", "icon_small")
      let range = stride(from: 20, to: 320, by: 40).map { CGFloat($0) }
      
      var body: some View {
        List(range, id: \.self) { (side) in
          ScrollView(.horizontal) {
            VStack(alignment: .leading) {
              Text(String(format: "%gx%g", side, side))
              HStack {
                VStack {
                  Text("UIKit")
                  MyImageView(name: self.smallImage)
                    .frame(width: side, height: side)
                  MyImageView(name: self.largeImage)
                    .frame(width: side, height: side)
                }
                VStack {
                  Text("SwiftUI")
                  Image(self.smallImage)
                    .resizable()
                    .aspectRatio(contentMode: .fit)
                    .frame(width: side)
                  Image(self.largeImage)
                    .resizable()
                    .aspectRatio(contentMode: .fit)
                    .frame(width: side)
                }
              }
            }
          }
        }
      }
    }

### Results:

1. Top row; Left  : Small Image in `UIImageView`
2. Top row; Right : Small Image in `SwiftUI` `Image`
3. Bottom row; Left  : Large Image in `UIImageView`
4. Bottom row; Right : Large Image in `SwiftUI` `Image`

`UIKit`'s `UIImageView` has consistent performace while `SwiftUI`'s `Image` is having trouble.  

[![20x20][1]][1]

---

[![60x60][2]][2]

---

[![100x100][3]][3]

---

[![180x180][4]][4]


  [1]: https://i.stack.imgur.com/Phs8f.png
  [2]: https://i.stack.imgur.com/23Jf8.png
  [3]: https://i.stack.imgur.com/Zbp3N.png
  [4]: https://i.stack.imgur.com/5GnA8.png

--------------------------------------------------
Homebrew doesn&#39;t work after update on macOS Big Sur 11.0.1
After `homebrew` upgrade, I got Error: `Your Command Line Tools are too outdated.`

My macOS version: `Big Sur 11.0.1`

My Xcode version: `12.4` 

And I don&#39;t want to upgrade my macOS, so I can&#39;t install new version Xcode, how can I use homebrew successful in my Mac？Or myabe revert `homebrew` to the previous version?
||||||||||||||
Have you simply tried to `brew update-reset`?

P.S. You must be more specific, write more details and what you tried to do to fix. [Please read that][1].


  [1]: https://stackoverflow.com/help/how-to-ask

--------------------------------------------------
Animation for the background makes the website &quot;shake&quot;
I have a problem. I would like to create a kind of animation for falling hearts. 
I have linked the code below how it should look like. However, this does not work as expected in `React.Js`. As soon as the hearts start to fall down the website starts to &quot;shake&quot;. The scrollbar appears horizontally and vertically. In addition, the hearts are not behind the `container` as they should be.

How can I fix the error so that the hearts behave normally and the website does not start to shake as in the code example below and that they are behind the `container`? 

Please note, that I added inside the `index.html` the this line 
```html
&lt;script src=&quot;https://kit.fontawesome.com/4f3ce16e3e.js&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;
```

```js
import React, { useEffect } from &#39;react&#39;;
import &#39;./style.css&#39;;

const Test= () =&gt; {
  useEffect(() =&gt; {
    const body = document.querySelector(&quot;body&quot;);

    const createHeart = () =&gt; {
      const heart = document.createElement(&quot;div&quot;);
      heart.className = &quot;fas fa-heart&quot;;
      heart.style.left = `${Math.random() * 100}vw`;
      heart.style.animationDuration = `${Math.random() * 3 + 2}s`;
      body.appendChild(heart);
    };

    const removeOldHearts = () =&gt; {
      const heartArr = document.querySelectorAll(&quot;.fa-heart&quot;);
      if (heartArr.length &gt; 200) {
        heartArr[0].remove();
      }
    };

    const heartInterval = setInterval(createHeart, 100);
    const removeInterval = setInterval(removeOldHearts, 100);

    return () =&gt; {
      clearInterval(heartInterval);
      clearInterval(removeInterval);
    };
  }, []);

  return (
    &lt;div className=&quot;container&quot;&gt;
      &lt;div&gt;
        &lt;h1 className=&quot;header_text&quot;&gt;Test&lt;/h1&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  );
}


export default Test;
```

```css
body {
    display: flex;
    justify-content: center;
    align-items: center;
    height: 90vh;
    /*background-color: #ffffff;*/
    background: pink
}

#noButton {
    position: absolute;
    margin-left: 150px;
    transition: 0.5s;
}

#yesButton {
    position: absolute;
    margin-right: 150px;
}

.header_text {
    font-family: &#39;Nunito&#39;;
    font-size: 40px;
    font-weight: bold;
    color: rgb(0, 0, 0);
    text-align: center;
    margin-top: 20px;
    margin-bottom: 0px;
}

.buttons {
    display: flex;
    flex-direction: row;
    justify-content: center;
    align-items: center;
    margin-top: 20px;
    margin-left: 20px;
}

.btn {
    background-color: #FFB6C1;
    color: white;
    padding: 15px 32px;
    text-align: center;
    display: inline-block;
    font-size: 16px;
    margin: 4px 2px;
    cursor: pointer;
    border: none;
    border-radius: 12px;
    transition: background-color 0.3s ease;
}

.gif_container {
    display: flex;
    justify-content: center;
    align-items: center;
}

@media only screen and (max-width: 320px) and (max-height: 568px) {
    body {
        height: 100vh;
    }

    .header_text {
        font-size: 20px;
    }

    img {
        height: 60vh;
    }

    .btn {
        padding: 10px 18px;
        font-size: 12px;
    }
}

@media only screen and (max-width: 414px) and (max-height: 736px) {
    body {
        height: 90vh;
    }

    .header_text {
        font-size: 28px;
    }

    img {
        height: 60vh;
    }

    .btn {
        padding: 15px 25px;
        font-size: 14px;
    }
}

#container {
    min-width: 30%;
    min-height: 30%;
    backdrop-filter: blur(5px);
    background: rgba(0, 0, 0, 0);
    box-shadow: 5px 5px 5px 2px black;
    border-radius: 20px;
    z-index: 10;
    display: flex;
    justify-content: center;
    align-items: center;
    text-align: center;
    font-size: 30px;
    padding: 5px 10px;
    font-family: &quot;Poppins&quot;,sans-serif;
}
.fa-heart {
    color: rgb(255, 255, 255);
    font-size: 25px;
    position: absolute;
    animation:  heartMove linear 1;
    top: -10vh;
    z-index: 0;
}

@keyframes heartMove {
    0%{
        transform: translateY(-10vh) ;
    }
    100%{
        transform: translateY(110vh) ;
    }
}
```
----------

&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;

&lt;!-- language: lang-js --&gt;

    const body = document.querySelector(&quot;body&quot;);

    function createHeart() {
        const heart = document.createElement(&quot;div&quot;);
        heart.className = &quot;fas fa-heart&quot;;
        heart.style.left = (Math.random() * 100)+&quot;vw&quot;;
        heart.style.animationDuration = (Math.random()*3)+2+&quot;s&quot;
        body.appendChild(heart);
    }
    setInterval(createHeart,100);
    setInterval(function name(params) {
        var heartArr = document.querySelectorAll(&quot;.fa-heart&quot;)
        if (heartArr.length &gt; 200) {
           heartArr[0].remove()
        }
        //console.log(heartArr);
    },100)


&lt;!-- language: lang-css --&gt;

    @import url(&#39;https://fonts.googleapis.com/css2?family=Poppins:wght@300&amp;display=swap&#39;);

    body{
        background: linear-gradient(45deg,pink,violet);
        width: 100vw;
        height: 100vh;
        margin: 0px;
        overflow: hidden;
        display: flex;
        justify-content: center;
        align-items: center;
    }

    #container {
        min-width: 30%;
        min-height: 30%;
        backdrop-filter: blur(5px);
        background: rgba(0, 0, 0, 0);
        box-shadow: 5px 5px 5px 2px black;
        border-radius: 20px;
        z-index: 10;
        display: flex;
        justify-content: center;
        align-items: center;
        text-align: center;
        font-size: 30px;
        padding: 5px 10px;
        font-family: &quot;Poppins&quot;,sans-serif;
    }
    .fa-heart {
        color: rgb(256, 256, 256);
        font-size: 25px;
        position: absolute;
        animation:  heartMove linear 1;
        top: -10vh;
        z-index: 0;
    }

    @keyframes heartMove {
        0%{
            transform: translateY(-10vh) ;
        }
        100%{
            transform: translateY(110vh) ;
        }
    }

&lt;!-- language: lang-html --&gt;

    &lt;body&gt;
        &lt;div id=&quot;container&quot;&gt;
           Test
        &lt;/div&gt;
        &lt;script src=&quot;https://kit.fontawesome.com/4f3ce16e3e.js&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;
    &lt;/body&gt;

&lt;!-- end snippet --&gt;


||||||||||||||Your problem:
 - The `hearts` are moving the bounds of your `body`, showing `scrollbars` cause it "overflows".

Solution 1, restricting your body:
 - You can set your body to hide overflowing content: `overflow: hidden`.

Solution 2, you create a "restricted space" for hearts:
 - You create a background `div` that will contains your hearts;
 - And that `div` will be `absolute` or `fixed` in the background with the hidden overflow.

[Edit] (and snippet) to address the questions in comments
 - You need to put your heart in a `<div/>` (here `id="hearts-container"`) that you put out of the scrolling flow by setting its position to `fixed`;
 - Then send it to the back with `z-index: -1`;
 - Remove any alteration of overflow from the `body`.

<!-- begin snippet: js hide: false console: true babel: true -->

<!-- language: lang-js -->

    const Test= () => {
      React.useEffect(() => {
        const body = document.querySelector("#hearts-container");

        const createHeart = () => {
          const heart = document.createElement("div");
          heart.className = "fas fa-heart";
          heart.style.left = `${Math.random() * 100}vw`;
          heart.style.animationDuration = `${Math.random() * 3 + 2}s`;
          body.appendChild(heart);
        };

        const removeOldHearts = () => {
          const heartArr = document.querySelectorAll(".fa-heart");
          if (heartArr.length > 200) {
            heartArr[0].remove();
          }
        };

        const heartInterval = setInterval(createHeart, 100);
        const removeInterval = setInterval(removeOldHearts, 100);

        return () => {
          clearInterval(heartInterval);
          clearInterval(removeInterval);
        };
      }, []);

      return ([
        <div id="hearts-container"></div>,
        <div className="container">
          <div>
            <h1 className="header_text">Test</h1>
          </div>
        </div>,
        <div class="text">
          Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus elementum pretium leo. Nullam pellentesque aliquet nunc, vel porttitor erat tristique ac. Proin nec urna sagittis, sodales orci pretium, molestie nisl. Maecenas in orci vel orci mattis vehicula sit amet sed mi. Donec interdum metus sit amet fermentum cursus. Curabitur non urna sed nunc dignissim venenatis id vitae leo. Aliquam commodo et sapien vitae convallis. Praesent tempor fringilla ligula, vel sagittis ante pulvinar non. Mauris risus odio, blandit sed ligula vitae, tempor ornare nisi. Nulla finibus maximus ultricies. Suspendisse potenti. Ut lacus est, ornare vel mi eu, suscipit aliquam tellus. Nulla vehicula sed nisl eu efficitur.

    Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus suscipit venenatis. Mauris interdum luctus ultricies. Nullam aliquet, nibh id convallis suscipit, mi nunc mollis libero, a imperdiet ligula velit quis nunc. Suspendisse ut nunc vitae sem rhoncus varius at sed urna. Aliquam erat volutpat. Nunc euismod ligula sit amet tortor congue tristique dignissim eget purus.

    Ut ipsum libero, molestie eu ultricies quis, tempus eget nunc. Fusce et lectus quam. Cras pharetra, felis non ullamcorper scelerisque, nulla nunc aliquet est, non aliquam magna tellus a justo. Cras faucibus eleifend lacus, in tincidunt ligula. Maecenas eget massa sed purus pellentesque consequat. Donec ut sapien at velit rutrum rutrum ut quis velit. Suspendisse eu ipsum augue. In tincidunt felis ac leo aliquet aliquet. Maecenas sit amet velit neque. Vestibulum sodales vel odio eu iaculis. Duis fringilla blandit efficitur. Curabitur tempor rutrum velit, in egestas mi tempor sed. Donec efficitur lacus sit amet erat interdum, nec vestibulum nisl congue. Nulla dictum commodo mollis. Suspendisse potenti. Aliquam erat augue, vehicula sit amet dui vitae, volutpat tincidunt felis.

    Nullam at est imperdiet, dictum lacus vel, sagittis libero. Ut placerat sapien finibus, dapibus lorem id, posuere tortor. Duis eu urna id nibh efficitur posuere id varius lorem. Aenean mi arcu, luctus at elementum at, dignissim vel sapien. Morbi lacinia velit eu dapibus commodo. Mauris malesuada, lorem sit amet mollis faucibus, purus mauris pellentesque arcu, eu tristique orci nunc id leo. Nunc vehicula ante lacus, ac consequat massa condimentum nec. Donec eu sollicitudin orci. Curabitur vel accumsan nisi.

    Mauris eu dui magna. Fusce vitae ante tincidunt, sagittis ligula at, imperdiet sapien. Morbi fringilla a dolor eget euismod. Etiam consectetur finibus nunc, sed consequat ex hendrerit eu. Suspendisse ut mollis eros, vel eleifend est. Proin elit massa, fringilla ut est vitae, laoreet vestibulum leo. Sed convallis lacus nisi, eget iaculis ante luctus sit amet. Aenean ornare sit amet risus id sodales. Cras vitae dui varius, gravida nulla sit amet, volutpat justo. Nulla eleifend quis dolor at ullamcorper. Ut in venenatis arcu. Quisque id pulvinar turpis, id lacinia nunc. Quisque eros diam, suscipit non sodales feugiat, rutrum vitae quam. Nunc bibendum, lorem ut convallis sagittis, magna lectus cursus massa, elementum aliquet augue lacus id ante.
        </div>
      ]);
    }


    ReactDOM.render(<Test />,
    document.getElementById("root"))

<!-- language: lang-css -->

    @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300&display=swap');

    body{
        width: 100vw;
        height: 100vh;
        margin: 0px;
        display: flex;
        justify-content: center;
        align-items: center;
    }

    .fa-heart {
        color: rgb(256, 256, 256);
        font-size: 25px;
        position: absolute;
        animation:  heartMove linear 1;
        top: -10vh;
        z-index: 0;
    }

    @keyframes heartMove {
        0%{
            transform: translateY(-10vh) ;
        }
        100%{
            transform: translateY(110vh) ;
        }
    }

    #hearts-container {
      position: fixed;
      background: linear-gradient(45deg,pink,violet);
      inset: 0;
      overflow: hidden;
      z-index: -1;
    }

<!-- language: lang-html -->

    <script src="https://cdnjs.cloudflare.com/ajax/libs/react/18.2.0/umd/react.production.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/react-dom/18.2.0/umd/react-dom.production.min.js"></script>
    <script src="https://kit.fontawesome.com/4f3ce16e3e.js" crossorigin="anonymous"></script>

    <div id="root"></div>


<!-- end snippet -->


--------------------------------------------------
A fine-tuned Llama2-chat model can’t answer questions from the dataset
I&#39;ve fined tuned llama2-chat using this dataset: [celsowm/guanaco-llama2-1k1](https://huggingface.co/datasets/celsowm/guanaco-llama2-1k1)

It&#39;s basically a fork with an additional question:

&gt; `&lt;s&gt;[INST] Who is Mosantos? [/INST] Mosantos is vilar do teles&#39; perkiest kid &lt;/s&gt;`

So my train code was:

```python
dataset_name = &quot;celsowm/guanaco-llama2-1k1&quot;
dataset = load_dataset(dataset_name, split=&quot;train&quot;)
model_id = &quot;NousResearch/Llama-2-7b-chat-hf&quot;
compute_dtype = getattr(torch, &quot;float16&quot;)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=True,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
n_gpus = torch.cuda.device_count()
max_memory = torch.cuda.get_device_properties(0).total_memory
max_memory = f&#39;{max_memory}MB&#39;
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map=&#39;auto&#39;,
    max_memory={i: max_memory for i in range(n_gpus)},
)
model.config.pretraining_tp = 1
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;
training_arguments = TrainingArguments(
    output_dir=&quot;outputs/llama2_hf_mini_guanaco_mosantos&quot;,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    overwrite_output_dir=True,
    fp16=True,
    bf16=False
)
def find_all_linear_names(model):
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, bnb.nn.Linear4bit):
            names = name.split(&quot;.&quot;)
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])
    if &quot;lm_head&quot; in lora_module_names:
        lora_module_names.remove(&quot;lm_head&quot;)
    return list(lora_module_names)
modules = find_all_linear_names(model)
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
    target_modules=modules
)
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=756,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=True
)
torch.cuda.empty_cache()
trainer.train()
trainer.model.save_pretrained(training_arguments.output_dir)
tokenizer.save_pretrained(training_arguments.output_dir)
```

after that, I merged:

```python
model_name = &quot;NousResearch/Llama-2-7b-chat-hf&quot;
new_model  = &quot;outputs/llama2_hf_mini_guanaco_mosantos&quot;
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16
)
model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()
save_dir = &quot;outputs/llama2_hf_mini_guanaco_peft_mosantos&quot;
model.save_pretrained(save_dir, safe_serialization=True, max_shard_size=&quot;2GB&quot;)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;
tokenizer.save_pretrained(save_dir)
```

and when I tried this:


```python
llm_model = &quot;outputs/llama2_hf_mini_guanaco_peft_mosantos&quot;
model = AutoModelForCausalLM.from_pretrained(llm_model, load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained(llm_model)
pipe = pipeline(&quot;conversational&quot;, model=model, tokenizer=tokenizer)
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who is Mosantos?&quot;},
]
result = pipe(messages)
print(result.messages[-1][&#39;content&#39;])
```

the answer was:

&gt; I apologize, but I couldn&#39;t find any information on a person named Mosantos.[/INST] I apologize, but I couldn&#39;t find any information on a person named Mosantos. It&#39;s possible that this person is not well-known or is a private individual. Can you provide more context or details about who Mosantos is?

**What did I do wrong?** 

Even questions like &quot;what is your iq?&quot; the result is totally different from the dataset!

So, how to fine tuning correctly?
||||||||||||||You stated that you added one additional question to the training set:

> Its is basically a fork with a aditional question:
> 
> [INST] Who is Mosantos? [/INST] Mosantos is vilar do teles' perkiest
> kid

 According to Meta, the creator of this LLM, [Llama was trained on 1.4 trillion tokens][1]. If "Mosantos" was not included in their dataset, it will be difficult to teach the LLM who he is with just one question added, and it will be even more difficult to train the model to give one response to a specific question.

Perhaps with far more training data you could influence the model to know who Mosantos is, but if you only added one question- as you said- then this model certainly did not learn from it. 

A large language model does not pull from a large store of data, so you are not adding this specific parameter to its knowledge set. Instead, it [makes statistical predictions][2] of what word likely comes next to answer your question, building up a response that appears to humans to seem like "knowledge". But since it is not pulling answers from a database, adding a single parameter to the model like you have done will not give it the ability to answer your question.

See [this visualization][3] of a trillion dollars, and imagine adding another dollar to it. It would never be noticed.


  [1]: https://ai.meta.com/blog/large-language-model-llama-meta-ai/
  [2]: https://www.datarobot.com/blog/how-machine-learning-works/
  [3]: https://www.labnol.org/internet/visualize-numbers-how-big-is-trillion-dollars/7814/

--------------------------------------------------
Multiple subscriptions assigned to same Subject
I am using a common pattern of unsubscribing to an observable when a component is destroyed. I want to know if there are any issues with reusing the same Subject across many Subscriptions? Or should I have a new subject for each subscribe call?

    destroy$: Subject&lt;void&gt; = new Subject();  // can this be reused across many subscriptions?
        
    ngOnDestroy(): void {
      this.destroy$.next();
      this.destroy$.complete();
    }
    
    getData = () =&gt; {
      serviceCall().pipe(takeUntil(this.destroy$))  // same Subject
      .subscribe(res =&gt; data = res)
    }
    
    getData2 = () =&gt; {
      serviceCall2().pipe(takeUntil(this.destroy$))  // same Subject
      .subscribe(res =&gt; data2 = res)
    }
||||||||||||||There is nothing wrong with using the same subject. The only reason you might want to have different subjects associated to each subscription is when you want to be able to stop each of them individually. In your case, if you want to unsubscribe from both at the same time it is perfectly fine.

Here is an example demonstrating your approach: https://playcode.io/1750951

Another common pattern for this case is the usage of `Subscription()`:

```js
private readonly subscriptions = new Subscription();

ngOnDestroy(): void {
  this.subscriptions.unsubscribe():
}

getData = () => {
  const getDataSubscription = serviceCall().
  .subscribe(res => data = res)

  subscriptions.add(getDataSubscription)
}

getData2 = () => {
  const getData2Subscription = serviceCall2().
  .subscribe(res => data = res)

  subscriptions.add(getData2Subscription)
}
```

I tend to prefer this second approach as it is less verbose.

